
@book{aalen2008,
  title = {Survival and {{Event History Analysis}}},
  author = {Aalen, Odd O. and Borgan, {\O}rnulf and Gjessing, H{\aa}kon K.},
  editor = {Gail, M. and Krickeberg, K. and Samet, J. and Tsiatis, A. and Wong, W.},
  year = {2008},
  series = {Statistics for {{Biology}} and {{Health}}},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-0-387-68560-1},
  isbn = {978-0-387-20287-7 978-0-387-68560-1},
  keywords = {cox,kaplan-meier,nelson-aalen,survival},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Aalen_et_al_2008_Survival_and_Event_History_Analysis.pdf}
}

@article{abadie2021,
  title = {Using {{Synthetic Controls}}: {{Feasibility}}, {{Data Requirements}}, and {{Methodological Aspects}}},
  shorttitle = {Using {{Synthetic Controls}}},
  author = {Abadie, Alberto},
  year = {2021},
  month = jun,
  journal = {Journal of Economic Literature},
  volume = {59},
  number = {2},
  pages = {391--425},
  issn = {0022-0515},
  doi = {10.1257/jel.20191450},
  abstract = {Probably because of their interpretability and transparent nature, synthetic controls have become widely applied in empirical research in economics and the social sciences. This article aims to provide practical guidance to researchers employing synthetic control methods. The article starts with an overview and an introduction to synthetic control estimation. The main sections discuss the advantages of the synthetic control framework as a research design, and describe the settings where synthetic controls provide reliable estimates and those where they may fail. The article closes with a discussion of recent extensions, related methods, and avenues for future research.},
  langid = {english},
  keywords = {Aggregate Productivity,Cross-Country Output Convergence,Diffusion Processes,done,Dynamic Quantile Regressions,Dynamic Treatment Effect Models,Economic Methodology; Multiple or Simultaneous Equation Models: Time-Series Models,State Space Models; Quantitative Policy Modeling; Macroeconomics: Production; Economic Integration; Empirical Studies of Economic Growth},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Abadie_2021_Using_Synthetic_Controls.pdf;/home/dede/Zotero/storage/BFTYCBRE/articles.html}
}

@book{abate2011,
  title = {{Geometria differenziale}},
  author = {Abate, Marco and Tovena, Francesca},
  year = {2011},
  month = jul,
  edition = {2011 edizione},
  publisher = {{Springer Verlag}},
  address = {{Milano}},
  abstract = {L'opera fornisce una introduzione alla geometria delle variet\`a differenziabili, illustrandone le principali propriet\`a e descrivendo le tecniche e gli strumenti usati per il loro studio.},
  isbn = {978-88-470-1919-5},
  langid = {italian},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Abate_Tovena_2011_Geometria_differenziale.pdf}
}

@article{abbe2018,
  title = {Community {{Detection}} and {{Stochastic Block Models}}: {{Recent Developments}}},
  shorttitle = {Community {{Detection}} and {{Stochastic Block Models}}},
  author = {Abbe, Emmanuel},
  year = {2018},
  journal = {Journal of Machine Learning Research},
  volume = {18},
  number = {177},
  pages = {1--86},
  issn = {1533-7928},
  abstract = {The stochastic block model (SBM) is a random graph model with planted clusters. It is widely employed as a canonical model to study clustering and community detection, and provides generally a fertile ground to study the statistical and computational tradeoffs that arise in network and data sciences. This note surveys the recent developments that establish the fundamental limits for community detection in the SBM, both with respect to information-theoretic and computational thresholds, and for various recovery requirements such as exact, partial and weak recovery (a.k.a., detection). The main results discussed are the phase transitions for exact recovery at the Chernoff-Hellinger threshold, the phase transition for weak recovery at the Kesten- Stigum threshold, the optimal distortion-SNR tradeoff for partial recovery, the learning of the SBM parameters and the gap between information-theoretic and computational thresholds. The note also covers some of the algorithms developed in the quest of achieving the limits, in particular two-round algorithms via graph-splitting, semi-definite programming, linearized belief propagation, classical and nonbacktracking spectral methods. A few open problems are also discussed.},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Abbe_2018_Community_Detection_and_Stochastic_Block_Models.pdf}
}

@book{abbott2015,
  title = {Understanding {{Analysis}}},
  author = {Abbott, Stephen},
  year = {2015},
  month = may,
  edition = {2nd ed. 2015, Corr. 2nd printing 2016 edition},
  publisher = {{Springer}},
  address = {{New York}},
  abstract = {This lively introductory text exposes the student to the rewards of a rigorous study of functions of a real variable. In each chapter, informal discussions of questions that give analysis its inherent fascination are followed by precise, but not overly formal, developments of the techniques needed to make sense of ~them. By focusing on the unifying themes of approximation and the resolution of paradoxes that arise in the transition from the finite to the infinite, the text turns what could be a daunting cascade of definitions and theorems into a coherent and engaging progression of ideas. Acutely aware of the need for rigor, the student is much better prepared to understand what constitutes a proper mathematical proof and how to write one.Fifteen years of classroom experience with the first edition of Understanding Analysis have solidified and refined the central narrative of the second edition. Roughly 150 new exercises join a selection of the best exercises from the first edition, and three more project-style sections have been added. Investigations of Euler's computation of {$\zeta$}(2), the Weierstrass Approximation \- Theorem, and the gamma function are now among the book's cohort of seminal results serving as motivation and payoff for the beginning student to master the methods of analysis.},
  isbn = {978-1-4939-2711-1},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Abbott_2015_Understanding_Analysis.pdf}
}

@article{abdella2019,
  title = {An Adaptive Thresholding-Based Process Variability Monitoring},
  author = {Abdella, Galal M. and Kim, Jinho and Kim, Sangahn and {Al-Khalifa}, Khalifa N. and Jeong, Myong K. (MK) and Hamouda, Abdel Magid and Elsayed, Elsayed A.},
  year = {2019},
  month = jul,
  journal = {Journal of Quality Technology},
  volume = {51},
  number = {3},
  pages = {242--256},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2019.1569952},
  abstract = {In high-dimensional processes, monitoring process variability is considerably difficult due to the large number of variables and the limited number of samples. Monitoring changes in the covariance matrix of a multivariate process is often used for monitoring process variability under the assumption that only a few elements in the covariance matrix are changed simultaneously from the in-control values. The existing LASSO-based covariance monitoring charts in the high-dimensional settings provide good performance in detecting some shift patterns depending on the prespecified tuning parameter. In practice, control charts that perform reasonably well over various shift patterns are desired when shift patterns are unknown. In this article, we propose a control chart based on an adaptive LASSO-thresholding for monitoring changes in the covariance matrix. The performance of the proposed chart, which is called the ALT-norm chart, is evaluated for various shift patterns and compared with the existing penalized likelihood-based methods. The results show the effectiveness of the proposed chart. Finally, we illustrate the advantages of the ALT-norm chart through simulated and real data from both the semiconductor industry and a high-dimensional milling process.},
  keywords = {adaptive thresholding estimation,done,monitoring covariance matrix,multivariate statistical process control},
  annotation = {\_eprint: https://doi.org/10.1080/00224065.2019.1569952},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Abdella_et_al_2019_An_adaptive_thresholding-based_process_variability_monitoring.pdf;/home/dede/Zotero/storage/W6IH4PH3/00224065.2019.html}
}

@article{adams2007,
  title = {Bayesian {{Online Changepoint Detection}}},
  author = {Adams, Ryan P. and Mackay, D.},
  year = {2007},
  abstract = {This work examines the case where the model parameters before and after the changepoint are independent and derives an online algorithm for exact inference of the most recent changepoint and its implementation is highly modular so that the algorithm may be applied to a variety of types of data. Changepoints are abrupt variations in the generative parameters of a data sequence. Online detection of changepoints is useful in modelling and prediction of time series in application areas such as finance, biometrics, and robotics. While frequentist methods have yielded online filtering and prediction techniques, most Bayesian papers have focused on the retrospective segmentation problem. Here we examine the case where the model parameters before and after the changepoint are independent and we derive an online algorithm for exact inference of the most recent changepoint. We compute the probability distribution of the length of the current ``run,'' or time since the last changepoint, using a simple message-passing algorithm. Our implementation is highly modular so that the algorithm may be applied to a variety of types of data. We illustrate this modularity by demonstrating the algorithm on three different real-world data sets.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Adams_Mackay_2007_Bayesian_Online_Changepoint_Detection.pdf}
}

@misc{adams2021,
  title = {{{PrincetonLIPS}}/Numpy-Hilbert-Curve},
  author = {Adams, Ryan P.},
  year = {2021},
  abstract = {Numpy implementation of Hilbert curves in arbitrary dimensions},
  copyright = {MIT License         ,                 MIT License},
  howpublished = {Princeton Laboratory for Intelligent Probabilistic Systems},
  keywords = {done}
}

@article{agarwal2021,
  title = {On {{Robustness}} of {{Principal Component Regression}}},
  author = {Agarwal, Anish and Shah, Devavrat and Shen, Dennis and Song, Dogyoon},
  year = {2021},
  month = may,
  journal = {Journal of the American Statistical Association},
  volume = {0},
  number = {0},
  pages = {1--15},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.2021.1928513},
  abstract = {Principal component regression (PCR) is a simple, but powerful and ubiquitously utilized method. Its effectiveness is well established when the covariates exhibit low-rank structure. However, its ability to handle settings with noisy, missing, and mixed-valued, that is, discrete and continuous, covariates is not understood and remains an important open challenge. As the main contribution of this work, we establish the robustness of PCR, without any change, in this respect and provide meaningful finite-sample analysis. To do so, we establish that PCR is equivalent to performing linear regression after preprocessing the covariate matrix via hard singular value thresholding (HSVT). As a result, in the context of counterfactual analysis using observational data, we show PCR is equivalent to the recently proposed robust variant of the synthetic control method, known as robust synthetic control (RSC). As an immediate consequence, we obtain finite-sample analysis of the RSC estimator that was previously absent. As an important contribution to the synthetic controls literature, we establish that an (approximate) linear synthetic control exists in the setting of a generalized factor model, or latent variable model; traditionally in the literature, the existence of a synthetic control needs to be assumed to exist as an axiom. We further discuss a surprising implication of the robustness property of PCR with respect to noise, that is, PCR can learn a good predictive model even if the covariates are tactfully transformed to preserve differential privacy. Finally, this work advances the state-of-the-art analysis for HSVT by establishing stronger guarantees with respect to the l2,{$\infty$} -norm rather than the Frobenius norm as is commonly done in the matrix estimation literature, which may be of interest in its own right.},
  keywords = {Error-in-variables regression,Hard singular value thresholding,Matrix estimation,Principal component regression,Synthetic controls,todo},
  annotation = {\_eprint: https://doi.org/10.1080/01621459.2021.1928513},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Agarwal_et_al_2021_On_Robustness_of_Principal_Component_Regression2.pdf;/home/dede/Zotero/storage/TG5IU7ZJ/01621459.2021.html}
}

@book{aggarwal2016,
  title = {Recommender {{Systems}}: {{The Textbook}}},
  shorttitle = {Recommender {{Systems}}},
  author = {Aggarwal, Charu C.},
  year = {2016},
  month = apr,
  edition = {1st ed. 2016 edition},
  publisher = {{Springer}},
  address = {{New York, NY}},
  isbn = {978-3-319-29657-9},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Aggarwal_2016_Recommender_Systems.pdf}
}

@article{agrawal1988,
  title = {Asymptotically Efficient Adaptive Allocation Rules for the Multiarmed Bandit Problem with Switching Cost},
  author = {Agrawal, R. and Hedge, M. and Teneketzis, D.},
  year = {1988},
  doi = {10.1109/9.7243},
  abstract = {The authors consider multiarmed bandit problems with switching cost, define uniformly good allocation rules, and restrict attention to such rules. They present a lower bound on the asymptotic performance of uniformly good allocation rules and construct an allocation scheme that achieves the bound. It is found that despite the inclusion of a switching cost the proposed allocation scheme achieves the same asymptotic performance as the optimal rule for the bandit problem without switching cost. This is made possible by grouping together samples into blocks of increasing sizes, thereby reducing the number of switches to O(log n). Finally, an optimal allocation scheme for a large class of distributions which includes members of the exponential family is illustrated. {$>$}},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Agrawal_et_al_1988_Asymptotically_efficient_adaptive_allocation_rules_for_the_multiarmed_bandit.pdf}
}

@article{agrawal1990,
  title = {Multi-Armed Bandit Problems with Multiple Plays and Switching Cost},
  author = {Agrawal, R. and Hegde, M. and Teneketzis, D.},
  year = {1990},
  month = apr,
  journal = {Stochastics and Stochastic Reports},
  volume = {29},
  number = {4},
  pages = {437--459},
  publisher = {{Taylor \& Francis}},
  issn = {1045-1129},
  doi = {10.1080/17442509008833627},
  abstract = {We consider multi-armed bandit problems with switching cost and multiple plays, define},
  keywords = {asymptotically efficient,block allocation,Multi-armed bandit,regret,switching cost,todo},
  annotation = {\_eprint: https://doi.org/10.1080/17442509008833627},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/R_et_al_1990_Multi-armed_bandit_problems_with_multiple_plays_and_switching_cost.pdf;/home/dede/Zotero/storage/63VKCQ49/17442509008833627.html}
}

@article{agrawal2012,
  title = {Analysis of {{Thompson Sampling}} for the Multi-Armed Bandit Problem},
  author = {Agrawal, Shipra and Goyal, Navin},
  year = {2012},
  month = apr,
  journal = {arXiv:1111.1797 [cs]},
  eprint = {1111.1797},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The multi-armed bandit problem is a popular model for studying exploration/exploitation trade-off in sequential decision problems. Many algorithms are now available for this well-studied problem. One of the earliest algorithms, given by W. R. Thompson, dates back to 1933. This algorithm, referred to as Thompson Sampling, is a natural Bayesian algorithm. The basic idea is to choose an arm to play according to its probability of being the best arm. Thompson Sampling algorithm has experimentally been shown to be close to optimal. In addition, it is efficient to implement and exhibits several desirable properties such as small regret for delayed feedback. However, theoretical understanding of this algorithm was quite limited. In this paper, for the first time, we show that Thompson Sampling algorithm achieves logarithmic expected regret for the multi-armed bandit problem. More precisely, for the two-armed bandit problem, the expected regret in time \$T\$ is \$O(\textbackslash frac\{\textbackslash ln T\}\{\textbackslash Delta\} + \textbackslash frac\{1\}\{\textbackslash Delta\^3\})\$. And, for the \$N\$-armed bandit problem, the expected regret in time \$T\$ is \$O([(\textbackslash sum\_\{i=2\}\^N \textbackslash frac\{1\}\{\textbackslash Delta\_i\^2\})\^2] \textbackslash ln T)\$. Our bounds are optimal but for the dependence on \$\textbackslash Delta\_i\$ and the constant factors in big-Oh.},
  archiveprefix = {arXiv},
  keywords = {68W40; 68Q25,Computer Science - Data Structures and Algorithms,Computer Science - Machine Learning,done,F.2.0},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Agrawal_Goyal_2012_Analysis_of_Thompson_Sampling_for_the_multi-armed_bandit_problem.pdf;/home/dede/Zotero/storage/97S9D9J2/1111.html}
}

@book{agresti2010,
  title = {{Analysis of Ordinal Categorical Data}},
  author = {Agresti, Alan},
  year = {2010},
  month = mar,
  edition = {2 edizione},
  publisher = {{John Wiley \& Sons Inc}},
  address = {{Hoboken, N.J}},
  abstract = {Statistical science's first coordinated manual of methods for analyzing ordered categorical data, now fully revised and updated, continues to present applications and case studies in fields as diverse as sociology, public health, ecology, marketing, and pharmacy. Analysis of Ordinal Categorical Data, Second Edition provides an introduction to basic descriptive and inferential methods for categorical data, giving thorough coverage of new developments and recent methods. Special emphasis is placed on interpretation and application of methods including an integrated comparison of the available strategies for analyzing ordinal data. Practitioners of statistics in government, industry (particularly pharmaceutical), and academia will want this new edition.},
  isbn = {978-0-470-08289-8},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Agresti_2010_Analysis_of_Ordinal_Categorical_Data.pdf}
}

@inproceedings{ahn2012,
  title = {Bayesian Posterior Sampling via Stochastic Gradient Fisher Scoring},
  booktitle = {Proceedings of the 29th {{International Coference}} on {{International Conference}} on {{Machine Learning}}},
  author = {Ahn, Sungjin and Korattikara, Anoop and Welling, Max},
  year = {2012},
  month = jun,
  series = {{{ICML}}'12},
  pages = {1771--1778},
  publisher = {{Omnipress}},
  address = {{Madison, WI, USA}},
  abstract = {In this paper we address the following question: "Can we approximately sample from a Bayesian posterior distribution if we are only allowed to touch a small mini-batch of data-items for every sample we generate?". An algorithm based on the Langevin equation with stochastic gradients (SGLD) was previously proposed to solve this, but its mixing rate was slow. By leveraging the Bayesian Central Limit Theorem, we extend the SGLD algorithm so that at high mixing rates it will sample from a normal approximation of the posterior, while for slow mixing rates it will mimic the behavior of SGLD with a pre-conditioner matrix. As a bonus, the proposed algorithm is reminiscent of Fisher scoring (with stochastic gradients) and as such an efficient optimizer during burn-in.},
  isbn = {978-1-4503-1285-1},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Ahn_et_al_2012_Bayesian_posterior_sampling_via_stochastic_gradient_fisher_scoring.pdf}
}

@book{aho1995,
  title = {{Foundations of Computer Science: C Edition}},
  shorttitle = {{Foundations of Computer Science}},
  author = {Aho, Alfred V. and Ullman, Jeffrey D.},
  year = {1995},
  month = feb,
  edition = {New edition edizione},
  publisher = {{W H Freeman \& Co}},
  address = {{New York}},
  abstract = {This text combines the theoretical foundations of computing with essential discrete mathematics. It follows the same organization as its predecessor, Foundations of Computer Science (also published by W.H. Freeman), with all examples and exercises in C.},
  isbn = {978-0-7167-8284-1},
  langid = {Inglese},
  keywords = {computer science,introduction},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Aho_Ullman_1995_Foundations_of_Computer_Science.pdf;/home/dede/Zotero/storage/C2R8SQBZ/init.vim}
}

@article{albert1993,
  title = {Bayesian {{Analysis}} of {{Binary}} and {{Polychotomous Response Data}}},
  author = {Albert, James H. and Chib, Siddhartha},
  year = {1993},
  journal = {Journal of the American Statistical Association},
  volume = {88},
  number = {422},
  pages = {669--679},
  publisher = {{[American Statistical Association, Taylor \& Francis, Ltd.]}},
  issn = {0162-1459},
  doi = {10.2307/2290350},
  abstract = {A vast literature in statistics, biometrics, and econometrics is concerned with the analysis of binary and polychotomous response data. The classical approach fits a categorical response regression model using maximum likelihood, and inferences about the model are based on the associated asymptotic theory. The accuracy of classical confidence statements is questionable for small sample sizes. In this article, exact Bayesian methods for modeling categorical response data are developed using the idea of data augmentation. The general approach can be summarized as follows. The probit regression model for binary outcomes is seen to have an underlying normal regression structure on latent continuous data. Values of the latent data can be simulated from suitable truncated normal distributions. If the latent data are known, then the posterior distribution of the parameters can be computed using standard results for normal linear models. Draws from this posterior are used to sample new latent data, and the process is iterated with Gibbs sampling. This data augmentation approach provides a general framework for analyzing binary regression models. It leads to the same simplification achieved earlier for censored regression models. Under the proposed framework, the class of probit regression models can be enlarged by using mixtures of normal distributions to model the latent data. In this normal mixture class, one can investigate the sensitivity of the parameter estimates to the choice of "link function," which relates the linear regression estimate to the fitted probabilities. In addition, this approach allows one to easily fit Bayesian hierarchical models. One specific model considered here reflects the belief that the vector of regression coefficients lies on a smaller dimension linear subspace. The methods can also be generalized to multinomial response models with \$J {$>$} 2\$ categories. In the ordered multinomial model, the J categories are ordered and a model is written linking the cumulative response probabilities with the linear regression structure. In the unordered multinomial model, the latent variables have a multivariate normal distribution with unknown variance-covariance matrix. For both multinomial models, the data augmentation method combined with Gibbs sampling is outlined. This approach is especially attractive for the multivariate probit model, where calculating the likelihood can be difficult.}
}

@book{albert2009,
  title = {Bayesian {{Computation}} with {{R}}},
  author = {Albert, Jim},
  year = {2009},
  series = {Use {{R}}!},
  edition = {Second},
  publisher = {{Springer-Verlag}},
  address = {{New York}},
  doi = {10.1007/978-0-387-92298-0},
  abstract = {There has been a dramatic growth in the development and application of Bayesian inferential methods. Some of this growth is due to the availability of powerful simulation-based algorithms to summarize posterior distributions. There has been also a growing interest in the use of the system R for statistical analyses. R's open source nature, free availability, and large number of contributor packages have made R the software of choice for many statisticians in education and industry. Bayesian Computation with R introduces Bayesian modeling by the use of computation using the R language. The early chapters present the basic tenets of Bayesian thinking by use of familiar one and two-parameter inferential problems. Bayesian computational methods such as Laplace's method, rejection sampling, and the SIR algorithm are illustrated in the context of a random effects model. The construction and implementation of Markov Chain Monte Carlo (MCMC) methods is introduced. These simulation-based algorithms are implemented for a variety of Bayesian applications such as normal and binary response regression, hierarchical modeling, order-restricted inference, and robust modeling. Algorithms written in R are used to develop Bayesian tests and assess Bayesian models by use of the posterior predictive distribution. The use of R to interface with WinBUGS, a popular MCMC computing language, is described with several illustrative examples. This book is a suitable companion book for an introductory course on Bayesian methods and is valuable to the statistical practitioner who wishes to learn more about the R language and Bayesian methodology. The LearnBayes package, written by the author and available from the CRAN website, contains all of the R functions described in the book. The second edition contains several new topics such as the use of mixtures of conjugate priors and the use of Zellner's g priors to choose between models in linear regression. There are more illustrations of the construction of informative prior distributions, such as the use of conditional means priors and multivariate normal priors in binary regressions. The new edition contains changes in the R code illustrations according to the latest edition of the LearnBayes package. Jim Albert is Professor of Statistics at Bowling Green State University. He is Fellow of the American Statistical Association and is past editor of The American Statistician. His books include Ordinal Data Modeling (with Val Johnson), Workshop Statistics: Discovery with Data, A Bayesian Approach (with Allan Rossman), and Bayesian Computation using Minitab.},
  isbn = {978-0-387-92297-3},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Albert_2009_Bayesian_Computation_with_R.pdf;/home/dede/Zotero/storage/MQICYPVZ/9780387922973.html}
}

@article{alghuried2020,
  title = {Anomaly Detection in Large-Scale Networks: {{A}} State-Space Decision Process},
  shorttitle = {Anomaly Detection in Large-Scale Networks},
  author = {Alghuried, Abdullah and Moghaddass, Ramin},
  year = {2020},
  month = aug,
  journal = {Journal of Quality Technology},
  volume = {0},
  number = {0},
  pages = {1--28},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2020.1805379},
  abstract = {A new data fusion and network analytics framework is proposed that is based on the topology of large-scale networks and the stochastic dependencies between nodes, edges, and sensor data. The framework can transform real-time sensor data collected from disparate sources in a network to detect the location of anomalies and the nodes that are impacted by the detected anomalies. By intelligently fuzing multidimensional sensor data based on the topology of a large-scale network, this article also contributes to big data analytics for network systems. We will show that the proposed framework not only brings computational benefits, but also results in better anomaly estimates leading to lower false alarm rates and higher detection rates.},
  keywords = {anomaly detection,fault detection,monitoring and control,network analytics,todo},
  annotation = {\_eprint: https://doi.org/10.1080/00224065.2020.1805379},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Alghuried_Moghaddass_2020_Anomaly_detection_in_large-scale_networks.pdf;/home/dede/Zotero/storage/4CEYPQ6G/00224065.2020.html}
}

@article{ali2020,
  title = {A Predictive {{Bayesian}} Approach to Sequential Time-between-Events Monitoring},
  author = {Ali, Sajid},
  year = {2020},
  journal = {Quality and Reliability Engineering International},
  volume = {36},
  number = {1},
  pages = {365--387},
  issn = {1099-1638},
  doi = {10.1002/qre.2580},
  abstract = {A fundamental problem with all process monitoring techniques is the requirement of a large Phase-I data set to establish control limits and overcome estimation error. This assumption of having a large Phase-I data set is very restrictive and often problematic, especially when the sampling is expensive or not available, eg, time-between-events (TBE) settings. Moreover, with the advancement in technology, quality practitioners are now more interested in online process monitoring. Therefore, the Bayesian methodology not only provides a natural solution for sequential and adaptive learning but also addresses the problem of a large Phase-I data set for setting up a monitoring structure. In this study, we propose Bayesian control charts for TBE assuming homogenous Poisson process. In particular, a predictive approach is adopted to introduce predictive limit control charts. Beside the Bayesian predictive Shewhart charts with dynamic control limits, a comparison of the frequentist sequential charts, designed by using unbiased and biased estimator of the process parameter, is also a part of the present study. To assess the predictive TBE chart performance in the presence of practitioner-to-practitioner variability, we use the average of the average run length (AARL) and the standard deviation of the in-control run length (SDARL).},
  langid = {english},
  keywords = {average run length (ARL),Bayesian process monitoring,doing,phase-I data,poisson process,predictive control limits,self-adaptive control charts,sequential process monitoring,time-between-events control charts},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/qre.2580},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Ali_2020_A_predictive_Bayesian_approach_to_sequential_time-between-events_monitoring.pdf;/home/dede/Zotero/storage/QUM2KTLK/qre.html}
}

@article{ali2020a,
  title = {On {{Designing}} a {{New Bayesian Dispersion Chart}} for {{Process Monitoring}}},
  author = {Ali, Sajid and Riaz, Muhammad},
  year = {2020},
  month = mar,
  journal = {Arabian Journal for Science and Engineering},
  volume = {45},
  number = {3},
  pages = {2093--2111},
  issn = {2191-4281},
  doi = {10.1007/s13369-019-04036-w},
  abstract = {Statistical quality control is an integral part of modern manufacturing, and control charts are widely used to improve and control the performance of a process. However, much of the work related to process monitoring has been done using frequentist approaches, whereas the Bayesian methodology has a practical advantage in process monitoring; that is, it can be used with a small phase-I dataset. In this article, we considered different symmetric and asymmetric loss functions for designing Bayesian control charts and noticed that the performance of the Bayesian charts is biased, i.e., in-control performance of the charts is not equal to the desired value for some loss functions and comparison among different Bayesian charts can be misleading. Therefore, to get the desired in-control performance under different loss functions, we propose a corrected design of the Bayesian control charts. In addition to posterior charts, we also discuss a predictive control chart to monitor the future observations in this study. Besides a simulation study, we also present two real data examples: the first example is to monitor the surface roughness of reamed holes in a particular metal part while the second is about monitoring the measurements of a diameter.},
  langid = {english}
}

@book{allen2011,
  title = {{Global Economic History: A Very Short Introduction}},
  shorttitle = {{Global Economic History}},
  author = {Allen, Robert C.},
  year = {2011},
  month = sep,
  publisher = {{OUP Oxford}},
  address = {{Oxford ; New York}},
  abstract = {Why are some countries rich and others poor? In 1500, the income differences were small, but they have grown dramatically since Columbus reached America. Since then, the interplay between geography, globalization, technological change, and economic policy has determined the wealth and poverty of nations. The industrial revolution was Britain's path breaking response to the challenge of globalization. Western Europe and North America joined Britain to form a club of rich nations by pursuing four polices-creating a national market by abolishing internal tariffs and investing in transportation, erecting an external tariff to protect their fledgling industries from British competition, banks to stabilize the currency and mobilize domestic savings for investment, and mass education to prepare people for industrial work.  Together these countries pioneered new technologies that have made them ever richer. Before the Industrial Revolution, most of the world's manufacturing was done in Asia, but industries from Casablanca to Canton were destroyed by western competition in the nineteenth century, and Asia was transformed into 'underdeveloped countries' specializing in agriculture. The spread of economic development has been slow since modern technology was invented to fit the needs of rich countries and is ill adapted to the economic and geographical conditions of poor countries. A few countries - Japan, Soviet Russia, South Korea, Taiwan, and perhaps China - have, nonetheless, caught up with the West through creative responses to the technological challenge and with Big Push industrialization that has achieved rapid growth through investment coordination. Whether other countries can emulate the success of East Asia is a challenge for the future.  ABOUT THE SERIES: The Very Short Introductions series from Oxford University Press contains hundreds of titles in almost every subject area. These pocket-sized books are the perfect way to get ahead in a new subject quickly. Our expert authors combine facts, analysis, perspective, new ideas, and enthusiasm to make interesting and challenging topics highly readable.},
  isbn = {978-0-19-959665-2},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Allen_2011_Global_Economic_History.pdf}
}

@book{allison2015,
  title = {The {{American Revolution}}: {{A Very Short Introduction}}},
  shorttitle = {The {{American Revolution}}},
  author = {Allison, Robert J.},
  year = {2015},
  series = {Very {{Short Introductions}}},
  edition = {First},
  publisher = {{Oxford University Press}},
  isbn = {978-0-19-022506-3},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Allison_2015_The_American_Revolution.epub}
}

@article{allison2017,
  title = {Russia and the Post-2014 International Legal Order: Revisionism and Realpolitik},
  shorttitle = {Russia and the Post-2014 International Legal Order},
  author = {Allison, Roy},
  year = {2017},
  month = may,
  journal = {International Affairs},
  volume = {93},
  number = {3},
  pages = {519--543},
  issn = {0020-5850},
  doi = {10.1093/ia/iix061},
  abstract = {Russia's annexation of Crimea and military intervention in eastern Ukraine, as well as Moscow's claims around these acts, challenge the post-Cold War territorial settlement and its underlying international legal principles. Unlike previous controversies over western-led interventions, a major power has used force to expand its territorial sphere. Russia's actions contradict its traditional focus on UN Charter principles and sovereignty in the wider international system. This article questions whether Russia has a serious agenda to gain support for revised understandings of international law, at least as applied to what it views as a Commonwealth of Independent States (CIS) region of entitlements. Or is Russia driven more by realpolitik, whereby legal discourse contributes to a strategic effort to force changes in the European territorial order, in the first instance in its direct neighbourhood? These uncertainties are contrasted to Russia's single-minded narrative on the threat of regime change. Moscow uses the spectre of `colour revolutions' to influence a variety of states in the wider international system. Overall, so far Russia has failed to shift legal understandings in its favour over Crimea/Ukraine. Yet it continues to view the CIS states as only partly sovereign, as located in a zone of exception. Moscow seems intent on changing the European security and territorial order in its favour, regardless of the language of legal principles it deploys in its wider international diplomacy.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Allison_2017_Russia_and_the_post-2014_international_legal_order.pdf;/home/dede/Zotero/storage/ATAGARCQ/3746574.html}
}

@article{alloway1991,
  title = {Control {{Chart Based}} on the {{Hodges-Lehmann Estimator}}},
  author = {Alloway, James A. and Raghavachari, M.},
  year = {1991},
  month = oct,
  journal = {Journal of Quality Technology},
  volume = {23},
  number = {4},
  pages = {336--347},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.1991.11979350},
  abstract = {A new variables control chart for location based on the Hodges-Lehmann estimator associated with the Wilcoxon signed rank statistic is presented. The proposed chart is nonparametric and hence maintains the nominal type I error specified. Calculations of control limits are simplified for certain conditions. The proposed approach compares favorably with the Shewhart control chart when the data are from a Gaussian distribution. Its performance is better than the traditional approach in the case of moderate sample sizes from long-tailed symmetric distributions. These properties make it well suited for early production runs of limited size when the distribution of the process statistic is unknown. The method may also be used to establish robust control limits.},
  keywords = {Hodges-Lehmann Estimator,Nonparametric Control Chart,Walsh Averages,Wilcoxon Signed Rank Statistic},
  annotation = {\_eprint: https://doi.org/10.1080/00224065.1991.11979350},
  file = {/home/dede/Zotero/storage/MK27B423/00224065.1991.html}
}

@incollection{alt2004,
  title = {Multivariate {{Quality Control}}},
  booktitle = {Encyclopedia of {{Statistical Sciences}}},
  author = {Alt, Frank B.},
  year = {2004},
  publisher = {{American Cancer Society}},
  doi = {10.1002/0471667196.ess1742},
  copyright = {Copyright \textcopyright{} 2004 by John Wiley \& Sons, Inc. All rights reserved.},
  isbn = {978-0-471-66719-3},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/0471667196.ess1742},
  file = {/home/dede/Zotero/storage/LG9VT964/0471667196.html}
}

@article{altshuler2013,
  title = {The {{Social Amplifier}}\textemdash{{Reaction}} of {{Human Communities}} to {{Emergencies}}},
  author = {Altshuler, Yaniv and Fire, Michael and Shmueli, Erez and Elovici, Yuval and Bruckstein, Alfred and Pentland, Alex (Sandy) and Lazer, David},
  year = {2013},
  month = aug,
  journal = {Journal of Statistical Physics},
  volume = {152},
  number = {3},
  pages = {399--418},
  issn = {1572-9613},
  doi = {10.1007/s10955-013-0759-z},
  abstract = {This paper develops a methodology to aggregate signals in a network regarding some hidden state of the world. We argue that focusing on edges around hubs will under certain circumstances amplify the faint signals disseminating in a network, allowing for more efficient detection of that hidden state. We apply this method to detecting emergencies in mobile phone data, demonstrating that under a broad range of cases and a constraint in how many edges can be observed at a time, focusing on the egocentric networks around key hubs will be more effective than sampling random edges. We support this conclusion analytically, through simulations, and with analysis of a dataset containing the call log data from a major mobile carrier in a European nation.},
  langid = {english},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Altshuler_et_al_2013_The_Social_Amplifierâ€”Reaction_of_Human_Communities_to_Emergencies.pdf}
}

@article{alvarez-melis2020,
  title = {Geometric {{Dataset Distances}} via {{Optimal Transport}}},
  author = {{Alvarez-Melis}, David and Fusi, Nicol{\`o}},
  year = {2020},
  month = feb,
  journal = {arXiv:2002.02923 [cs, stat]},
  eprint = {2002.02923},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {The notion of task similarity is at the core of various machine learning paradigms, such as domain adaptation and meta-learning. Current methods to quantify it are often heuristic, make strong assumptions on the label sets across the tasks, and many are architecture-dependent, relying on task-specific optimal parameters (e.g., require training a model on each dataset). In this work we propose an alternative notion of distance between datasets that (i) is model-agnostic, (ii) does not involve training, (iii) can compare datasets even if their label sets are completely disjoint and (iv) has solid theoretical footing. This distance relies on optimal transport, which provides it with rich geometry awareness, interpretable correspondences and well-understood properties. Our results show that this novel distance provides meaningful comparison of datasets, and correlates well with transfer learning hardness across various experimental settings and datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,distance,metric spaces,optimal mass transport,Statistics - Machine Learning,todo,Wasserstein distance},
  file = {/home/dede/Zotero/storage/PFJYAMZ7/Alvarez-Melis_Fusi_2020_Geometric Dataset Distances via Optimal Transport.pdf;/home/dede/Zotero/storage/I3VXCPUJ/2002.html}
}

@article{aly2017,
  title = {Optimal Design of the Adaptive Exponentially Weighted Moving Average Control Chart over a Range of Mean Shifts},
  author = {Aly, Aya A. and Hamed, Ramadan M. and Mahmoud, Mahmoud A.},
  year = {2017},
  month = feb,
  journal = {Communications in Statistics - Simulation and Computation},
  volume = {46},
  number = {2},
  pages = {890--902},
  publisher = {{Taylor \& Francis}},
  issn = {0361-0918},
  doi = {10.1080/03610918.2014.983650},
  abstract = {The adaptive exponentially weighted moving average (AEWMA) control chart is a smooth combination of the Shewhart and exponentially weighted moving average (EWMA) control charts. This chart was proposed by Cappizzi and Masarotto (2003) to achieve a reasonable performance for both small and large shifts. Cappizzi and Masarotto (2003) used a pair of shifts in designing their control chart. In this study, however, the process mean shift is considered as a random variable with a certain probability distribution and the AEWMA control chart is optimized for a wide range of mean shifts according to that probability distribution and not just for a pair of shifts. Using the Markov chain technique, the results show that the new optimization design can improve the performance of the AEWMA control chart from an overall point of view relative to the various designs presented by Cappizzi and Masarotto (2003). Optimal design parameters that achieve the desired in-control average run length (ARL) are computed in several cases and formulas used to find approximately their values are given. Using these formulas, the practitioner can compute the optimal design parameters corresponding to any desired in-control ARL without the need to apply the optimization procedure. The results obtained by these formulas are very promising and would particularly facilitate the design of the AEWMA control chart for any in-control ARL value.},
  keywords = {62P25,90C30,AEWMA,ARL,Markov chain,Statistical process control},
  annotation = {\_eprint: https://doi.org/10.1080/03610918.2014.983650},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Aly_et_al_2017_Optimal_design_of_the_adaptive_exponentially_weighted_moving_average_control.pdf;/home/dede/Zotero/storage/LGK67EHP/03610918.2014.html}
}

@article{aly2021,
  title = {An Adaptive Exponentially Weighted Moving Average Control Chart for Poisson Processes},
  author = {Aly, Aya A. and Saleh, Nesma A. and Mahmoud, Mahmoud A.},
  year = {2021},
  month = oct,
  journal = {Quality Engineering},
  volume = {33},
  number = {4},
  pages = {627--640},
  publisher = {{Taylor \& Francis}},
  issn = {0898-2112},
  doi = {10.1080/08982112.2021.1956535},
  abstract = {The Adaptive Exponentially Weighted Moving Average (AEWMA) control chart is known to be effective in detecting range of shifts simultaneously. Moreover, the AEWMA chart is known to diminish the inertia effect. The AEWMA chart is usually investigated while assuming that the monitored process follows a continuous distribution; commonly the normal distribution. In practice, however, monitored data could be of a discrete-type. We aim in this study to propose a discrete-version from the AEWMA chart; namely the Poisson AEWMA chart. The chart is compared with its counterparts; the Poisson EWMA chart and Poisson CUSUM chart using the ARL and RMI metrics. Our results show that the Poisson AEWMA chart performs more efficiently in detecting shifts of various sizes with an RMI value approaching zero. The Poisson CUSUM chart has the worst performance. Moreover, the proposed Poisson AEWMA chart is capable of detecting shifts faster than an approach based on normal approximation even for large values of the mean defects. In addition, the superiority of the Poisson AEWMA chart in diminishing the inertia effect is illustrated through a numerical example. The example shows that the Poisson AEWMA chart is capable of detecting out of control situations very fast even if the chart statistic is in a disadvantageous position before a shift occurs.},
  keywords = {ARL,control charts,done,Poisson distribution,process monitoring and control,Statistical quality control},
  annotation = {\_eprint: https://doi.org/10.1080/08982112.2021.1956535},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Aly_et_al_2021_An_adaptive_exponentially_weighted_moving_average_control_chart_for_poisson.pdf;/home/dede/Zotero/storage/5K5X4JDG/08982112.2021.html}
}

@article{aly2022,
  title = {An Adaptive {{EWMA}} Control Chart for Monitoring Zero-Inflated {{Poisson}} Processes},
  author = {Aly, Aya A. and Saleh, Nesma A. and Mahmoud, Mahmoud A.},
  year = {2022},
  month = apr,
  journal = {Communications in Statistics - Simulation and Computation},
  volume = {51},
  number = {4},
  pages = {1564--1577},
  publisher = {{Taylor \& Francis}},
  issn = {0361-0918},
  doi = {10.1080/03610918.2019.1676437},
  abstract = {Zero-Inflated Poisson (ZIP) distribution is used to model count data with excessive zeros. In this article, we develop and design an adaptive exponentially weighted moving average (AEWMA) control chart for monitoring ZIP processes. A Markov Chain approach is used to approximate the performance measures; namely the average run length (ARL) and standard deviation of run length (SDRL) of the AEWMA chart. The chart performance is assessed using optimized design parameters that provide the smallest ARL for a range of shifts. A performance comparison of the ZIP-AEWMA chart is conducted with the competing charts in terms of the relative mean index (RMI) metric. Results show that the ZIP-AEWMA chart has superior performance over the competing charts for a wide range of process shifts, especially when the probability of excessive zeros in data is high. The proposed chart is also applied on a real-life application to demonstrate its use. We highly recommend the use of the AEWMA chart for monitoring ZIP processes.},
  keywords = {Adaptive exponentially weighted moving average,Markov chain,Optimal design,Statistical process monitoring,Zero-inflated Poisson distribution},
  annotation = {\_eprint: https://doi.org/10.1080/03610918.2019.1676437},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Aly_et_al_2022_An_adaptive_EWMA_control_chart_for_monitoring_zero-inflated_Poisson_processes.pdf;/home/dede/Zotero/storage/P4ST4CN6/03610918.2019.html}
}

@book{ambrosio2008,
  title = {Gradient {{Flows}}: {{In Metric Spaces}} and in the {{Space}} of {{Probability Measures}}},
  shorttitle = {Gradient {{Flows}}},
  author = {Ambrosio, Luigi and Gigli, Nicola and Savare, Giuseppe},
  year = {2008},
  month = mar,
  edition = {2nd edition},
  publisher = {{Birkh\"auser}},
  address = {{Basel}},
  abstract = {The book is devoted to the theory of gradient flows in the general framework of metric spaces, and in the more specific setting of the space of probability measures, which provide a surprising link between optimal transportation theory and many evolutionary PDE's related to (non)linear diffusion. Particular emphasis is given to the convergence of the implicit time discretization method and to the error estimates for this discretization, extending the well established theory in Hilbert spaces. The book is split in two main parts that can be read independently of each other.},
  isbn = {978-3-7643-8721-1},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Ambrosio_et_al_2008_Gradient_Flows.pdf}
}

@article{amin1991,
  title = {A Nonparametric Exponentially Weighted Moving Average Control Scheme},
  author = {Amin, Raid W. and Searcy, Alice J.},
  year = {1991},
  month = jan,
  journal = {Communications in Statistics - Simulation and Computation},
  volume = {20},
  number = {4},
  pages = {1049--1072},
  publisher = {{Taylor \& Francis}},
  issn = {0361-0918},
  doi = {10.1080/03610919108812996},
  abstract = {A common assumption when evaluating the properties of Exponentially Weighted Moving Average (EWMA) control procedures for controlling the process mean is the observations are normal with known variance. In this article we propose a nonparametric control procedure that can be used when the underlying distribution is not know there is not enough information on the variance or shape of the distribution. Its average run length properties are less affected than the corresponding parametric EWMA procedure when autocorrelation between the observations is present. The procedure nonparametric EWMA procedure is based on Wilcoxon signed-rank statistics ranking is within groups. Our simulation results show that the proposed control procedure is less efficient than the parametric -EWMA procedure when the distribution is not and it can be considerably more efficient than the parametric procedure for heavy- distributions. The proposed procedure is insensitive to misspecification of the van and its ARL properties of the control procedure are affected relatively little by cho the weighting parameter {$\lambda$}.},
  keywords = {Average Run Length,Control Charts,Nonparametric Statistics,Process Control,Wilcoxon S Rank Statistic},
  annotation = {\_eprint: https://doi.org/10.1080/03610919108812996},
  file = {/home/dede/Zotero/storage/9KXTPFWE/03610919108812996.html}
}

@inproceedings{aminnayeri2016,
  title = {A Risk Adjusted Self-Starting {{Bernoulli CUSUM}} Control Chart with Dynamic Probability Control Limits},
  author = {Aminnayeri, M. and Sogandi, F.},
  year = {2016},
  doi = {10.22060/MISCJ.2016.833},
  abstract = {103 AIJ Modeling, Identification, Simulation and Control, Vol 48, No. 2, Fall 2016 Please cite this article using: Aminnayeri, M., and Sogandi, F., 2016. ``A Risk-Adjusted Self-Starting Bernoulli CUSUM Control Chart with Dynamic Probability Control Limits''. Amirkabir International Journal of Modeling, Identification, Simulation and Control, 48(2), pp. 103\textendash 110. DOI: 10.22060/miscj.2016.833 URL: http://miscj.aut.ac.ir/article\_833.html *Corresponding Author, Email: mjnayeri@aut.ac.it A Risk-Adjusted Self-Starting Bernoulli CUSUM Control Chart with Dynamic Probability Control Limits},
  keywords = {done},
  file = {/home/dede/Zotero/storage/YGNDD6NC/Aminnayeri and Sogandi - 2016 - A risk adjusted self-starting Bernoulli CUSUM cont.pdf}
}

@inproceedings{amiri2016,
  title = {A Self-Starting Control Chart for Simultaneous Monitoring of Mean and Variance of Autocorrelated Simple Linear Profile},
  booktitle = {2016 {{IEEE International Conference}} on {{Industrial Engineering}} and {{Engineering Management}} ({{IEEM}})},
  author = {Amiri, Amirhossein and Ghashghaei, Reza and Khosravi, Peyman},
  year = {2016},
  month = dec,
  pages = {209--213},
  issn = {2157-362X},
  doi = {10.1109/IEEM.2016.7797866},
  abstract = {Sometimes, quality of a process can be described by a functional relationship between response variables and explanatory variables which called profile. In some situations, there is an autocorrelation structure within a profile. Most of the times in real practice there is no enough data to estimate the process parameters. In this case, we can use a self-starting control chart which does not need preliminary data to start monitoring in start-up stages. In this paper, we consider a simple linear profile in the presence of a first order autoregressive (AR(1)) autocorrelation structure within profile and propose a self-starting control chart to monitor mean and variance of a simple linear profile simultaneously.},
  keywords = {Control charts,Correlation,Distribution functions,Monitoring,Process control,Random variables,Recursive residuals,Self-starting control chart,Simple linear profile,simultaneous monitoring,Standards},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Amiri_et_al_2016_A_self-starting_control_chart_for_simultaneous_monitoring_of_mean_and_variance.pdf;/home/dede/Zotero/storage/XKJZ8MBH/7797866.html}
}

@article{anderlucci2017,
  title = {The {{Importance}} of {{Being Clustered}}: {{Uncluttering}} the {{Trends}} of {{Statistics}} from 1970 to 2015},
  shorttitle = {The {{Importance}} of {{Being Clustered}}},
  author = {Anderlucci, Laura and Montanari, Angela and Viroli, Cinzia},
  year = {2017},
  month = sep,
  journal = {Statistical Science},
  volume = {34},
  doi = {10.1214/18-STS686},
  abstract = {In this paper we retrace the recent history of statistics by analyzing all the papers published in five prestigious statistical journals since 1970, namely: Annals of Statistics, Biometrika, Journal of the American Statistical Association, Journal of the Royal Statistical Society, series B and Statistical Science. The aim is to construct a kind of "taxonomy" of the statistical papers by organizing and by clustering them in main themes. In this sense being identified in a cluster means being important enough to be uncluttered in the vast and interconnected world of the statistical research. Since the main statistical research topics naturally born, evolve or die during time, we will also develop a dynamic clustering strategy, where a group in a time period is allowed to migrate or to merge into different groups in the following one. Results show that statistics is a very dynamic and evolving science, stimulated by the rise of new research questions and types of data.},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Anderlucci_et_al_2017_The_Importance_of_Being_Clustered3.pdf}
}

@book{andersen1996,
  title = {{Statistical Models Based on Counting Processes}},
  author = {Andersen, Per Kragh and Borgan, Ornulf and Gill, Richard D. and Keiding, Niels},
  year = {1996},
  edition = {1st ed. 1993. Corr. 4th printing 1996 edizione},
  publisher = {{Springer Nature}},
  address = {{New York Berlin Heidelberg}},
  isbn = {978-0-387-94519-4},
  langid = {Inglese}
}

@incollection{andersen2014,
  title = {{{ARCH}} and {{GARCH Models}}},
  booktitle = {Wiley {{StatsRef}}: {{Statistics Reference Online}}},
  author = {Andersen, Torben and Bollerslev, Tim and Hadi, Ali},
  editor = {Balakrishnan, N. and Colton, Theodore and Everitt, Brian and Piegorsch, Walter and Ruggeri, Fabrizio and Teugels, Jozef L.},
  year = {2014},
  month = sep,
  pages = {stat03491},
  publisher = {{John Wiley \& Sons, Ltd}},
  address = {{Chichester, UK}},
  doi = {10.1002/9781118445112.stat03491},
  isbn = {978-1-118-44511-2},
  langid = {english},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Andersen_et_al_2014_ARCH_and_GARCH_Models.pdf}
}

@article{andreoli2018,
  title = {A Conjugate Prior for the {{Dirichlet}} Distribution},
  author = {Andreoli, Jean-Marc},
  year = {2018},
  journal = {arXiv:1811.05266},
  eprint = {1811.05266},
  eprinttype = {arxiv},
  abstract = {This note investigates a conjugate class for the Dirichlet distribution class in the exponential family.},
  archiveprefix = {arXiv},
  keywords = {60E99,Computer Science - Machine Learning,dirichlet prior boojum,done,Statistics - Machine Learning},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Andreoli_2018_A_conjugate_prior_for_the_Dirichlet_distribution.pdf;/home/dede/Zotero/storage/98FA3V42/1811.html}
}

@article{andrieu2008,
  title = {A Tutorial on Adaptive {{MCMC}}},
  author = {Andrieu, Christophe and Thoms, Johannes},
  year = {2008},
  month = dec,
  journal = {Statistics and Computing},
  volume = {18},
  number = {4},
  pages = {343--373},
  issn = {0960-3174, 1573-1375},
  doi = {10.1007/s11222-008-9110-y},
  langid = {english},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Andrieu_Thoms_2008_A_tutorial_on_adaptive_MCMC.pdf}
}

@book{aneiros2020,
  title = {Functional and {{High-Dimensional Statistics}} and {{Related Fields}}},
  editor = {Aneiros, Germ{\'a}n and Horov{\'a}, Ivana and Hu{\v s}kov{\'a}, Marie and Vieu, Philippe},
  year = {2020},
  series = {Contributions to {{Statistics}}},
  publisher = {{Springer International Publishing}},
  doi = {10.1007/978-3-030-47756-1},
  abstract = {This book presents the latest research on the statistical analysis of functional, high-dimensional and other complex data, addressing methodological and computational aspects, as well as real-world applications. It covers topics like classification, confidence bands, density estimation, depth, diagnostic tests, dimension reduction, estimation on manifolds, high- and infinite-dimensional statistics, inference on functional data, networks, operatorial statistics, prediction, regression, robustness, sequential learning, small-ball probability, smoothing, spatial data, testing, and topological object data analysis, and includes applications in automobile engineering, criminology, drawing recognition, economics, environmetrics, medicine, mobile phone data, spectrometrics and urban environments. The book gathers selected, refereed contributions presented at the Fifth International Workshop on Functional and Operatorial Statistics (IWFOS) in Brno, Czech Republic. The workshop was originally to be held on June 24-26, 2020, but had to be postponed as a consequence of the COVID-19 pandemic. Initiated by the Working Group on Functional and Operatorial Statistics at the University of Toulouse in 2008, the IWFOS workshops provide a forum to discuss the latest trends and advances in functional statistics and related fields, and foster the exchange of ideas and international collaboration in the field.},
  isbn = {978-3-030-47755-4},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Aneiros_et_al_2020_Functional_and_High-Dimensional_Statistics_and_Related_Fields.pdf;/home/dede/Zotero/storage/ZGZL7FEB/9783030477554.html}
}

@inproceedings{angelosante2009,
  title = {{{RLS-weighted Lasso}} for Adaptive Estimation of Sparse Signals},
  booktitle = {2009 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}},
  author = {Angelosante, Daniele and Giannakis, Georgios B.},
  year = {2009},
  month = apr,
  pages = {3245--3248},
  issn = {2379-190X},
  doi = {10.1109/ICASSP.2009.4960316},
  abstract = {The batch least-absolute shrinkage and selection operator (Lasso) has well-documented merits for estimating sparse signals of interest emerging in various applications, where observations adhere to parsimonious linear regression models. To cope with linearly growing complexity and memory requirements that batch Lasso estimators face when processing observations sequentially, the present paper develops a recursive Lasso algorithm that can also track slowly-varying sparse signals of interest. Performance analysis reveals that recursive Lasso can either estimate consistently the sparse signal's support or its nonzero entries, but not both. This motivates the development of a weighted version of the recursive Lasso scheme with weights obtained from the recursive least-squares (RLS) algorithm. The resultant RLS-weighted Lasso algorithm provably estimates sparse signals consistently. Simulated tests compare competing alternatives and corroborate the performance of the novel algorithms in estimating time-invariant and tracking slow-varying signals under sparsity constraints.},
  keywords = {Adaptive estimation,Collaboration,done,Government,Image coding,Input variables,Lasso,Linear regression,Performance analysis,Recursive estimation,Resonance light scattering,Signal processing,Sparsity,Tracking,Variable Selection},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Angelosante_Giannakis_2009_RLS-weighted_Lasso_for_adaptive_estimation_of_sparse_signals.pdf;/home/dede/Zotero/storage/ENTRGKI9/4960316.html}
}

@book{ansari2014,
  title = {Iran: {{A Very Short Introduction}}},
  shorttitle = {Iran},
  author = {Ansari, Ali},
  year = {2014},
  series = {Very {{Short Introductions}}},
  publisher = {{Oxford University Press}},
  isbn = {978-0-19-966934-9},
  file = {/home/dede/Zotero/storage/9KSYX8QP/(Very Short Introductions) Ali Ansari - Iran_ A Very Short Introduction-Oxford University Press (2014).epub}
}

@article{antoch2001,
  title = {Permutation Tests in Change Point Analysis},
  author = {Antoch, Jarom{\'{\i}}r and Hu{\v s}kov{\'a}, Marie},
  year = {2001},
  month = may,
  journal = {Statistics \& Probability Letters},
  volume = {53},
  number = {1},
  pages = {37--46},
  issn = {0167-7152},
  doi = {10.1016/S0167-7152(01)00009-8},
  abstract = {The critical values for various tests for changes in location model are obtained through the use of permutation tests principle. Theoretical results show that in the limit these new ``permutation tests'' behave in the same way as the ``classical tests'' stemming from both maximum likelihood and Bayes principles. However, the results of the simulation study show that the permutation tests behave considerably better than the corresponding classical tests if measured by the critical values attained.},
  langid = {english},
  keywords = {Change(s) in location model,Monte Carlo,Permutation tests,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Antoch_HuÅ¡kovÃ¡_2001_Permutation_tests_in_change_point_analysis.pdf;/home/dede/Zotero/storage/C9WXLUF6/S0167715201000098.html}
}

@book{applebaum2009,
  title = {L\'evy {{Processes}} and {{Stochastic Calculus}}},
  author = {Applebaum, David},
  year = {2009},
  series = {Cambridge {{Studies}} in {{Advanced Mathematics}}},
  edition = {Second},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/CBO9780511809781},
  abstract = {L\'evy processes form a wide and rich class of random process, and have many applications ranging from physics to finance. Stochastic calculus is the mathematics of systems interacting with random noise. Here, the author ties these two subjects together, beginning with an introduction to the general theory of L\'evy processes, then leading on to develop the stochastic calculus for L\'evy processes in a direct and accessible way. This fully revised edition now features a number of new topics. These include: regular variation and subexponential distributions; necessary and sufficient conditions for L\'evy processes to have finite moments; characterisation of L\'evy processes with finite variation; Kunita's estimates for moments of L\'evy type stochastic integrals; new proofs of Ito representation and martingale representation theorems for general L\'evy processes; multiple Wiener-L\'evy integrals and chaos decomposition; an introduction to Malliavin calculus; an introduction to stability theory for L\'evy-driven SDEs.},
  isbn = {978-0-521-73865-1},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Applebaum_2009_LÃ©vy_Processes_and_Stochastic_Calculus.pdf;/home/dede/Zotero/storage/EZY3XBIM/4AC698D37D3D8E57D099B73ADF4ACB11.html}
}

@book{argiento2019,
  title = {Bayesian {{Statistics}} and {{New Generations}}: {{BAYSM}} 2018, {{Warwick}}, {{UK}}, {{July}} 2-3 {{Selected Contributions}}},
  shorttitle = {Bayesian {{Statistics}} and {{New Generations}}},
  editor = {Argiento, Raffaele and Durante, Daniele and Wade, Sara},
  year = {2019},
  series = {Springer {{Proceedings}} in {{Mathematics}} \& {{Statistics}}},
  publisher = {{Springer International Publishing}},
  doi = {10.1007/978-3-030-30611-3},
  abstract = {This book presents a selection of peer-reviewed contributions to the fourth Bayesian Young Statisticians Meeting, BAYSM 2018, held at the University of Warwick on 2-3 July 2018. The meeting provided a valuable opportunity for young researchers, MSc students, PhD students, and postdocs interested in Bayesian statistics to connect with the broader Bayesian community. The proceedings offer cutting-edge papers on a wide range of topics in Bayesian statistics, identify important challenges and investigate promising methodological approaches, while also assessing current methods and stimulating applications. The book is intended for a broad audience of statisticians, and demonstrates how theoretical, methodological, and computational aspects are often combined in the Bayesian framework to successfully tackle complex problems.},
  isbn = {978-3-030-30610-6},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Argiento_et_al_2019_Bayesian_Statistics_and_New_Generations.pdf;/home/dede/Zotero/storage/DIXNDUHM/9783030306106.html}
}

@book{arnold2008,
  title = {{A First Course in Order Statistics}},
  author = {Arnold, Barry C. and Balakrishnan, N. and Nagaraja, H. N.},
  year = {2008},
  month = apr,
  publisher = {{Society for Industrial and Applied Mathematics}},
  address = {{Philadelphia, PA}},
  isbn = {978-0-89871-648-1},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Arnold_et_al_2008_A_First_Course_in_Order_Statistics.pdf}
}

@book{ash1999,
  title = {Probability and {{Measure Theory}}},
  author = {Ash, Robert B. and {Dol{\'e}ans-Dade}, Catherine A.},
  year = {1999},
  month = dec,
  edition = {2nd edition},
  publisher = {{Academic Press}},
  address = {{San Diego}},
  abstract = {Probability and Measure Theory, Second Edition, is a text for a graduate-level course in probability that includes essential background topics in analysis. It provides extensive coverage of conditional probability and expectation, strong laws of large numbers, martingale theory, the central limit theorem, ergodic theory, and Brownian motion.},
  isbn = {978-0-12-065202-0},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Ash_DolÃ©ans-Dade_1999_Probability_and_Measure_Theory.pdf}
}

@article{athey2018,
  title = {Generalized {{Random Forests}}},
  author = {Athey, Susan and Tibshirani, Julie and Wager, Stefan},
  year = {2018},
  month = apr,
  journal = {arXiv:1610.01271 [econ, stat]},
  eprint = {1610.01271},
  eprinttype = {arxiv},
  primaryclass = {econ, stat},
  abstract = {We propose generalized random forests, a method for non-parametric statistical estimation based on random forests (Breiman, 2001) that can be used to fit any quantity of interest identified as the solution to a set of local moment equations. Following the literature on local maximum likelihood estimation, our method considers a weighted set of nearby training examples; however, instead of using classical kernel weighting functions that are prone to a strong curse of dimensionality, we use an adaptive weighting function derived from a forest designed to express heterogeneity in the specified quantity of interest. We propose a flexible, computationally efficient algorithm for growing generalized random forests, develop a large sample theory for our method showing that our estimates are consistent and asymptotically Gaussian, and provide an estimator for their asymptotic variance that enables valid confidence intervals. We use our approach to develop new methods for three statistical tasks: non-parametric quantile regression, conditional average partial effect estimation, and heterogeneous treatment effect estimation via instrumental variables. A software implementation, grf for R and C++, is available from CRAN.},
  archiveprefix = {arXiv},
  keywords = {Economics - Econometrics,Statistics - Machine Learning,Statistics - Methodology,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Athey_et_al_2018_Generalized_Random_Forests.pdf;/home/dede/Zotero/storage/L9M9H26L/1610.html}
}

@book{athreya2006,
  title = {Measure Theory and Probability Theory},
  author = {Athreya, Krishna B. and Lahiri, Soumendra N.},
  year = {2006},
  publisher = {{Springer Science \& Business Media}},
  abstract = {This book arose out of two graduate courses that the authors have taught duringthepastseveralyears;the?rstonebeingonmeasuretheoryfollowed by the second one on advanced probability theory. The traditional approach to a ?rst course in measure theory, such as in Royden (1988), is to teach the Lebesgue measure on the real line, then the p di?erentation theorems of Lebesgue, L -spaces on R, and do general m- sure at the end of the course with one main application to the construction of product measures. This approach does have the pedagogic advantage of seeing one concrete case ?rst before going to the general one. But this also has the disadvantage in making many students' perspective on m- sure theory somewhat narrow. It leads them to think only in terms of the Lebesgue measure on the real line and to believe that measure theory is intimately tied to the topology of the real line. As students of statistics, probability, physics, engineering, economics, and biology know very well, there are mass distributions that are typically nonuniform, and hence it is useful to gain a general perspective. This book attempts to provide that general perspective right from the beginning. The opening chapter gives an informal introduction to measure and integration theory. It shows that the notions of ?-algebra of sets and countable additivity of a set function are dictated by certain very na- ral approximation procedures from practical applications and that they are not just some abstract ideas.},
  isbn = {978-0-387-35434-7},
  keywords = {Business \& Economics / Operations Research,Computers / Computer Science,Mathematics / Calculus,Mathematics / Mathematical Analysis,Mathematics / Probability \& Statistics / General,Mathematics / Probability \& Statistics / Stochastic Processes}
}

@incollection{atkinson2014,
  title = {Optimal {{Design}} of {{Experiments}}},
  booktitle = {Wiley {{StatsRef}}: {{Statistics Reference Online}}},
  author = {Atkinson, A. C. and Fedorov, V. V.},
  editor = {Balakrishnan, N. and Colton, Theodore and Everitt, Brian and Piegorsch, Walter and Ruggeri, Fabrizio and Teugels, Jozef L.},
  year = {2014},
  month = sep,
  pages = {stat00911},
  publisher = {{John Wiley \& Sons, Ltd}},
  address = {{Chichester, UK}},
  doi = {10.1002/9781118445112.stat00911},
  isbn = {978-1-118-44511-2},
  langid = {english},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Atkinson_Fedorov_2014_Optimal_Design_of_Experiments.pdf}
}

@inproceedings{atwi2011,
  title = {A Case Study in Robust Quickest Detection for Hidden {{Markov}} Models},
  booktitle = {Proceedings of the 2011 {{American Control Conference}}},
  author = {Atwi, Aliaa and Savla, Ketan and Dahleh, Munther A.},
  year = {2011},
  month = jun,
  pages = {780--785},
  issn = {2378-5861},
  doi = {10.1109/ACC.2011.5991027},
  abstract = {We consider the problem of detecting rare events in a real data set with structural interdependencies. The real data set is modeled using hidden Markov models (HMMs), and rare event detection is viewed as a variant of the quickest detection problem. We assess the feasibility of two quickest detection frameworks recently suggested. The first method is based on dynamic programming and follows a Bayesian approach, and the second method is a non-Bayesian approximate cumulative sum (CUSUM) algorithm. We discuss implementation considerations for each method and show their performance through simulations for a real data set. In addition, we examine, through simulations, the robustness of the CUSUM-based method when the rare event model is not exactly known but belongs to a known class of models.},
  keywords = {Data models,Delay,Hidden Markov models,Markov processes,Probability,Robustness,Tin},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Atwi_et_al_2011_A_case_study_in_robust_quickest_detection_for_hidden_Markov_models.pdf;/home/dede/Zotero/storage/NJZJQHB9/5991027.html}
}

@article{auer2002,
  title = {Finite-Time {{Analysis}} of the {{Multiarmed Bandit Problem}}},
  author = {Auer, Peter and {Cesa-Bianchi}, Nicol{\`o} and Fischer, Paul},
  year = {2002},
  month = may,
  journal = {Machine Learning},
  volume = {47},
  pages = {235--256},
  doi = {10.1023/A:1013689704352},
  abstract = {Reinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for a balance between exploring the environment to find profitable actions while taking the empirically best action as often as possible. A popular measure of a policy's success in addressing this dilemma is the regret, that is the loss due to the fact that the globally optimal policy is not followed all the times. One of the simplest examples of the exploration/exploitation dilemma is the multi-armed bandit problem. Lai and Robbins were the first ones to show that the regret for this problem has to grow at least logarithmically in the number of plays. Since then, policies which asymptotically achieve this regret have been devised by Lai and Robbins and many others. In this work we show that the optimal logarithmic regret is also achievable uniformly over time, with simple and efficient policies, and for all reward distributions with bounded support.},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Auer_et_al_2002_Finite-time_Analysis_of_the_Multiarmed_Bandit_Problem.pdf}
}

@article{averre2015,
  title = {Russia, Humanitarian Intervention and the {{Responsibility}} to {{Protect}}: The Case of {{Syria}}},
  shorttitle = {Russia, Humanitarian Intervention and the {{Responsibility}} to {{Protect}}},
  author = {Averre, Dered and Davies, Lance},
  year = {2015},
  month = jul,
  journal = {International Affairs},
  volume = {91},
  number = {4},
  pages = {813--834},
  issn = {0020-5850},
  doi = {10.1111/1468-2346.12343},
  abstract = {Western analysis perceives Russian approaches to issues of humanitarian intervention and the Responsibility to Protect (R2P) as running counter to western-inspired international norms. This debate has surfaced with some vigour over Russia's policy in the Syria conflict where, in order to protect its strategic interests in Syria, an obstructionist Moscow has been accused of ignoring humanitarian considerations and allowing time for the Assad regime to crush the opposition by vetoing a resolution threatening to impose sanctions. While Russian approaches are undoubtedly explained by a desire to maximize its growing political influence and trade advantages to serve its legitimate foreign policy interests, and while Moscow's attitudes to intervention and R2P exhibit important differences from those of the major western liberal democracies, its arguments are in fact framed within a largely rational argument rooted in `traditional' state-centred international law. This article first highlights key arguments in the scholarly literature on intervention and R2P before going on to examine the evolution of Russian views on these issues. The analysis then focuses on the extent to which Moscow's arguments impact on international legal debates on the Libya and Syria conflicts. The article then seeks to explore how Russian approaches to intervention/R2P reflect fundamental trends in its foreign policy thinking and its quest for legitimacy in a negotiated international order. Finally, it attempts to raise some important questions regarding Russia's role in the future direction of the intervention/R2P debates.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/AVERRE_DAVIES_2015_Russia,_humanitarian_intervention_and_the_Responsibility_to_Protect.pdf;/home/dede/Zotero/storage/RIUPQW7S/2326866.html}
}

@article{ayoob2005,
  title = {The Future of Political {{Islam}}: The Importance of External Variables},
  shorttitle = {The Future of Political {{Islam}}},
  author = {Ayoob, Mohammed},
  year = {2005},
  month = oct,
  journal = {International Affairs},
  volume = {81},
  number = {5},
  pages = {951--961},
  issn = {0020-5850},
  doi = {10.1111/j.1468-2346.2005.00496.x},
  abstract = {Much has been written about the potential of political Islam to affect in major ways the future of Muslim societies and polities around the world. However, most analyses of political Islam that explicitly try to assess its future potential concentrate on its innate characteristics as a political ideology with the propensity to mobilize its adherents for purposes of regime change or social transformation or both. Therefore, these analyses emphasize the inherent nature of, and the in-built contradictions within, Islamism. Far less has been written about the environment external to the phenomenon of Islamism, namely, the milieu in which Islamist groups operate and propagate their ideology. Moreover, only a minuscule portion of the writings on political Islam try explicitly to analyse the impact that variables external to the inherent characteristics of political Islam are likely to have on Islamism's future prospects. This article attempts to fill this gap by putting Islamism in a wider perspective and by analysing the impact of environmental factors external to Islamism as an ideology, and largely outside the control of Islamists, on the future potential of political Islam.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Ayoob_2005_The_future_of_political_Islam.pdf;/home/dede/Zotero/storage/5JPP669E/2434966.html}
}

@article{aytacoglu2021,
  title = {Controlling the Conditional False Alarm Rate for the {{MEWMA}} Control Chart},
  author = {Ayta{\c c}o{\u g}lu, Burcu and Driscoll, Anne R. and Woodall, William H.},
  year = {2021},
  month = jul,
  journal = {Journal of Quality Technology},
  volume = {0},
  number = {0},
  pages = {1--16},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2021.1947162},
  abstract = {An integral part of the design of control charts, including the multivariate exponentially weighted moving average (MEWMA) control chart, is the determination of the appropriate control limits for prospective monitoring. Methods using Markov chain analyses, integral equations, and simulation have been proposed to determine the MEWMA chart limits when the limits are based on a specified in-control average run length (ARL) value. A drawback of the usual approach is that the conditional false alarm rate (CFAR) for these charts varies over time in what might be in an unexpected and undesirable way. We define the CFAR as the probability of a false alarm given no previous false alarm. We do not condition on the results of a Phase I sample, as done by others, in studies of the effect of estimation error on control chart performance. We propose the use of dynamic probability control limits (DPCLs) to keep the CFAR constant over time at a specified value. The CFAR at any time, however, could be controlled to be any specified value using our approach. Using simulation, we determine the DPCLs for the MEWMA control chart being used to monitor the mean vector with an assumed known variance-covariance matrix. We consider cases where the sample size is both fixed and time-varying. For varying sample sizes, the DPCLs adapt automatically to any change in the sample size distribution. In all cases, the CFAR is held closely to a fixed value and the resulting in-control run length performance follows closely to that of the geometric distribution.},
  keywords = {Average run length,dynamic probability control limits,exponentially weighted moving average,multivariate quality control,statistical process monitoring},
  annotation = {\_eprint: https://doi.org/10.1080/00224065.2021.1947162},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Aytacoglu_et_al_2021_Controlling_the_conditional_false_alarm_rate_for_the_MEWMA_control_chart.pdf;/home/dede/Zotero/storage/7WXATHLI/00224065.2021.html}
}

@article{azarnoush2016,
  title = {Monitoring {{Temporal Homogeneity}} in {{Attributed Network Streams}}},
  author = {Azarnoush, Bahareh and Paynabar, Kamran and Bekki, Jennifer and Runger, George},
  year = {2016},
  month = jan,
  journal = {Journal of Quality Technology},
  volume = {48},
  number = {1},
  pages = {28--43},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2016.11918149},
  abstract = {Network modeling and analysis has become a fundamental tool for studying various complex systems. This paper proposes an extension of statistical monitoring to network streams, which is crucial for effective decision-making in various applications. To this end, a model for the probability of edge existence as a function of vertex attributes is constructed and a likelihood method is developed to monitor the underlying network model. The method is flexible to detect any form of anomaly that arises from different network edge-formation mechanisms. Experiments on simulated and real network streams depict the properties and benefits of the method compared with existing methods in the literature.},
  keywords = {Change Detection,Dynamic Network,Likelihood-Ratio Test,todo},
  annotation = {\_eprint: https://doi.org/10.1080/00224065.2016.11918149},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Azarnoush_et_al_2016_Monitoring_Temporal_Homogeneity_in_Attributed_Network_Streams.pdf;/home/dede/Zotero/storage/ETACUSY9/00224065.2016.html}
}

@article{azzalini1985,
  title = {A {{Class}} of {{Distributions Which Includes}} the {{Normal Ones}}},
  author = {Azzalini, A.},
  year = {1985},
  journal = {Scandinavian Journal of Statistics},
  volume = {12},
  number = {2},
  pages = {171--178},
  publisher = {{[Board of the Foundation of the Scandinavian Journal of Statistics, Wiley]}},
  issn = {0303-6898},
  abstract = {A new class of density functions depending on a shape parameter {$\lambda$} is introduced, such that {$\lambda$}=0 corresponds to the standard normal density. The properties of this class of density functions are studied.},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Azzalini_1985_A_Class_of_Distributions_Which_Includes_the_Normal_Ones.pdf}
}

@article{azzalini1993,
  title = {On the {{Use}} of {{Nonparametric Regression}} for {{Checking Linear Relationships}}},
  author = {Azzalini, Adelchi and Bowman, Adrian},
  year = {1993},
  journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
  volume = {55},
  number = {2},
  pages = {549--557},
  issn = {2517-6161},
  doi = {10.1111/j.2517-6161.1993.tb01923.x},
  abstract = {The problem of checking the linearity of a regression relationship is addressed through the idea of smoothing of a residual plot. A pseudolikelihood ratio test statistic, which measures the distance between the nonparametric and the parametric models, is derived as a ratio of quadratic forms. The distribution of this statistic under the null hypothesis of linearity is calculated numerically by using Johnson curves. A power study shows the new statistic to be more sensitive to non-linearity than the Durbin-Watson statistic.},
  copyright = {\textcopyright{} 1993 Royal Statistical Society},
  langid = {english},
  keywords = {done,durbin-watson test,johnson curves,model checking,nonparametric regression,pseudolikelihood ratio test,residuals,testing linearity},
  annotation = {\_eprint: https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.2517-6161.1993.tb01923.x},
  file = {/home/dede/Zotero/storage/V398L2J2/j.2517-6161.1993.tb01923.html}
}

@article{azzalini1996,
  title = {The {{Multivariate Skew-Normal Distribution}}},
  author = {Azzalini, A. and {dalla Valle}, A.},
  year = {1996},
  journal = {Biometrika},
  volume = {83},
  number = {4},
  pages = {715--726},
  publisher = {{[Oxford University Press, Biometrika Trust]}},
  issn = {0006-3444},
  abstract = {The paper extends earlier work on the so-called skew-normal distribution, a family of distributions including the normal, but with an extra parameter to regulate skewness. The present work introduces a multivariate parametric family such that the marginal densities are scalar skew-normal, and studies its properties, with special emphasis on the bivariate case.}
}

@book{azzalini2001,
  title = {Inferenza Statistica: Una Presentazione Basata Sul Concetto Di Verosimiglianza},
  shorttitle = {Inferenza Statistica},
  author = {Azzalini, A.},
  year = {2001},
  edition = {Second},
  publisher = {{Springer-Verlag}},
  abstract = {Il concetto di verosimiglianza gioca un ruolo fondamentale nell'impostazione corrente della Statistica, sia per introdurre nozioni generali della teoria che per lo sviluppo di metodi specifici. Questo libro presenta un'esposizione della teoria statistica basata sulla verosimiglianza, osservata dal punto di vista della "teoria classica", e dimostra come il corpo principale delle tecniche statistiche attualmente in uso possano essere desunte da un numero limitato di concetti-chiave. L'attuale edizione integra la precedente con un capitolo sui modelli lineari generalizzati e con altri aggiornamenti quali numerose illustrazioni numeriche, basate su applicazioni reali, che facilitano la percezione della rilevanza operativa dei metodi presentati.},
  isbn = {978-88-470-0130-5},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Azzalini_2001_Inferenza_statistica.pdf}
}

@book{azzalini2012,
  title = {Data {{Analysis}} and {{Data Mining}}: {{An Introduction}}},
  shorttitle = {Data {{Analysis}} and {{Data Mining}}},
  author = {Azzalini, Adelchi and Scarpa, Bruno},
  year = {2012},
  publisher = {{Oxford University Press}},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Azzalini_Scarpa_2012_Data_Analysis_and_Data_Mining.pdf}
}

@book{azzalini2014,
  title = {The {{Skew-Normal}} and {{Related Families}}},
  author = {Azzalini, Adelchi and Capitanio, Antonella},
  year = {2014},
  publisher = {{Cambridge University Press}},
  abstract = {Interest in the skew-normal and related families of distributions has grown enormously over recent years, as theory has advanced, challenges of data have grown, and computational tools have made substantial progress. This comprehensive treatment, blending theory and practice, will be the standard resource for statisticians and applied researchers. Assuming only basic knowledge of (non-measure-theoretic) probability and statistical inference, the book is accessible to the wide range of researchers who use statistical modelling techniques. Guiding readers through the main concepts and results, it covers both the probability and the statistics sides of the subject, in the univariate and multivariate settings. The theoretical development is complemented by numerous illustrations and applications to a range of fields including quantitative finance, medical statistics, environmental risk studies, and industrial and business efficiency. The author's freely available R package sn, available from CRAN, equips readers to put the methods into action with their own data.},
  isbn = {978-1-107-02927-9},
  langid = {english},
  keywords = {Business \& Economics / Econometrics,Business \& Economics / Finance / General,Business \& Economics / Statistics,Mathematics / Probability \& Statistics / General}
}

@article{azzam2006,
  title = {Islamism Revisited},
  author = {Azzam, Maha},
  year = {2006},
  month = nov,
  journal = {International Affairs},
  volume = {82},
  number = {6},
  pages = {1119--1132},
  issn = {0020-5850},
  doi = {10.1111/j.1468-2346.2006.00591.x},
  abstract = {While contemporary Islamism is a response to the domestic political situation in many Muslim countries, as well as to US and western policies in the Muslim world, it is also about religious assertion and the forging of an alternative ideology, not only as a means of empowerment but also as a means of establishing a particular social order. The debate is as much an internal one as it is a global one and in many ways mirrors the struggles of the nineteenth and twentieth centuries for political and cultural independence.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Azzam_2006_Islamism_revisited.pdf;/home/dede/Zotero/storage/EEF3X2ZA/2435021.html}
}

@inproceedings{baadel2016,
  title = {Overlapping Clustering: {{A}} Review},
  shorttitle = {Overlapping Clustering},
  booktitle = {2016 {{SAI Computing Conference}} ({{SAI}})},
  author = {Baadel, Said and Thabtah, Fadi and Lu, Joan},
  year = {2016},
  month = jul,
  pages = {233--237},
  publisher = {{IEEE}},
  address = {{London, United Kingdom}},
  doi = {10.1109/SAI.2016.7555988},
  isbn = {978-1-4673-8460-5},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Baadel_et_al_2016_Overlapping_clustering.pdf}
}

@article{bahdanau2016,
  title = {Neural {{Machine Translation}} by {{Jointly Learning}} to {{Align}} and {{Translate}}},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  year = {2016},
  month = may,
  journal = {arXiv:1409.0473 [cs, stat]},
  eprint = {1409.0473},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Bahdanau_et_al_2016_Neural_Machine_Translation_by_Jointly_Learning_to_Align_and_Translate.pdf;/home/dede/Zotero/storage/SUCT6GJU/1409.html}
}

@article{bai2020,
  title = {Spike-and-{{Slab Group Lassos}} for {{Grouped Regression}} and {{Sparse Generalized Additive Models}}},
  author = {Bai, Ray and Moran, Gemma E. and Antonelli, Joseph L. and Chen, Yong and Boland, Mary R.},
  year = {2020},
  month = may,
  journal = {Journal of the American Statistical Association},
  volume = {0},
  number = {0},
  pages = {1--14},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.2020.1765784},
  abstract = {\textendash We introduce the spike-and-slab group lasso (SSGL) for Bayesian estimation and variable selection in linear regression with grouped variables. We further extend the SSGL to sparse generalized additive models (GAMs), thereby introducing the first nonparametric variant of the spike-and-slab lasso methodology. Our model simultaneously performs group selection and estimation, while our fully Bayes treatment of the mixture proportion allows for model complexity control and automatic self-adaptivity to different levels of sparsity. We develop theory to uniquely characterize the global posterior mode under the SSGL and introduce a highly efficient block coordinate ascent algorithm for maximum a posteriori estimation. We further employ de-biasing methods to provide uncertainty quantification of our estimates. Thus, implementation of our model avoids the computational intensiveness of Markov chain Monte Carlo in high dimensions. We derive posterior concentration rates for both grouped linear regression and sparse GAMs when the number of covariates grows at nearly exponential rate with sample size. Finally, we illustrate our methodology through extensive simulations and data analysis. Supplementary materials for this article are available online.},
  keywords = {High-dimensional regression,Interaction detection,Maximum a posteriori estimation,Nonparametric regression,Spike-and-slab lasso,todo,Variable selection},
  annotation = {\_eprint: https://doi.org/10.1080/01621459.2020.1765784},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Bai_et_al_2020_Spike-and-Slab_Group_Lassos_for_Grouped_Regression_and_Sparse_Generalized.pdf;/home/dede/Zotero/storage/JVL9QQS5/01621459.2020.html}
}

@article{bai2021,
  title = {Spike-and-{{Slab Meets LASSO}}: {{A Review}} of the {{Spike-and-Slab LASSO}}},
  shorttitle = {Spike-and-{{Slab Meets LASSO}}},
  author = {Bai, Ray and Rockova, Veronika and George, Edward I.},
  year = {2021},
  month = may,
  journal = {arXiv:2010.06451 [stat]},
  eprint = {2010.06451},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {High-dimensional data sets have become ubiquitous in the past few decades, often with many more covariates than observations. In the frequentist setting, penalized likelihood methods are the most popular approach for variable selection and estimation in high-dimensional data. In the Bayesian framework, spike-and-slab methods are commonly used as probabilistic constructs for high-dimensional modeling. Within the context of linear regression, Rockova and George (2018) introduced the spike-and-slab LASSO (SSL), an approach based on a prior which provides a continuum between the penalized likelihood LASSO and the Bayesian point-mass spike-and-slab formulations. Since its inception, the spike-and-slab LASSO has been extended to a variety of contexts, including generalized linear models, factor analysis, graphical models, and nonparametric regression. The goal of this paper is to survey the landscape surrounding spike-and-slab LASSO methodology. First we elucidate the attractive properties and the computational tractability of SSL priors in high dimensions. We then review methodological developments of the SSL and outline several theoretical developments. We illustrate the methodology on both simulated and real datasets.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Bai_et_al_2021_Spike-and-Slab_Meets_LASSO.pdf;/home/dede/Zotero/storage/HF9VQPD6/2010.html}
}

@article{bakir1979,
  title = {A {{Nonparametric Procedure}} for {{Process Control Based}} on {{Within-Group Ranking}}},
  author = {Bakir, Saad T. and Reynolds, Marion R.},
  year = {1979},
  journal = {Technometrics},
  volume = {21},
  number = {2},
  pages = {175--183},
  publisher = {{[Taylor \& Francis, Ltd., American Statistical Association, American Society for Quality]}},
  issn = {0040-1706},
  doi = {10.2307/1268514},
  abstract = {A nonparametric procedure is developed for the problem of quickly detecting any shift in the mean of a sequence of observations from a specified control value. The proposed procedure is based on Wilcoxon signed-rank statistics where ranking is within groups. A cumulative sum control chart type stopping rule is used with the Wilcoxon statistics. Using a Markov chain approach, the average run length of the procedure can be computed exactly for any distribution for which the distribution of the Wilcoxon signed-rank statistic is known. The procedure has the same average run length for any continuous distribution which is symmetric about the control value.}
}

@article{baldi1989,
  title = {Neural Networks and Principal Component Analysis: {{Learning}} from Examples without Local Minima},
  shorttitle = {Neural Networks and Principal Component Analysis},
  author = {Baldi, Pierre and Hornik, Kurt},
  year = {1989},
  month = jan,
  journal = {Neural Networks},
  volume = {2},
  number = {1},
  pages = {53--58},
  issn = {0893-6080},
  doi = {10.1016/0893-6080(89)90014-2},
  abstract = {We consider the problem of learning from examples in layered linear feed-forward neural networks using optimization methods, such as back propagation, with respect to the usual quadratic error function E of the connection weights. Our main result is a complete description of the landscape attached to E in terms of principal component analysis. We show that E has a unique minimum corresponding to the projection onto the subspace generated by the first principal vectors of a covariance matrix associated with the training patterns. All the additional critical points of E are saddle points (corresponding to projections onto subspaces generated by higher order vectors). The auto-associative case is examined in detail. Extensions and implications for the learning algorithms are discussed.},
  langid = {english},
  keywords = {Back propagation,Learning,Neural networks,Principal component analysis,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Baldi_Hornik_1989_Neural_networks_and_principal_component_analysis.pdf}
}

@article{balogh2004,
  title = {The {{Bimodal Galaxy Color Distribution}}: {{Dependence}} on {{Luminosity}} and {{Environment}}},
  shorttitle = {The {{Bimodal Galaxy Color Distribution}}},
  author = {Balogh, Michael L. and Baldry, Ivan K. and Nichol, Robert and Miller, Chris and Bower, Richard and Glazebrook, Karl},
  year = {2004},
  month = nov,
  journal = {The Astrophysical Journal},
  volume = {615},
  number = {2},
  pages = {L101-L104},
  issn = {0004-637X, 1538-4357},
  doi = {10.1086/426079},
  langid = {english},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Balogh_et_al_2004_The_Bimodal_Galaxy_Color_Distribution2.pdf}
}

@book{baltagi2008,
  title = {{Econometric Analysis of Panel Data}},
  author = {Baltagi, Badi H.},
  year = {2008},
  month = apr,
  edition = {4 edizione},
  publisher = {{John Wiley \& Sons Inc}},
  address = {{Chichester, UK ; Hoboken, NJ}},
  abstract = {Written by one of the world's leading researchers and writers in the field, Econometric Analysis of Panel Data has become established as the leading textbook for postgraduate courses in panel data. This new edition reflects the rapid developments in the field covering the vast research that has been conducted on panel data since its initial publication. Featuring the most recent empirical examples from panel data literature, data sets are also provided as well as the programs to implement the estimation and testing procedures described in the book. These programs will be made available via an accompanying website which will also contain solutions to end of chapter exercises that will appear in the book.The text has been fully updated with new material on dynamic panel data models and recent results on non-linear panel models and in particular work on limited dependent variables panel data models.},
  isbn = {978-0-470-51886-1},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Baltagi_2008_Econometric_Analysis_of_Panel_Data.pdf}
}

@article{bandyopadhyay2016,
  title = {Nonparametric Spatial Models for Clustered Ordered Periodontal Data},
  author = {Bandyopadhyay, Dipankar and Canale, Antonio},
  year = {2016},
  month = aug,
  journal = {Journal of the Royal Statistical Society. Series C, Applied statistics},
  volume = {65},
  number = {4},
  pages = {619--640},
  issn = {0035-9254},
  doi = {10.1111/rssc.12150},
  abstract = {Clinical attachment level (CAL) is regarded as the most popular measure to assess periodontal disease (PD). These probed tooth-site level measures are usually rounded and recorded as whole numbers (in mm) producing clustered (site measures within a mouth) error-prone ordinal responses representing some ordering of the underlying PD progression. In addition, it is hypothesized that PD progression can be spatially-referenced, i.e., proximal tooth-sites share similar PD status in comparison to sites that are distantly located. In this paper, we develop a Bayesian multivariate probit framework for these ordinal responses where the cut-point parameters linking the observed ordinal CAL levels to the latent underlying disease process can be fixed in advance. The latent spatial association characterizing conditional independence under Gaussian graphs is introduced via a nonparametric Bayesian approach motivated by the probit stick-breaking process, where the components of the stick-breaking weights follows a multivariate Gaussian density with the precision matrix distributed as G-Wishart. This yields a computationally simple, yet robust and flexible framework to capture the latent disease status leading to a natural clustering of tooth-sites and subjects with similar PD status (beyond spatial clustering), and improved parameter estimation through sharing of information. Both simulation studies and application to a motivating PD dataset reveal the advantages of considering this flexible nonparametric ordinal framework over other alternatives.},
  pmcid = {PMC4979584},
  pmid = {27524839},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Bandyopadhyay_Canale_2016_Nonparametric_spatial_models_for_clustered_ordered_periodontal_data2.pdf}
}

@inproceedings{banerjee2005,
  title = {Model-Based Overlapping Clustering},
  booktitle = {Proceedings of the Eleventh {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery in Data Mining},
  author = {Banerjee, Arindam and Krumpelman, Chase and Ghosh, Joydeep and Basu, Sugato and Mooney, Raymond J.},
  year = {2005},
  month = aug,
  series = {{{KDD}} '05},
  pages = {532--537},
  publisher = {{Association for Computing Machinery}},
  address = {{Chicago, Illinois, USA}},
  doi = {10.1145/1081870.1081932},
  abstract = {While the vast majority of clustering algorithms are partitional, many real world datasets have inherently overlapping clusters. Several approaches to finding overlapping clusters have come from work on analysis of biological datasets. In this paper, we interpret an overlapping clustering model proposed by Segal et al. [23] as a generalization of Gaussian mixture models, and we extend it to an overlapping clustering model based on mixtures of any regular exponential family distribution and the corresponding Bregman divergence. We provide the necessary algorithm modifications for this extension, and present results on synthetic data as well as subsets of 20-Newsgroups and EachMovie datasets.},
  isbn = {978-1-59593-135-1},
  keywords = {Bregman divergences,done,exponential model,graphical model,high-dimensional clustering,overlapping clustering},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Banerjee_et_al_2005_Model-based_overlapping_clustering.pdf}
}

@article{banerjee2008,
  title = {Gaussian Predictive Process Models for Large Spatial Data Sets},
  author = {Banerjee, Sudipto and Gelfand, Alan E. and Finley, Andrew O. and Sang, Huiyan},
  year = {2008},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {70},
  number = {4},
  pages = {825--848},
  issn = {1467-9868},
  doi = {10.1111/j.1467-9868.2008.00663.x},
  abstract = {Summary. With scientific data available at geocoded locations, investigators are increasingly turning to spatial process models for carrying out statistical inference. Over the last decade, hierarchical models implemented through Markov chain Monte Carlo methods have become especially popular for spatial modelling, given their flexibility and power to fit models that would be infeasible with classical methods as well as their avoidance of possibly inappropriate asymptotics. However, fitting hierarchical spatial models often involves expensive matrix decompositions whose computational complexity increases in cubic order with the number of spatial locations, rendering such models infeasible for large spatial data sets. This computational burden is exacerbated in multivariate settings with several spatially dependent response variables. It is also aggravated when data are collected at frequent time points and spatiotemporal process models are used. With regard to this challenge, our contribution is to work with what we call predictive process models for spatial and spatiotemporal data. Every spatial (or spatiotemporal) process induces a predictive process model (in fact, arbitrarily many of them). The latter models project process realizations of the former to a lower dimensional subspace, thereby reducing the computational burden. Hence, we achieve the flexibility to accommodate non-stationary, non-Gaussian, possibly multivariate, possibly spatiotemporal processes in the context of large data sets. We discuss attractive theoretical properties of these predictive processes. We also provide a computational template encompassing these diverse settings. Finally, we illustrate the approach with simulated and real data sets.},
  copyright = {\textcopyright{} 2008 Royal Statistical Society},
  langid = {english},
  keywords = {Co-regionalization,Gaussian processes,Hierarchical modelling,Kriging,Markov chain Monte Carlo methods,Multivariate spatial processes,Spaceâ€“time processes,todo},
  annotation = {\_eprint: https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9868.2008.00663.x},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Banerjee_et_al_2008_Gaussian_predictive_process_models_for_large_spatial_data_sets.pdf;/home/dede/Zotero/storage/6EBKQI7P/j.1467-9868.2008.00663.html}
}

@article{banerjee2013,
  title = {Efficient {{Gaussian}} Process Regression for Large Datasets},
  author = {Banerjee, Anjishnu and Dunson, David B. and Tokdar, Surya T.},
  year = {2013},
  month = mar,
  journal = {Biometrika},
  volume = {100},
  number = {1},
  pages = {75--89},
  issn = {0006-3444},
  doi = {10.1093/biomet/ass068},
  abstract = {Gaussian processes are widely used in nonparametric regression, classification and spatiotemporal modelling, facilitated in part by a rich literature on their theoretical properties. However, one of their practical limitations is expensive computation, typically on the order of n3 where n is the number of data points, in performing the necessary matrix inversions. For large datasets, storage and processing also lead to computational bottlenecks, and numerical stability of the estimates and predicted values degrades with increasing n. Various methods have been proposed to address these problems, including predictive processes in spatial data analysis and the subset-of-regressors technique in machine learning. The idea underlying these approaches is to use a subset of the data, but this raises questions concerning sensitivity to the choice of subset and limitations in estimating fine-scale structure in regions that are not well covered by the subset. Motivated by the literature on compressive sensing, we propose an alternative approach that involves linear projection of all the data points onto a lower-dimensional subspace. We demonstrate the superiority of this approach from a theoretical perspective and through simulated and real data examples.},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Banerjee_et_al_2013_Efficient_Gaussian_process_regression_for_large_datasets.pdf;/home/dede/Zotero/storage/62HGXPV8/193404.html}
}

@book{banerjee2014,
  title = {Hierarchical {{Modeling}} and {{Analysis}} for {{Spatial Data}}},
  author = {Banerjee, Sudipto and Carlin, Bradley P. and Gelfand, Alan E.},
  year = {2014},
  month = sep,
  edition = {2nd Edition},
  publisher = {{Chapman and Hall/CRC}},
  address = {{Boca Raton}},
  abstract = {Keep Up to Date with the Evolving Landscape of Space and Space-Time Data Analysis and Modeling Since the publication of the first edition, the statistical landscape has substantially changed for analyzing space and space-time data. More than twice the size of its predecessor, Hierarchical Modeling and Analysis for Spatial Data, Second Edition reflects the major growth in spatial statistics as both a research area and an area of application. New to the Second Edition   New chapter on spatial point patterns developed primarily from a modeling perspective New chapter on big data that shows how the predictive process handles reasonably large datasets New chapter on spatial and spatiotemporal gradient modeling that incorporates recent developments in spatial boundary analysis and wombling New chapter on the theoretical aspects of geostatistical (point-referenced) modeling  Greatly expanded chapters on methods for multivariate and spatiotemporal modeling New special topics sections on data fusion/assimilation and spatial analysis for data on extremes Double the number of exercises  Many more color figures integrated throughout the text Updated computational aspects, including the latest version of WinBUGS, the new flexible spBayes software, and assorted R packages  The Only Comprehensive Treatment of the Theory, Methods, and Software This second edition continues to provide a complete treatment of the theory, methods, and application of hierarchical modeling for spatial and spatiotemporal data. It tackles current challenges in handling this type of data, with increased emphasis on observational data, big data, and the upsurge of associated software tools. The authors also explore important application domains, including environmental science, forestry, public health, and real estate.},
  isbn = {978-1-4398-1917-3},
  langid = {english},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Banerjee_et_al_2014_Hierarchical_Modeling_and_Analysis_for_Spatial_Data.pdf}
}

@article{barber2015,
  title = {Controlling the False Discovery Rate via Knockoffs},
  author = {Barber, Rina Foygel and Cand{\`e}s, Emmanuel J.},
  year = {2015},
  month = oct,
  journal = {The Annals of Statistics},
  volume = {43},
  number = {5},
  pages = {2055--2085},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/15-AOS1337},
  abstract = {In many fields of science, we observe a response variable together with a large number of potential explanatory variables, and would like to be able to discover which variables are truly associated with the response. At the same time, we need to know that the false discovery rate (FDR)\textemdash the expected fraction of false discoveries among all discoveries\textemdash is not too high, in order to assure the scientist that most of the discoveries are indeed true and replicable. This paper introduces the knockoff filter, a new variable selection procedure controlling the FDR in the statistical linear model whenever there are at least as many observations as variables. This method achieves exact FDR control in finite sample settings no matter the design or covariates, the number of variables in the model, or the amplitudes of the unknown regression coefficients, and does not require any knowledge of the noise level. As the name suggests, the method operates by manufacturing knockoff variables that are cheap\textemdash their construction does not require any new data\textemdash and are designed to mimic the correlation structure found within the existing variables, in a way that allows for accurate FDR control, beyond what is possible with permutation-based methods. The method of knockoffs is very general and flexible, and can work with a broad class of test statistics. We test the method in combination with statistics from the Lasso for sparse regression, and obtain empirical results showing that the resulting method has far more power than existing selection rules when the proportion of null variables is high.},
  keywords = {62F03,62J05,false discovery rate (FDR),Lasso,Martingale theory,permutation methods,sequential hypothesis testing,Variable selection},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Barber_CandÃ¨s_2015_Controlling_the_false_discovery_rate_via_knockoffs.pdf;/home/dede/Zotero/storage/MXMIPE4U/15-AOS1337.html}
}

@article{bardwell2019,
  title = {Most {{Recent Changepoint Detection}} in {{Panel Data}}},
  author = {Bardwell, Lawrence and Fearnhead, Paul and Eckley, Idris A. and Smith, Simon and Spott, Martin},
  year = {2019},
  month = jan,
  journal = {Technometrics},
  volume = {61},
  number = {1},
  pages = {88--98},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.2018.1438926},
  abstract = {Detecting recent changepoints in time-series can be important for short-term prediction, as we can then base predictions just on the data since the changepoint. In many applications, we have panel data, consisting of many related univariate time-series. We present a novel approach to detect sets of most recent changepoints in such panel data that aims to pool information across time-series, so that we preferentially infer a most recent change at the same time-point in multiple series. Our approach is computationally efficient as it involves analysing each time-series independently to obtain a profile-likelihood like quantity that summarizes the evidence for the series having either no change or a specific value for its most recent changepoint. We then post-process this output from each time-series to obtain a potentially small set of times for the most recent changepoints, and, for each time, the set of series that has their most recent changepoint at that time. We demonstrate the usefulness of this method on two datasets: forecasting events in a telecommunications network and inference about changes in the net asset ratio for a panel of US firms.},
  keywords = {Breakpoints,Changepoints,Forecasting,Panel data,Structural breaks,todo},
  annotation = {\_eprint: https://doi.org/10.1080/00401706.2018.1438926},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Bardwell_et_al_2019_Most_Recent_Changepoint_Detection_in_Panel_Data.pdf;/home/dede/Zotero/storage/672TFVWH/00401706.2018.html}
}

@book{barndorff-nielsen1989,
  title = {Asymptotic {{Techniques}} for {{Use}} in {{Statistics}}},
  author = {{Barndorff-Nielsen}, O. E. and Cox, D. R.},
  year = {1989},
  month = jan,
  publisher = {{Springer}},
  abstract = {The use in statistical theory of approximate arguments based on such methods as local linearization (the delta method) and approxi\- mate normality has a long history. Such ideas play at least three roles. First they may give simple approximate answers to distributional problems where an exact solution is known in principle but difficult to implement. The second role is to yield higher-order expansions from which the accuracy of simple approximations may be assessed and where necessary improved. Thirdly the systematic development of a theoretical approach to statistical inference that will apply to quite general families of statistical models demands an asymptotic formulation, as far as possible one that will recover 'exact' results where these are available. The approximate arguments are developed by supposing that some defining quantity, often a sample size but more generally an amount of information, becomes large: it must be stressed that this is a technical device for generating approximations whose adequacy always needs assessing, rather than a 'physical' limiting notion. Of the three roles outlined above, the first two are quite close to the traditional roles of asymptotic expansions in applied mathematics and much ofthe very extensive literature on the asymptotic expansion of integrals and of the special functions of mathematical physics is quite directly relevant, although the recasting of these methods into a probability mould is quite often enlightening.},
  isbn = {978-0-412-31400-1},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Barndorff-Nielsen_Cox_1989_Asymptotic_Techniques_for_Use_in_Statistics.pdf}
}

@unpublished{barnes2021,
  title = {Matrix {{Forensics}}},
  author = {Barnes, Richard},
  year = {2021},
  month = oct,
  abstract = {Collection of Matrix/Linear Algebra Information},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Barnes_2021_Matrix_Forensics.pdf}
}

@book{barnett1999,
  title = {Comparative Statistical Inference},
  author = {Barnett, Vic},
  year = {1999},
  publisher = {{John Wiley \& Sons}},
  isbn = {978-0-471-97643-1},
  keywords = {Mathematics / Probability \& Statistics / General,Mathematics / Probability \& Statistics / Stochastic Processes},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Barnett_1999_Comparative_statistical_inference.pdf}
}

@book{bartholomew2011,
  title = {Latent Variable Models and Factor Analysis : A Unified Approach},
  shorttitle = {Latent Variable Models and Factor Analysis},
  author = {Bartholomew, David J.},
  year = {2011},
  edition = {3rd ed.},
  publisher = {{Wiley,}},
  address = {{Chichester, West Sussex :}},
  isbn = {978-0-470-97192-5},
  keywords = {factor analysis,latent class},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Bartholomew_2011_Latent_variable_models_and_factor_analysis.pdf;/home/dede/Zotero/storage/BXJBP44Z/9573550.html}
}

@book{bartolucci2012,
  title = {Latent {{Markov Models}} for {{Longitudinal Data}}},
  author = {Bartolucci, Francesco and Farcomeni, Alessio and Pennoni, Fulvia},
  year = {2012},
  series = {Chapman \& {{Hall}}/{{CRC Statistics}} in the {{Social}} and {{Behavioral Sciences}}},
  publisher = {{Chapman and Hall/CRC}},
  abstract = {Drawing on the authors' extensive research in the analysis of categorical longitudinal data, Latent Markov Models for Longitudinal Data focuses on the formulation of latent Markov models and the practical use of these models. Numerous examples illustrate how latent Markov models are used in econom},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Bartolucci_et_al_2012_Latent_Markov_Models_for_Longitudinal_Data.pdf;/home/dede/Zotero/storage/ZIQIBPDF/9781439817087.html}
}

@inproceedings{basioti2021,
  title = {Single {{Image Restoration}} with {{Generative Priors}}},
  booktitle = {2021 {{IEEE International Conference}} on {{Image Processing}} ({{ICIP}})},
  author = {Basioti, Kalliopi and Moustakides, George V.},
  year = {2021},
  month = sep,
  pages = {1679--1683},
  issn = {2381-8549},
  doi = {10.1109/ICIP42928.2021.9506768},
  abstract = {Generative models can be used, as an alternative to conventional probability densities, to capture the statistical behavior of complicated datasets. Unlike probability densities with which the generation of realizations may become a challenging task, generative models have an inherent ability to easily produce realizations, which, in the case of natural images can be extremely realistic. In many image restoration problems, such as deblurring, colorization, inpainting, super-resolution, etc., probability densities are used as priors, one may therefore wonder whether we can, instead, adopt generative models. Indeed such methods have appeared in the literature, but they require exact knowledge of the transformations responsible for the data distortion and involve regularizer terms with weights that require adjustment. Our approach, by combining maximum a-posteriori probability with maximum likelihood estimation, can successfully restore images in both blind and non-blind modes without the need to fine-tune any regularization parameters. Simulations on deblurring, colorization, and image separation problems with exact knowledge of the transformation demonstrate improved image quality, reduced computational cost compared to existing methods. Comparable results are also enjoyed when the distortion models contain unknown parameters.},
  keywords = {Bayes procedures,Blind image restoration/separation,Computational modeling,Distortion,Generative modeling,Image restoration,Image separation,Mathematical model,Probability,Superresolution,Task analysis,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Basioti_Moustakides_2021_Single_Image_Restoration_with_Generative_Priors.pdf;/home/dede/Zotero/storage/PWMXR8V8/9506768.html}
}

@book{bass2011,
  title = {{Stochastic Processes}},
  author = {Bass, Richard F.},
  year = {2011},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge ; New York}},
  abstract = {This comprehensive guide to stochastic processes gives a complete overview of the theory and addresses the most important applications. Pitched at a level accessible to beginning graduate students and researchers from applied disciplines, it is both a course book and a rich resource for individual readers. Subjects covered include Brownian motion, stochastic calculus, stochastic differential equations, Markov processes, weak convergence of processes and semigroup theory. Applications include the Black\textendash Scholes formula for the pricing of derivatives in financial mathematics, the Kalman\textendash Bucy filter used in the US space program and also theoretical applications to partial differential equations and analysis. Short, readable chapters aim for clarity rather than full generality. More than 350 exercises are included to help readers put their new-found knowledge to the test and to prepare them for tackling the research literature.},
  isbn = {978-1-107-00800-7},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Bass_2011_Stochastic_Processes.pdf}
}

@article{bassetti2006,
  title = {On Minimum {{Kantorovich}} Distance Estimators},
  author = {Bassetti, Federico and Bodini, Antonella and Regazzini, Eugenio},
  year = {2006},
  month = jul,
  journal = {Statistics \& Probability Letters},
  volume = {76},
  number = {12},
  pages = {1298--1302},
  issn = {0167-7152},
  doi = {10.1016/j.spl.2006.02.001},
  abstract = {This article introduces estimators defined as minimizers of Kantorovich distances between statistical models and empirical distributions. Existence, measurability and consistency of these estimators are studied. A few significant examples illustrate the applicability of the theoretical results dealt with in the paper.},
  langid = {english},
  keywords = {Consistency of point estimators,Kantorovich distance,Minimum dissimilarity estimators,Minimum Kantorovich distance estimators,optimal transport,todo,Wasserstein distance},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Bassetti_et_al_2006_On_minimum_Kantorovich_distance_estimators.pdf;/home/dede/Zotero/storage/AINPG762/S0167715206000381.html}
}

@book{basso2009,
  title = {{Permutation Tests for Stochastic Ordering and ANOVA: Theory and Applications with R: 194}},
  shorttitle = {{Permutation Tests for Stochastic Ordering and ANOVA}},
  author = {Basso, Dario and Pesarin, Fortunato and Salmaso, Luigi and Solari, Aldo},
  year = {2009},
  month = apr,
  edition = {2009\textdegree{} edizione},
  publisher = {{Springer}},
  address = {{London ; New York}},
  isbn = {978-0-387-85955-2},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Basso_et_al_2009_Permutation_Tests_for_Stochastic_Ordering_and_ANOVA.pdf}
}

@article{bastani2020,
  title = {Online {{Decision Making}} with {{High-Dimensional Covariates}}},
  author = {Bastani, Hamsa and Bayati, Mohsen},
  year = {2020},
  month = jan,
  journal = {Operations Research},
  volume = {68},
  number = {1},
  pages = {276--294},
  publisher = {{INFORMS}},
  issn = {0030-364X},
  doi = {10.1287/opre.2019.1902},
  abstract = {Big data have enabled decision makers to tailor decisions at the individual level in a variety of domains, such as personalized medicine and online advertising. Doing so involves learning a model of decision rewards conditional on individual-specific covariates. In many practical settings, these covariates are high dimensional; however, typically only a small subset of the observed features are predictive of a decision's success. We formulate this problem as a K-armed contextual bandit with high-dimensional covariates and present a new efficient bandit algorithm based on the LASSO estimator. We prove that our algorithm's cumulative expected regret scales at most polylogarithmically in the covariate dimension d; to the best of our knowledge, this is the first such bound for a contextual bandit. The key step in our analysis is proving a new tail inequality that guarantees the convergence of the LASSO estimator despite the non-i.i.d. data induced by the bandit policy. Furthermore, we illustrate the practical relevance of our algorithm by evaluating it on a simplified version of a medication dosing problem. A patient's optimal medication dosage depends on the patient's genetic profile and medical records; incorrect initial dosage may result in adverse consequences, such as stroke or bleeding. We show that our algorithm outperforms existing bandit methods and physicians in correctly dosing a majority of patients.},
  keywords = {adaptive treatment allocation,contextual bandits,done,high-dimensional statistics,LASSO,online learning,personalized decision making,simulation,statistics,Stochastic Models},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Bastani_Bayati_2020_Online_Decision_Making_with_High-Dimensional_Covariates.pdf}
}

@article{basu1955,
  title = {On Statistics Independent of a Complete Sufficient Statistic},
  author = {Basu, Debabrata},
  year = {1955},
  journal = {Sankhy\=a: The Indian Journal of Statistics},
  volume = {15},
  keywords = {done}
}

@article{battey2022,
  title = {Some {{Perspectives}} on {{Inference}} in {{High Dimensions}}},
  author = {Battey, H. S. and Cox, D. R.},
  year = {2022},
  month = feb,
  journal = {Statistical Science},
  volume = {37},
  number = {1},
  pages = {110--122},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/21-STS824},
  abstract = {With very large amounts of data, important aspects of statistical analysis may appear largely descriptive in that the role of probability sometimes seems limited or totally absent. The main emphasis of the present paper lies on contexts where formulation in terms of a probabilistic model is feasible and fruitful but to be at all realistic large numbers of unknown parameters need consideration. Then many of the standard approaches to statistical analysis, for instance direct application of the method of maximum likelihood, or the use of flat priors, often encounter difficulties. After a brief discussion of broad conceptual issues, we provide some new perspectives on aspects of high-dimensional statistical theory, emphasizing a number of open problems.},
  keywords = {inference,likelihood,model uncertainty,nuisance parameters,parameter orthogonalization,Sparsity,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Battey_Cox_2022_Some_Perspectives_on_Inference_in_High_Dimensions.pdf;/home/dede/Zotero/storage/WEDJN364/21-STS824.html}
}

@article{battiston2018,
  title = {Multi-{{Armed Bandit}} for {{Species Discovery}}: {{A Bayesian Nonparametric Approach}}},
  shorttitle = {Multi-{{Armed Bandit}} for {{Species Discovery}}},
  author = {Battiston, Marco and Favaro, Stefano and Teh, Yee Whye},
  year = {2018},
  month = jan,
  journal = {Journal of the American Statistical Association},
  volume = {113},
  number = {521},
  pages = {455--466},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.2016.1261711},
  abstract = {Let (P1, \ldots, PJ) denote J populations of animals from distinct regions. A priori, it is unknown which species are present in each region and what are their corresponding frequencies. Species are shared among populations and each species can be present in more than one region with its frequency varying across populations. In this article, we consider the problem of sequentially sampling these populations to observe the greatest number of different species. We adopt a Bayesian nonparametric approach and endow (P1, \ldots, PJ) with a hierarchical Pitman\textendash Yor process prior. As a consequence of the hierarchical structure, the J unknown discrete probability measures share the same support, that of their common random base measure. Given this prior choice, we propose a sequential rule that, at every time step, given the information available up to that point, selects the population from which to collect the next observation. Rather than picking the population with the highest posterior estimate of producing a new value, the proposed rule includes a Thompson sampling step to better balance the exploration\textendash exploitation trade-off. We also propose an extension of the algorithm to deal with incidence data, where multiple observations are collected in a time period. The performance of the proposed algorithms is assessed through a simulation study and compared to three other strategies. Finally, we compare these algorithms using a dataset of species of trees, collected from different plots in South America. Supplementary materials for this article are available online.},
  keywords = {todo},
  annotation = {\_eprint: https://doi.org/10.1080/01621459.2016.1261711},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Battiston_et_al_2018_Multi-Armed_Bandit_for_Species_Discovery.pdf;/home/dede/Zotero/storage/57IRYM4S/01621459.2016.html}
}

@article{bayarri2005,
  title = {A {{Bayesian}}, {{Sequential Look}} at u-{{Control Charts}}},
  author = {Bayarri, M. and {Garc{\'i}a-donato}, G.},
  year = {2005},
  month = may,
  journal = {Technometrics},
  volume = {47},
  doi = {10.1198/004017005000000085},
  abstract = {We extend the usual implementation of u-control charts (uCCs) in two ways. First, we overcome the restrictive (and often inadequate) assumptions of the Poisson model; next, we eliminate the need for the questionable base period by using a sequential procedure. We use empirical Bayes (EB) and Bayes methods and compare them with the traditional frequentist implementation. EB methods are somewhat easy to implement, and they deal nicely with extra-Poisson variability (and, at the same time, informally check the adequacy of the Poisson assumption). However. they still need the base period. The sequential, full Bayes approach, on the other hand, also avoids this drawback of traditional u-charts. The implementation requires numerical simulation, and also use of a prior distribution. Several possibilities for both objective and informative priors are explored. We argue that the sequential, full Bayesian uCC is a powerful and versatile tool for process monitoring.},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Bayarri_GarcÃ­a-donato_2005_A_Bayesian,_Sequential_Look_at_u-Control_Charts.pdf}
}

@article{bayer2020,
  title = {The Look-Elsewhere Effect from a Unified {{Bayesian}} and Frequentist Perspective},
  author = {Bayer, Adrian E. and Seljak, Uro{\v s}},
  year = {2020},
  month = oct,
  journal = {Journal of Cosmology and Astroparticle Physics},
  volume = {2020},
  number = {10},
  pages = {009},
  publisher = {{IOP Publishing}},
  issn = {1475-7516},
  doi = {10.1088/1475-7516/2020/10/009},
  langid = {english},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Bayer_Seljak_2020_The_look-elsewhere_effect_from_a_unified_Bayesian_and_frequentist_perspective.pdf;/home/dede/Zotero/storage/GBW77K45/009.html}
}

@inproceedings{bazot2011,
  title = {A {{Bernoulli-Gaussian}} Model for Gene Factor Analysis},
  booktitle = {2011 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Bazot, Cecile and Dobigeon, Nicolas and Tourneret, Jean-Yves and Hero, Alfred O.},
  year = {2011},
  month = may,
  pages = {5996--5999},
  publisher = {{IEEE}},
  address = {{Prague}},
  doi = {10.1109/ICASSP.2011.5947728},
  isbn = {978-1-4577-0538-0 978-1-4577-0539-7},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Bazot_et_al_2011_A_Bernoulli-Gaussian_model_for_gene_factor_analysis.pdf}
}

@book{beaumont1980,
  title = {Intermediate {{Mathematical Statistics}}},
  author = {Beaumont, G. P.},
  year = {1980},
  publisher = {{Springer Netherlands}},
  doi = {10.1007/978-94-009-5794-7},
  abstract = {This book covers those basic topics which usually form the core of intermediate courses in statistical theory; it is largely about estima\- tion and hypothesis testing. It is intended for undergraduates following courses in statistics but is also suitable preparatory read\- ing for some postgraduate courses. It is assumed that the reader has completed an introductory course which covered probability, random variables, moments and the sampling distributions. The level of mathematics required does not go beyond first year calculus. In case the reader has not acquired much facility in handling matrices, the results in least squares estimation are first obtained directly and then given an (optional) matrix formulation. If techniques for changing from one set of variables to another have not been met, then the appendix on these topics should be studied first. The same appendix contains essential discussion of the order statistics which are frequently used for illustrative purposes. Introductory courses usually include the elements of hypothesis testing and of point and interval estimation though the treatment must perforce become rather thin since at that stage it is difficult to provide adequate justifications for some procedures-plausible though they may seem. This text discusses these important topics in considerable detail, starting from scratch. The level is nowhere advanced and proofs of asymptotic results are omitted. Methods deriving from the Bayesian point of view are gradually introduced and alternate with the more usual techniques.},
  isbn = {978-0-412-15480-5},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Beaumont_1980_Intermediate_Mathematical_Statistics.pdf;/home/dede/Zotero/storage/3SNQZTQC/9780412154805.html}
}

@article{beaumont1992,
  title = {The {{General History}} of the {{Second World War}}},
  author = {Beaumont, Joan},
  editor = {Kitchen, Martin and Parker, R. A. C. and Willmott, H. P. and Keegan, John and Ellis, John and Wilt, Alan F.},
  year = {1992},
  journal = {The International History Review},
  volume = {14},
  number = {4},
  pages = {753--766},
  publisher = {{Taylor \& Francis, Ltd.}},
  issn = {0707-5332},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Beaumont_1992_The_General_History_of_the_Second_World_War.pdf}
}

@article{bell2014,
  title = {A {{Distribution-Free Multivariate Phase I Location Control Chart}} for {{Subgrouped Data}} from {{Elliptical Distributions}}},
  author = {Bell, Richard C. and {Jones-Farmer}, L. Allison and Billor, Nedret},
  year = {2014},
  month = oct,
  journal = {Technometrics},
  volume = {56},
  number = {4},
  pages = {528--538},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.2013.879264},
  abstract = {In quality control, a proper Phase I analysis is essential to the success of Phase II monitoring. A literature review reveals no distribution-free Phase I multivariate techniques in existence. This research develops a Phase I location control chart for multivariate elliptical processes. The resulting in-control reference sample can then be used to estimate the parameters for Phase II monitoring. Using Monte Carlo simulation, the proposed method is compared with the Hotelling's T2 Phase I chart. Although Hotelling's T2 chart is preferred when the data are multivariate normal, the proposed method is shown to perform significantly better under nonnormality. This article has supplementary material online.},
  keywords = {Data depth,done,Mahalanobis depth,Mean rank chart,Nonparametric,Outliers,Retrospective analysis,Robust estimators},
  annotation = {\_eprint: https://doi.org/10.1080/00401706.2013.879264},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Bell_et_al_2014_A_Distribution-Free_Multivariate_Phase_I_Location_Control_Chart_for_Subgrouped.pdf;/home/dede/Zotero/storage/KGBD6ZRI/00401706.2013.html}
}

@book{ben-tal1987,
  title = {{Lectures on Modern Convex Optimization: Analysis, Algorithms, and Engineering Applications}},
  shorttitle = {{Lectures on Modern Convex Optimization}},
  author = {{Ben-Tal}, Aharon and Nemirovski, Arkadi},
  year = {1987},
  publisher = {{Society for Industrial and Applied Mathematics}},
  address = {{Philadelphia, PA}},
  abstract = {Here is a book devoted to well-structured and thus efficiently solvable convex optimization problems, with emphasis on conic quadratic and semidefinite programming. The authors present the basic theory underlying these problems as well as their numerous applications in engineering, including synthesis of filters, Lyapunov stability analysis, and structural design. The authors also discuss the complexity issues and provide an overview of the basic theory of state-of-the-art polynomial time interior point methods for linear, conic quadratic, and semidefinite programming. The book's focus on well-structured convex problems in conic form allows for unified theoretical and algorithmical treatment of a wide spectrum of important optimization problems arising in applications.},
  isbn = {978-0-89871-491-3},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Ben-Tal_Nemirovski_1987_Lectures_on_Modern_Convex_Optimization.pdf}
}

@article{bengio2012,
  title = {Practical Recommendations for Gradient-Based Training of Deep Architectures},
  author = {Bengio, Yoshua},
  year = {2012},
  month = sep,
  journal = {arXiv:1206.5533 [cs]},
  eprint = {1206.5533},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Learning algorithms related to artificial neural networks and in particular for Deep Learning may seem to involve many bells and whistles, called hyper-parameters. This chapter is meant as a practical guide with recommendations for some of the most commonly used hyper-parameters, in particular in the context of learning algorithms based on back-propagated gradient and gradient-based optimization. It also discusses how to deal with the fact that more interesting results can be obtained when allowing one to adjust many hyper-parameters. Overall, it describes elements of the practice used to successfully and efficiently train and debug large-scale and often deep multi-layer neural networks. It closes with open questions about the training difficulties observed with deeper architectures.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Bengio_2012_Practical_recommendations_for_gradient-based_training_of_deep_architectures.pdf;/home/dede/Zotero/storage/4V34FZK8/1206.html}
}

@article{benjamini1995,
  title = {Controlling the {{False Discovery Rate}}: {{A Practical}} and {{Powerful Approach}} to {{Multiple Testing}}},
  shorttitle = {Controlling the {{False Discovery Rate}}},
  author = {Benjamini, Yoav and Hochberg, Yosef},
  year = {1995},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  volume = {57},
  number = {1},
  pages = {289--300},
  publisher = {{[Royal Statistical Society, Wiley]}},
  issn = {0035-9246},
  abstract = {The common approach to the multiplicity problem calls for controlling the familywise error rate (FWER). This approach, though, has faults, and we point out a few. A different approach to problems of multiple significance testing is presented. It calls for controlling the expected proportion of falsely rejected hypotheses-the false discovery rate. This error rate is equivalent to the FWER when all hypotheses are true but is smaller otherwise. Therefore, in problems where the control of the false discovery rate rather than that of the FWER is desired, there is potential for a gain in power. A simple sequential Bonferroni-type procedure is proved to control the false discovery rate for independent test statistics, and a simulation study shows that the gain in power is substantial. The use of the new procedure and the appropriateness of the criterion are illustrated with examples.},
  keywords = {todo}
}

@article{benjamini2001,
  title = {The Control of the False Discovery Rate in Multiple Testing under Dependency},
  author = {Benjamini, Yoav and Yekutieli, Daniel},
  year = {2001},
  month = aug,
  journal = {The Annals of Statistics},
  volume = {29},
  number = {4},
  pages = {1165--1188},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/aos/1013699998},
  abstract = {Benjamini and Hochberg suggest that the false discovery rate may be the appropriate error rate to control in many applied multiple testing problems. A simple procedure was given there as an FDR controlling procedure for independent test statistics and was shown to be much more powerful than comparable procedures which control the traditional familywise error rate. We prove that this same procedure also controls the false discovery rate when the test statistics have positive regression dependency on each of the test statistics corresponding to the true null hypotheses. This condition for positive dependency is general enough to cover many problems of practical interest, including the comparisons of many treatments with a single control, multivariate normal test statistics with positive correlation matrix and multivariate \$t\$. Furthermore, the test statistics may be discrete, and the tested hypotheses composite without posing special difficulties. For all other forms of dependency, a simple conservative modification of the procedure controls the false discovery rate. Thus the range of problems for which a procedure with proven FDR control can be offered is greatly increased.},
  keywords = {47N30,62G30,62J15,comparisons with control,discrete test statistics,FDR,Hochbergâ€™s procedure,MTP2 densities,Multiple comparisons procedures,multiple endpoints many-to-one comparisons,positive regression dependency,Simesâ€™equality,todo,unidimensional latent variables},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Benjamini_Yekutieli_2001_The_control_of_the_false_discovery_rate_in_multiple_testing_under_dependency.pdf;/home/dede/Zotero/storage/GVTU4JRF/1013699998.html}
}

@article{bennett2010,
  title = {Neural {{Correlates}} of {{Interspecies Perspective Taking}} in the {{Post-Mortem Atlantic Salmon}} : {{An Argument For Proper Multiple Comparisons Correction}}},
  author = {Bennett, C. M. and Baird, A. and Miller, M. and Wolford, G.},
  year = {2010},
  journal = {Journal of Serendipitous and Unexpected Results},
  abstract = {Neural Correlates of Interspecies Perspective Taking in the Post-Mortem Atlantic Salmon: An Argument For Proper Multiple Comparisons Correction Craig M. Bennett 1{${_\ast}$}, Abigail A. Baird 2, Michael B. Miller 1 and George L. Wolford 3 Department of Psychology, University of California at Santa Barbara, Santa Barbara, CA 93106 Department of Psychology, Blodgett Hall, Vassar College, Poughkeepsie, NY 12604 Department of Psychological and Brain Sciences, Moore Hall, Dartmouth College, Hanover, NH 03755},
  langid = {english},
  keywords = {todo},
  file = {/home/dede/Zotero/storage/JVZDTADT/3ace3864cc5ada47b31a74d8fea91edb48bc019d.html}
}

@article{bentley2014,
  title = {Strategic Taboos: Chemical Weapons and {{US}} Foreign Policy},
  shorttitle = {Strategic Taboos},
  author = {Bentley, Michelle},
  year = {2014},
  month = sep,
  journal = {International Affairs},
  volume = {90},
  number = {5},
  pages = {1033--1048},
  issn = {0020-5850},
  doi = {10.1111/1468-2346.12155},
  abstract = {This article examines US President Barack Obama's foreign policy rhetoric on Syria, specifically in relation to the threat of chemical weapons and the prohibitionary taboo surrounding their use. It contends that Obama's rhetorical construction of the taboo is not simply a commitment to the control of these horrific weapons (where such arms have been comprehended as so extensively vile as to preclude their employment), but that this also represents the strategic linguistic exploitation of these normative ideals in order to directly shape policy. By analysing of presidential speeches made during the conflict, it demonstrates that Obama has manipulated pre-existing conceptions of chemical weapons as taboo, and also as forms of weapons of mass destruction, to deliberately construct policy in line with his own political ambitions\textemdash most notably as a way of forcing a multilateral solution to the situation in Syria. This article challenges existing perceptions of the chemical weapons taboo as an inherently normative constraint, arguing that this instead comprises a more agency-driven construct. Static notions of the taboo must be abandoned and subsequently replaced with a framework of understanding that recognizes how the taboo can be used as a deliberate driver of foreign policy.},
  file = {/home/dede/Zotero/storage/HV6L3FRN/2326748.html}
}

@article{berger1999,
  title = {Integrated Likelihood Methods for Eliminating Nuisance Parameters},
  author = {Berger, James O. and Liseo, Brunero and Wolpert, Robert L.},
  year = {1999},
  month = feb,
  journal = {Statistical Science},
  volume = {14},
  number = {1},
  pages = {1--28},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/ss/1009211804},
  abstract = {Elimination of nuisance parameters is a central problem in statistical inference and has been formally studied in virtually all approaches to inference. Perhaps the least studied approach is elimination of nuisance parameters through integration, in the sense that this is viewed as an almost incidental byproduct of Bayesian analysis and is hence not something which is deemed to require separate study. There is, however, considerable value in considering integrated likelihood on its own, especially versions arising from default or noninformative priors. In this paper, we review such common integrated likelihoods and discuss their strengths and weaknesses relative to other methods.},
  keywords = {marginal likelihood,nuisance parameters,profile likelihood,reference priors},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Berger_et_al_1999_Integrated_likelihood_methods_for_eliminating_nuisance_parameters.pdf;/home/dede/Zotero/storage/BXNQR4HR/1009211804.html}
}

@book{berlin2014,
  title = {Personal Impressions},
  author = {Berlin, Isaiah and Hardy, Henry and Annan, Noel and Lee, Hermione},
  year = {2014},
  edition = {Third ed},
  publisher = {{Princeton University Press}},
  isbn = {978-0-691-15770-2},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Berlin_et_al_2014_Personal_impressions.epub}
}

@article{bernton2019,
  title = {On Parameter Estimation with the {{Wasserstein}} Distance},
  author = {Bernton, Espen and Jacob, Pierre E. and Gerber, Mathieu and Robert, Christian P.},
  year = {2019},
  month = may,
  journal = {arXiv:1701.05146 [math, stat]},
  eprint = {1701.05146},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  abstract = {Statistical inference can be performed by minimizing, over the parameter space, the Wasserstein distance between model distributions and the empirical distribution of the data. We study asymptotic properties of such minimum Wasserstein distance estimators, complementing results derived by Bassetti, Bodini and Regazzini in 2006. In particular, our results cover the misspecified setting, in which the data-generating process is not assumed to be part of the family of distributions described by the model. Our results are motivated by recent applications of minimum Wasserstein estimators to complex generative models. We discuss some difficulties arising in the approximation of these estimators and illustrate their behavior in several numerical experiments. Two of our examples are taken from the literature on approximate Bayesian computation and have likelihood functions that are not analytically tractable. Two other examples involve misspecified models.},
  archiveprefix = {arXiv},
  keywords = {generative models,Kantorovich distance,Mathematics - Statistics Theory,minimum distance estimation,optimal transport,parameter inference,Statistics - Computation,Statistics - Methodology,todo,Wasserstein distance},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Bernton_et_al_2019_On_parameter_estimation_with_the_Wasserstein_distance.pdf;/home/dede/Zotero/storage/RNX2SK33/1701.html}
}

@article{bernton2019a,
  title = {Approximate {{Bayesian}} Computation with the {{Wasserstein}} Distance},
  author = {Bernton, Espen and Jacob, Pierre E. and Gerber, Mathieu and Robert, Christian P.},
  year = {2019},
  month = apr,
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {81},
  number = {2},
  eprint = {1905.03747},
  eprinttype = {arxiv},
  pages = {235--269},
  issn = {1369-7412, 1467-9868},
  doi = {10.1111/rssb.12312},
  abstract = {A growing number of generative statistical models do not permit the numerical evaluation of their likelihood functions. Approximate Bayesian computation (ABC) has become a popular approach to overcome this issue, in which one simulates synthetic data sets given parameters and compares summaries of these data sets with the corresponding observed values. We propose to avoid the use of summaries and the ensuing loss of information by instead using the Wasserstein distance between the empirical distributions of the observed and synthetic data. This generalizes the well-known approach of using order statistics within ABC to arbitrary dimensions. We describe how recently developed approximations of the Wasserstein distance allow the method to scale to realistic data sizes, and propose a new distance based on the Hilbert space-filling curve. We provide a theoretical study of the proposed method, describing consistency as the threshold goes to zero while the observations are kept fixed, and concentration properties as the number of observations grows. Various extensions to time series data are discussed. The approach is illustrated on various examples, including univariate and multivariate g-and-k distributions, a toggle switch model from systems biology, a queueing model, and a L\textbackslash 'evy-driven stochastic volatility model.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Bernton_et_al_2019_Approximate_Bayesian_computation_with_the_Wasserstein_distance.pdf;/home/dede/Zotero/storage/CW2X85XJ/1905.html}
}

@book{berry2013,
  title = {{Bandit Problems: Sequential Allocation of Experiments}},
  shorttitle = {{Bandit Problems}},
  author = {Berry, Donald A. and Fristedt, Bert},
  year = {2013},
  month = mar,
  edition = {Reprint edizione},
  publisher = {{Springer Verlag}},
  address = {{Dordrecht}},
  isbn = {978-94-015-3713-1},
  langid = {Inglese}
}

@article{betancourt2018,
  title = {A {{Conceptual Introduction}} to {{Hamiltonian Monte Carlo}}},
  author = {Betancourt, Michael},
  year = {2018},
  month = jul,
  journal = {arXiv:1701.02434 [stat]},
  eprint = {1701.02434},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Hamiltonian Monte Carlo has proven a remarkable empirical success, but only recently have we begun to develop a rigorous understanding of why it performs so well on difficult problems and how it is best applied in practice. Unfortunately, that understanding is confined within the mathematics of differential geometry which has limited its dissemination, especially to the applied communities for which it is particularly important. In this review I provide a comprehensive conceptual account of these theoretical foundations, focusing on developing a principled intuition behind the method and its optimal implementations rather of any exhaustive rigor. Whether a practitioner or a statistician, the dedicated reader will acquire a solid grasp of how Hamiltonian Monte Carlo works, when it succeeds, and, perhaps most importantly, when it fails.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Betancourt_2018_A_Conceptual_Introduction_to_Hamiltonian_Monte_Carlo.pdf;/home/dede/Zotero/storage/7NJ9SCTM/1701.html}
}

@article{betancourt2020,
  title = {Random {{Partition Models}} for {{Microclustering Tasks}}},
  author = {Betancourt, Brenda and Zanella, Giacomo and Steorts, Rebecca C.},
  year = {2020},
  month = oct,
  journal = {Journal of the American Statistical Association},
  volume = {0},
  number = {0},
  pages = {1--13},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.2020.1841647},
  abstract = {Traditional Bayesian random partition models assume that the size of each cluster grows linearly with the number of data points. While this is appealing for some applications, this assumption is not appropriate for other tasks such as entity resolution (ER), modeling of sparse networks, and DNA sequencing tasks. Such applications require models that yield clusters whose sizes grow sublinearly with the total number of data points\textemdash the microclustering property. Motivated by these issues, we propose a general class of random partition models that satisfy the microclustering property with well-characterized theoretical properties. Our proposed models overcome major limitations in the existing literature on microclustering models, namely a lack of interpretability, identifiability, and full characterization of model asymptotic properties. Crucially, we drop the classical assumption of having an exchangeable sequence of data points, and instead assume an exchangeable sequence of clusters. In addition, our framework provides flexibility in terms of the prior distribution of cluster sizes, computational tractability, and applicability to a large number of microclustering tasks. We establish theoretical properties of the resulting class of priors, where we characterize the asymptotic behavior of the number of clusters and of the proportion of clusters of a given size. Our framework allows a simple and efficient Markov chain Monte Carlo algorithm to perform statistical inference. We illustrate our proposed methodology on the microclustering task of ER, where we provide a simulation study and real experiments on survey panel data.},
  keywords = {Bayesian random partition models,Entity resolution,Exchangeable partition probability function,Gibbs partitions,Microclustering,Record linkage,todo},
  annotation = {\_eprint: https://doi.org/10.1080/01621459.2020.1841647},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Betancourt_et_al_2020_Random_Partition_Models_for_Microclustering_Tasks.pdf;/home/dede/Zotero/storage/FVHHHG3I/01621459.2020.html}
}

@article{bezdek1984,
  title = {{{FCM}}\textemdash the {{Fuzzy C-Means}} Clustering-Algorithm},
  author = {Bezdek, James and Ehrlich, Robert and Full, William},
  year = {1984},
  journal = {Computers \& Geosciences},
  volume = {10},
  pages = {191--203},
  abstract = {This paper transmits a FORTRAN-IV coding of the fuzzy c-means (FCM) clustering program. The FCM program is applicable to a wide variety of geostatistical data analysis problems. This program generates fuzzy partitions and prototypes for any set of numerical data. These partitions are useful for corroborating known substructures or suggesting substructure in unexplored data. The clustering criterion used to aggregate subsets is a generalized least-squares objective function. Features of this program include a choice of three norms (Euclidean, Diagonal, or Mahalonobis), an adjustable weighting factor that essentially controls sensitivity to noise, acceptance of variable numbers of clusters, and outputs that include several measures of cluster validity.},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Bezdek_et_al_1984_FCMâ€”the_Fuzzy_C-Means_clustering-algorithm.pdf}
}

@article{bhattacharjee2020,
  title = {Change {{Point Estimation}} in a {{Dynamic Stochastic Block Model}}},
  author = {Bhattacharjee, Monika and Banerjee, Moulinath and Michailidis, George},
  year = {2020},
  journal = {Journal of Machine Learning Research},
  volume = {21},
  number = {107},
  pages = {1--59},
  issn = {1533-7928},
  abstract = {We consider the problem of estimating the location of a single change point in a network generated by a dynamic stochastic block model mechanism. This model produces community structure in the network that exhibits change at a single time epoch. We propose two methods of estimating the change point, together with the model parameters, before and after its occurrence. The first employs a least-squares criterion function and takes into consideration the full structure of the stochastic block model and is evaluated at each point in time. Hence, as an intermediate step, it requires estimating the community structure based on a clustering algorithm at every time point. The second method comprises the following two steps: in the first one, a least-squares function is used and evaluated at each time point, but ignoring the community structure and only considering a random graph generating mechanism exhibiting a change point. Once the change point is identified, in the second step, all network data before and after it are used together with a clustering algorithm to obtain the corresponding community structures and subsequently estimate the generating stochastic block model parameters. The first method, since it requires knowledge of the community structure and hence clustering at every point in time, is significantly more computationally expensive than the second one. On the other hand, it requires a significantly less stringent identifiability condition for consistent estimation of the change point and the model parameters than the second method; however, it also requires a condition on the misclassification rate of misallocating network nodes to their respective communities that may fail to hold in many realistic settings. Despite the apparent stringency of the identifiability condition for the second method, we show that networks generated by a stochastic block mechanism exhibiting a change in their structure can easily satisfy this condition under a multitude of scenarios, including merging/splitting communities, nodes joining another community, etc. Further, for both methods under their respective identifiability and certain additional regularity conditions, we establish rates of convergence and derive the asymptotic distributions of the change point estimators. The results are illustrated on synthetic data. In summary, this work provides an in-depth investigation of the novel problem of change point analysis for networks generated by stochastic block models, identifies key conditions for the consistent estimation of the change point, and proposes a computationally fast algorithm that solves the problem in many settings that occur in applications. Finally, it discusses challenges posed by employing clustering algorithms in this problem, that require additional investigation for their full resolution.},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Bhattacharjee_et_al_2020_Change_Point_Estimation_in_a_Dynamic_Stochastic_Block_Model.pdf}
}

@article{bhattacharya1981,
  title = {A {{Nonparametric Control Chart}} for {{Detecting Small Disorders}}},
  author = {Bhattacharya, P. K. and Jr, Dargan Frierson},
  year = {1981},
  month = may,
  journal = {The Annals of Statistics},
  volume = {9},
  number = {3},
  pages = {544--554},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/aos/1176345458},
  abstract = {We consider sequential observation of independent random variables \$X\_1,\textbackslash cdots, X\_N\$ whose distribution changes from \$F\$ to \$G\$ after the first \$\textbackslash lbrack N\textbackslash theta \textbackslash rbrack\$ variables. The object is to detect the unknown change-point quickly without too many false alarms. A nonparametric control chart based on partial weighted sums of sequential ranks is proposed. It is shown that if the change from \$F\$ to \$G\$ is small, then as \$N \textbackslash rightarrow \textbackslash infty\$, the appropriately scaled and linearly interpolated graph of partial rank sums converges to a Brownian motion on which a drift sets in at time \$\textbackslash theta\$. Using this, the asymptotic performance of the one-sided control chart is compared with one based on partial sums of the \$X\$'s. Location change, scale change and contamination are considered. It is found that for distributions with heavy tails, the control chart based on ranks stops more frequently and faster than the one based on the \$X\$'s. Performance of the two procedures are also tested on simulated data and the outcomes are compatible with the theoretical results.},
  keywords = {62E20,62G99,62L10,62N10,Brownian motion,Nonparametric control chart,sequential rank,weak convergence},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Bhattacharya_Jr_1981_A_Nonparametric_Control_Chart_for_Detecting_Small_Disorders.pdf;/home/dede/Zotero/storage/IZDYABP8/1176345458.html}
}

@article{bhattacharya2011,
  title = {Sparse {{Bayesian}} Infinite Factor Models},
  author = {Bhattacharya, Anirban and Dunson, David},
  year = {2011},
  month = jan,
  journal = {Biometrika},
  volume = {98},
  pages = {291--306},
  doi = {10.2307/23076151},
  abstract = {We focus on sparse modelling of high-dimensional covariance matrices using Bayesian latent factor models. We propose a multiplicative gamma process shrinkage prior on the factor loadings which allows introduction of infinitely many factors, with the loadings increasingly shrunk towards zero as the column index increases. We use our prior on a parameter-expanded loading matrix to avoid the order dependence typical in factor analysis models and develop an efficient Gibbs sampler that scales well as data dimensionality increases. The gain in efficiency is achieved by the joint conjugacy property of the proposed prior, which allows block updating of the loadings matrix. We propose an adaptive Gibbs sampler for automatically truncating the infinite loading matrix through selection of the number of important factors. Theoretical results are provided on the support of the prior and truncation approximation bounds. A fast algorithm is proposed to produce approximate Bayes estimates. Latent factor regression methods are developed for prediction and variable selection in applications with high-dimensional correlated predictors. Operating characteristics are assessed through simulation studies, and the approach is applied to predict survival times from gene expression data. Copyright 2011, Oxford University Press.},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Bhattacharya_Dunson_2011_Sparse_Bayesian_infinite_factor_models.pdf}
}

@article{bhattacharya2015,
  title = {Dirichlet\textendash{{Laplace Priors}} for {{Optimal Shrinkage}}},
  author = {Bhattacharya, Anirban and Pati, Debdeep and Pillai, Natesh S. and Dunson, David B.},
  year = {2015},
  month = oct,
  journal = {Journal of the American Statistical Association},
  volume = {110},
  number = {512},
  pages = {1479--1490},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.2014.960967},
  abstract = {Penalized regression methods, such as L1 regularization, are routinely used in high-dimensional applications, and there is a rich literature on optimality properties under sparsity assumptions. In the Bayesian paradigm, sparsity is routinely induced through two-component mixture priors having a probability mass at zero, but such priors encounter daunting computational problems in high dimensions. This has motivated continuous shrinkage priors, which can be expressed as global-local scale mixtures of Gaussians, facilitating computation. In contrast to the frequentist literature, little is known about the properties of such priors and the convergence and concentration of the corresponding posterior distribution. In this article, we propose a new class of Dirichlet\textendash Laplace priors, which possess optimal posterior concentration and lead to efficient posterior computation. Finite sample performance of Dirichlet\textendash Laplace priors relative to alternatives is assessed in simulated and real data examples.},
  pmid = {27019543},
  keywords = {Bayesian,Convergence rate,High-dimensional,L1,Lasso,Penalized regression,Regularization,Shrinkage prior.},
  annotation = {\_eprint: https://doi.org/10.1080/01621459.2014.960967},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Bhattacharya_et_al_2015_Dirichletâ€“Laplace_Priors_for_Optimal_Shrinkage.pdf;/home/dede/Zotero/storage/Z7Y486XP/01621459.2014.html}
}

@article{bhuyan2014,
  title = {Network {{Anomaly Detection}}: {{Methods}}, {{Systems}} and {{Tools}}},
  shorttitle = {Network {{Anomaly Detection}}},
  author = {Bhuyan, M. H. and Bhattacharyya, D. and Kalita, J.},
  year = {2014},
  journal = {IEEE Communications Surveys \& Tutorials},
  doi = {10.1109/SURV.2013.052213.00046},
  abstract = {This paper provides a structured and comprehensive overview of various facets of network anomaly detection so that a researcher can become quickly familiar with every aspect of network anomalies detection. Network anomaly detection is an important and dynamic research area. Many network intrusion detection methods and systems (NIDS) have been proposed in the literature. In this paper, we provide a structured and comprehensive overview of various facets of network anomaly detection so that a researcher can become quickly familiar with every aspect of network anomaly detection. We present attacks normally encountered by network intrusion detection systems. We categorize existing network anomaly detection methods and systems based on the underlying computational techniques used. Within this framework, we briefly describe and compare a large number of network anomaly detection methods and systems. In addition, we also discuss tools that can be used by network defenders and datasets that researchers in network anomaly detection can use. We also highlight research directions in network anomaly detection.},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Bhuyan_et_al_2014_Network_Anomaly_Detection.pdf}
}

@book{bickel2015,
  title = {{Mathematical Statistics: Basic Ideas and Selected Topics}},
  shorttitle = {{Mathematical Statistics}},
  author = {Bickel, Peter J. and Doksum, Kjell A.},
  year = {2015},
  month = apr,
  edition = {2nd ed},
  volume = {1},
  publisher = {{Chapman and Hall/CRC}},
  address = {{Boca Raton}},
  abstract = {Mathematical Statistics: Basic Ideas and Selected Topics, Volume I, Second Edition presents fundamental, classical statistical concepts at the doctorate level. It covers estimation, prediction, testing, confidence sets, Bayesian analysis, and the general approach of decision theory. This edition gives careful proofs of major results and explains how the theory sheds light on the properties of practical methods.   The book first discusses non- and semiparametric models before covering parameters and parametric models. It then offers a detailed treatment of maximum likelihood estimates (MLEs) and examines the theory of testing and confidence regions, including optimality theory for estimation and elementary robustness considerations. It next presents basic asymptotic approximations with one-dimensional parameter models as examples. The book also describes inference in multivariate (multiparameter) models, exploring asymptotic normality and optimality of MLEs, Wald and Rao statistics, generalized linear models, and more.  Mathematical Statistics: Basic Ideas and Selected Topics, Volume II will be published in 2015. It will present important statistical concepts, methods, and tools not covered in Volume I.},
  isbn = {978-1-4987-2380-0},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Bickel_Doksum_2015_Mathematical_Statistics.pdf}
}

@book{billingsley2012,
  title = {Probability and {{Measure}}},
  author = {Billingsley, Patrick},
  year = {2012},
  month = feb,
  edition = {Anniversary edition},
  publisher = {{Wiley}},
  address = {{Hoboken, N.J}},
  abstract = {Praise for the Third Edition "It is, as far as I'm concerned, among the best books in math ever written....if you are a mathematician and want to have the top reference in probability, this is it." (Amazon.com, January 2006) A complete and comprehensive classic in probability and measure theory Probability and Measure, Anniversary Edition by Patrick Billingsley celebrates the achievements and advancements that have made this book a classic in its field for the past 35 years. Now re-issued in a new style and format, but with the reliable content that the third edition was revered for, this Anniversary Edition builds on its strong foundation of measure theory and probability with Billingsley's unique writing style. In recognition of 35 years of publication, impacting tens of thousands of readers, this Anniversary Edition has been completely redesigned in a new, open and user-friendly way in order to appeal to university-level students. This book adds a new foreward by Steve Lally of the Statistics Department at The University of Chicago in order to underscore the many years of successful publication and world-wide popularity and emphasize the educational value of this book. The Anniversary Edition contains features including:   An improved treatment of Brownian motion   Replacement of queuing theory with ergodic theory   Theory and applications used to illustrate real-life situations   Over 300 problems with corresponding, intensive notes and solutions   Updated bibliography   An extensive supplement of additional notes on the problems and chapter commentaries   Patrick Billingsley was a first-class, world-renowned authority in probability and measure theory at a leading U.S. institution of higher education. He continued to be an influential probability theorist until his unfortunate death in 2011. Billingsley earned his Bachelor's Degree in Engineering from the U.S. Naval Academy where he served as an officer. he went on to receive his Master's Degree and doctorate in Mathematics from Princeton University.Among his many professional awards was the Mathematical Association of America's Lester R. Ford Award for mathematical exposition. His achievements through his long and esteemed career have solidified Patrick Billingsley's place as a leading authority in the field and been a large reason for his books being regarded as classics. This Anniversary Edition of Probability and Measure offers advanced students, scientists, and engineers an integrated introduction to measure theory and probability. Like the previous editions, this Anniversary Edition is a key resource for students of mathematics, statistics, economics, and a wide variety of disciplines that require a solid understanding of probability theory.},
  isbn = {978-1-118-12237-2},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Billingsley_2012_Probability_and_Measure.pdf}
}

@article{binette2020,
  title = {({{Almost}}) {{All}} of {{Entity Resolution}}},
  author = {Binette, Olivier and Steorts, Rebecca C.},
  year = {2020},
  month = aug,
  journal = {arXiv:2008.04443 [cs, stat]},
  eprint = {2008.04443},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Whether the goal is to estimate the number of people that live in a congressional district, to estimate the number of individuals that have died in an armed conflict, or to disambiguate individual authors using bibliographic data, all these applications have a common theme - integrating information from multiple sources. Before such questions can be answered, databases must be cleaned and integrated in a systematic and accurate way, commonly known as record linkage, de-duplication, or entity resolution. In this article, we review motivational applications and seminal papers that have led to the growth of this area. Specifically, we review the foundational work that began in the 1940's and 50's that have led to modern probabilistic record linkage. We review clustering approaches to entity resolution, semi- and fully supervised methods, and canonicalization, which are being used throughout industry and academia in applications such as human rights, official statistics, medicine, citation networks, among others. Finally, we discuss current research topics of practical importance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Databases,Statistics - Machine Learning,Statistics - Methodology,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Binette_Steorts_2020_(Almost)_All_of_Entity_Resolution.pdf;/home/dede/Zotero/storage/37TJRDW6/2008.html}
}

@article{bisbee2019,
  title = {{{BARP}}: {{Improving Mister P Using Bayesian Additive Regression Trees}}},
  shorttitle = {{{BARP}}},
  author = {Bisbee, James},
  year = {2019},
  month = nov,
  journal = {American Political Science Review},
  volume = {113},
  number = {4},
  pages = {1060--1065},
  publisher = {{Cambridge University Press}},
  issn = {0003-0554, 1537-5943},
  doi = {10.1017/S0003055419000480},
  abstract = {Multilevel regression and post-stratification (MRP) is the current gold standard for extrapolating opinion data from nationally representative surveys to smaller geographic units. However, innovations in nonparametric regularization methods can further improve the researcher's ability to extrapolate opinion data to a geographic unit of interest. I test an ensemble of regularization algorithms and find that there is room for substantial improvement on the multilevel model via more flexible methods of regularization. I propose a modified version of MRP that replaces the multilevel model with a nonparametric approach called Bayesian additive regression trees (BART or, when combined with post-stratification, BARP). I compare both methods across a number of data contexts, demonstrating the benefits of applying more powerful regularization methods to extrapolate opinion data to target geographical units. I provide an R package that implements the BARP method.},
  langid = {english},
  keywords = {todo}
}

@book{bishop2006,
  title = {{Pattern Recognition And Machine Learning}},
  author = {Bishop, Christopher M.},
  year = {2006},
  month = aug,
  publisher = {{Springer Nature}},
  address = {{New York}},
  abstract = {This is the first textbook on pattern recognition to present the Bayesian viewpoint. The book presents approximate inference algorithms that permit fast approximate answers in situations where exact answers are not feasible. It uses graphical models to describe probability distributions when no other books apply graphical models to machine learning. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory.},
  isbn = {978-0-387-31073-2},
  langid = {Inglese}
}

@article{blackwell1973,
  title = {Ferguson {{Distributions Via P\'olya Urn Schemes}}},
  author = {Blackwell, David and MacQueen, James B.},
  year = {1973},
  month = mar,
  journal = {The Annals of Statistics},
  volume = {1},
  number = {2},
  pages = {353--355},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/aos/1176342372},
  abstract = {The Polya urn scheme is extended by allowing a continuum of colors. For the extended scheme, the distribution of colors after \$n\$ draws is shown to converge as \$n \textbackslash rightarrow \textbackslash infty\$ to a limiting discrete distribution \$\textbackslash mu\^\textbackslash ast\$. The distribution of \$\textbackslash mu\^\textbackslash ast\$ is shown to be one introduced by Ferguson and, given \$\textbackslash mu\^\textbackslash ast\$, the colors drawn from the urn are shown to be independent with distribution \$\textbackslash mu\^\textbackslash ast\$.},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Blackwell_MacQueen_1973_Ferguson_Distributions_Via_Polya_Urn_Schemes.pdf;/home/dede/Zotero/storage/X7ISWSJN/1176342372.html}
}

@article{blazquez-garcia2020,
  title = {A Review on Outlier/Anomaly Detection in Time Series Data},
  author = {{Bl{\'a}zquez-Garc{\'i}a}, Ane and Conde, Angel and Mori, Usue and Lozano, Jose A.},
  year = {2020},
  month = feb,
  journal = {arXiv:2002.04236 [cs, stat]},
  eprint = {2002.04236},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Recent advances in technology have brought major breakthroughs in data collection, enabling a large amount of data to be gathered over time and thus generating time series. Mining this data has become an important task for researchers and practitioners in the past few years, including the detection of outliers or anomalies that may represent errors or events of interest. This review aims to provide a structured and comprehensive state-of-the-art on outlier detection techniques in the context of time series. To this end, a taxonomy is presented based on the main aspects that characterize an outlier detection technique.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/BlÃ¡zquez-GarcÃ­a_et_al_2020_A_review_on_outlier-anomaly_detection_in_time_series_data.pdf;/home/dede/Zotero/storage/Z4TUM2EY/2002.html}
}

@article{blei2011,
  title = {Distance {{Dependent Chinese Restaurant Processes}}},
  author = {Blei, David M. and Frazier, Peter I.},
  year = {2011},
  journal = {Journal of Machine Learning Research},
  volume = {12},
  number = {74},
  pages = {2461--2488},
  issn = {1533-7928},
  keywords = {todo},
  file = {/home/dede/Zotero/storage/NEHJYDCC/Blei and Frazier - 2011 - Distance Dependent Chinese Restaurant Processes.pdf;/home/dede/Zotero/storage/FJIEJHR2/blei11a.html}
}

@article{blei2017,
  title = {Variational {{Inference}}: {{A Review}} for {{Statisticians}}},
  shorttitle = {Variational {{Inference}}},
  author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
  year = {2017},
  month = apr,
  journal = {Journal of the American Statistical Association},
  volume = {112},
  number = {518},
  eprint = {1601.00670},
  eprinttype = {arxiv},
  pages = {859--877},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.2017.1285773},
  abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this paper, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this class of algorithms.},
  archiveprefix = {arXiv},
  keywords = {Algorithms; Statistical Computing; Computationally Intensive Methods,Computer Science - Machine Learning,done,Statistics - Computation,Statistics - Machine Learning,variational inference},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Blei_et_al_2017_Variational_Inference.pdf;/home/dede/Zotero/storage/Y3IQZPKI/1601.html}
}

@article{blum1954,
  title = {Multidimensional {{Stochastic Approximation Methods}}},
  author = {Blum, Julius R.},
  year = {1954},
  month = dec,
  journal = {The Annals of Mathematical Statistics},
  volume = {25},
  number = {4},
  pages = {737--744},
  issn = {0003-4851},
  doi = {10.1214/aoms/1177728659},
  langid = {english},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Blum_1954_Multidimensional_Stochastic_Approximation_Methods.pdf}
}

@book{bobrowski2005,
  title = {Functional {{Analysis}} for {{Probability}} and {{Stochastic Processes}}: {{An Introduction}}},
  shorttitle = {Functional {{Analysis}} for {{Probability}} and {{Stochastic Processes}}},
  author = {Bobrowski, Adam},
  year = {2005},
  month = sep,
  edition = {1 edition},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge, UK ; New York}},
  abstract = {Designed for students of probability and stochastic processes, as well as for students of functional analysis, specifically, this volume presents some chosen parts of functional analysis that can help clarify probability and stochastic processes. The subjects range from basic Hilbert and Banach spaces, through weak topologies and Banach algebras, to the theory of semigroups of bounded linear operators. Numerous standard and non-standard examples and exercises make the book suitable as a course textbook or for self-study.},
  isbn = {978-0-521-53937-1},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Bobrowski_2005_Functional_Analysis_for_Probability_and_Stochastic_Processes.pdf}
}

@techreport{bodoia,
  title = {{{MapReduce Algorithms}} for K-Means {{Clustering}}},
  author = {Bodoia, Max},
  pages = {11},
  langid = {english},
  keywords = {done},
  file = {/home/dede/Zotero/storage/8ZYHJTVD/Bodoia - MapReduce Algorithms for k-means Clustering.pdf}
}

@book{boettke1994,
  title = {Collapse of {{Development Planning}}},
  author = {Boettke, Peter J.},
  year = {1994},
  month = jul,
  publisher = {{NYU Press}},
  address = {{New York}},
  isbn = {978-0-8147-1225-2},
  langid = {english}
}

@article{boonchong2007,
  title = {Proposed {{Short Runs Multivariate Control Charts}} for the {{Process Mean}}},
  author = {Boon Chong, Michael and Quah, S.},
  year = {2007},
  month = feb,
  journal = {Quality Engineering},
  volume = {14(4)},
  pages = {603--621},
  doi = {10.1081/QEN-120003562},
  abstract = {In recent years, there is a trend in manufacturing industries to produce smaller lot sizes or low volume production. This trend is due to increased emphasis on just-in-time techniques (JIT), synchronous manufacturing, job-shop settings, the reduction of in-process inventory, and costs. The term used to describe such low volume production is ``Short Runs Production'' or more commonly ``Short Runs.'' In such an environment, the run of a process is short, usually fewer than 50. Therefore, a principal practical problem is the need to chart a large number of different processes and the consequent large number of charts required. This article discusses proposed multivariate control charts for short runs based on individual measurements and subgrouped data.}
}

@book{boothby2002,
  title = {An {{Introduction}} to {{Differentiable Manifolds}} and {{Riemannian Geometry}}, {{Revised}}},
  author = {Boothby, William M.},
  year = {2002},
  month = aug,
  edition = {2 edition},
  publisher = {{Academic Press}},
  address = {{Amsterdam ; New York}},
  abstract = {The second edition of An Introduction to Differentiable Manifolds and Riemannian Geometry, Revised has sold over 6,000 copies since publication in 1986 and this revision will make it even more useful. This is the only book available that is approachable by "beginners" in this subject. It has become an essential introduction to the subject for mathematics students, engineers, physicists, and economists who need to learn how to apply these vital methods. It is also the only book that thoroughly reviews certain areas of advanced calculus that are necessary to understand the subject.},
  isbn = {978-0-12-116051-7},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Boothby_2002_An_Introduction_to_Differentiable_Manifolds_and_Riemannian_Geometry,_Revised.pdf}
}

@article{botev2017,
  title = {The {{Normal Law Under Linear Restrictions}}: {{Simulation}} and {{Estimation}} via {{Minimax Tilting}}},
  shorttitle = {The {{Normal Law Under Linear Restrictions}}},
  author = {Botev, Z. I.},
  year = {2017},
  month = jan,
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {79},
  number = {1},
  eprint = {1603.04166},
  eprinttype = {arxiv},
  pages = {125--148},
  issn = {13697412},
  doi = {10.1111/rssb.12162},
  abstract = {Simulation from the truncated multivariate normal distribution in high dimensions is a recurrent problem in statistical computing, and is typically only feasible using approximate MCMC sampling. In this article we propose a minimax tilting method for exact iid simulation from the truncated multivariate normal distribution. The new methodology provides both a method for simulation and an efficient estimator to hitherto intractable Gaussian integrals. We prove that the estimator possesses a rare vanishing relative error asymptotic property. Numerical experiments suggest that the proposed scheme is accurate in a wide range of setups for which competing estimation schemes fail. We give an application to exact iid simulation from the Bayesian posterior of the probit regression model.},
  archiveprefix = {arXiv},
  keywords = {65C05; 68W20,done,Statistics - Computation},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Botev_2017_The_Normal_Law_Under_Linear_Restrictions.pdf;/home/dede/Zotero/storage/246L5GV7/1603.html}
}

@misc{botev2020,
  title = {Truncated {{Multivariate Normal}} and {{Student Distributions}}},
  author = {Botev, Z. I. and Belzile, Leo},
  year = {2020},
  abstract = {A collection of functions to deal with the truncated univariate and multivariate normal and Student distributions, described in Botev (2017) {$<$}doi:10.1111/rssb.12162{$>$} and Botev and L'Ecuyer (2015) {$<$}doi:10.1109/WSC.2015.7408180{$>$}.},
  annotation = {R package version 2.2.1},
  file = {/home/dede/Zotero/storage/GD4WAFKW/TruncatedNormal.html}
}

@incollection{boucheron2004,
  title = {Concentration {{Inequalities}}},
  booktitle = {Advanced {{Lectures}} on {{Machine Learning}}: {{ML Summer Schools}} 2003, {{Canberra}}, {{Australia}}, {{February}} 2 - 14, 2003, {{T\"ubingen}}, {{Germany}}, {{August}} 4 - 16, 2003, {{Revised Lectures}}},
  author = {Boucheron, St{\'e}phane and Lugosi, G{\'a}bor and Bousquet, Olivier},
  editor = {Bousquet, Olivier and {von Luxburg}, Ulrike and R{\"a}tsch, Gunnar},
  year = {2004},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {208--240},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-28650-9_9},
  abstract = {Concentration inequalities deal with deviations of functions of independent random variables from their expectation. In the last decade new tools have been introduced making it possible to establish simple and powerful inequalities. These inequalities are at the heart of the mathematical analysis of various problems in machine learning and made it possible to derive new efficient algorithms. This text attempts to summarize some of the basic tools.},
  isbn = {978-3-540-28650-9},
  langid = {english},
  keywords = {Conditional Entropy,Empirical Process,Independent Random Variable,Moment Generate Function,Relative Entropy,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Boucheron_et_al_2004_Concentration_Inequalities.pdf}
}

@article{bourazas2021,
  title = {Predictive {{Control Charts}} ({{PCC}}): {{A Bayesian}} Approach in Online Monitoring of Short Runs},
  shorttitle = {Predictive {{Control Charts}} ({{PCC}})},
  author = {Bourazas, Konstantinos and Kiagias, Dimitrios and Tsiamyrtzis, Panagiotis},
  year = {2021},
  month = may,
  journal = {Journal of Quality Technology},
  volume = {0},
  number = {0},
  pages = {1--35},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2021.1916413},
  abstract = {Performing online monitoring for short horizon data is a challenging, though cost effective benefit. Self-starting methods attempt to address this issue adopting a hybrid scheme that executes calibration and monitoring simultaneously. In this work, we propose a Bayesian alternative that will utilize prior information and possible historical data (via power priors), offering a head-start in online monitoring, putting emphasis on outlier detection. For cases of complete prior ignorance, the objective Bayesian version will be provided. Charting will be based on the predictive distribution and the methodological framework will be derived in a general way, to facilitate discrete and continuous data from any distribution that belongs to the regular exponential family (with Normal, Poisson and Binomial being the most representative). Being in the Bayesian arena, we will be able to not only perform process monitoring, but also draw online inference regarding the unknown process parameter(s). An extended simulation study will evaluate the proposed methodology against frequentist based competitors and it will cover topics regarding prior sensitivity and model misspecification robustness. A continuous and a discrete real data set will illustrate its use in practice. Technical details, algorithms, guidelines on prior elicitation and R-codes are provided in appendices and supplementary material. Short production runs and online phase I monitoring are among the best candidates to benefit from the developed methodology.},
  keywords = {done,Online phase I monitoring,outlier detection,regular exponential family,self-starting,statistical process control and monitoring},
  annotation = {\_eprint: https://doi.org/10.1080/00224065.2021.1916413},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Bourazas_et_al_2021_Predictive_Control_Charts_(PCC).pdf;/home/dede/Zotero/storage/UGHKEMBQ/00224065.2021.html}
}

@phdthesis{bourazas2021a,
  title = {Self-Starting {{Methods}} in {{Bayesian Statistical Process Control}} \& {{Monitoring}}},
  author = {Bourazas, Konstantinos},
  year = {2021},
  school = {Athens University},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Bourazas_2021_Self-starting_Methods_in_Bayesian_Statistical_Process_Control_&_Monitoring.pdf}
}

@article{box1951,
  title = {On the {{Experimental Attainment}} of {{Optimum Conditions}}},
  author = {Box, G. E. P. and Wilson, K. B.},
  year = {1951},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  volume = {13},
  number = {1},
  pages = {1--45},
  publisher = {{[Royal Statistical Society, Wiley]}},
  issn = {0035-9246},
  keywords = {todo}
}

@book{box2005,
  title = {Statistics for {{Experimenters}}: {{Design}}, {{Innovation}}, and {{Discovery}}},
  shorttitle = {Statistics for {{Experimenters}}},
  author = {Box, George E. P. and Hunter, J. Stuart and Hunter, William G.},
  year = {2005},
  month = may,
  edition = {2nd edition},
  publisher = {{Wiley-Interscience}},
  address = {{Hoboken, N.J}},
  isbn = {978-0-471-71813-0},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Box_et_al_2005_Statistics_for_Experimenters.pdf}
}

@book{boyd2004,
  title = {Convex {{Optimization}}},
  author = {Boyd, Stephen and Vandenberghe, Lieven},
  year = {2004},
  edition = {1 edition},
  publisher = {{Cambridge University Press}},
  abstract = {Convex optimization problems arise frequently in many different fields. This book provides a comprehensive introduction to the subject, and shows in detail how such problems can be solved numerically with great efficiency. The book begins with the basic elements of convex sets and functions, and then describes various classes of convex optimization problems. Duality and approximation techniques are then covered, as are statistical estimation techniques. Various geometrical problems are then presented, and there is detailed discussion of unconstrained and constrained minimization problems, and interior-point methods. The focus of the book is on recognizing convex optimization problems and then finding the most appropriate technique for solving them. It contains many worked examples and homework exercises and will appeal to students, researchers and practitioners in fields such as engineering, computer science, mathematics, statistics, finance and economics.},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Boyd_Vandenberghe_2004_Convex_Optimization.pdf}
}

@article{boyles2000,
  title = {Phase {{I Analysis}} for {{Autocorrelated Processes}}},
  author = {Boyles, Russell A.},
  year = {2000},
  month = oct,
  journal = {Journal of Quality Technology},
  volume = {32},
  number = {4},
  pages = {395--409},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2000.11980025},
  abstract = {There has been recent interest in statistical process control for autocorrelated processes. Previous researchers have not distinguished models of autocorrelated common-cause variation from the actual behavior of baseline data. Standard estimators of common-cause parameters can be severely biased when assignable causes are present. A new estimation method given here overcomes this difficulty for the first-order autoregressive common-cause model. The method is illustrated with real data sets and assessed via simulation.},
  keywords = {Autocorrelation,Autoregressive Moving Average Model,Common-Cause Variation,Level Shifts,Outliers,Retrospective Analysis,Structural Model,Time Series Analysis,Trends},
  annotation = {\_eprint: https://doi.org/10.1080/00224065.2000.11980025},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Boyles_2000_Phase_I_Analysis_for_Autocorrelated_Processes.pdf;/home/dede/Zotero/storage/8NW83T8X/00224065.2000.html}
}

@book{breiman1984,
  title = {{Classification and Regression Trees}},
  author = {Breiman, Leo and Friedman, Jerome and Olshen, Richard Avery and Stone, Charles John},
  year = {1984},
  month = jan,
  edition = {1 edizione},
  publisher = {{Chapman and Hall/CRC}},
  address = {{Boca Raton}},
  abstract = {The methodology used to construct tree structured rules is the focus of this monograph. Unlike many other statistical procedures, which moved from pencil and paper to calculators, this text's use of trees was unthinkable before computers. Both the practical and theoretical sides have been developed in the authors' study of tree methods. Classification and Regression Trees reflects these two sides, covering the use of trees as a data analysis method, and in a more mathematical framework, proving some of their fundamental properties.},
  isbn = {978-0-412-04841-8},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Breiman_et_al_1984_Classification_and_Regression_Trees.pdf}
}

@article{breiman1996,
  title = {Bagging Predictors},
  author = {Breiman, Leo},
  year = {1996},
  month = aug,
  journal = {Machine Learning},
  volume = {24},
  number = {2},
  pages = {123--140},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/BF00058655},
  langid = {english},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Breiman_1996_Bagging_predictors.pdf}
}

@article{breiman2001,
  title = {Random {{Forests}}},
  author = {Breiman, Leo},
  year = {2001},
  journal = {Machine Learning},
  volume = {45},
  number = {1},
  pages = {5--32},
  issn = {08856125},
  doi = {10.1023/A:1010933404324},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Breiman_2001_Random_Forests.pdf}
}

@article{breiman2001a,
  title = {Statistical {{Modeling}}: {{The Two Cultures}}},
  shorttitle = {Statistical {{Modeling}}},
  author = {Breiman, Leo},
  year = {2001},
  journal = {Statistical Science},
  volume = {16},
  number = {3},
  pages = {199--231},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/ss/1009213726},
  abstract = {There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown. The statistical community has been committed to the almost exclusive use of data models. This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems. Algorithmic modeling, both in theory and practice, has developed rapidly in fields outside statistics. It can be used both on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. If our goal as a field is to use data to solve problems, then we need to move away from exclusive dependence on data models and adopt a more diverse set of tools.},
  langid = {english},
  mrnumber = {MR1874152},
  zmnumber = {1059.62505},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Breiman_2001_Statistical_Modeling.pdf;/home/dede/Zotero/storage/JQ9XCF65/1009213726.html}
}

@book{bremaud2014,
  title = {{Fourier Analysis and Stochastic Processes}},
  author = {Br{\'e}maud, Pierre},
  year = {2014},
  edition = {2014\textdegree{} edizione},
  publisher = {{Springer}},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/BrÃ©maud_2014_Fourier_Analysis_and_Stochastic_Processes.pdf}
}

@book{bremaud2020,
  title = {{Markov Chains: Gibbs Fields, Monte Carlo Simulation and Queues}},
  shorttitle = {{Markov Chains}},
  author = {Br{\'e}maud, Pierre},
  year = {2020},
  edition = {Second},
  publisher = {{Springer Nature}},
  isbn = {978-3-030-45981-9},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/BrÃ©maud_2020_Markov_Chains.pdf}
}

@book{bressan2012,
  title = {{Lecture Notes on Functional Analysis: With Applications to Linear Partial Differential Equations}},
  shorttitle = {{Lecture Notes on Functional Analysis}},
  author = {Bressan, Alberto},
  year = {2012},
  publisher = {{Amer Mathematical Society}},
  address = {{Providence, Rhode Island}},
  abstract = {This textbook is addressed to graduate students in mathematics or other disciplines who wish to understand the essential concepts of functional analysis and their applications to partial differential equations. The book is intentionally concise, presenting all the fundamental concepts and results but omitting the more specialized topics. Enough of the theory of Sobolev spaces and semigroups of linear operators is included as needed to develop significant applications to elliptic, parabolic, and hyperbolic PDEs. Throughout the book, care has been taken to explain the connections between theorems in functional analysis and familiar results of finite-dimensional linear algebra. The main concepts and ideas used in the proofs are illustrated with a large number of figures. A rich collection of homework problems is included at the end of most chapters. The book is suitable as a text for a one-semester graduate course.},
  isbn = {978-0-8218-8771-4},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Bressan_2012_Lecture_Notes_on_Functional_Analysis.pdf}
}

@book{breuilly2016,
  title = {{The Oxford Handbook of the History of Nationalism}},
  author = {Breuilly, John},
  year = {2016},
  edition = {Reprint edizione},
  publisher = {{OUP Oxford}},
  address = {{Oxford}},
  isbn = {978-0-19-876820-3},
  langid = {Inglese},
  keywords = {doing},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Breuilly_2016_The_Oxford_Handbook_of_the_History_of_Nationalism.epub}
}

@book{brezis2010,
  title = {{Functional Analysis, Sobolev Spaces and Partial Differential Equations}},
  author = {Brezis, Haim},
  year = {2010},
  month = nov,
  edition = {2011 edizione},
  publisher = {{Springer}},
  address = {{New York ; London}},
  abstract = {This is a completely revised English edition of the important Analyse fonctionnelle (1983). It contains a wealth of problems and exercises to guide the reader. It is also the first single-volume textbook to cover related fields of functional analysis and PDEs.},
  isbn = {978-0-387-70913-0},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Brezis_2010_Functional_Analysis,_Sobolev_Spaces_and_Partial_Differential_Equations.pdf}
}

@book{broadberry2021,
  title = {The {{Cambridge Economic History}} of the {{Modern World}}: {{Volume}} 2, 1870 to the {{Present}}},
  shorttitle = {The {{Cambridge Economic History}} of the {{Modern World}}},
  editor = {Broadberry, Stephen and Fukao, Kyoji},
  year = {2021},
  month = aug,
  publisher = {{Cambridge University Press}},
  address = {{Cambridge, United Kingdom ; New York, NY}},
  isbn = {978-1-107-15948-8},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Broadberry_Fukao_2021_The_Cambridge_Economic_History_of_the_Modern_World2.pdf}
}

@book{broadberry2021a,
  title = {{The Cambridge Economic History of the Modern World: Volume 1, 1700 to 1870}},
  shorttitle = {{The Cambridge Economic History of the Modern World}},
  author = {Broadberry, Stephen and Fukao, Kyoji},
  year = {2021},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge, United Kingdom ; New York, NY}},
  isbn = {978-1-107-15945-7},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Broadberry_Fukao_2021_The_Cambridge_Economic_History_of_the_Modern_World.pdf}
}

@book{brockwell2009,
  title = {{Time Series: Theory and Methods}},
  shorttitle = {{Time Series}},
  author = {Brockwell, Peter J.},
  year = {2009},
  month = apr,
  edition = {2\textdegree{} edizione},
  publisher = {{Springer}},
  address = {{New York, N.Y}},
  isbn = {978-1-4419-0319-8},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Brockwell_2009_Time_Series.pdf}
}

@book{brockwell2016,
  title = {Introduction to {{Time Series}} and {{Forecasting}}},
  author = {Brockwell, Peter J. and Davis, Richard A.},
  year = {2016},
  series = {Springer {{Texts}} in {{Statistics}}},
  edition = {Third},
  publisher = {{Springer-Verlag}},
  address = {{New York}},
  doi = {10.1007/b97391},
  abstract = {Some of the key mathematical results are stated without proof in order to make the underlying theory accessible to a wider audience. The book assumes a knowledge only of basic calculus, matrix algebra, and elementary statistics. The emphasis is on methods and the analysis of data sets. The logic and tools of model-building for stationary and nonstationary time series are developed in detail and numerous exercises, many of which make use of the included computer package, provide the reader with ample opportunity to develop skills in this area. The core of the book covers stationary processes, ARMA and ARIMA processes, multivariate time series and state-space models, with an optional chapter on spectral analysis. Additional topics include harmonic regression, the Burg and Hannan-Rissanen algorithms, unit roots, regression with ARMA errors, structural models, the EM algorithm, generalized state-space models with applications to time series of count data, exponential smoothing, the Holt-Winters and ARAR forecasting algorithms, transfer function models and intervention analysis. Brief introductions are also given to cointegration and to nonlinear, continuous-time and long-memory models. The time series package included in the back of the book is a slightly modified version of the package ITSM, published separately as ITSM for Windows, by Springer-Verlag, 1994. It does not handle such large data sets as ITSM for Windows, but like the latter, runs on IBM-PC compatible computers under either DOS or Windows (version 3.1 or later). The programs are all menu-driven so that the reader can immediately apply the techniques in the book to time series data, with a minimal investment of time in the computational and algorithmic aspects of the analysis.},
  isbn = {978-1-4757-7750-5},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Brockwell_Davis_2002_Introduction_to_Time_Series_and_Forecasting.pdf;/home/dede/Zotero/storage/6QB7ZJ5B/9781475777505.html}
}

@article{broderick2011,
  title = {Beta Processes, Stick-Breaking, and Power Laws},
  author = {Broderick, Tamara and Jordan, Michael I. and Pitman, Jim},
  year = {2011},
  month = sep,
  journal = {arXiv:1106.0539 [stat]},
  eprint = {1106.0539},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {The beta-Bernoulli process provides a Bayesian nonparametric prior for models involving collections of binary-valued features. A draw from the beta process yields an infinite collection of probabilities in the unit interval, and a draw from the Bernoulli process turns these into binary-valued features. Recent work has provided stick-breaking representations for the beta process analogous to the well-known stick-breaking representation for the Dirichlet process. We derive one such stick-breaking representation directly from the characterization of the beta process as a completely random measure. This approach motivates a three-parameter generalization of the beta process, and we study the power laws that can be obtained from this generalized beta process. We present a posterior inference algorithm for the beta-Bernoulli process that exploits the stick-breaking representation, and we present experimental results for a discrete factor-analysis model.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Machine Learning,Statistics - Methodology,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Broderick_et_al_2011_Beta_processes,_stick-breaking,_and_power_laws.pdf;/home/dede/Zotero/storage/AH6KG5UE/1106.html}
}

@article{brodsky2010,
  title = {Sequential {{Detection}} and {{Estimation}} of {{Change-Points}}},
  author = {Brodsky, Boris},
  year = {2010},
  month = apr,
  journal = {Sequential Analysis},
  volume = {29},
  number = {2},
  pages = {217--233},
  publisher = {{Taylor \& Francis}},
  issn = {0747-4946},
  doi = {10.1080/07474941003741284},
  abstract = {The problem of sequential detection and estimation of a change-point is considered. The `large parameter' approach for solving this problem is proposed. A two-stage method is designed according to this approach, which includes the nonparametric version of the CUSUM procedure for sequential change-point detection and the modified Kolmogorov-Smirnov test for the retrospective estimation of the detected change-point. The asymptotic effectiveness of this method is proved for dependent observations (Theorems 2.1 and 3.1) when the large parameter infinitely increases. Monte Carlo study of this method for the Gaussian case (changes in mean and dispersion) is performed. The a priori theoretical lower bounds are proved for new performance measures in sequential change-point detection and retrospective estimation (Theorems 5.1 and 5.2) and the asymptotic optimality of the proposed method is demonstrated.},
  keywords = {62L10,Asymptotic optimality,Change-point,Change-point estimation,Performance measure,Sequential detection},
  annotation = {\_eprint: https://doi.org/10.1080/07474941003741284},
  file = {/home/dede/Zotero/storage/DDW79RMC/07474941003741284.html}
}

@book{brooks2011,
  title = {{Handbook of Markov Chain Monte Carlo}},
  author = {Brooks, Steve and Gelman, Andrew and Jones, Galin and Meng, Xiao-Li},
  year = {2011},
  month = may,
  edition = {1 edizione},
  publisher = {{Chapman and Hall/CRC}},
  address = {{Boca Raton}},
  abstract = {Since their popularization in the 1990s, Markov chain Monte Carlo (MCMC) methods have revolutionized statistical computing and have had an especially profound impact on the practice of Bayesian statistics. Furthermore, MCMC methods have enabled the development and use of intricate models in an astonishing array of disciplines as diverse as fisheries science and economics. The wide-ranging practical importance of MCMC has sparked an expansive and deep investigation into fundamental Markov chain theory.  The Handbook of Markov Chain Monte Carlo provides a reference for the broad audience of developers and users of MCMC methodology interested in keeping up with cutting-edge theory and applications. The first half of the book covers MCMC foundations, methodology, and algorithms. The second half considers the use of MCMC in a variety of practical applications including in educational research, astrophysics, brain imaging, ecology, and sociology.  The in-depth introductory section of the book allows graduate students and practicing scientists new to MCMC to become thoroughly acquainted with the basic theory, algorithms, and applications. The book supplies detailed examples and case studies of realistic scientific problems presenting the diversity of methods used by the wide-ranging MCMC community. Those familiar with MCMC methods will find this book a useful refresher of current theory and recent developments.},
  isbn = {978-1-4200-7941-8},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Brooks_et_al_2011_Handbook_of_Markov_Chain_Monte_Carlo.pdf}
}

@article{brown1975,
  title = {Techniques for {{Testing}} the {{Constancy}} of {{Regression Relationships}} over {{Time}}},
  author = {Brown, R. L. and Durbin, J. and Evans, J. M.},
  year = {1975},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  volume = {37},
  number = {2},
  pages = {149--192},
  publisher = {{[Royal Statistical Society, Wiley]}},
  issn = {0035-9246},
  abstract = {Methods for studying the stability over time of regression relationships are considered. Recursive residuals, defined to be uncorrelated with zero means and constant variance, are introduced and tests based on the cusum and cusum of squares of recursive residuals are developed. Further techniques based on moving regressions, in which the regression model is fitted from a segment of data which is moved along the series, and on regression models whose coefficients are polynomials in time are studied. The Quandt log-likelihood ratio statistic is considered. Emphasis is placed on the use of graphical methods. The techniques proposed have been embodied in a comprehensive computer program, TIMVAR. Use of the techniques is illustrated by applying them to three sets of data.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Brown_et_al_1975_Techniques_for_Testing_the_Constancy_of_Regression_Relationships_over_Time.pdf}
}

@article{brown2018,
  title = {Japan's Security Cooperation with {{Russia}}: Neutralizing the Threat of a {{China}}\textendash{{Russia}} United Front},
  shorttitle = {Japan's Security Cooperation with {{Russia}}},
  author = {Brown, James D. J.},
  year = {2018},
  month = jul,
  journal = {International Affairs},
  volume = {94},
  number = {4},
  pages = {861--882},
  issn = {0020-5850},
  doi = {10.1093/ia/iiy031},
  abstract = {The growing threats from North Korea and China, as well as heightened concerns about the long-term reliability of the United States, have led Japan to cultivate closer security relations with other regional players. Priority has been placed on Australia, India, South Korea and south-east Asian countries, yet Japan's National Security Strategy also states that cooperation with Russia is crucial. This is controversial since the pursuit of closer ties continued despite Russia's intervention in Ukraine and since the policy sets Japan apart from its US ally. The purpose of this article is to explain the nature, motivations and prospects of this emerging security relationship. The article's key finding is that the Japanese security elite do not regard Russia as a significant threat since a clear differentiation is made between Russia's actions in eastern Europe and east Asia. Additionally, cooperation is considered vital to neutralize the danger of a prospective China\textendash Russia united front. These and related incentives have encouraged Japan to explore security cooperation with Russia, including via 2+2 meetings and search and rescue drills. However, due to a range of international and domestic constraints, Japan's security ties with Russia are likely to remain less developed than those with other partners in the region.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Brown_2018_Japan's_security_cooperation_with_Russia.pdf;/home/dede/Zotero/storage/AR5MDLVC/5039990.html}
}

@article{bubeck2015,
  title = {Convex {{Optimization}}: {{Algorithms}} and {{Complexity}}},
  shorttitle = {Convex {{Optimization}}},
  author = {Bubeck, S{\'e}bastien},
  year = {2015},
  month = nov,
  journal = {arXiv:1405.4980 [cs, math, stat]},
  eprint = {1405.4980},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {This monograph presents the main complexity theorems in convex optimization and their corresponding algorithms. Starting from the fundamental theory of black-box optimization, the material progresses towards recent advances in structural optimization and stochastic optimization. Our presentation of black-box optimization, strongly influenced by Nesterov's seminal book and Nemirovski's lecture notes, includes the analysis of cutting plane methods, as well as (accelerated) gradient descent schemes. We also pay special attention to non-Euclidean settings (relevant algorithms include Frank-Wolfe, mirror descent, and dual averaging) and discuss their relevance in machine learning. We provide a gentle introduction to structural optimization with FISTA (to optimize a sum of a smooth and a simple non-smooth term), saddle-point mirror prox (Nemirovski's alternative to Nesterov's smoothing), and a concise description of interior point methods. In stochastic optimization we discuss stochastic gradient descent, mini-batches, random coordinate descent, and sublinear algorithms. We also briefly touch upon convex relaxation of combinatorial problems and the use of randomness to round solutions, as well as random walks based methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computational Complexity,Computer Science - Machine Learning,Mathematics - Numerical Analysis,Mathematics - Optimization and Control,Statistics - Machine Learning,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Bubeck_2015_Convex_Optimization.pdf;/home/dede/Zotero/storage/EMFD2NPG/1405.html}
}

@article{buchholz2008,
  title = {On Properties of Predictors Derived with a Two-Step Bootstrap Model Averaging Approach\textemdash{{A}} Simulation Study in the Linear Regression Model},
  author = {Buchholz, Anika and Holl{\"a}nder, Norbert and Sauerbrei, Willi},
  year = {2008},
  month = jan,
  journal = {Computational Statistics \& Data Analysis},
  volume = {52},
  number = {5},
  pages = {2778--2793},
  issn = {01679473},
  doi = {10.1016/j.csda.2007.10.007},
  langid = {english},
  keywords = {todo}
}

@article{buhlmann2002,
  title = {Bootstraps for {{Time Series}}},
  author = {B{\"u}hlmann, Peter},
  year = {2002},
  journal = {Statistical Science},
  volume = {17},
  number = {1},
  pages = {52--72},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0883-4237},
  abstract = {We review and compare block, sieve and local bootstraps for time series and thereby illuminate theoretical aspects of the procedures as well as their performance on finite-sample data. Our view is selective with the intention of providing a new and fair picture of some particular aspects of bootstrapping time series. The generality of the block bootstrap is contrasted with sieve bootstraps. We discuss implementational advantages and disadvantages. We argue that two types of sieve often outperform the block method, each of them in its own important niche, namely linear and categorical processes. Local bootstraps, designed for nonparametric smoothing problems, are easy to use and implement but exhibit in some cases low performance.},
  keywords = {doing},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/BÃ¼hlmann_2002_Bootstraps_for_Time_Series.pdf}
}

@book{buhlmann2011,
  title = {{Statistics for High-Dimensional Data: Methods, Theory and Applications}},
  shorttitle = {{Statistics for High-Dimensional Data}},
  author = {Buhlmann, Peter and Geer, Sara Van De},
  year = {2011},
  edition = {2011\textdegree{} edizione},
  publisher = {{Springer-Nature New York Inc}},
  address = {{Heidelberg ; New York}},
  abstract = {Modern statistics deals with large and complex data sets, and consequently with models containing a large number of parameters. This book presents a detailed account of recently developed approaches, including the Lasso and versions of it for various models, boosting methods, undirected graphical modeling, and procedures controlling false positive selections.A special characteristic of the book is that it contains comprehensive mathematical theory on high-dimensional statistics combined with methodology, algorithms and illustrations with real data examples. This in-depth approach highlights the methodsÂ’ great potential and practical applicability in a variety of settings. As such, it is a valuable resource for researchers, graduate students and experts in statistics, applied mathematics and computer science.},
  isbn = {978-3-642-20191-2},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Buhlmann_Geer_2011_Statistics_for_High-Dimensional_Data.pdf}
}

@article{buhlmann2014,
  title = {Discussion: "{{A}} Significance Test for the Lasso"},
  shorttitle = {Discussion},
  author = {B{\"u}hlmann, Peter and Meier, Lukas and {van de Geer}, Sara},
  year = {2014},
  month = apr,
  journal = {The Annals of Statistics},
  volume = {42},
  number = {2},
  eprint = {1405.6792},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  issn = {0090-5364},
  doi = {10.1214/13-AOS1175A},
  abstract = {Discussion of "A significance test for the lasso" by Richard Lockhart, Jonathan Taylor, Ryan J. Tibshirani, Robert Tibshirani [arXiv:1301.7161].},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/BÃ¼hlmann_et_al_2014_Discussion.pdf;/home/dede/Zotero/storage/ZCHDWNJJ/1405.html}
}

@article{buja1989,
  title = {Linear {{Smoothers}} and {{Additive Models}}},
  author = {Buja, Andreas and Hastie, Trevor and Tibshirani, Robert},
  year = {1989},
  month = jun,
  journal = {The Annals of Statistics},
  volume = {17},
  number = {2},
  pages = {453--510},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/aos/1176347115},
  abstract = {We study linear smoothers and their use in building nonparametric regression models. In the first part of this paper we examine certain aspects of linear smoothers for scatterplots; examples of these are the running-mean and running-line, kernel and cubic spline smoothers. The eigenvalue and singular value decompositions of the corresponding smoother matrix are used to describe qualitatively a smoother, and several other topics such as the number of degrees of freedom of a smoother are discussed. In the second part of the paper we describe how linear smoothers can be used to estimate the additive model, a powerful nonparametric regression model, using the "back-fitting algorithm." We show that backfitting is the Gauss-Seidel iterative method for solving a set of normal equations associated with the additive model. We provide conditions for consistency and nondegeneracy and prove convergence for the backfitting and related algorithms for a class of smoothers that includes cubic spline smoothers.},
  keywords = {62G05,65D10,Additive model,Gauss-Seidel algorithm,nonparametric,regression,semiparametric,Smoother},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Buja_et_al_1989_Linear_Smoothers_and_Additive_Models.pdf;/home/dede/Zotero/storage/LNBBAKME/1176347115.html}
}

@inproceedings{buja2016,
  title = {Models as {{Approximations II}}: {{A General Theory}} of {{Model-Robust Regression}}},
  shorttitle = {Models as {{Approximations}} --- {{Part II}}},
  author = {Buja, A. and Berk, R. and Brown, L. and George, E. and Kuchibhotla, A. and Zhao, Linda H.},
  year = {2016},
  abstract = {We discuss a model-robust theory for general types of regression in the simplest case of iid observations. The theory replaces the parameters of parametric models with statistical functionals, to be called ``regression functionals'' and defined on large non-parametric classes of joint x-y distributions without assuming a working model. Examples of regression functionals are the slopes of OLS linear equations at largely arbitrary x-y distributions (see Part I). More generally, regression functionals can be defined by minimizing objective functions or solving estimating equations at joint x-y distributions. The role of parametric models is reduced to heuristics for generating objective functions and estimating equations without assuming them as correct. In this framework it is possible to achieve the following: (1) explicate the meaning of mis/well-specification for regression functionals, (2) decompose sampling variability into two components, one due to the conditional response distributions and another due to the regressor distribution interacting (conspiring) with misspecification, (3) exhibit plug-in (and hence sandwich) estimators of standard error as limiting cases of x-y bootstrap estimators. AMS 2000 subject classifications: Primary 62J05, 62J20, 62F40; secondary 62F35, 62A10.},
  keywords = {doing},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Buja_et_al_2016_Models_as_Approximations_---_Part_II.pdf}
}

@article{buja2019,
  title = {Models as {{Approximations I}}: {{Consequences Illustrated}} with {{Linear Regression}}},
  shorttitle = {Models as {{Approximations I}}},
  author = {Buja, Andreas and Berk, Richard and Brown, Lawrence and George, Edward and Pitkin, Emil and Traskin, Mikhail and Zhao, Linda and Zhang, Kai},
  year = {2019},
  month = jul,
  journal = {arXiv:1404.1578 [stat]},
  eprint = {1404.1578},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {In the early 1980s Halbert White inaugurated a "model-robust'' form of statistical inference based on the "sandwich estimator'' of standard error. This estimator is known to be "heteroskedasticity-consistent", but it is less well-known to be "nonlinearity-consistent'' as well. Nonlinearity, however, raises fundamental issues because in its presence regressors are not ancillary, hence can't be treated as fixed. The consequences are deep: (1)\textasciitilde population slopes need to be re-interpreted as statistical functionals obtained from OLS fits to largely arbitrary joint \$\textbackslash xy\$\textasciitilde distributions; (2)\textasciitilde the meaning of slope parameters needs to be rethought; (3)\textasciitilde the regressor distribution affects the slope parameters; (4)\textasciitilde randomness of the regressors becomes a source of sampling variability in slope estimates; (5)\textasciitilde inference needs to be based on model-robust standard errors, including sandwich estimators or the \$\textbackslash xy\$\textasciitilde bootstrap. In theory, model-robust and model-trusting standard errors can deviate by arbitrary magnitudes either way. In practice, significant deviations between them can be detected with a diagnostic test.},
  archiveprefix = {arXiv},
  keywords = {skimmed,Statistics - Methodology},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Buja_et_al_2019_Models_as_Approximations_I.pdf;/home/dede/Zotero/storage/Y628BZUG/1404.html}
}

@book{bunton2013,
  title = {The {{Palestinian-Israeli}} Conflict : A Very Short Introduction},
  shorttitle = {The {{Palestinian-Israeli}} Conflict},
  author = {Bunton and Martin},
  year = {2013},
  series = {Very Short Introductions 359 {{Palestinian}} 1},
  edition = {1. edition},
  publisher = {{Oxford University Press, USA}},
  isbn = {978-0-19-960393-0},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Bunton_Martin_2013_The_Palestinian-Israeli_conflict.epub}
}

@book{burnham1964,
  title = {{Suicide of the West: An Essay on the Meaning and Destiny of Liberalism}},
  shorttitle = {{Suicide of the West}},
  author = {Burnham, James and O'Sullivan, John and Kimball, Roger},
  year = {1964},
  edition = {Revised edizione},
  publisher = {{Encounter Books}},
  address = {{New York}},
  isbn = {978-1-59403-783-2},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Burnham_et_al_1964_Suicide_of_the_West.epub}
}

@article{burtini2015,
  title = {A {{Survey}} of {{Online Experiment Design}} with the {{Stochastic Multi-Armed Bandit}}},
  author = {Burtini, Giuseppe and Loeppky, Jason and Lawrence, Ramon},
  year = {2015},
  month = nov,
  journal = {arXiv:1510.00757 [cs, stat]},
  eprint = {1510.00757},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Adaptive and sequential experiment design is a well-studied area in numerous domains. We survey and synthesize the work of the online statistical learning paradigm referred to as multi-armed bandits integrating the existing research as a resource for a certain class of online experiments. We first explore the traditional stochastic model of a multi-armed bandit, then explore a taxonomic scheme of complications to that model, for each complication relating it to a specific requirement or consideration of the experiment design context. Finally, at the end of the paper, we present a table of known upper-bounds of regret for all studied algorithms providing both perspectives for future theoretical work and a decision-making tool for practitioners looking for theoretical guarantees.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Burtini_et_al_2015_A_Survey_of_Online_Experiment_Design_with_the_Stochastic_Multi-Armed_Bandit.pdf;/home/dede/Zotero/storage/TCMPUTYG/1510.html}
}

@article{busoniu2018,
  title = {Reinforcement Learning for Control: {{Performance}}, Stability, and Deep Approximators},
  shorttitle = {Reinforcement Learning for Control},
  author = {Bu{\c s}oniu, Lucian and {de Bruin}, Tim and Toli{\'c}, Domagoj and Kober, Jens and Palunko, Ivana},
  year = {2018},
  month = jan,
  journal = {Annual Reviews in Control},
  volume = {46},
  pages = {8--28},
  issn = {1367-5788},
  doi = {10.1016/j.arcontrol.2018.09.005},
  abstract = {Reinforcement learning (RL) offers powerful algorithms to search for optimal controllers of systems with nonlinear, possibly stochastic dynamics that are unknown or highly uncertain. This review mainly covers artificial-intelligence approaches to RL, from the viewpoint of the control engineer. We explain how approximate representations of the solution make RL feasible for problems with continuous states and control actions. Stability is a central concern in control, and we argue that while the control-theoretic RL subfield called adaptive dynamic programming is dedicated to it, stability of RL largely remains an open question. We also cover in detail the case where deep neural networks are used for approximation, leading to the field of deep RL, which has shown great success in recent years. With the control practitioner in mind, we outline opportunities and pitfalls of deep RL; and we close the survey with an outlook that \textendash{} among other things \textendash{} points out some avenues for bridging the gap between control and artificial-intelligence RL techniques.},
  langid = {english},
  keywords = {Adaptive dynamic programming,Deep learning,Function approximation,Optimal control,Reinforcement learning,Stability,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/BuÅŸoniu_et_al_2018_Reinforcement_learning_for_control.pdf;/home/dede/Zotero/storage/SJNCTQJC/S1367578818301184.html}
}

@article{butz1969,
  title = {Convergence with {{Hilbert}}'s Space Filling Curve},
  author = {Butz, Arthur R.},
  year = {1969},
  month = may,
  journal = {Journal of Computer and System Sciences},
  volume = {3},
  number = {2},
  pages = {128--146},
  issn = {00220000},
  doi = {10.1016/S0022-0000(69)80010-3},
  langid = {english},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Butz_1969_Convergence_with_Hilbert's_space_filling_curve.pdf}
}

@article{cabana2021,
  title = {Robust Multivariate Control Chart Based on Shrinkage for Individual Observations},
  author = {Cabana, Elisa and Lillo, Rosa E.},
  year = {2021},
  month = jun,
  journal = {Journal of Quality Technology},
  volume = {0},
  number = {0},
  pages = {1--26},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2021.1930617},
  abstract = {A robust multivariate quality control technique for individual observations is proposed, based on the robust reweighted shrinkage estimators. A simulation study is done to check the performance and compare the method with the classical Hotelling approach, and the robust alternative based on the reweighted minimum covariance determinant estimator. The results show the appropriateness of the method even when the dimension or the Phase I contamination are high, with both independent and correlated variables, showing additional advantages about computational efficiency. The approach is illustrated with two real data-set examples from production processes.},
  keywords = {Hotelling T2,multivariate process control,reweighted MCD,reweighted shrinkage estimator},
  annotation = {\_eprint: https://doi.org/10.1080/00224065.2021.1930617},
  file = {/home/dede/Zotero/storage/8V8JKVY5/00224065.2021.html}
}

@article{cai2020,
  title = {Distances between Probability Distributions of Different Dimensions},
  author = {Cai, Yuhang and Lim, Lek-Heng},
  year = {2020},
  month = nov,
  journal = {arXiv:2011.00629 [cs, math, stat]},
  eprint = {2011.00629},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {Comparing probability distributions is an indispensable and ubiquitous task in machine learning and statistics. The most common way to compare a pair of Borel probability measures is to compute a metric between them, and by far the most widely used notions of metric are the Wasserstein metric and the total variation metric. The next most common way is to compute a divergence between them, and in this case almost every known divergences such as those of Kullback-Leibler, Jensen-Shannon, R\textbackslash 'enyi, and many more, are special cases of the \$f\$-divergence. Nevertheless these metrics and divergences may only be computed, in fact, are only defined, when the pair of probability measures are on spaces of the same dimension. How would one measure, say, a Wasserstein distance between the uniform distribution on the interval \$[-1,1]\$ and a Gaussian distribution on \$\textbackslash mathbb\{R\}\^3\$? We will show that various common notions of metrics and divergences can be extended in a completely natural manner to Borel probability measures defined on spaces of different dimensions, e.g., one on \$\textbackslash mathbb\{R\}\^m\$ and another on \$\textbackslash mathbb\{R\}\^n\$ where \$m, n\$ are distinct, so as to give a meaningful answer to the previous question.},
  archiveprefix = {arXiv},
  keywords = {28A33; 28A50; 46E27; 49Q22; 60E05; 94A17,Computer Science - Information Theory,Mathematics - Probability,Mathematics - Statistics Theory,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Cai_Lim_2020_Distances_between_probability_distributions_of_different_dimensions.pdf;/home/dede/Zotero/storage/6JQZIEYL/2011.html}
}

@article{camacho2016,
  title = {{{PCA-based}} Multivariate Statistical Network Monitoring for Anomaly Detection},
  author = {Camacho, Jos{\'e} and {P{\'e}rez-Villegas}, Alejandro and {Garc{\'i}a-Teodoro}, Pedro and {Maci{\'a}-Fern{\'a}ndez}, Gabriel},
  year = {2016},
  month = jun,
  journal = {Computers \& Security},
  volume = {59},
  pages = {118--137},
  issn = {01674048},
  doi = {10.1016/j.cose.2016.02.008},
  langid = {english},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Camacho_et_al_2016_PCA-based_multivariate_statistical_network_monitoring_for_anomaly_detection.pdf}
}

@article{canale2011,
  title = {Bayesian {{Kernel Mixtures}} for {{Counts}}},
  author = {Canale, Antonio and Dunson, David B.},
  year = {2011},
  month = dec,
  journal = {Journal of the American Statistical Association},
  volume = {106},
  number = {496},
  pages = {1528--1539},
  issn = {0162-1459},
  doi = {10.1198/jasa.2011.tm10552},
  abstract = {Although Bayesian nonparametric mixture models for continuous data are well developed, there is a limited literature on related approaches for count data. A common strategy is to use a mixture of Poissons, which unfortunately is quite restrictive in not accounting for distributions having variance less than the mean. Other approaches include mixing multinomials, which requires finite support, and using a Dirichlet process prior with a Poisson base measure, which does not allow smooth deviations from the Poisson. As a broad class of alternative models, we propose to use nonparametric mixtures of rounded continuous kernels. An efficient Gibbs sampler is developed for posterior computation, and a simulation study is performed to assess performance. Focusing on the rounded Gaussian case, we generalize the modeling framework to account for multivariate count data, joint modeling with continuous and categorical variables, and other complications. The methods are illustrated through applications to a developmental toxicity study and marketing data. This article has .},
  pmcid = {PMC3329131},
  pmid = {22523437},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Canale_Dunson_2011_Bayesian_Kernel_Mixtures_for_Counts.pdf}
}

@article{canale2016,
  title = {Constrained Functional Time Series: {{Applications}} to the {{Italian}} Gas Market},
  shorttitle = {Constrained Functional Time Series},
  author = {Canale, Antonio and Vantini, Simone},
  year = {2016},
  month = oct,
  journal = {International Journal of Forecasting},
  volume = {32},
  number = {4},
  pages = {1340--1351},
  issn = {0169-2070},
  doi = {10.1016/j.ijforecast.2016.05.002},
  abstract = {Motivated by market dynamic modelling in the Italian Natural Gas Balancing Platform, we propose a model for analyzing time series of functions, subject to equality and inequality constraints at the two edges of the domain, respectively, such as daily demand and offer curves. Specifically, we provide the constrained functions with suitable pre-Hilbert structures, and introduce a useful isometric bijective map that associates each possible bounded and monotonic function to an unconstrained one. We introduce a functional-to-functional autoregressive model that is used to forecast future demand/offer functions, and estimate the model via the minimization of a penalized mean squared error of prediction, with a penalty term based on the Hilbert\textendash Schmidt squared norm of autoregressive lagged operators. The approach is of general interest and could be generalized to any situation in which one has to deal with functions that are subject to the above constraints which evolve over time.},
  langid = {english},
  keywords = {Autoregressive model,Demand and offer model,done,Energy forecasting,Functional data analysis,Functional ridge regression},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Canale_Vantini_2016_Constrained_functional_time_series.pdf;/home/dede/Zotero/storage/T2I5YBWZ/S0169207016300620.html}
}

@incollection{canale2016a,
  title = {Bayesian {{Nonparametrics}}},
  booktitle = {Wiley {{StatsRef}}: {{Statistics Reference Online}}},
  author = {Canale, Antonio and Lijoi, Antonio and Pr{\"u}nster, Igor},
  editor = {Balakrishnan, N. and Colton, Theodore and Everitt, Brian and Piegorsch, Walter and Ruggeri, Fabrizio and Teugels, Jozef L.},
  year = {2016},
  month = aug,
  pages = {1--11},
  publisher = {{John Wiley \& Sons, Ltd}},
  address = {{Chichester, UK}},
  doi = {10.1002/9781118445112.stat07850},
  isbn = {978-1-118-44511-2},
  langid = {english},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Canale_et_al_2016_Bayesian_Nonparametrics.pdf}
}

@article{canale2016b,
  title = {Multiscale {{Bernstein}} Polynomials for Densities},
  author = {Canale, Antonio and Dunson, David B.},
  year = {2016},
  journal = {Statistica Sinica},
  volume = {26},
  number = {3},
  pages = {1175--1195},
  publisher = {{Institute of Statistical Science, Academia Sinica}},
  issn = {1017-0405},
  abstract = {Our focus is on constructing a multiscale nonparametric prior for densities. The Bayes density estimation literature is dominated by single scale methods, with the exception of Polya trees, which favor overly-spiky densities even when the truth is smooth. We propose a multiscale Bernstein polynomial family of priors, which produce smooth realizations that do not rely on hard partitioning of the support. At each level in an infinitely-deep binary tree, we place a beta dictionary density; within a scale the densities are equivalent to Bernstein polynomials. Using a stick-breaking characterization, stochastically decreasing weights are allocated to the finer scale dictionary elements. A slice sampler is used for posterior computation, and properties are described. The method characterizes densities with locally-varying smoothness, and can produce a sequence of coarse to fine density estimates. An extension for Bayesian testing of group differences is introduced and applied to DNA methylation array data.},
  keywords = {done}
}

@article{canale2017,
  title = {{{msBP}}: {{An R Package}} to {{Perform Bayesian Nonparametric Inference Using Multiscale Bernstein Polynomials Mixtures}}},
  shorttitle = {{{msBP}}},
  author = {Canale, Antonio},
  year = {2017},
  month = jun,
  journal = {Journal of Statistical Software},
  volume = {78},
  number = {1},
  pages = {1--19},
  issn = {1548-7660},
  doi = {10.18637/jss.v078.i06},
  copyright = {Copyright (c) 2017 Antonio Canale},
  langid = {english},
  keywords = {binary trees,density estimation,multiscale stick-breaking,multiscale testing,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Canale_2017_msBP.pdf;/home/dede/Zotero/storage/Q46VB3XQ/v078i06.html}
}

@article{canale2020,
  title = {Importance Conditional Sampling for {{Pitman-Yor}} Mixtures},
  author = {Canale, Antonio and Corradin, Riccardo and Nipoti, Bernardo},
  year = {2020},
  month = jan,
  journal = {arXiv:1906.08147 [stat]},
  eprint = {1906.08147},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Nonparametric mixture models based on the Pitman-Yor process represent a flexible tool for density estimation and clustering. Natural generalization of the popular class of Dirichlet process mixture models, they allow for more robust inference on the number of components characterizing the distribution of the data. We propose a new sampling strategy for such models, named importance conditional sampling (ICS), which combines appealing properties of existing methods, including easy interpretability and straightforward quantification of posterior uncertainty. An extensive simulation study highlights the efficiency of the proposed method which, unlike other conditional samplers, is robust to the specification of the parameters characterizing the Pitman-Yor process. The ICS also proves more efficient than the marginal sampler, as soon as the sample size is not small, and, importantly, the step to update latent parameters is fully parallelizable. We further show that the ICS approach can be naturally extended to other classes of computationally demanding models, such as nonparametric mixture models for partially exchangeable data. The behaviour of our method is illustrated by analysing a rich dataset from the Collaborative Perinatal Project.},
  archiveprefix = {arXiv},
  keywords = {65C60; 62F15; 65C40; 62G07,Statistics - Computation,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Canale_et_al_2020_Importance_conditional_sampling_for_Pitman-Yor_mixtures.pdf;/home/dede/Zotero/storage/HWJTU3X3/1906.html}
}

@article{candes2017,
  title = {Panning for {{Gold}}: {{Model-X Knockoffs}} for {{High-dimensional Controlled Variable Selection}}},
  shorttitle = {Panning for {{Gold}}},
  author = {Candes, Emmanuel and Fan, Yingying and Janson, Lucas and Lv, Jinchi},
  year = {2017},
  month = dec,
  journal = {arXiv:1610.02351 [math, stat]},
  eprint = {1610.02351},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  abstract = {Many contemporary large-scale applications involve building interpretable models linking a large set of potential covariates to a response in a nonlinear fashion, such as when the response is binary. Although this modeling problem has been extensively studied, it remains unclear how to effectively control the fraction of false discoveries even in high-dimensional logistic regression, not to mention general high-dimensional nonlinear models. To address such a practical problem, we propose a new framework of \$model\$-\$X\$ knockoffs, which reads from a different perspective the knockoff procedure (Barber and Cand\textbackslash `es, 2015) originally designed for controlling the false discovery rate in linear models. Whereas the knockoffs procedure is constrained to homoscedastic linear models with \$n\textbackslash ge p\$, the key innovation here is that model-X knockoffs provide valid inference from finite samples in settings in which the conditional distribution of the response is arbitrary and completely unknown. Furthermore, this holds no matter the number of covariates. Correct inference in such a broad setting is achieved by constructing knockoff variables probabilistically instead of geometrically. To do this, our approach requires the covariates be random (independent and identically distributed rows) with a distribution that is known, although we provide preliminary experimental evidence that our procedure is robust to unknown/estimated distributions. To our knowledge, no other procedure solves the \$controlled\$ variable selection problem in such generality, but in the restricted settings where competitors exist, we demonstrate the superior power of knockoffs through simulations. Finally, we apply our procedure to data from a case-control study of Crohn's disease in the United Kingdom, making twice as many discoveries as the original analysis of the same data.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory,Statistics - Applications,Statistics - Methodology,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Candes_et_al_2017_Panning_for_Gold.pdf;/home/dede/Zotero/storage/V3XCB4KG/1610.html}
}

@article{cao2018,
  title = {Nearly {{Optimal Adaptive Procedure}} for {{Piecewise-Stationary Bandit}}: A {{Change-Point Detection Approach}}},
  shorttitle = {Nearly {{Optimal Adaptive Procedure}} for {{Piecewise-Stationary Bandit}}},
  author = {Cao, Y. and Wen, Zheng and Kveton, B. and Xie, Yao},
  year = {2018},
  journal = {undefined},
  abstract = {This paper considers a scenario in which the arms\&\#39; reward distributions may change in a piecewise-stationary fashion at unknown time steps, and motivate and propose a learning algorithm called M-UCB, which can detect and adapt to changes, for the considered scenario. Multi-armed bandit (MAB) is a class of online learning problems where a learning agent aims to maximize its expected cumulative reward while repeatedly selecting to pull arms with unknown reward distributions. In this paper, we consider a scenario in which the arms\&\#39; reward distributions may change in a piecewise-stationary fashion at unknown time steps. By connecting change-detection techniques with classic UCB algorithms, we motivate and propose a learning algorithm called M-UCB, which can detect and adapt to changes, for the considered scenario. We also establish an \$O(\textbackslash sqrt\{MKT\textbackslash log T\})\$ regret bound for M-UCB, where \$T\$ is the number of time steps, \$K\$ is the number of arms, and \$M\$ is the number of stationary segments. \% and \$\textbackslash Delta\$ is the gap between the expected rewards of the optimal and best suboptimal arms. Comparison with the best available lower bound shows that M-UCB is nearly optimal in \$T\$ up to a logarithmic factor. We also compare M-UCB with state-of-the-art algorithms in a numerical experiment based on a public Yahoo! dataset. In this experiment, M-UCB achieves about \$50 \textbackslash\%\$ regret reduction with respect to the best performing state-of-the-art algorithm.},
  langid = {english},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Cao_et_al_2018_Nearly_Optimal_Adaptive_Procedure_for_Piecewise-Stationary_Bandit.pdf;/home/dede/Zotero/storage/4A6E7WPA/4f301c97d00b955c497bab3a6b7f4cd13a5610a1.html}
}

@article{cao2021,
  title = {Exploiting Low-Rank Covariance Structures for Computing High-Dimensional Normal and {{Student-t}} Probabilities},
  author = {Cao, Jian and Genton, Marc G. and Keyes, David E. and Turkiyyah, George M.},
  year = {2021},
  month = jan,
  journal = {Statistics and Computing},
  volume = {31},
  number = {1},
  pages = {2},
  issn = {1573-1375},
  doi = {10.1007/s11222-020-09978-y},
  abstract = {We present a preconditioned Monte Carlo method for computing high-dimensional multivariate normal and Student-t probabilities arising in spatial statistics. The approach combines a tile-low-rank representation of covariance matrices with a block-reordering scheme for efficient quasi-Monte Carlo simulation. The tile-low-rank representation decomposes the high-dimensional problem into many diagonal-block-size problems and low-rank connections. The block-reordering scheme reorders between and within the diagonal blocks to reduce the impact of integration variables from right to left, thus improving the Monte Carlo convergence rate. Simulations up to dimension 65,536 suggest that the new method can improve the run time by an order of magnitude compared with the hierarchical quasi-Monte Carlo method and two orders of magnitude compared with the dense quasi-Monte Carlo method. Our method also forms a strong substitute for the approximate conditioning methods as a more robust estimation with error guarantees. An application study to wind stochastic generators is provided to illustrate that the new computational method makes the maximum likelihood estimation feasible for high-dimensional skew-normal random fields.},
  langid = {english},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Cao_et_al_2021_Exploiting_low-rank_covariance_structures_for_computing_high-dimensional_normal.pdf}
}

@article{cao2022,
  title = {Scalable Computation of Predictive Probabilities in Probit Models with {{Gaussian}} Process Priors},
  author = {Cao, Jian and Durante, Daniele and Genton, Marc G.},
  year = {2022},
  month = jan,
  journal = {arXiv:2009.01471 [stat]},
  eprint = {2009.01471},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Predictive models for binary data are fundamental in various fields, and the growing complexity of modern applications has motivated several flexible specifications for modeling the relationship between the observed predictors and the binary responses. A widely-implemented solution is to express the probability parameter via a probit mapping of a Gaussian process indexed by predictors. However, unlike for continuous settings, there is a lack of closed-form results for predictive distributions in binary models with Gaussian process priors. Markov chain Monte Carlo methods and approximation strategies provide common solutions to this problem, but state-of-the-art algorithms are either computationally intractable or inaccurate in moderate-to-high dimensions. In this article, we aim to cover this gap by deriving closed-form expressions for the predictive probabilities in probit Gaussian processes that rely either on cumulative distribution functions of multivariate Gaussians or on functionals of multivariate truncated normals. To evaluate these quantities we develop novel scalable solutions based on tile-low-rank Monte Carlo methods for computing multivariate Gaussian probabilities, and on mean-field variational approximations of multivariate truncated normals. Closed-form expressions for the marginal likelihood and for the posterior distribution of the Gaussian process are also discussed. As shown in simulated and real-world empirical studies, the proposed methods scale to dimensions where state-of-the-art solutions are impractical.},
  archiveprefix = {arXiv},
  keywords = {skimmed,Statistics - Computation},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Cao_et_al_2022_Scalable_computation_of_predictive_probabilities_in_probit_models_with_Gaussian.pdf;/home/dede/Zotero/storage/CTECRHZY/2009.html}
}

@article{capizzi2001,
  title = {Design of Change Detection Algorithms Based on the Generalized Likelihood Ratio Test},
  author = {Capizzi, Giovanna},
  year = {2001},
  journal = {Environmetrics},
  volume = {12},
  number = {8},
  pages = {749--756},
  issn = {1099-095X},
  doi = {10.1002/env.497},
  abstract = {A design procedure for detecting additive changes in a state-space model is proposed. Since the mean of the observations after the change is unknown, detection algorithms based on the generalized likelihood ratio test, GLR, and on window-limited type GLR, are considered. As Lai (1995) pointed out, it is very difficult to find a satisfactory choice of both window size and threshold for these change detection algorithms. The basic idea of this article is to estimate, through the stochastic approximation of Robbins and Monro, the threshold value which satisfies a constraint on the mean between false alarms, for a specified window size. A convenient stopping rule, based on the first passage time of an F-statistic below a fixed boundary, is used to terminate the iterative approximation. Then, the window size which produces the most desirable out-of-control ARL, for a fixed value of the in-control ARL, can be selected. These change detection algorithms are applied to detect biases on the measurements of ozone, recorded from one monitoring site of Bologna (Italy). Comparisons of the ARL profiles reveal that the full-GLR scheme provides much more protection than the window-limited GLR schemes against small shifts in the process, but the modified window-limited GLR provides more protection against large shifts. Copyright \textcopyright{} 2001 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {air pollution,design procedure,doing,false alarms,fault detection,generalized likelihood ratio test},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/env.497},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Capizzi_2001_Design_of_change_detection_algorithms_based_on_the_generalized_likelihood_ratio.pdf;/home/dede/Zotero/storage/IFQQ6GFP/env.html}
}

@article{capizzi2003,
  title = {An {{Adaptive Exponentially Weighted Moving Average Control Chart}}},
  author = {Capizzi, Giovanna and Masarotto, Guido},
  year = {2003},
  journal = {Technometrics},
  volume = {45},
  number = {3},
  pages = {199--207},
  publisher = {{[Taylor \& Francis, Ltd., American Statistical Association, American Society for Quality]}},
  issn = {0040-1706},
  abstract = {Lucas and Saccucci showed that exponentially weighted moving average (EWMA) control charts can be designed to quickly detect either small or large shifts in the mean of a sequence of independent observations. But a single EWMA chart cannot perform well for small and large shifts simultaneously. Furthermore, in the worst-case situation, this scheme requires a few observations to overcome its initial inertia. The main goal of this article is to suggest an adaptive EWMA (AEWMA) chart that weights the past observations of the monitored process using a suitable function of the current "error." The resulting scheme can be viewed as a smooth combination of a Shewhart chart and an EWMA chart. A design procedure for the new control schemes is suggested. Comparisons of the standard and worst-case average run length profiles of the new scheme with those of different control charts show that AEWMA schemes offer a more balanced protection against shifts of different sizes.},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Capizzi_Masarotto_2003_An_Adaptive_Exponentially_Weighted_Moving_Average_Control_Chart2.pdf}
}

@article{capizzi2008,
  title = {Practical {{Design}} of {{Generalized Likelihood Ratio Control Charts}} for {{Autocorrelated Data}}},
  author = {Capizzi, Giovanna and Masarotto, Guido},
  year = {2008},
  month = aug,
  journal = {Technometrics},
  volume = {50},
  number = {3},
  pages = {357--370},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1198/004017008000000280},
  abstract = {Control charts based on generalized likelihood ratio (GLR) tests are attractive from both theoretical and practical points of view. In particular, in the case of an autocorrelated process, the GLR test uses the information contained in the time-varying response after a change and, as shown by Apley and Shi, is able to outperfom traditional control charts applied to residuals. In addition, a GLR chart provides estimates of the magnitude and the time of occurrence of the change. In this article we present a practical approach to implementating GLR charts for monitoring an autoregressive moving average process assuming that only a phase I sample is available. The proposed approach, based on automatic time series identification, estimates the GLR control limits through stochastic approximation using bootstrap resampling and thus is able to take into account the uncertainty about the underlying model. A Monte Carlo study shows that our methodology can be used to design, in a semiautomatic fashion, a GLR chart with a prescribed rate of false alarms when as few as 50 phase I observations are available. A real example is used to illustrate the designing procedure.},
  keywords = {Automatic modeling,Autoregressive moving average model,Bootstrap,done,Statistical process control,Stochastic approximation,Uncertainty modeling},
  annotation = {\_eprint: https://doi.org/10.1198/004017008000000280},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Capizzi_Masarotto_2008_Practical_Design_of_Generalized_Likelihood_Ratio_Control_Charts_for.pdf;/home/dede/Zotero/storage/8CXVWLV6/004017008000000280.html}
}

@article{capizzi2009,
  title = {Bootstrap-Based Design of Residual Control Charts},
  author = {Capizzi, Giovanna and Masarotto, Guido},
  year = {2009},
  journal = {IIE Transactions},
  volume = {41},
  number = {4},
  pages = {275--286},
  publisher = {{Taylor \& Francis}},
  issn = {0740-817X},
  doi = {10.1080/07408170802120059},
  abstract = {One approach to monitoring autocorrelated data consists in applying a control chart to the residuals of a time series model estimated from process observations. Recent research shows that the impact of estimation error on the run length properties of the resulting charts is not negligible. In this paper a general strategy for implementing residual-based control schemes is investigated. The designing procedure uses the AR-sieve approximation assuming that the process allows an autoregressive representation of order infinity. The run length distribution is estimated using bootstrap resampling in order to account for uncertainty in the estimated parameters. Control limits that satisfy a given constraint on the false alarm rate are computed via stochastic approximation. The proposed procedure is investigated for three residual-based control charts: generalized likelihood ratio, cumulative sum and exponentially weighted moving average. Results show that the bootstrap approach safeguards against an undesirably high rate of false alarms. In addition, the out-of-control bootstrap chart sensitivity seems to be comparable to that of charts designed under the assumption that the estimated model is equal to the true generating process. [Supplementary materials are available for this article. Go to the publisher's online edition of IIE Transactions for the following free supplemental resource: Appendix]},
  keywords = {autocorrelated data,control charts,done,On-line quality control,sieve bootstrap,time series,uncertainty modeling},
  annotation = {\_eprint: https://doi.org/10.1080/07408170802120059},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Capizzi_Masarotto_2009_Bootstrap-based_design_of_residual_control_charts.pdf;/home/dede/Zotero/storage/VPPPQH29/07408170802120059.html}
}

@article{capizzi2010,
  title = {Self-{{Starting CUSCORE Control Charts}} for {{Individual Multivariate Observations}}},
  author = {Capizzi, Giovanna and Masarotto, Guido},
  year = {2010},
  journal = {Journal of Quality Technology},
  volume = {42},
  number = {2},
  pages = {136--151},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2010.11917812},
  abstract = {In some manufacturing settings, such as during process start-up and in the case of short production runs, process parameters are unknown, and Phase I samples cannot be gathered to accurately estimate control limits for prospective monitoring. Self-starting charts can be applied to these low-volume applications. In this article, two new self-starting multivariate control charts, both based on a CUSCORE-type procedure, are proposed for monitoring the unknown mean of a multivariate normal distribution. These charting procedures, which weight current observations according to the information contained in the fault signature, are able to outperform the previously suggested self-starting charts, which neglect the dynamic pattern of the mean change.},
  keywords = {Change-Point Detection,Q-Chart,Recursive Residuals,skimmed,Statistical Process Control},
  annotation = {\_eprint: https://doi.org/10.1080/00224065.2010.11917812},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Capizzi_Masarotto_2010_Self-Starting_CUSCORE_Control_Charts_for_Individual_Multivariate_Observations.pdf;/home/dede/Zotero/storage/XH2BUMAR/00224065.2010.html}
}

@inproceedings{capizzi2010a,
  title = {Linear Profile Monitoring Using an Adaptive {{EWMA}} Control Chart},
  booktitle = {2010 {{IEEE International Conference}} on {{Industrial Engineering}} and {{Engineering Management}}},
  author = {Capizzi, Giovanna and Masarotto, Guido},
  year = {2010},
  month = dec,
  pages = {1425--1429},
  issn = {2157-362X},
  doi = {10.1109/IEEM.2010.5674339},
  abstract = {A novel adaptive Exponentially Weighted Moving Average scheme, called AEWMA3, is suggested for monitoring the stability of a linear relationship between a quality characteristic and an explanatory variable. Comparisons with an analogous non-adaptive scheme points to a better overall performance of the suggested scheme in a variety of out-of-control scenarios. In addition, since a single AEWMA3 chart is able to quickly detect both small and large shifts, its practical design is simpler than that of other competing control charts.},
  keywords = {Adaptive Control Charts,Control charts,Exponentially Weighted Moving Average,Least squares approximation,Mathematical model,Monitoring,Process control,Profile Monitoring,Smoothing methods,Stability analysis,Statistical Process Control},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Capizzi_Masarotto_2010_Linear_profile_monitoring_using_an_adaptive_EWMA_control_chart.pdf;/home/dede/Zotero/storage/FIC9I43C/5674339.html}
}

@article{capizzi2011,
  title = {A {{Least Angle Regression Control Chart}} for {{Multidimensional Data}}},
  author = {Capizzi, Giovanna and Masarotto, Guido},
  year = {2011},
  month = aug,
  journal = {Technometrics},
  volume = {53},
  number = {3},
  pages = {285--296},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1198/TECH.2011.10027},
  abstract = {In multidimensional applications, it is very rare that all variables shift at the same time. A statistical process control procedure would have superior efficiency when limited to the subset of variables likely responsible for the out-of-control conditions. The key idea of this article consists of combining a variable selection method with a multivariate control chart to detect changes in both the mean and variability of a multidimensional process with Gaussian errors. In particular, we develop a control chart for Phase II monitoring which integrates the least angle regression algorithm with a multivariate exponentially weighted moving average. Comparisons with related multivariate control schemes demonstrate the efficiency of the proposed control chart in a wide range of practical applications, including profile and multistage process monitoring. Further, the proposed scheme may also provide valuable diagnostic information for fault isolation. Supplemental materials, including an R package, are available online.},
  keywords = {Change-point detection,done,Exponentially weighted moving average,Multistage process,Profile monitoring,Statistical process control,Variable selection},
  annotation = {\_eprint: https://doi.org/10.1198/TECH.2011.10027},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Capizzi_Masarotto_2011_A_Least_Angle_Regression_Control_Chart_for_Multidimensional_Data.pdf;/home/dede/Zotero/storage/GP39YYTI/TECH.2011.html}
}

@article{capizzi2012,
  title = {Adaptive {{Generalized Likelihood Ratio Control Charts}} for {{Detecting Unknown Patterned Mean Shifts}}},
  author = {Capizzi, Giovanna and Masarotto, Guido},
  year = {2012},
  month = oct,
  journal = {Journal of Quality Technology},
  volume = {44},
  number = {4},
  pages = {281--303},
  issn = {0022-4065, 2575-6230},
  doi = {10.1080/00224065.2012.11917902},
  langid = {english},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Capizzi_Masarotto_2012_Adaptive_Generalized_Likelihood_Ratio_Control_Charts_for_Detecting_Unknown.pdf}
}

@article{capizzi2012a,
  title = {An {{Enhanced Control Chart}} for {{Start-Up Processes}} and {{Short Runs}}},
  author = {Capizzi, Giovanna and Masarotto, Guido},
  year = {2012},
  journal = {Quality Technology \& Quantitative Management},
  volume = {9},
  number = {2},
  pages = {189--202},
  issn = {1684-3703},
  doi = {10.1080/16843703.2012.11673285},
  langid = {english},
  keywords = {toRead},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Capizzi_Masarotto_2012_An_Enhanced_Control_Chart_for_Start-Up_Processes_and_Short_Runs.pdf}
}

@article{capizzi2013,
  title = {Phase {{I Distribution-Free Analysis}} of {{Univariate Data}}},
  author = {Capizzi, Giovanna and Masarotto, Guido},
  year = {2013},
  month = jul,
  journal = {Journal of Quality Technology},
  volume = {45},
  pages = {273--284},
  doi = {10.1080/00224065.2013.11917938},
  abstract = {In Phase I analysis, data are used retrospectively for checking process stability and defining the in-control state. Most Phase I control charts are based on the assumption of normally distributed observations. However, distribution-free methods appear to be ideal candidates for Phase I applications. Indeed, because little information is available, it is difficult to validate a distributional assumption in Phase I or at least at its beginning stage. In addition, as has been noted in the literature, this assumption cannot be checked before process stability is established. In this article, we propose a new distribution-free Phase I procedure for univariate observations. The suggested method, based on recursive segmentation and permutation, detects single or multiple mean and/or scale shifts. A simulation study shows that our method compares favorably with parametric control charts when the process is normally distributed and performs better than other nonparametric control charts when the process distribution is skewed or heavy tailed. An R package can be found in the supplemental materials.},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Capizzi_Masarotto_2013_Phase_I_Distribution-Free_Analysis_of_Univariate_Data.pdf}
}

@article{capizzi2015,
  title = {Recent {{Advances}} in {{Process Monitoring}}: {{Nonparametric}} and {{Variable-Selection Methods}} for {{Phase I}} and {{Phase II}}},
  shorttitle = {Recent {{Advances}} in {{Process Monitoring}}},
  author = {Capizzi, Giovanna},
  year = {2015},
  month = jan,
  journal = {Quality Engineering},
  volume = {27},
  number = {1},
  pages = {44--67},
  issn = {0898-2112, 1532-4222},
  doi = {10.1080/08982112.2015.968046},
  langid = {english},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Capizzi_2015_Recent_Advances_in_Process_Monitoring.pdf}
}

@inproceedings{capizzi2015a,
  title = {Comparison of {{Phase II Control Charts Based}} on {{Variable Selection Methods}}},
  author = {Capizzi, G. and Masarotto, G.},
  year = {2015},
  doi = {10.1007/978-3-319-12355-4_10},
  abstract = {VS-based control charts are compared in various out-of-control scenarios characterizing modern manufacturing environments such as high-dimensional data, profile, and multistage process monitoring to provide practical guidelines for choosing a suitable VS-based monitoring scheme. In recent years, control charts based on variable selection (VS) algorithms have been suggested for monitoring multivariate data. These charts share the common idea that process faults usually affect a small fraction of the monitored quality characteristics. Thus, VS methods can be used to identify the subset of the variables for which the shift may have occurred. However, the suggested VS-based control charts differ in many aspects such as the particular VS algorithm and the type of control statistic. In this paper, we compare VS-based control charts in various out-of-control scenarios characterizing modern manufacturing environments such as high-dimensional data, profile, and multistage process monitoring. The main aim of this paper is to provide practical guidelines for choosing a suitable VS-based monitoring scheme.},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Capizzi_Masarotto_2015_Comparison_of_Phase_II_Control_Charts_Based_on_Variable_Selection_Methods.pdf}
}

@article{capizzi2016,
  title = {Efficient Control Chart Calibration by Simulated Stochastic Approximation},
  author = {Capizzi, Giovanna and Masarotto, Guido},
  year = {2016},
  journal = {IIE Transactions},
  volume = {48},
  number = {1},
  pages = {57--65},
  publisher = {{Taylor \& Francis}},
  issn = {0740-817X},
  doi = {10.1080/0740817X.2015.1055392},
  abstract = {The accurate determination of control limits is crucial in statistical process control. The usual approach consists in computing the limits so that the in-control run-length distribution has some desired properties; for example, a prescribed mean. However, as a consequence of the increasing complexity of process data, the run-length of many control charts discussed in the recent literature can be studied only through simulation. Furthermore, in some scenarios, such as profile and autocorrelated data monitoring, the limits cannot be tabulated in advance, and when different charts are combined, the control limits depend on a multidimensional vector of parameters. In this article, we propose the use of stochastic approximation methods for control chart calibration and discuss enhancements for their implementation (e.g., the initialization of the algorithm, an adaptive choice of the gain, a suitable stopping rule for the iterative process, and the advantages of using multicore workstations). Examples are used to show that simulated stochastic approximation provides a reliable and fully automatic approach for computing the control limits in complex applications. An R package implementing the algorithm is available in the supplemental materials.},
  keywords = {control charts,done,multi-chart schemes,statistical process control,Stochastic approximation,stochastic root finding},
  annotation = {\_eprint: https://doi.org/10.1080/0740817X.2015.1055392},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Capizzi_Masarotto_2016_Efficient_control_chart_calibration_by_simulated_stochastic_approximation.pdf;/home/dede/Zotero/storage/DULZS8HW/0740817x.2015.html}
}

@article{capizzi2017,
  title = {Phase {{I Distribution-Free Analysis}} of {{Multivariate Data}}},
  author = {Capizzi, Giovanna and Masarotto, Guido},
  year = {2017},
  month = oct,
  journal = {Technometrics},
  volume = {59},
  number = {4},
  pages = {484--495},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.2016.1272494},
  abstract = {In this study, a new distribution-free Phase I control chart for retrospectively monitoring multivariate data is developed. The suggested approach, based on the multivariate signed ranks, can be applied to individual or subgrouped data for detection of location shifts with an arbitrary pattern (e.g., isolated, transitory, sustained, progressive, etc.). The procedure is complemented with a LASSO-based post-signal diagnostic method for identification of the shifted variables. A simulation study shows that the method compares favorably with parametric control charts when the process is normally distributed, and largely outperforms other multivariate nonparametric control charts when the process distribution is skewed or heavy-tailed. An R package can be found in the supplementary material.},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Capizzi_Masarotto_2017_Phase_I_Distribution-Free_Analysis_of_Multivariate_Data.pdf;/home/dede/Zotero/storage/BRPHH56T/00401706.2016.html}
}

@article{capizzi2020,
  title = {Guaranteed In-Control Control Chart Performance with Cautious Parameter Learning},
  author = {Capizzi, Giovanna and Masarotto, Guido},
  year = {2020},
  journal = {Journal of Quality Technology},
  volume = {52},
  number = {4},
  pages = {385--403},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2019.1640096},
  abstract = {Parameter estimation has a large impact on control chart performance. Recently, widened control limits have been suggested to guarantee an acceptable in-control behavior. However, the consequence is a reduced ability to detect a real change in the process. In order to overcome this undesired effect, we explore an alternative design based on a delayed updating of parameter estimates. We consider an application to the Shewhart X, EWMA, and CUSUM control charts for the process mean. This approach is simple to implement, reduces the variation of the in-control average run lengths, and significantly improves the out-of-control performance.},
  keywords = {control limits,done,estimation effects,recursive estimation,statistical process control,statistical process monitoring},
  annotation = {\_eprint: https://doi.org/10.1080/00224065.2019.1640096},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Capizzi_Masarotto_2020_Guaranteed_in-control_control_chart_performance_with_cautious_parameter_learning.pdf;/home/dede/Zotero/storage/G7692GCF/00224065.2019.html}
}

@article{caron2008,
  title = {Bayesian {{Inference}} for {{Linear Dynamic Models With Dirichlet Process Mixtures}}},
  author = {Caron, F. and Davy, M. and Doucet, A. and Duflos, E. and Vanheeghe, P.},
  year = {2008},
  month = jan,
  journal = {IEEE Transactions on Signal Processing},
  volume = {56},
  number = {1},
  pages = {71--84},
  issn = {1941-0476},
  doi = {10.1109/TSP.2007.900167},
  abstract = {Using Kalman techniques, it is possible to perform optimal estimation in linear Gaussian state-space models. Here, we address the case where the noise probability density functions are of unknown functional form. A flexible Bayesian nonparametric noise model based on Dirichlet process mixtures is introduced. Efficient Markov chain Monte Carlo and sequential Monte Carlo methods are then developed to perform optimal batch and sequential estimation in such contexts. The algorithms are applied to blind deconvolution and change point detection. Experimental results on synthetic and real data demonstrate the efficiency of this approach in various contexts.},
  keywords = {Bayesian methods,Bayesian nonparametrics,Change detection algorithms,Dirichlet process mixture (DPM),Filtering,Gaussian noise,Kalman filters,Markov chain Monte Carlo (MCMC),Monte Carlo methods,Noise shaping,particle filter,Probability density function,Rao-Blackwellization,Raoâ€“Blackwellization,Smoothing methods,State estimation,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Caron_et_al_2008_Bayesian_Inference_for_Linear_Dynamic_Models_With_Dirichlet_Process_Mixtures.pdf;/home/dede/Zotero/storage/JA68Y4TS/4359522.html}
}

@article{carpenter1997,
  title = {Examining {{Associations}} between {{Occupation}} and {{Health}} by {{Using Routinely Collected Data}}},
  author = {Carpenter, Lucy M. and Maconochie, Noreen E. S. and Roman, Eve and Cox, D. R.},
  year = {1997},
  journal = {Journal of the Royal Statistical Society. Series A (Statistics in Society)},
  volume = {160},
  number = {3},
  pages = {507--521},
  publisher = {{[Wiley, Royal Statistical Society]}},
  issn = {0964-1998},
  abstract = {When examining a large number of associations simultaneously, as happens when routinely collected data are used to assess associations between occupation and health, it is not obvious how best to identify associations requiring further investigation since some risks may be high, or low, by chance alone. We have developed an approach to deal with this problem which is relatively easy to apply and appropriate to applications where data are not too sparse. Observed to expected ratios are estimated using an empirical Bayes procedure. Anomalous associations can be seen as outliers in a normal probability plot of the log-ratios. The method is illustrated in the analysis of 252 000 cancers registered in men in England during 1981-87.}
}

@article{carvalho2010,
  title = {The Horseshoe Estimator for Sparse Signals},
  author = {Carvalho, C. M. and Polson, N. G. and Scott, J. G.},
  year = {2010},
  month = jun,
  journal = {Biometrika},
  volume = {97},
  number = {2},
  pages = {465--480},
  issn = {0006-3444, 1464-3510},
  doi = {10.1093/biomet/asq017},
  langid = {english},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Carvalho_et_al_2010_The_horseshoe_estimator_for_sparse_signals.pdf}
}

@article{casella1985,
  title = {An {{Introduction}} to {{Empirical Bayes Data Analysis}}},
  author = {Casella, George},
  year = {1985},
  journal = {The American Statistician},
  volume = {39},
  number = {2},
  pages = {83--87},
  publisher = {{[American Statistical Association, Taylor \& Francis, Ltd.]}},
  issn = {0003-1305},
  doi = {10.2307/2682801},
  abstract = {Empirical Bayes methods have been shown to be powerful data-analysis tools in recent years. The empirical Bayes model is much richer than either the classical or the ordinary Bayes model and often provides superior estimates of parameters. An introduction to some empirical Bayes methods is given, and these methods are illustrated with two examples.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Casella_1985_An_Introduction_to_Empirical_Bayes_Data_Analysis.pdf}
}

@book{casella2002,
  title = {{Statistical Inference}},
  author = {Casella, George and Berger, Roger L.},
  year = {2002},
  publisher = {{Duxbury Pr}},
  address = {{Australia ; Pacific Grove, CA}},
  abstract = {This book builds theoretical statistics from the first principles of probability theory. Starting from the basics of probability, the authors develop the theory of statistical inference using techniques, definitions, and concepts that are statistical and are natural extensions and consequences of previous concepts. Intended for first-year graduate students, this book can be used for students majoring in statistics who have a solid mathematics background. It can also be used in a way that stresses the more practical uses of statistical theory, being more concerned with understanding basic statistical concepts and deriving reasonable statistical procedures for a variety of situations, and less concerned with formal optimality investigations.},
  isbn = {978-0-534-24312-8},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Casella_Berger_2002_Statistical_Inference.pdf;/home/dede/Documents/MEGA/universita/zotero-pdf/Casella_Berger_2002_Statistical_Inference2.pdf}
}

@article{castagliola2013,
  title = {The Variable Sample Size t Control Chart for Monitoring Short Production Runs},
  author = {Castagliola, Philippe and Celano, Giovanni and Fichera, Sergio and Nenes, George},
  year = {2013},
  month = jun,
  journal = {The International Journal of Advanced Manufacturing Technology},
  volume = {66},
  number = {9},
  pages = {1353--1366},
  issn = {1433-3015},
  doi = {10.1007/s00170-012-4413-8},
  abstract = {Starting the online monitoring of a quality characteristic by means of a control chart at the beginning of a short production run is often a challenging issue for quality practitioners: in fact, the frequent absence of preliminary information prevents from getting a precise estimate of the characteristic mean and standard deviation. Furthermore, for short runs having a finite rolling horizon, the number of inspections scheduled within the run can be too small to get sufficient samples allowing the phase~I implementation of the chart to be completed. Recently, t control charts have been proposed as efficient means to overcome this problem because they do not need any phase~I tentative control limits definition or preliminary process knowledge. In this paper, a variable sample size (VSS) version of the t~chart is proposed. Adaptive control charts have been implemented with success in long runs: here, the performance of the variable sample size strategy is investigated for a chart used in a short run. The statistical performance of the VSS t~chart is compared with the one of the fixed-parameter (FP) t~chart for both scenarios of fixed and unknown shift size, with the latter situation being frequent in short-run manufacturing environments. An extensive numerical investigation reveals the potential benefits of the proposed chart. When the statistical design is optimized with respect to a fixed value of the shift size {$\delta$}, the VSS t~chart has a better statistical performance than the FP t~chart for moderate to large values of {$\delta$}. Conversely, for the unknown shift size condition, the VSS t~chart always outperforms the FP t~chart for in-control average sample sizes ASS0\,{$>$}\,7. An illustrative example shows the implementation of the VSS during the production of a finite lot of mechanical parts.},
  langid = {english}
}

@article{castillo1995,
  title = {A Kalman Filtering Process Control Scheme with an Application in Semiconductor Short Run Manufacturing},
  author = {Castillo, Enrique Del and Montgomery, Douglas C.},
  year = {1995},
  journal = {Quality and Reliability Engineering International},
  volume = {11},
  number = {2},
  pages = {101--105},
  issn = {0748-8017},
  doi = {10.1002/qre.4680110205},
  abstract = {A quality control chart for monitoring a short run process during the start-up phase is presented in this article. The chart is based on the Kalman filter recursive equations being applied to a stable process where the process variance is unknown prior to the start of the production run. The run length properties of this control scheme are discussed. It is shown that for the proposed scheme the run length properties are independent of the unknown process variance and that these properties are appropriate for monitoring a stable process during start-up. An economic model for the optimal design of the control scheme is presented and illustrated with a wet etching process used in semiconductor manufacturing.}
}

@article{castillo1996,
  title = {A Review of Statistical Process Control Techniques for Short Run Manufacturing Systems},
  author = {Castillo, Enrique Del and Grayson, James M. and Montgomery, Douglas C. and Runger, George C.},
  year = {1996},
  month = jan,
  journal = {Communications in Statistics - Theory and Methods},
  volume = {25},
  number = {11},
  pages = {2723--2737},
  publisher = {{Taylor \& Francis}},
  issn = {0361-0926},
  doi = {10.1080/03610929608831866},
  abstract = {Because manufacturing lot sizes continue to shrink, statistical process control methods for short production runs are increasingly important. We review and comment on the assumptions, advantages and disadvantages of alternatives, Traditional methods well as more recent developments are described and contrasted.},
  keywords = {average run length,control charts,just-in-time,quality control},
  annotation = {\_eprint: https://doi.org/10.1080/03610929608831866},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Castillo_et_al_1996_A_review_of_statistical_process_control_techniques_for_short_run_manufacturing.pdf;/home/dede/Zotero/storage/2SZ9J43N/03610929608831866.html}
}

@article{catalina2020,
  title = {Projection {{Predictive Inference}} for {{Generalized Linear}} and {{Additive Multilevel Models}}},
  author = {Catalina, Alejandro and B{\"u}rkner, Paul-Christian and Vehtari, Aki},
  year = {2020},
  month = oct,
  journal = {arXiv:2010.06994 [stat]},
  eprint = {2010.06994},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Projection predictive inference is a decision theoretic Bayesian approach that decouples model estimation from decision making. Given a reference model previously built including all variables present in the data, projection predictive inference projects its posterior onto a constrained space of a subset of variables. Variable selection is then performed by sequentially adding relevant variables until predictive performance is satisfactory. Previously, projection predictive inference has been demonstrated only for generalized linear models (GLMs) and Gaussian processes (GPs) where it showed superior performance to competing variable selection procedures. In this work, we extend projection predictive inference to support variable and structure selection for generalized linear multilevel models (GLMMs) and generalized additive multilevel models (GAMMs). Our simulative and real-word experiments demonstrate that our method can drastically reduce the model complexity required to reach reference predictive performance and achieve good frequency properties.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Computation,Statistics - Methodology,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Catalina_et_al_2020_Projection_Predictive_Inference_for_Generalized_Linear_and_Additive_Multilevel.pdf;/home/dede/Zotero/storage/KRZ8GSJ7/2010.html}
}

@incollection{cavanaugh2016,
  title = {Model {{Selection}}: {{Bayesian Information Criterion}}},
  shorttitle = {Model {{Selection}}},
  booktitle = {Wiley {{StatsRef}}: {{Statistics Reference Online}}},
  author = {Cavanaugh, Joseph E.},
  year = {2016},
  pages = {1--3},
  publisher = {{American Cancer Society}},
  doi = {10.1002/9781118445112.stat00247.pub2},
  abstract = {This article presents an overview of the Bayesian information criterion (BIC), along with its motivation and some of its asymptotic optimality properties. It also compares and contrasts the criterion to the Akaike information criterion (AIC).},
  copyright = {Copyright \textcopyright{} 2016 John Wiley \& Sons, Ltd. All rights reserved.},
  isbn = {978-1-118-44511-2},
  langid = {english},
  keywords = {Akaike information criterion,Bayes factors,model selection criterion,Schwarz information criterion,todo},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118445112.stat00247.pub2},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Cavanaugh_2016_Model_Selection.pdf;/home/dede/Zotero/storage/97Y85P2H/9781118445112.stat00247.html}
}

@inproceedings{cazabet2010,
  title = {Detection of {{Overlapping Communities}} in {{Dynamical Social Networks}}},
  booktitle = {Proceedings of the 2010 {{IEEE Second International Conference}} on {{Social Computing}}, {{SocialCom}} / {{IEEE International Conference}} on {{Privacy}}, {{Security}}, {{Risk}} and {{Trust}}, {{PASSAT}} 2010, {{Minneapolis}}, {{Minnesota}}, {{USA}}, {{August}} 20-22, 2010},
  author = {Cazabet, Remy and Amblard, Fr{\'e}d{\'e}ric and Hanachi, Chihab},
  year = {2010},
  month = aug,
  pages = {314},
  doi = {10.1109/SocialCom.2010.51},
  abstract = {Community detection on networks is a well-known problem encountered in many fields, for which the existing algorithms are inefficient 1) at capturing overlaps in-between communities, 2) at detecting communities having disparities in size and density 3) at taking into account the networks' dynamics. In this paper, we propose a new algorithm (iLCD) for community detection using a radically new approach. Taking into account the dynamics of the network, it is designed for the detection of strongly overlapping communities. We first explain the main principles underlying the iLCD algorithm, introducing the two notions of intrinsic communities and longitudinal detection, and detail the algorithm. Then, we illustrate its efficiency in the case of a citation network, and then compare it with existing most efficient algorithms using a standard generator of community-based networks, the LFR benchmark.},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Cazabet_et_al_2010_Detection_of_Overlapping_Communities_in_Dynamical_Social_Networks.pdf}
}

@article{centofanti2021,
  title = {Functional {{Regression Control Chart}}},
  author = {Centofanti, Fabio and Lepore, Antonio and Menafoglio, Alessandra and Palumbo, Biagio and Vantini, Simone},
  year = {2021},
  month = jul,
  journal = {Technometrics},
  volume = {63},
  number = {3},
  pages = {281--294},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.2020.1753581},
  abstract = {The modern development of data acquisition technologies in many industrial processes is facilitating the collection of quality characteristics that are apt to be modeled as functions, which are usually referred to as profiles. At the same time, measurements of concurrent variables, which are related to the quality characteristic profiles, are often available in a functional form as well, and usually referred to as covariates. To adjust the monitoring of the quality characteristic profiles by the effect of this additional information, a new functional control chart is elaborated on the residuals obtained from a function-on-function linear regression of the quality characteristic profile on the functional covariates. By means of a Monte Carlo simulation study, the proposed control chart is compared with other control charts already appeared in the literature and some remarks are given on its use in presence of covariate mean shifts. Furthermore, a real-case study in the shipping industry is presented with the purpose of monitoring ship fuel consumption and thus, CO2 emissions from a Ro-Pax ship, with particular regard to detecting their reduction after a specific energy efficiency initiative.},
  keywords = {Functional data analysis,Multivariate functional linear regression,Profile monitoring,Statistical process control,todo},
  annotation = {\_eprint: https://doi.org/10.1080/00401706.2020.1753581},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Centofanti_et_al_2021_Functional_Regression_Control_Chart.pdf;/home/dede/Zotero/storage/LYRFPUSW/00401706.2020.html}
}

@book{ceron2013,
  title = {{Social Media e Sentiment Analysis: L'evoluzione dei fenomeni sociali attraverso la Rete}},
  shorttitle = {{Social Media e Sentiment Analysis}},
  author = {Ceron, Andrea and Curini, Luigi and Iacus, Stefano Maria},
  year = {2013},
  month = may,
  publisher = {{Springer}},
  abstract = {Due miliardi e mezzo di utenti internet, oltre un miliardo di account Facebook, 550 milioni di profili Twitter. Che parlano, discutono, si confrontano sui temi pi\`u svariati. Un flusso in continuo divenire di informazioni che d\`a sostanza ogni giorno al mondo dei Big Data. Ma come si analizza concretamente il "sentiment" della Rete? Quali sono i pregi e i limiti dei diversi metodi esistenti? E a quali domande possiamo dare una risposta? Dopo aver presentato le varie tecniche di analisi testuale applicate ai social media, questo libro discute di come l'informazione presente in Rete sia in grado di aiutarci a meglio comprendere il presente e a fare previsioni sul futuro riguardo a una molteplicit\`a di fenomeni sociali, che spaziano dall'andamento dei mercati finanziari, alla diffusione di malattie, alle rivolte e ai sommovimenti popolari fino ai risultati dei talent show, prima di concentrarsi su due casi specifici: l'andamento della felicit\`a degli italiani giorno per giorno, e i risultati delle campagne elettorali in Francia, Stati Uniti e Italia tra il 2012 e il 2013.},
  isbn = {978-88-470-5531-5},
  langid = {italian},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Ceron_et_al_2013_Social_Media_e_Sentiment_Analysis.pdf}
}

@book{ceron2016,
  title = {Politics and {{Big Data}}: {{Nowcasting}} and {{Forecasting Elections}} with {{Social Media}}},
  shorttitle = {Politics and {{Big Data}}},
  author = {Ceron, Andrea and Curini, Luigi and Iacus, Stefano Maria},
  year = {2016},
  publisher = {{Routledge}},
  address = {{London ; New York, NY}},
  abstract = {The importance of social media as a way to monitor an electoral campaign is well established. Day-by-day, hour-by-hour evaluation of the evolution of online ideas and opinion allows observers and scholars to monitor trends and momentum in public opinion well before traditional polls. However, there are difficulties in recording and analyzing often brief, unverified comments while the unequal age, gender, social and racial representation among social media users can produce inaccurate forecasts of final polls. Reviewing the different techniques employed using social media to nowcast and forecast elections, this book assesses its achievements and limitations while presenting a new technique of "sentiment analysis" to improve upon them. The authors carry out a meta-analysis of the existing literature to show the conditions under which social media-based electoral forecasts prove most accurate while new case studies from France, the United States and Italy demonstrate how much more accurate "sentiment analysis" can prove.},
  isbn = {978-1-4724-6666-2},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Ceron_et_al_2016_Politics_and_Big_Data.pdf}
}

@article{ceron2016a,
  title = {{{iSA}}: {{A}} Fast, Scalable and Accurate Algorithm for Sentiment Analysis of Social Media Content},
  shorttitle = {{{iSA}}},
  author = {Ceron, Andrea and Curini, Luigi and Iacus, Stefano Maria},
  year = {2016},
  month = nov,
  journal = {Information Sciences},
  volume = {367--368},
  pages = {105--124},
  issn = {0020-0255},
  doi = {10.1016/j.ins.2016.05.052},
  abstract = {We present iSA (integrated sentiment analysis), a novel algorithm designed for social networks and Web 2.0 sphere (Twitter, blogs, etc.) opinion analysis, i.e. developed for the digital environments characterized by abundance of noise compared to the amount of information. Instead of performing an individual classification and then aggregate the predicted values, iSA directly estimates the aggregated distribution of opinions. Based on supervised hand-coding rather than NLP techniques or ontological dictionaries, iSA is a language-agnostic algorithm (based on human coders' abilities). iSA exploits a dimensionality reduction approach which makes it scalable, fast, memory efficient, stable and statistically accurate. The cross-tabulation of opinions is possible with iSA thanks to its stability. Through empirical analysis it will be shown when iSA outperforms machine learning techniques of individual classification (e.g. SVM, Random Forests, etc) as well as the only other alternative for aggregated sentiment analysis known as ReadMe.},
  langid = {english},
  keywords = {done,Opinion mining,Sentiment analysis,Twitter analysis},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Ceron_et_al_2016_iSA.pdf}
}

@article{chae2012,
  title = {Spatiotemporal Social Media Analytics for Abnormal Event Detection and Examination Using Seasonal-Trend Decomposition: 2012 {{IEEE Conference}} on {{Visual Analytics Science}} and {{Technology}}, {{VAST}} 2012},
  shorttitle = {Spatiotemporal Social Media Analytics for Abnormal Event Detection and Examination Using Seasonal-Trend Decomposition},
  author = {Chae, Junghoon and Thom, Dennis and Bosch, Harald and Jang, Yun and Maciejewski, Ross and Ebert, David S. and Ertl, Thomas},
  year = {2012},
  month = dec,
  journal = {IEEE Conference on Visual Analytics Science and Technology 2012, VAST 2012 - Proceedings},
  series = {{{IEEE Conference}} on {{Visual Analytics Science}} and {{Technology}} 2012, {{VAST}} 2012 - {{Proceedings}}},
  pages = {143--152},
  issn = {9781467347532},
  doi = {10.1109/VAST.2012.6400557},
  abstract = {Recent advances in technology have enabled social media services to support space-time indexed data, and internet users from all over the world have created a large volume of time-stamped, geo-located data. Such spatiotemporal data has immense value for increasing situational awareness of local events, providing insights for investigations and understanding the extent of incidents, their severity, and consequences, as well as their time-evolving nature. In analyzing social media data, researchers have mainly focused on finding temporal trends according to volume-based importance. Hence, a relatively small volume of relevant messages may easily be obscured by a huge data set indicating normal situations. In this paper, we present a visual analytics approach that provides users with scalable and interactive social media data analysis and visualization including the exploration and examination of abnormal topics and events within various social media data sources, such as Twitter, Flickr and YouTube. In order to find and understand abnormal events, the analyst can first extract major topics from a set of selected messages and rank them probabilistically using Latent Dirichlet Allocation. He can then apply seasonal trend decomposition together with traditional control chart methods to find unusual peaks and outliers within topic time series. Our case studies show that situational awareness can be improved by incorporating the anomaly and trend examination techniques into a highly interactive visual analysis process.},
  keywords = {H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Information filtering; relevance feedback,H.5.2 [Information Interfaces and Presentation]: User Interfaces - GUI,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Chae_et_al_2012_Spatiotemporal_social_media_analytics_for_abnormal_event_detection_and.pdf}
}

@article{chakraborti2008,
  title = {Phase {{I Statistical Process Control Charts}}: {{An Overview}} and {{Some Results}}},
  shorttitle = {Phase {{I Statistical Process Control Charts}}},
  author = {Chakraborti, S. and Human, S. W. and Graham, M. A.},
  year = {2008},
  month = dec,
  journal = {Quality Engineering},
  volume = {21},
  number = {1},
  pages = {52--62},
  publisher = {{Taylor \& Francis}},
  issn = {0898-2112},
  doi = {10.1080/08982110802445561},
  abstract = {In practice, Phase I analysis constitutes an integral part of an overall SPC program in which control charts play a crucial role. An overview of the literature on Phase I parametric control charts for univariate variables data is presented. Since the Phase I signaling events are dependent and multiple signaling events are to be dealt with simultaneously in making an in-control or out-of-control decision, the joint distribution of the charting statistics is used to control the false alarm probability, which is defined as the probability of at least one false alarm, while designing the charts. An example is given. Concluding remarks include suggestions regarding future research problems.},
  keywords = {change-point,exponential distribution,false alarm probability (FAP),false alarm rate (FAR),mean,normal distribution,Phase II,prospective,reference only,retrospective,variance},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Chakraborti_et_al_2008_Phase_I_Statistical_Process_Control_Charts.pdf}
}

@article{chakraborti2019,
  title = {Nonparametric (Distribution-Free) Control Charts: {{An}} Updated Overview and Some Results},
  shorttitle = {Nonparametric (Distribution-Free) Control Charts},
  author = {Chakraborti, Subha and Graham, Marien},
  year = {2019},
  month = may,
  journal = {Quality Engineering},
  volume = {31},
  pages = {1--22},
  doi = {10.1080/08982112.2018.1549330},
  abstract = {Control charts that are based on assumption(s) of a specific form for the underlying process distribution are referred to as parametric control charts. There are many applications where there is insufficient information to justify such assumption(s) and, consequently, control charting techniques with a minimal set of distributional assumption requirements are in high demand. To this end, nonparametric or distribution-free control charts have been proposed in recent years. The charts have stable in-control properties, are robust against outliers and can be surprisingly efficient in comparison with their parametric counterparts. Chakraborti and some of his colleagues provided review papers on nonparametric control charts in 2001, 2007 and 2011, respectively. These papers have been received with considerable interest and attention by the community. However, the literature on nonparametric statistical process/quality control/monitoring has grown exponentially and because of this rapid growth, an update is deemed necessary. In this article, we bring these reviews forward to 2017, discussing some of the latest developments in the area. Moreover, unlike the past reviews, which did not include the multivariate charts, here we review both univariate and multivariate nonparametric control charts. We end with some concluding remarks.},
  keywords = {doing},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Chakraborti_Graham_2019_Nonparametric_(distribution-free)_control_charts.pdf}
}

@article{chan2020,
  title = {The Multi-Armed Bandit Problem: {{An}} Efficient Nonparametric Solution},
  shorttitle = {The Multi-Armed Bandit Problem},
  author = {Chan, Hock Peng},
  year = {2020},
  month = feb,
  journal = {The Annals of Statistics},
  volume = {48},
  number = {1},
  pages = {346--373},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/19-AOS1809},
  abstract = {Lai and Robbins (Adv. in Appl. Math. 6 (1985) 4\textendash 22) and Lai (Ann. Statist. 15 (1987) 1091\textendash 1114) provided efficient parametric solutions to the multi-armed bandit problem, showing that arm allocation via upper confidence bounds (UCB) achieves minimum regret. These bounds are constructed from the Kullback\textendash Leibler information of the reward distributions, estimated from specified parametric families. In recent years, there has been renewed interest in the multi-armed bandit problem due to new applications in machine learning algorithms and data analytics. Nonparametric arm allocation procedures like \$\textbackslash epsilon \$-greedy, Boltzmann exploration and BESA were studied, and modified versions of the UCB procedure were also analyzed under nonparametric settings. However, unlike UCB these nonparametric procedures are not efficient under general parametric settings. In this paper, we propose efficient nonparametric procedures.},
  keywords = {62L05,efficiency,KL-UCB,subsampling,Thompson sampling,todo,UCB},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Chan_2020_The_multi-armed_bandit_problem.pdf;/home/dede/Zotero/storage/5SXT9CN9/19-AOS1809.html}
}

@book{chandler1973,
  title = {The {{Campaigns}} of {{Napoleon}}},
  author = {Chandler, David G.},
  year = {1973},
  month = mar,
  publisher = {{Scribner}},
  isbn = {978-0-02-523660-8},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Chandler_1973_The_Campaigns_of_Napoleon.epub}
}

@article{chang2017,
  title = {{{AEWMA}} t {{Control Chart}} for {{Short Production Runs}}},
  author = {Chang, Zhiyuan and Sun, Jinsheng},
  year = {2017},
  month = oct,
  journal = {Journal of Systems Science and Information},
  volume = {4},
  number = {5},
  pages = {444--459},
  publisher = {{De Gruyter}},
  issn = {2512-6660},
  doi = {10.21078/JSSI-2016-444-16},
  abstract = {Owing to the limited number of inspections during a short run process, it is impossible to get the correct estimate of the population mean and standard deviation during Phase I implementation of control chart. The t control chart proposed recently can overcome this problem. The EWMA t control chart has been proposed to monitor the process mean, but a single EWMA t control chart cannot perform well for small and large shifts simultaneously, which is known as the ``inertia problem''. The adaptive varying smoothing parameter EWMA (AEWMA) control chart can overcome the inertia problem. In this paper, the AEWMA t control chart for short run process is proposed. The truncated average run length and the probability of trigger a signal are adopted to test the performance of short run AEWMA t chart. Based on the investigation of the joint effect of control chart parameters on the performance of AEWMA t chart, a new optimization algorithm is proposed for statistical design of the AEWMA control chart. Simulations are performed for perfect and imperfect setup conditions, the results show that the AEWMA t control chart performs better than the EWMA t control chart.},
  langid = {english},
  keywords = {AEWMA control chart,Markov chain,short run,t chart,TARL},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Chang_Sun_2017_AEWMA_t_Control_Chart_for_Short_Production_Runs.pdf}
}

@book{chartrand2012,
  title = {{Mathematical Proofs: A Transition to Advanced Mathematics}},
  shorttitle = {{Mathematical Proofs}},
  author = {Chartrand, Gary and Polimeni, Albert D. and Zhang, Ping},
  year = {2012},
  edition = {3\textdegree{} edizione},
  publisher = {{Pearson College Div}},
  isbn = {978-0-321-79709-4},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Chartrand_et_al_2012_Mathematical_Proofs.pdf}
}

@article{chatterjee2009,
  title = {Distribution-Free Cumulative Sum Control Charts Using Bootstrap-Based Control Limits},
  author = {Chatterjee, Snigdhansu and Qiu, Peihua},
  year = {2009},
  journal = {The Annals of Applied Statistics},
  volume = {3},
  number = {1},
  pages = {349--369},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {1932-6157, 1941-7330},
  doi = {10.1214/08-AOAS197},
  abstract = {This paper deals with phase II, univariate, statistical process control when a set of in-control data is available, and when both the in-control and out-of-control distributions of the process are unknown. Existing process control techniques typically require substantial knowledge about the in-control and out-of-control distributions of the process, which is often difficult to obtain in practice. We propose (a) using a sequence of control limits for the cumulative sum (CUSUM) control charts, where the control limits are determined by the conditional distribution of the CUSUM statistic given the last time it was zero, and (b) estimating the control limits by bootstrap. Traditionally, the CUSUM control chart uses a single control limit, which is obtained under the assumption that the in-control and out-of-control distributions of the process are Normal. When the normality assumption is not valid, which is often true in applications, the actual in-control average run length, defined to be the expected time duration before the control chart signals a process change, is quite different from the nominal in-control average run length. This limitation is mostly eliminated in the proposed procedure, which is distribution-free and robust against different choices of the in-control and out-of-control distributions.},
  keywords = {Cumulative sum control charts,distribution-free procedures,done,nonparametric model,Resampling,robustness,statistical process control},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Chatterjee_Qiu_2009_Distribution-free_cumulative_sum_control_charts_using_bootstrap-based_control.pdf;/home/dede/Zotero/storage/E5XUXZES/08-AOAS197.html}
}

@book{chen2000,
  title = {Monte {{Carlo Methods}} in {{Bayesian Computation}}},
  author = {Chen, Ming-Hui and Shao, Qi-Man and Ibrahim, Joseph G.},
  year = {2000},
  series = {Springer {{Series}} in {{Statistics}}},
  publisher = {{Springer-Verlag}},
  address = {{New York}},
  doi = {10.1007/978-1-4612-1276-8},
  abstract = {Sampling from the posterior distribution and computing posterior quanti\- ties of interest using Markov chain Monte Carlo (MCMC) samples are two major challenges involved in advanced Bayesian computation. This book examines each of these issues in detail and focuses heavily on comput\- ing various posterior quantities of interest from a given MCMC sample. Several topics are addressed, including techniques for MCMC sampling, Monte Carlo (MC) methods for estimation of posterior summaries, improv\- ing simulation accuracy, marginal posterior density estimation, estimation of normalizing constants, constrained parameter problems, Highest Poste\- rior Density (HPD) interval calculations, computation of posterior modes, and posterior computations for proportional hazards models and Dirichlet process models. Also extensive discussion is given for computations in\- volving model comparisons, including both nested and nonnested models. Marginal likelihood methods, ratios of normalizing constants, Bayes fac\- tors, the Savage-Dickey density ratio, Stochastic Search Variable Selection (SSVS), Bayesian Model Averaging (BMA), the reverse jump algorithm, and model adequacy using predictive and latent residual approaches are also discussed. The book presents an equal mixture of theory and real applications.},
  isbn = {978-0-387-98935-8},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Chen_et_al_2000_Monte_Carlo_Methods_in_Bayesian_Computation.pdf;/home/dede/Zotero/storage/U7P3QNNE/9780387989358.html}
}

@book{chen2002,
  title = {Stochastic {{Approximation}} and {{Its Applications}}},
  author = {Chen, Han-Fu},
  year = {2002},
  month = aug,
  edition = {2002nd edition},
  publisher = {{Springer}},
  address = {{Dordrecht ; Boston}},
  isbn = {978-1-4020-0806-1},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Chen_2002_Stochastic_Approximation_and_Its_Applications.pdf}
}

@article{chen2010,
  title = {A Two-Sample Test for High-Dimensional Data with Applications to Gene-Set Testing},
  author = {Chen, Song Xi and Qin, Ying-Li},
  year = {2010},
  month = apr,
  journal = {The Annals of Statistics},
  volume = {38},
  number = {2},
  pages = {808--835},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/09-AOS716},
  abstract = {We propose a two-sample test for the means of high-dimensional data when the data dimension is much larger than the sample size. Hotelling's classical T2 test does not work for this ``large p, small n'' situation. The proposed test does not require explicit conditions in the relationship between the data dimension and sample size. This offers much flexibility in analyzing high-dimensional data. An application of the proposed test is in testing significance for sets of genes which we demonstrate in an empirical study on a leukemia data set.},
  keywords = {60K35,62G10,62H15,gene-set testing,high dimension,large p small n,martingale central limit theorem,multiple comparison,skimmed},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Chen_Qin_2010_A_two-sample_test_for_high-dimensional_data_with_applications_to_gene-set.pdf;/home/dede/Zotero/storage/3BL48N29/09-AOS716.html}
}

@inproceedings{chen2013,
  title = {Combinatorial {{Multi-Armed Bandit}}: {{General Framework}} and {{Applications}}},
  shorttitle = {Combinatorial {{Multi-Armed Bandit}}},
  booktitle = {Proceedings of the 30th {{International Conference}} on {{Machine Learning}}},
  author = {Chen, Wei and Wang, Yajun and Yuan, Yang},
  year = {2013},
  month = feb,
  pages = {151--159},
  publisher = {{PMLR}},
  issn = {1938-7228},
  abstract = {We define a general framework for a large class of combinatorial multi-armed bandit (CMAB) problems, where simple arms with unknown istributions  form \textbackslash em super arms. In each round, a super arm is played and the outcomes of its related simple arms are observed, which helps the selection of super arms in future rounds. The reward of the super arm depends on the outcomes of played arms, and it only needs to satisfy two mild assumptions, which allow a large class of nonlinear reward instances. We assume the availability of an ({$\alpha$},{$\beta$})-approximation oracle that takes the  means of the distributions of arms and outputs a super arm that with probability {$\beta$}generates  an {$\alpha$}fraction of the optimal expected reward. The objective of a CMAB algorithm is to minimize \textbackslash em ({$\alpha$},{$\beta$})-approximation regret, which is the difference in total expected reward between the {$\alpha\beta$}fraction of expected reward when always playing the optimal super arm, and the expected reward of playing super arms according to the algorithm. We provide CUCB algorithm that achieves O(\textbackslash log n) regret, where n is the number of rounds played, and we further provide distribution-independent bounds for a large class of reward functions. Our regret analysis is tight in that it matches the bound for classical MAB problem up to a constant factor, and it significantly improves the regret bound in a recent paper on combinatorial bandits with linear rewards. We apply our CMAB framework to two new applications, probabilistic maximum coverage (PMC) for online advertising and social influence maximization for viral marketing, both having nonlinear reward structures.},
  langid = {english},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Chen_et_al_2013_Combinatorial_Multi-Armed_Bandit.pdf}
}

@article{chen2013a,
  title = {Stress {{Functions}} for {{Nonlinear Dimension Reduction}}, {{Proximity Analysis}}, and {{Graph Drawing}}},
  author = {Chen, Lisha and Buja, Andreas},
  year = {2013},
  journal = {Journal of Machine Learning Research},
  volume = {14},
  number = {Apr},
  pages = {1145--1173},
  issn = {ISSN 1533-7928},
  abstract = {Multidimensional scaling (MDS) is the art of reconstructing pointsets (embeddings) from pairwise distance data, and as such it is at the basis of several approaches to nonlinear dimension reduction and manifold learning. At present, MDS lacks a unifying methodology as it consists of a discrete collection of proposals that differ in their optimization criteria, called ''stress functions''. To correct this situation we propose (1) to embed many of the extant stress functions in a parametric family of stress functions, and (2) to replace the ad hoc choice among discrete proposals with a principled parameter selection method. This methodology yields the following benefits and problem solutions: (a )It provides guidance in tailoring stress functions to a given data situation, responding to the fact that no single stress function dominates all others across all data situations; (b) the methodology enriches the supply of available stress functions; (c) it helps our understanding of stress functions by replacing the comparison of discrete proposals with a characterization of the effect of parameters on embeddings; (d) it builds a bridge to graph drawing, which is the related but not identical art of constructing embeddings from graphs.},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Chen_Buja_2013_Stress_Functions_for_Nonlinear_Dimension_Reduction,_Proximity_Analysis,_and.pdf}
}

@article{chen2015,
  title = {Cluster-{{Based Profile Analysis}} in {{Phase I}}},
  author = {Chen, Yajuan and Birch, Jeffrey B. and Woodall, William H.},
  year = {2015},
  month = jan,
  journal = {Journal of Quality Technology},
  volume = {47},
  number = {1},
  pages = {14--29},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2015.11918103},
  abstract = {Our proposed technique, referred to as cluster-based profile monitoring, incorporates a cluster-analysis phase to aid in determining the possible existence of profiles in the historical data set resulting from an out-of-control process. The proposed method first replaces the data from each sampled unit with an estimated profile, using some appropriate regression method, and clusters the profiles based on their estimated parameter vectors. This yields an initial main cluster, which contains at least half the profiles. The initial estimated parameters for the population average (PA) profile are obtained by fitting a linear mixed model to those profiles in the main cluster. Profiles that are not contained in the initial main cluster may be iteratively added to the main cluster and the mixed model used to update the estimated parameters for the PA profile. Those profiles contained in the final main cluster are considered as resulting from the in-control process while those not included are considered as resulting from an out-of-control process. A simulated example, a Monte Carlo study, and an application demonstrate the performance advantages of this proposed method over a non-cluster based method with respect to more accurate estimates of the PA parameters and improved classification performance criteria.When the profiles can be represented by m {$\rho$} \texttimes{} 1 vectors, the profile monitoring process is equivalent to the detection of multivariate outliers. For this reason, we also compare our proposed method to a popular method used to identify outliers when dealing with a multivariate response. Our study demonstrates that, when the out-of-control process corresponds to a sustained shift, the cluster-based method using the successive difference estimator is clearly the superior method, among those methods we considered, based on all performance criteria.},
  keywords = {Control Chart,done,Mixed Models,Outliers,Profile Monitoring,Robust,Statistical Process Control},
  annotation = {\_eprint: https://doi.org/10.1080/00224065.2015.11918103},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Chen_et_al_2015_Cluster-Based_Profile_Analysis_in_Phase_I.pdf;/home/dede/Zotero/storage/2V3AAXHB/00224065.2015.html}
}

@article{chen2016,
  title = {A {{Distribution-Free Multivariate Control Chart}}},
  author = {Chen, Nan and Zi, Xuemin and Zou, Changliang},
  year = {2016},
  month = oct,
  journal = {Technometrics},
  volume = {58},
  number = {4},
  pages = {448--459},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.2015.1049750},
  abstract = {Monitoring multivariate quality variables or data streams remains an important and challenging problem in statistical process control (SPC). Although the multivariate SPC has been extensively studied in the literature, designing distribution-free control schemes are still challenging and yet to be addressed well. This article develops a new nonparametric methodology for monitoring location parameters when only a small reference dataset is available. The key idea is to construct a series of conditionally distribution-free test statistics in the sense that their distributions are free of the underlying distribution given the empirical distribution functions. The conditional probability that the charting statistic exceeds the control limit at present given that there is no alarm before the current time point can be guaranteed to attain a specified false alarm rate. The success of the proposed method lies in the use of data-dependent control limits, which are determined based on the observations online rather than decided before monitoring. Our theoretical and numerical studies show that the proposed control chart is able to deliver satisfactory in-control run-length performance for any distributions with any dimension. It is also very efficient in detecting multivariate process shifts when the process distribution is heavy-tailed or skewed. Supplementary materials for this article are available online.},
  keywords = {Conditionally distribution-free,Empirical distribution,Nonparametric procedure,Robustness,Self-starting,Statistical process control.,todo},
  annotation = {\_eprint: https://doi.org/10.1080/00401706.2015.1049750},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Chen_et_al_2016_A_Distribution-Free_Multivariate_Control_Chart.pdf;/home/dede/Zotero/storage/9CLU6F34/00401706.2015.html}
}

@article{chen2017,
  title = {Unique {{Entity Estimation}} with {{Application}} to the {{Syrian Conflict}}},
  author = {Chen, Beidi and Shrivastava, Anshumali and Steorts, Rebecca C.},
  year = {2017},
  month = oct,
  journal = {arXiv:1710.02690 [cs, stat]},
  eprint = {1710.02690},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Entity resolution identifies and removes duplicate entities in large, noisy databases and has grown in both usage and new developments as a result of increased data availability. Nevertheless, entity resolution has tradeoffs regarding assumptions of the data generation process, error rates, and computational scalability that make it a difficult task for real applications. In this paper, we focus on a related problem of unique entity estimation, which is the task of estimating the unique number of entities and associated standard errors in a data set with duplicate entities. Unique entity estimation shares many fundamental challenges of entity resolution, namely, that the computational cost of all-to-all entity comparisons is intractable for large databases. To circumvent this computational barrier, we propose an efficient (near-linear time) estimation algorithm based on locality sensitive hashing. Our estimator, under realistic assumptions, is unbiased and has provably low variance compared to existing random sampling based approaches. In addition, we empirically show its superiority over the state-of-the-art estimators on three real applications. The motivation for our work is to derive an accurate estimate of the documented, identifiable deaths in the ongoing Syrian conflict. Our methodology, when applied to the Syrian data set, provides an estimate of \$191,874 \textbackslash pm 1772\$ documented, identifiable deaths, which is very close to the Human Rights Data Analysis Group (HRDAG) estimate of 191,369. Our work provides an example of challenges and efforts involved in solving a real, noisy challenging problem where modeling assumptions may not hold.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Databases,Statistics - Applications,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Chen_et_al_2017_Unique_Entity_Estimation_with_Application_to_the_Syrian_Conflict.pdf;/home/dede/Zotero/storage/Z7ACHVMS/1710.html}
}

@article{chen2018,
  title = {Combinatorial {{Multi-Armed Bandit}} with {{General Reward Functions}}},
  author = {Chen, Wei and Hu, Wei and Li, Fu and Li, Jian and Liu, Yu and Lu, Pinyan},
  year = {2018},
  month = jul,
  journal = {arXiv:1610.06603 [cs, stat]},
  eprint = {1610.06603},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {In this paper, we study the stochastic combinatorial multi-armed bandit (CMAB) framework that allows a general nonlinear reward function, whose expected value may not depend only on the means of the input random variables but possibly on the entire distributions of these variables. Our framework enables a much larger class of reward functions such as the \$\textbackslash max()\$ function and nonlinear utility functions. Existing techniques relying on accurate estimations of the means of random variables, such as the upper confidence bound (UCB) technique, do not work directly on these functions. We propose a new algorithm called stochastically dominant confidence bound (SDCB), which estimates the distributions of underlying random variables and their stochastically dominant confidence bounds. We prove that SDCB can achieve \$O(\textbackslash log\{T\})\$ distribution-dependent regret and \$\textbackslash tilde\{O\}(\textbackslash sqrt\{T\})\$ distribution-independent regret, where \$T\$ is the time horizon. We apply our results to the \$K\$-MAX problem and expected utility maximization problems. In particular, for \$K\$-MAX, we provide the first polynomial-time approximation scheme (PTAS) for its offline problem, and give the first \$\textbackslash tilde\{O\}(\textbackslash sqrt T)\$ bound on the \$(1-\textbackslash epsilon)\$-approximation regret of its online problem, for any \$\textbackslash epsilon{$>$}0\$.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Machine Learning,Statistics - Machine Learning,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Chen_et_al_2018_Combinatorial_Multi-Armed_Bandit_with_General_Reward_Functions.pdf;/home/dede/Zotero/storage/TFTEQVGK/1610.html}
}

@article{chen2020,
  title = {China and {{Russia}} in {{R2P}} Debates at the {{UN Security Council}}},
  author = {Chen, Zheng and Yin, Hang},
  year = {2020},
  month = may,
  journal = {International Affairs},
  volume = {96},
  number = {3},
  pages = {787--805},
  issn = {0020-5850},
  doi = {10.1093/ia/iiz229},
  abstract = {While China and Russia's general policies towards the Responsibility to Protect (R2P) are similar, the two reveal nuanced differences in addressing specific emergencies. Both express support for the first two pillars of R2P while resisting coercive intervention under its aegis, as they share anxieties of domestic political security and concerns about their international image. Nonetheless, addressing cases like the Syrian crisis, Russian statements are more assertive and even aggressive while Chinese ones are usually vague and reactive. This article highlights the two states' different tones through computer-assisted text analyses. It argues that diplomatic styles reflect Russian and Chinese perceptions of their own place in the evolving international order. Experiences in past decades create divergent reference points and status prospects for them, which leads to their different strategies in signalling Great Power status. As Beijing is optimistic about its status-rising prospects, it exercises more self-restraint in order to avoid external containments and is reluctant to act as an independent `spoiler'. Meanwhile, Moscow interprets its Great Power status more from a frame of `loss' and therefore is inclined to adopt a sterner approach to signal its status. Although their policies complement each other on many occasions, there is nothing akin to a Sino\textendash Russian `bloc'.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Chen_Yin_2020_China_and_Russia_in_R2P_debates_at_the_UN_Security_Council.pdf;/home/dede/Zotero/storage/W2PJEJE6/5712435.html}
}

@article{chen2021,
  title = {Data-Driven Selection of the Number of Change-Points via Error Rate Control},
  author = {Chen, Hui and Ren, Haojie and Yao, Fang and Zou, Changliang},
  year = {2021},
  month = nov,
  journal = {Journal of the American Statistical Association},
  volume = {0},
  number = {0},
  pages = {1--14},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.2021.1999820},
  abstract = {In multiple change-point analysis, one of the main difficulties is to determine the number of change-points. Various consistent selection methods, including the use of Schwarz information criterion and cross-validation, have been proposed to balance the model fitting and complexity. However, there is lack of systematic approaches to provide theoretical guarantee of significance in determining the number of changes. In this paper, we introduce a data-adaptive selection procedure via error rate control based on order-preserving sample-splitting, which is applicable to most existing change-point methods. The key idea is to construct a series of statistics with global symmetry property and then utilize the symmetry to derive a data-driven threshold. Under this general framework, we are able to rigorously investigate the false discovery proportion control, and show that the proposed method controls the false discovery rate (FDR) asymptotically under mild conditions while retaining the true change-points. Numerical experiments indicate that our selection procedure works well for many change-detection methods and is able to yield accurate FDR control in finite samples. Keywords: Empirical distribution; False discovery rate; Multiple change-point model; Sample-splitting; Symmetry; Uniform convergence.},
  keywords = {doing},
  annotation = {\_eprint: https://doi.org/10.1080/01621459.2021.1999820},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Chen_et_al_2021_Data-driven_selection_of_the_number_of_change-points_via_error_rate_control.pdf}
}

@article{cheng2014,
  title = {A {{Distribution-Free Multivariate Control Chart}} for {{Phase I Applications}}},
  author = {Cheng, Ching-Ren and Shiau, Jyh-Jen Horng},
  year = {2014},
  journal = {Quality and Reliability Engineering International},
  volume = {31},
  number = {1},
  pages = {97--111},
  issn = {1099-1638},
  doi = {10.1002/qre.1751},
  abstract = {The purpose of this paper is to provide a novel distribution-free control chart for monitoring the location parameter vector of a multivariate process in phase I analysis. To be robust to the process distribution, the spatial sign statistic that defines the multivariate direction of an observation is used to construct a Shewhart-type control chart for detecting out-of-control observations in historical phase I data. The proposed control chart is distribution free in the sense that the false-positive rate (or false alarm rate), the proportion of wrongly classified in-control samples, can be controlled at the specified value for elliptical-direction distributions. In addition, we demonstrate through simulation studies that the false-positive rate of the proposed chart is robust to the shift size of the out-of-control condition if we only delete the most extreme out-of-control observation at each iteration of phase I analysis. Compared with the traditional Hotelling's T2 control chart and some of its robust versions, the proposed chart is generally more powerful in detecting out-of-control observations and more robust to the normality assumption. Copyright \textcopyright{} 2014 John Wiley \& Sons, Ltd.},
  copyright = {Copyright \textcopyright{} 2014 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {distribution free,done,multivariate control chart,NSPC,phase I analysis,spatial sign},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/qre.1751},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Cheng_Shiau_2014_A_Distribution-Free_Multivariate_Control_Chart_for_Phase_I_Applications.pdf;/home/dede/Zotero/storage/AN25BISL/qre.html}
}

@article{chiou2016,
  title = {Multivariate Functional Linear Regression and Prediction},
  author = {Chiou, Jeng-Min and Yang, Ya-Fang and Chen, Yu-Ting},
  year = {2016},
  month = apr,
  journal = {Journal of Multivariate Analysis},
  series = {Special {{Issue}} on {{Statistical Models}} and {{Methods}} for {{High}} or {{Infinite Dimensional Spaces}}},
  volume = {146},
  pages = {301--312},
  issn = {0047-259X},
  doi = {10.1016/j.jmva.2015.10.003},
  abstract = {We propose a multivariate functional linear regression (mFLR) approach to analysis and prediction of multivariate functional data in cases in which both the response and predictor variables contain multivariate random functions. The mFLR model, coupled with the multivariate functional principal component analysis approach, takes the advantage of cross-correlation between component functions within the multivariate response and predictor variables, respectively. The estimate of the matrix of bivariate regression functions is consistent in the sense of the multi-dimensional Gram\textendash Schmidt norm and is asymptotically normally distributed. The prediction intervals of the multivariate random trajectories are available for predictive inference. We show the finite sample performance of mFLR by a simulation study and illustrate the method through predicting multivariate traffic flow trajectories for up-to-date and partially observed traffic streams.},
  langid = {english},
  keywords = {Functional prediction,Functional principal component analysis,Functional regression,Multivariate functional data,Stochastic processes,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Chiou_et_al_2016_Multivariate_functional_linear_regression_and_prediction.pdf;/home/dede/Zotero/storage/S3FES66Z/S0047259X15002535.html}
}

@article{chipman2010,
  title = {{{BART}}: {{Bayesian}} Additive Regression Trees},
  shorttitle = {{{BART}}},
  author = {Chipman, Hugh A. and George, Edward I. and McCulloch, Robert E.},
  year = {2010},
  month = mar,
  journal = {The Annals of Applied Statistics},
  volume = {4},
  number = {1},
  pages = {266--298},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {1932-6157, 1941-7330},
  doi = {10.1214/09-AOAS285},
  abstract = {We develop a Bayesian ``sum-of-trees'' model where each tree is constrained by a regularization prior to be a weak learner, and fitting and inference are accomplished via an iterative Bayesian backfitting MCMC algorithm that generates samples from a posterior. Effectively, BART is a nonparametric Bayesian regression approach which uses dimensionally adaptive random basis elements. Motivated by ensemble methods in general, and boosting algorithms in particular, BART is defined by a statistical model: a prior and a likelihood. This approach enables full posterior inference including point and interval estimates of the unknown regression function as well as the marginal effects of potential predictors. By keeping track of predictor inclusion frequencies, BART can also be used for model-free variable selection. BART's many features are illustrated with a bake-off against competing methods on 42 different data sets, with a simulation experiment and on a drug discovery classification problem.},
  keywords = {Bayesian backfitting,boosting,CART,â€Žclassificationâ€Ž,ensemble,MCMC,Nonparametric regression,probit model,random basis,regularizatio,sum-of-trees model,todo,Variable selection,weak learner},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Chipman_et_al_2010_BART.pdf;/home/dede/Zotero/storage/LCWJCKMG/09-AOAS285.html}
}

@article{chopin2002,
  title = {A {{Sequential Particle Filter Method}} for {{Static Models}}},
  author = {Chopin, Nicolas},
  year = {2002},
  journal = {Biometrika},
  volume = {89},
  number = {3},
  pages = {539--551},
  publisher = {{[Oxford University Press, Biometrika Trust]}},
  issn = {0006-3444},
  abstract = {Particle filter methods are complex inference procedures, which combine importance sampling and Monte Carlo schemes in order to explore consistently a sequence of multiple distributions of interest. We show that such methods can also offer an efficient estimation tool in 'static' set-ups, in which case \$\textbackslash pi (\textbackslash theta \textbackslash mid y\_1,..., y\_N)\$ (n {$<$} N) is the only posterior distribution of interest but the preliminary exploration of partial posteriors \$\textbackslash pi (\textbackslash theta \textbackslash mid y\_1,..., y\_n)\$ makes it possible to save computing time. A complete algorithm is proposed for independent or Markov models. Our method is shown to challenge other common estimation procedures in terms of robustness and execution time, especially when the sample size is important. Two classes of examples, mixture models and discrete generalised linear models, are discussed and illustrated by numerical results.},
  keywords = {doing},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Chopin_2002_A_Sequential_Particle_Filter_Method_for_Static_Models.pdf}
}

@book{chopin2020,
  title = {An {{Introduction}} to {{Sequential Monte Carlo}}},
  author = {Chopin, Nicolas and Papaspiliopoulos, Omiros},
  year = {2020},
  series = {Springer {{Series}} in {{Statistics}}},
  publisher = {{Springer International Publishing}},
  doi = {10.1007/978-3-030-47845-2},
  abstract = {This book provides a general introduction to Sequential Monte Carlo (SMC) methods, also known as particle filters. These methods have become a staple for the sequential analysis of data in such diverse fields as signal processing, epidemiology, machine learning, population ecology, quantitative finance, and robotics.The coverage is comprehensive, ranging from the underlying theory to computational implementation, methodology, and diverse applications in various areas of science. This is achieved by describing SMC algorithms as particular cases of a general framework, which involves concepts such as Feynman-Kac distributions, and tools such as importance sampling and resampling. This general framework is used consistently throughout the book.Extensive coverage is provided on sequential learning (filtering, smoothing) of state-space (hidden Markov) models, as this remains an important application of SMC methods. More recent applications, such as parameter estimation of these models (through e.g. particle Markov chain Monte Carlo techniques) and the simulation of challenging probability distributions (in e.g. Bayesian inference or rare-event problems), are also discussed.The book may be used either as a graduate text on Sequential Monte Carlo methods and state-space modeling, or as a general reference work on the area. Each chapter includes a set of exercises for self-study, a comprehensive bibliography, and a ``Python corner,'' which discusses the practical implementation of the methods covered. In addition, the book comes with an open source Python library, which implements all the algorithms described in the book, and contains all the programs that were used to perform the numerical experiments.},
  isbn = {978-3-030-47844-5},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Chopin_Papaspiliopoulos_2020_An_Introduction_to_Sequential_Monte_Carlo.pdf;/home/dede/Zotero/storage/YJPJSVB7/9783030478445.html}
}

@article{christensen1992,
  title = {Case-{{Deletion Diagnostics}} for {{Mixed Models}}},
  author = {Christensen, Ronald and Pearson, Larry M. and Johnson, Wesley},
  year = {1992},
  journal = {Technometrics},
  volume = {34},
  number = {1},
  pages = {38--45},
  publisher = {{[Taylor \& Francis, Ltd., American Statistical Association, American Society for Quality]}},
  issn = {0040-1706},
  doi = {10.2307/1269550},
  abstract = {Mixed linear models arise in many areas of application. Standard estimation methods for mixed models are sensitive to bizarre observations. Such influential observations can completely distort an analysis and lead to inappropriate actions and conclusions. We develop case-deletion diagnostics for detecting influential observations in mixed linear models. Diagnostics for both fixed effects and variance components are proposed. Computational formulas are given that make the procedures feasible. The methods are illustrated using examples.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Christensen_et_al_1992_Case-Deletion_Diagnostics_for_Mixed_Models.pdf}
}

@book{christensen2019,
  title = {Advanced {{Linear Modeling}}: {{Statistical Learning}} and {{Dependent Data}}},
  shorttitle = {Advanced {{Linear Modeling}}},
  author = {Christensen, Ronald},
  year = {2019},
  month = dec,
  edition = {3rd ed. 2019 edition},
  publisher = {{Springer}},
  address = {{Cham, Switzerland}},
  isbn = {978-3-030-29163-1},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Christensen_2019_Advanced_Linear_Modeling.pdf}
}

@book{chung2000,
  title = {A {{Course}} in {{Probability Theory}}},
  author = {Chung, Kai Lai},
  year = {2000},
  month = oct,
  edition = {3rd edition},
  publisher = {{Academic Press}},
  address = {{San Diego}},
  abstract = {Since the publication of the first edition of this classic textbook over thirty years ago, tens of thousands of students have used A Course in Probability Theory. New in this edition is an introduction to measure theory that expands the market, as this treatment is more consistent with current courses. While there are several books on probability, Chung's book is considered a classic, original work in probability theory due to its elite level of sophistication.},
  isbn = {978-0-12-174151-8},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Chung_2000_A_Course_in_Probability_Theory,_Third_Edition.pdf}
}

@book{cifarelli1989,
  title = {Statistica Bayesiana: Appunti Ad Uso Degli Studenti},
  shorttitle = {Statistica Bayesiana},
  author = {Cifarelli, Donato Michele and Muliere, Pietro},
  year = {1989},
  publisher = {{Gianni Iuculano Editore}},
  isbn = {978-88-7072-114-0},
  lccn = {519.54},
  keywords = {Statistica matematica bayesiana}
}

@book{cinlar2011,
  title = {Probability and {{Stochastics}}},
  author = {{\c C}inlar, Erhan},
  year = {2011},
  series = {Graduate {{Texts}} in {{Mathematics}}},
  volume = {261},
  address = {{New York}},
  doi = {10.1007/978-0-387-87859-1},
  isbn = {978-0-387-87858-4 978-0-387-87859-1},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Ã‡inlar_2011_Probability_and_Stochastics.pdf}
}

@article{cipolli2017,
  title = {Computationally Tractable Approximate and Smoothed {{Polya}} Trees},
  author = {Cipolli, William and Hanson, Timothy},
  year = {2017},
  month = jan,
  journal = {Statistics and Computing},
  volume = {27},
  number = {1},
  pages = {39--51},
  issn = {0960-3174, 1573-1375},
  doi = {10.1007/s11222-016-9652-3},
  langid = {english},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Cipolli_Hanson_2017_Computationally_tractable_approximate_and_smoothed_Polya_trees.pdf}
}

@book{cohen2017,
  title = {The {{Big Stick}}: {{The Limits}} of {{Soft Power}} and the {{Necessity}} of {{Military Force}}},
  shorttitle = {The {{Big Stick}}},
  author = {Cohen, Eliot A.},
  year = {2017},
  month = jan,
  edition = {1st edition},
  publisher = {{Basic Books}},
  address = {{New York}},
  isbn = {978-0-465-04472-6},
  langid = {english},
  keywords = {doing},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Cohen_2017_The_Big_Stick.epub}
}

@book{congdon2014,
  title = {Applied {{Bayesian Modelling}}},
  author = {Congdon, Peter},
  year = {2014},
  series = {Wiley {{Series}} in {{Probability}} and {{Statistics}}},
  publisher = {{Wiley}},
  abstract = {The use of Bayesian statistics has grown significantly in recent years, and will undoubtedly continue to do so. Applied Bayesian Modelling is the follow-up to the author's best selling book, Bayesian Statistical Modelling, and focuses on the potential applications of Bayesian techniques in a wide range of important topics in the social and health sciences. The applications are illustrated through many real-life examples and software implementation in WINBUGS - a popular software package that offers a simplified and flexible approach to statistical modelling. The book gives detailed explanations for each example - explaining fully the choice of model for each particular problem. The book  * Provides a broad and comprehensive account of applied Bayesian modelling.  * Describes a variety of model assessment methods and the flexibility of Bayesian prior specifications.  * Covers many application areas, including panel data models, structural equation and other multivariate structure models, spatial analysis, survival analysis and epidemiology.  * Provides detailed worked examples in WINBUGS to illustrate the practical application of the techniques described. All WINBUGS programs are available from an ftp site.  The book provides a good introduction to Bayesian modelling and data analysis for a wide range of people involved in applied statistical analysis, including researchers and students from statistics, and the health and social sciences. The wealth of examples makes this book an ideal reference for anyone involved in statistical modelling and analysis.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Congdon_2014_Applied_Bayesian_Modelling.pdf}
}

@book{conn2008,
  title = {{Introduction to Derivative-Free Optimization}},
  author = {Conn, Andrew R. and Scheinberg, Katya and Vicente, Lu{\'i}s N.},
  year = {2008},
  publisher = {{Society for Industrial and Applied Mathematics}},
  address = {{Philadelphia}},
  abstract = {The absence of derivatives, often combined with the presence of noise or lack of smoothness, is a major challenge for optimization. This book explains how sampling and model techniques are used in derivative-free methods and how these methods are designed to efficiently and rigorously solve optimization problems. Although readily accessible to readers with a modest background in computational mathematics, it is also intended to be of interest to researchers in the field. Introduction to Derivative-Free Optimization is the first contemporary comprehensive treatment of optimization without derivatives. This book covers most of the relevant classes of algorithms from direct search to model-based approaches. It contains a comprehensive description of the sampling and modeling tools needed for derivative-free optimization; these tools allow the reader to better analyze the convergent properties of the algorithms and identify their differences and similarities.},
  isbn = {978-0-89871-668-9},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Conn_et_al_2008_Introduction_to_Derivative-Free_Optimization.pdf}
}

@article{conover2017,
  title = {The {{Sequential Normal Scores Transformation}}},
  author = {Conover, W. J. and Tercero, Victor G. and {Cordero-Franco}, Alvaro E.},
  year = {2017},
  month = jul,
  journal = {Sequential Analysis},
  volume = {36},
  number = {3},
  eprint = {1705.03496},
  eprinttype = {arxiv},
  pages = {397--414},
  issn = {0747-4946, 1532-4176},
  doi = {10.1080/07474946.2017.1360091},
  abstract = {The sequential analysis of series often requires nonparametric procedures, where the most powerful ones frequently use rank transformations. Re-ranking the data sequence after each new observation can become too intensive computationally. This led to the idea of sequential ranks, where only the most recent observation is ranked. However, difficulties finding, or approximating, the null distribution of the statistics may have contributed to the lack of popularity of these methods. In this paper, we propose transforming the sequential ranks into sequential normal scores which are independent, and asymptotically standard normal random variables. Thus original methods based on the normality assumption may be used. A novel approach permits the inclusion of a priori information in the form of quantiles. It is developed as a strategy to increase the sensitivity of the scoring statistic. The result is a powerful convenient method to analyze non-normal data sequences. Also, four variations of sequential normal scores are presented using examples from the literature. Researchers and practitioners might find this approach useful to develop nonparametric procedures to address new problems extending the use of parametric procedures when distributional assumptions are not met. These methods are especially useful with large data streams where efficient computational methods are required.},
  archiveprefix = {arXiv},
  keywords = {62L10; 62Gxx,Mathematics - Statistics Theory},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Conover_et_al_2017_The_Sequential_Normal_Scores_Transformation.pdf;/home/dede/Zotero/storage/A3TSHDF5/1705.html}
}

@article{conover2018,
  title = {A {{Look}} at {{Sequential Normal Scores}} and {{How They Apply}} to {{Financial Data Analysis}}},
  author = {Conover, W. J. and {Tercero-G{\'o}mez}, V{\'i}ctor G. and {Cordero-Franco}, Alvaro E.},
  year = {2018},
  month = apr,
  journal = {Journal of Applied Mathematics and Physics},
  volume = {6},
  number = {4},
  pages = {787--816},
  publisher = {{Scientific Research Publishing}},
  doi = {10.4236/jamp.2018.64069},
  abstract = {Statistical methods for analyzing economic data need to be timely, accurate and easy to compute. To accomplish this, parametric models are often assumed, but they are at best approximate, and often lack a good fit in the tails of the distribution where much of the interesting data are concentrated. Therefore, nonparametric methods have been extensively examined as alternatives to the constrictive assumptions of parametric models. This paper examines the use of Sequential Normal Scores (SNS) for transforming time series data with unknown distributions into time series data that are approximately standard-normally distributed. Particular attention is directed toward detecting outliers (out-of-control values), and applying subsequent analytic methods such as CUSUMs and Exponentially Weighted Moving Average (EWMA) schemes. Two examples of stock market data are presented for illustration.},
  copyright = {http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Conover_et_al_2018_A_Look_at_Sequential_Normal_Scores_and_How_They_Apply_to_Financial_Data_Analysis.pdf;/home/dede/Zotero/storage/MYKNIPNP/paperinformation.html}
}

@article{considine2019,
  title = {Contests of Legitimacy and Value: The {{Treaty}} on the {{Prohibition}} of {{Nuclear Weapons}} and the Logic of Prohibition},
  shorttitle = {Contests of Legitimacy and Value},
  author = {Considine, Laura},
  year = {2019},
  month = sep,
  journal = {International Affairs},
  volume = {95},
  number = {5},
  pages = {1075--1092},
  issn = {0020-5850},
  doi = {10.1093/ia/iiz103},
  abstract = {The recently adopted Treaty on the Prohibition of Nuclear Weapons (TPNW) has caused much debate and controversy in global nuclear politics. Given that the stated goal of the TPNW supporters (states and NGOs alike) is to embed the treaty in the structures of nuclear governance and to strengthen its normative power, how likely is the TPNW to achieve these objectives? The article argues that the unique structures of legitimacy and value within which nuclear weapons are enmeshed place particular complications on the normative force of the TPNW as compared to previous humanitarian arms control initiatives, which has implications for the way in which the TPNW can function to consolidate a prohibitionary norm on nuclear weapons possession. The article uses the framing of legitimacy to analyse the complex structures within which the TPNW was adopted and within which it will enter into force, particularly focusing on the TPNW's relationship to the Treaty on the Non-Proliferation of Nuclear Weapons (NPT). The article concludes that consolidation may require a further challenge to the existing structures of nuclear order than state actors have, so far, been willing to make. This work is based on first-hand observations from the TPNW negotiations and interviews with civil society actors at the United Nations in New York in June and July 2017.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Considine_2019_Contests_of_legitimacy_and_value.pdf;/home/dede/Zotero/storage/WRTNY6NE/5537418.html}
}

@article{cook2006,
  title = {Validation of {{Software}} for {{Bayesian Models Using Posterior Quantiles}}},
  author = {Cook, Samantha R and Gelman, Andrew and Rubin, Donald B},
  year = {2006},
  month = sep,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {15},
  number = {3},
  pages = {675--692},
  publisher = {{Taylor \& Francis}},
  issn = {1061-8600},
  doi = {10.1198/106186006X136976},
  abstract = {This article presents a simulation-based method designed to establish the computational correctness of software developed to fit a specific Bayesian model, capitalizing on properties of Bayesian posterior distributions. We illustrate the validation technique with two examples. The validation method is shown to find errors in software when they exist and, moreover, the validation output can be informative about the nature and location of such errors. We also compare our method with that of an earlier approach.},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Cook_et_al_2006_Validation_of_Software_for_Bayesian_Models_Using_Posterior_Quantiles.pdf;/home/dede/Zotero/storage/AZJX9IQH/106186006X136976.html}
}

@book{cormen2009,
  title = {Introduction to {{Algorithms}}, 3rd {{Edition}}},
  author = {Cormen, Thomas H. and Leiserson, Charles E. and Rivest, Ronald L. and Stein, Clifford},
  year = {2009},
  month = jul,
  edition = {3rd edition},
  publisher = {{The MIT Press}},
  address = {{Cambridge, Mass}},
  abstract = {The latest edition of the essential text and professional reference, with substantial new material on such topics as vEB trees, multithreaded algorithms, dynamic programming, and edge-based flow.Some books on algorithms are rigorous but incomplete; others cover masses of material but lack rigor. Introduction to Algorithms uniquely combines rigor and comprehensiveness. The book covers a broad range of algorithms in depth, yet makes their design and analysis accessible to all levels of readers. Each chapter is relatively self-contained and can be used as a unit of study. The algorithms are described in English and in a pseudocode designed to be readable by anyone who has done a little programming. The explanations have been kept elementary without sacrificing depth of coverage or mathematical rigor.The first edition became a widely used text in universities worldwide as well as the standard reference for professionals. The second edition featured new chapters on the role of algorithms, probabilistic analysis and randomized algorithms, and linear programming. The third edition has been revised and updated throughout. It includes two completely new chapters, on van Emde Boas trees and multithreaded algorithms, substantial additions to the chapter on recurrence (now called ``Divide-and-Conquer''), and an appendix on matrices. It features improved treatment of dynamic programming and greedy algorithms and a new notion of edge-based flow in the material on flow networks. Many exercises and problems have been added for this edition. The international paperback edition is no longer available; the hardcover is available worldwide.},
  isbn = {978-0-262-03384-8},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Cormen_et_al_2009_Introduction_to_Algorithms,_3rd_Edition.pdf}
}

@book{cornell1995,
  title = {The {{Beginnings}} of {{Rome}}: {{Italy}} and {{Rome}} from the {{Bronze Age}} to the {{Punic Wars}}},
  shorttitle = {The {{Beginnings}} of {{Rome}}},
  author = {Cornell, T. J.},
  year = {1995},
  month = sep,
  edition = {1st edition},
  publisher = {{Routledge}},
  address = {{London ; New York}},
  isbn = {978-0-415-01596-7},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Cornell_1995_The_Beginnings_of_Rome.epub}
}

@book{coulmas2019,
  title = {{Identity: A Very Short Introduction}},
  shorttitle = {{Identity}},
  author = {Coulmas, Florian},
  year = {2019},
  month = feb,
  edition = {Illustrated edizione},
  publisher = {{OUP Oxford}},
  langid = {Inglese}
}

@incollection{cox,
  title = {Appendix {{A}}: {{A}} Brief History},
  author = {Cox, D.}
}

@book{cox1974,
  title = {{Theoretical Statistics}},
  author = {Cox, D.R. and Hinkley, D.V.},
  year = {1974},
  publisher = {{Chapman \& Hall/CRC}},
  address = {{Boca Raton}},
  abstract = {A text that stresses the general concepts of the theory of statistics Theoretical Statistics provides a systematic statement of the theory of statistics, emphasizing general concepts rather than mathematical rigor. Chapters 1 through 3 provide an overview of statistics and discuss some of the basic philosophical ideas and problems behind statistical procedures. Chapters 4 and 5 cover hypothesis testing with simple and null hypotheses, respectively. Subsequent chapters discuss non-parametrics, interval estimation, point estimation, asymptotics, Bayesian procedure, and deviation theory. Student familiarity with standard statistical techniques is assumed.},
  isbn = {978-0-412-16160-5},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Cox_Hinkley_1979_Theoretical_Statistics.pdf}
}

@article{cox1990,
  title = {Role of {{Models}} in {{Statistical Analysis}}},
  author = {Cox, D. R.},
  year = {1990},
  journal = {Statistical Science},
  volume = {5},
  number = {2},
  pages = {169--174},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0883-4237},
  abstract = {A number of distinct roles are identified for probability models used in the analysis of data. Examples are outlined. Some general issues arising in the formulation of such models are discussed.},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Cox_1990_Role_of_Models_in_Statistical_Analysis.pdf}
}

@article{cox1994,
  title = {Some Comments on the Teaching of Stochastic Processes to Engineers},
  author = {Cox, David and Davison, Anthony},
  year = {1994},
  month = dec,
  journal = {International Journal of Continuing Engineering Education and Life-Long Learnin},
  volume = {4},
  pages = {24--30},
  doi = {10.1504/IJCEELL.1994.030285},
  abstract = {The contribution of stochastic processes in modelling dynamic phenomenena is reviewed. The key ideas are identified, the importance of careful formulation is emphasised and the roles of analytical solution and simulation discussed. A simple example is investigated in a little detail.},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Cox_Davison_1994_Some_comments_on_the_teaching_of_stochastic_processes_to_engineers.pdf}
}

@article{cox1997,
  title = {The {{Current Position}} of {{Statistics}}: {{A Personal View}}},
  shorttitle = {The {{Current Position}} of {{Statistics}}},
  author = {Cox, D. R.},
  year = {1997},
  journal = {International Statistical Review},
  volume = {65},
  number = {3},
  pages = {261--276},
  issn = {1751-5823},
  doi = {10.1111/j.1751-5823.1997.tb00305.x},
  abstract = {Some current aspects of statistical work are reviewed under three broad headings, applied probability modelling, design of investigations and statistical analysis and interpretation of data. The emphasis is on applications in science and science-based technology, although some incidental comments are made about statistics in public affairs. While no technical details are given, there is some discussion of potential fields for development. The choice of topics reflects the author's personal interests.},
  langid = {english},
  keywords = {Analysis of data,Applied probability models,Design of investigations,done,Formal inference,Government statistics,Stochastic models},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Cox_1997_The_Current_Position_of_Statistics.pdf;/home/dede/Zotero/storage/UGHCF8QZ/j.1751-5823.1997.tb00305.html}
}

@article{cox2002,
  title = {American Power before and after 11 {{September}}: Dizzy with Success?},
  shorttitle = {American Power before and after 11 {{September}}},
  author = {Cox, Michael},
  year = {2002},
  month = apr,
  journal = {International Affairs},
  volume = {78},
  number = {2},
  pages = {261--276},
  issn = {0020-5850},
  doi = {10.1111/1468-2346.00250},
  abstract = {One of the most interesting consequences of the war against international terrorism is the discovery by many analysts of American power. However, if the experts had been more attentive they might have noticed that a power shift in favour of the United States is not just some recent phenomenon arising from US victory over the Taliban or the new Bush military build-up. Rather, it can, and should be, traced back to important trends of the early 1990s. What the war has done is to reveal the extent of America's renaissance in the postwar decade while its position as true hegemon was being consolidated. However, victory in war may not bring order in peace if the United States does not draw the correct lessons.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Cox_2002_American_power_before_and_after_11_September.pdf;/home/dede/Zotero/storage/RH5HDZGC/2434740.html}
}

@book{cox2011,
  title = {{Principles of Applied Statistics}},
  author = {Cox, D. R. and Donnelly, Christl A.},
  year = {2011},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge, UK ; New York}},
  isbn = {978-1-107-64445-8},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Cox_Donnelly_2011_Principles_of_Applied_Statistics.pdf}
}

@incollection{cressie2014,
  title = {Space-{{Time Kalman Filter}}},
  booktitle = {Wiley {{StatsRef}}: {{Statistics Reference Online}}},
  author = {Cressie, Noel and Wikle, Christopher K.},
  editor = {Balakrishnan, N. and Colton, Theodore and Everitt, Brian and Piegorsch, Walter and Ruggeri, Fabrizio and Teugels, Jozef L.},
  year = {2014},
  month = sep,
  pages = {stat07813},
  publisher = {{John Wiley \& Sons, Ltd}},
  address = {{Chichester, UK}},
  doi = {10.1002/9781118445112.stat07813},
  isbn = {978-1-118-44511-2},
  langid = {english},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Cressie_Wikle_2014_Space-Time_Kalman_Filter.pdf}
}

@article{cribari-neto2010,
  title = {Beta {{Regression}} in {{R}}},
  author = {{Cribari-Neto}, Francisco and Zeileis, Achim},
  year = {2010},
  journal = {Journal of Statistical Software},
  volume = {34},
  number = {1},
  pages = {1--24},
  issn = {1548-7660},
  doi = {10.18637/jss.v034.i02},
  copyright = {Copyright (c) 2009 Francisco Cribari-Neto, Achim Zeileis},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Cribari-Neto_Zeileis_2010_Beta_Regression_in_R.pdf;/home/dede/Zotero/storage/C54N3GRR/v034i02.html}
}

@article{crosier1988,
  title = {Multivariate {{Generalizations}} of {{Cumulative Sum Quality-Control Schemes}}},
  author = {Crosier, Ronald B.},
  year = {1988},
  journal = {Technometrics},
  volume = {30},
  number = {3},
  pages = {291--303},
  publisher = {{[Taylor \& Francis, Ltd., American Statistical Association, American Society for Quality]}},
  issn = {0040-1706},
  doi = {10.2307/1270083},
  abstract = {This article presents the design procedures and average run lengths for two multivariate cumulative sum (CUSUM) quality-control procedures. The first CUSUM procedure reduces each multivariate observation to a scalar and then forms a CUSUM of the scalars. The second CUSUM procedure forms a CUSUM vector directly from the observations. These two procedures are compared with each other and with the multivariate Shewhart chart. Other multivariate quality-control procedures are mentioned. Robustness, the fast initial response feature for CUSUM schemes, and combined Shewhart-CUSUM schemes are discussed.},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Crosier_1988_Multivariate_Generalizations_of_Cumulative_Sum_Quality-Control_Schemes.pdf}
}

@article{crowder2001,
  title = {Small {{Sample Properties}} of an {{Adaptive Filter Applied}} to {{Low Volume SPC}}},
  author = {Crowder, S. and Eshleman, L.},
  year = {2001},
  doi = {10.1080/00224065.2001.11980045},
  abstract = {An adaptive filtering technique that assumes initial process parameters are unknown, and updates the parameters as more data become available is developed, and it is shown that far fewer data values are needed than is typically recommended for process control applications. In many manufacturing environments, such as the nuclear weapons complex, emphasis has shifted from the regular production and delivery of large orders to infrequent small orders. However, the challenge to maintain the same high quality and reliability standards while building much smaller lot sizes remains. To meet this challenge, specific areas need more attention, including fast and on-target process start-up, low volume statistical process control, process characterization with small experiments, and estimating reliability given few actual performance tests of the product. In this paper we address the issue of low volume statistical process control. We investigate an adaptive filtering approach to process monitoring with a relatively short time series of autocorrelated data. The emphasis is on estimation and minimization of mean squared error rather than the traditional hypothesis testing and run length analyses associated with process control charting. We develop an adaptive filtering technique that assumes initial process parameters are unknown, and updates the parameters as more data become available. Using simulation techniques, we study the data requirements (the length of a time series of autocorrelated data) necessary to adequately estimate process parameters. We show that far fewer data values are needed than is typically recommended for process control applications. We also demonstrate the techniques with a case study from the nuclear weapons manufacturing complex.}
}

@article{cumings1993,
  title = {"{{Revising Postrevisionism}}," or, {{The Poverty}} of {{Theory}} in {{Diplomatic History}}},
  author = {Cumings, Bruce},
  year = {1993},
  journal = {Diplomatic History},
  volume = {17},
  number = {4},
  pages = {539--569},
  publisher = {{Oxford University Press}},
  issn = {0145-2096},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Cumings_1993_Revising_Postrevisionism,_or,_The_Poverty_of_Theory_in_Diplomatic_History.pdf}
}

@article{dai2014,
  title = {Monitoring Profile Trajectories with Dynamic Time Warping Alignment},
  author = {Dai, Chenxu and Wang, Kaibo and Jin, Ran},
  year = {2014},
  journal = {Quality and Reliability Engineering International},
  volume = {30},
  number = {6},
  pages = {815--827},
  publisher = {{Wiley Online Library}},
  keywords = {toRead},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Dai_et_al_2014_Monitoring_profile_trajectories_with_dynamic_time_warping_alignment.pdf}
}

@article{darling1967,
  title = {Iterated {{Logarithm Inequalities}}},
  author = {Darling, D. A. and Robbins, Herbert},
  year = {1967},
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  volume = {57},
  number = {5},
  pages = {1188--1192},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Darling_Robbins_1967_Iterated_Logarithm_Inequalities.pdf}
}

@article{darling1967a,
  title = {Confidence Sequences for Mean, Variance, and Median},
  author = {Darling, D. A. and Robbins, Herbert},
  year = {1967},
  journal = {Proceedings of the National Academy of Sciences},
  volume = {58},
  number = {1},
  pages = {66--68},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.58.1.66},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Darling_Robbins_1967_Confidence_sequences_for_mean,_variance,_and_median.pdf}
}

@book{dasgupta2011,
  title = {Probability for Statistics and Machine Learning: Fundamentals and Advanced Topics},
  shorttitle = {Probability for Statistics and Machine Learning},
  author = {DasGupta, Anirban},
  year = {2011},
  publisher = {{Springer-Verlag}},
  address = {{New York}},
  abstract = {This book provides a versatile and lucid treatment of classic as well as modern probability theory, while integrating them with core topics in statistical theory and also some key tools in machine learning. It is written in an extremely accessible style, with elaborate motivating discussions and numerous worked out examples and exercises. The book has 20 chapters on a wide range of topics, 423 worked out examples, and 808 exercises. It is unique in its unification of probability and statistics, its coverage and its superb exercise sets, detailed bibliography, and in its substantive treatment of many topics of current importance.This book can be used as a text for a year long graduate course in statistics, computer science, or mathematics, for self-study, and as an invaluable research reference on probabiliity and its applications. Particularly worth mentioning are the treatments of distribution theory, asymptotics, simulation and Markov Chain Monte Carlo, Markov chains and martingales, Gaussian processes, VC theory, probability metrics, large deviations, bootstrap, the EM algorithm, confidence intervals, maximum likelihood and Bayes estimates, exponential families, kernels, and Hilbert spaces, and a self contained complete review of univariate probability.},
  isbn = {978-1-4419-9633-6},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/DasGupta_2011_Probability_for_statistics_and_machine_learning.pdf}
}

@book{davison1997,
  title = {Bootstrap {{Methods}} and Their {{Application}}},
  author = {Davison, A. C. and Hinkley, D. V.},
  year = {1997},
  series = {Cambridge {{Series}} in {{Statistical}} and {{Probabilistic Mathematics}}},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/CBO9780511802843},
  abstract = {Bootstrap methods are computer-intensive methods of statistical analysis, which use simulation to calculate standard errors, confidence intervals, and significance tests. The methods apply for any level of modelling, and so can be used for fully parametric, semiparametric, and completely nonparametric analysis. This 1997 book gives a broad and up-to-date coverage of bootstrap methods, with numerous applied examples, developed in a coherent way with the necessary theoretical basis. Applications include stratified data; finite populations; censored and missing data; linear, nonlinear, and smooth regression models; classification; time series and spatial problems. Special features of the book include: extensive discussion of significance tests and confidence intervals; material on various diagnostic methods; and methods for efficient computation, including improved Monte Carlo simulation. Each chapter includes both practical and theoretical exercises. S-Plus programs for implementing the methods described in the text are available from the supporting website.},
  isbn = {978-0-521-57471-6},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Davison_Hinkley_1997_Bootstrap_Methods_and_their_Application.pdf;/home/dede/Zotero/storage/H4YML7MF/ED2FD043579F27952363566DC09CBD6A.html}
}

@book{davison2003,
  title = {{Statistical Models}},
  author = {Davison, A. C.},
  year = {2003},
  edition = {1 edizione},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  abstract = {Models and likelihood are the backbone of modern statistics. This 2003 book gives an integrated development of these topics that blends theory and practice, intended for advanced undergraduate and graduate students, researchers and practitioners. Its breadth is unrivaled, with sections on survival analysis, missing data, Markov chains, Markov random fields, point processes, graphical models, simulation and Markov chain Monte Carlo, estimating functions, asymptotic approximations, local likelihood and spline regressions as well as on more standard topics such as likelihood and linear and generalized linear models. Each chapter contains a wide range of problems and exercises. Practicals in the S language designed to build computing and data analysis skills, and a library of data sets to accompany the book, are available over the Web.},
  isbn = {978-0-521-73449-3},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Davison_2008_Statistical_Models.pdf}
}

@article{dean2008,
  title = {{{MapReduce}}: Simplified Data Processing on Large Clusters},
  shorttitle = {{{MapReduce}}},
  author = {Dean, Jeffrey and Ghemawat, Sanjay},
  year = {2008},
  month = jan,
  journal = {Communications of the ACM},
  volume = {51},
  number = {1},
  pages = {107--113},
  issn = {0001-0782, 1557-7317},
  abstract = {MapReduce is a programming model and an associated implementation for processing and generating large data sets. Users specify a map function that processes a key/value pair to generate a set of intermediate key/value pairs, and a reduce function that merges all intermediate values associated with the same intermediate key. Many real world tasks are expressible in this model, as shown in the paper.},
  langid = {english},
  keywords = {done},
  file = {/home/dede/Zotero/storage/WCX6QD8H/Dean and Ghemawat - 2008 - MapReduce simplified data processing on large clu.pdf}
}

@book{dean2015,
  title = {Handbook of {{Design}} and {{Analysis}} of {{Experiments}}},
  editor = {Dean, Angela and Morris, Max and Stufken, John and Bingham, Derek},
  year = {2015},
  month = jun,
  edition = {1st edition},
  publisher = {{Chapman and Hall/CRC}},
  address = {{Boca Raton}},
  isbn = {978-1-4665-0433-2},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Dean_et_al_2015_Handbook_of_Design_and_Analysis_of_Experiments.pdf}
}

@article{definetti1931,
  title = {{Sul significato soggettivo della probabilit\`a}},
  author = {{de Finetti}, Bruno},
  year = {1931},
  journal = {Fundamenta Mathematicae},
  volume = {17},
  number = {1},
  pages = {298--329},
  issn = {0016-2736},
  langid = {italian},
  file = {/home/dede/Zotero/storage/AFLFMQNV/212523.html}
}

@article{definetti1937,
  title = {{La pr\'evision : ses lois logiques, ses sources subjectives}},
  shorttitle = {{La pr\'evision}},
  author = {{de Finetti}, Bruno},
  year = {1937},
  journal = {Annales de l'institut Henri Poincar\'e},
  volume = {7},
  number = {1},
  pages = {1--68},
  langid = {french},
  keywords = {todo},
  file = {/home/dede/Zotero/storage/TJEUGVTZ/de Finetti - 1937 - La prÃ©vision  ses lois logiques, ses sources subj.pdf;/home/dede/Zotero/storage/9G92DWSJ/AIHP_1937__7_1_1_0.html}
}

@article{degroot1987,
  title = {A {{Conversation}} with {{George Box}}},
  author = {DeGroot, Morris H.},
  year = {1987},
  journal = {Statistical Science},
  volume = {2},
  number = {3},
  pages = {239--258},
  issn = {0883-4237},
  keywords = {conversation,discussion,done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/DeGroot_1987_A_Conversation_with_George_Box.pdf}
}

@book{degroot2011,
  title = {Probability and {{Statistics}}},
  author = {DeGroot, Morris H. and Schervish, Mark J.},
  year = {2011},
  month = jan,
  edition = {4th edition},
  publisher = {{Pearson}},
  address = {{Boston}},
  isbn = {978-0-321-50046-5},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/DeGroot_Schervish_2011_Probability_and_Statistics.pdf;/home/dede/Documents/MEGA/universita/zotero-pdf/DeGroot_Schervish_2011_Probability_and_Statistics2.pdf}
}

@inproceedings{delathauwer2009,
  title = {A Survey of Tensor Methods},
  booktitle = {2009 {{IEEE International Symposium}} on {{Circuits}} and {{Systems}}},
  author = {De Lathauwer, Lieven},
  year = {2009},
  month = may,
  pages = {2773--2776},
  issn = {2158-1525},
  doi = {10.1109/ISCAS.2009.5118377},
  abstract = {Matrix decompositions have always been at the heart of signal, circuit and system theory. In particular, the singular value decomposition (SVD) has been an important tool. There is currently a shift of paradigm in the algebraic foundations of these fields. Quite recently, nonnegative matrix factorization (NMF) has been shown to outperform SVD at a number of tasks. Increasing research efforts are spent on the study and application of decompositions of higher-order tensors or multi-way arrays. This paper is a partial survey on tensor generalizations of the SVD and their applications. We also touch on nonnegative tensor factorizations.},
  keywords = {Circuits and systems,Control systems,Data mining,Frequency,Heart,Matrices,Matrix decomposition,Signal processing,Singular value decomposition,Tensile stress,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/De_Lathauwer_2009_A_survey_of_tensor_methods.pdf;/home/dede/Zotero/storage/4MW22PAI/5118377.html}
}

@article{delcastillo1994,
  title = {Short-Run Statistical Process Control: {{Q}} -{{Chart}} Enhancements and Alternative Methods},
  shorttitle = {Short-Run Statistical Process Control},
  author = {{del Castillo}, Enrique and Montgomery, Douglas},
  year = {1994},
  month = mar,
  journal = {Quality and Reliability Engineering International},
  volume = {10},
  pages = {87--97},
  doi = {10.1002/qre.4680100203},
  abstract = {In processes where the length of the production run is short, data to estimate the process parameters and control limits may not be available prior to the start of production, and because of the short run time, traditional methods for establishing control charts cannot be easily applied. Recently, Q charts have been proposed to address this problem. We study the average run length (ARL) of Q charts for a normally distributed variable assuming that a sustained shift occurs in the quality characteristic. It is shown that in some cases Q charts do not exhibit adequate ARL performance. Modifications that enhance the ARL properties of Q charts are presented. Some alternatives to Q charts are also discussed. For the case of a known process target two alternative methods are presented: an exponentially weighted moving average (EWMA) method and an adaptive Kalman filtering method. It is shown that both methods have better ARL performance than Q charts for that case. For the case of both process parameters unknown, an adaptive Kalman filtering method used with a tracking signal provides an ARL performance that improves as better estimates of the process mean and variance are given. A practical example illustrates the tracking signal method for the case when the process parameters are unknown.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/del_Castillo_Montgomery_1994_Short-run_statistical_process_control.pdf}
}

@article{delnegro2018,
  title = {A Bayesian Approach for Inference on Probabilistic Surveys},
  author = {Del Negro, Marco and Casarin, Roberto and Bassetti, Federico},
  year = {2018},
  number = {2016},
  keywords = {bayesian inference,bayesian nonparametric,casters,done,inflation credibility,survey of professional fore-}
}

@book{delsol2017,
  title = {{Anti-Piketty: Capital for the 21st Century}},
  shorttitle = {{Anti-Piketty}},
  author = {Delsol, Jean-philippe and Lecaussin, Nicolas and Martin, Emmanuel},
  year = {2017},
  publisher = {{Cato Inst}},
  address = {{Washington, D.C}},
  isbn = {978-1-944424-25-1},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Delsol_et_al_2017_Anti-Piketty.epub}
}

@article{demicheaux2020,
  title = {Depth for {{Curve Data}} and {{Applications}}},
  author = {{de Micheaux}, Pierre Lafaye and Mozharovskyi, Pavlo and Vimond, Myriam},
  year = {2020},
  month = mar,
  journal = {Journal of the American Statistical Association},
  volume = {0},
  number = {0},
  pages = {1--17},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.2020.1745815},
  abstract = {In 1975, John W. Tukey defined statistical data depth as a function that determines the centrality of an arbitrary point with respect to a data cloud or to a probability measure. During the last decades, this seminal idea of data depth evolved into a powerful tool proving to be useful in various fields of science. Recently, extending the notion of data depth to the functional setting attracted a lot of attention among theoretical and applied statisticians. We go further and suggest a notion of data depth suitable for data represented as curves, or trajectories, which is independent of the parameterization. We show that our curve depth satisfies theoretical requirements of general depth functions that are meaningful for trajectories. We apply our methodology to diffusion tensor brain images and also to pattern recognition of handwritten digits and letters. Supplementary materials for this article are available online.},
  keywords = {Classification,Data depth,DD-plot,DT-MRI fibers,Nonparametric statistics,todo,Unparameterized curves},
  annotation = {\_eprint: https://doi.org/10.1080/01621459.2020.1745815},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/de_Micheaux_et_al_2020_Depth_for_Curve_Data_and_Applications.pdf;/home/dede/Zotero/storage/LRR4LXXC/01621459.2020.html}
}

@article{demidenko2005,
  title = {Influence Analysis for Linear Mixed-Effects Models},
  author = {Demidenko, Eugene and Stukel, Therese A.},
  year = {2005},
  journal = {Statistics in Medicine},
  volume = {24},
  number = {6},
  pages = {893--909},
  issn = {1097-0258},
  doi = {10.1002/sim.1974},
  abstract = {In this paper, we extend several regression diagnostic techniques commonly used in linear regression, such as leverage, infinitesimal influence, case deletion diagnostics, Cook's distance, and local influence to the linear mixed-effects model. In each case, the proposed new measure has a direct interpretation in terms of the effects on a parameter of interest, and collapses to the familiar linear regression measure when there are no random effects. The new measures are explicitly defined functions and do not necessitate re-estimation of the model, especially for cluster deletion diagnostics. The basis for both the cluster deletion diagnostics and Cook's distance is a generalization of Miller's simple update formula for case deletion for linear models. Pregibon's infinitesimal case deletion diagnostics is adapted to the linear mixed-effects model. A simple compact matrix formula is derived to assess the local influence of the fixed-effects regression coefficients. Finally, a link between the local influence approach and Cook's distance is established. These influence measures are applied to an analysis of 5-year Medicare reimbursements to colon cancer patients to identify the most influential observations and their effects on the fixed-effects coefficients. Copyright \textcopyright{} 2004 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {case deletion,infinitesimal influence,local influence,random effects,repeated measurements,sensitivity analysis},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.1974},
  file = {/home/dede/Zotero/storage/VXU6BI32/Demidenko and Stukel - 2005 - Influence analysis for linear mixed-effects models.pdf;/home/dede/Zotero/storage/4JUCWWCD/sim.html}
}

@article{deshpande2019,
  title = {Simultaneous {{Variable}} and {{Covariance Selection With}} the {{Multivariate Spike-and-Slab LASSO}}},
  author = {Deshpande, Sameer K. and Ro{\v c}kov{\'a}, Veronika and George, Edward I.},
  year = {2019},
  month = oct,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {28},
  number = {4},
  pages = {921--931},
  publisher = {{Taylor \& Francis}},
  issn = {1061-8600},
  doi = {10.1080/10618600.2019.1593179},
  abstract = {We propose a Bayesian procedure for simultaneous variable and covariance selection using continuous spike-and-slab priors in multivariate linear regression models where q possibly correlated responses are regressed onto p predictors. Rather than relying on a stochastic search through the high-dimensional model space, we develop an ECM algorithm similar to the EMVS procedure of Ro\v{c}kov\'a and George targeting modal estimates of the matrix of regression coefficients and residual precision matrix. Varying the scale of the continuous spike densities facilitates dynamic posterior exploration and allows us to filter out negligible regression coefficients and partial covariances gradually. Our method is seen to substantially outperform regularization competitors on simulated data. We demonstrate our method with a re-examination of data from a recent observational study of the effect of playing high school football on several later-life cognition, psychological, and socio-economic outcomes. An R package, scripts for replicating examples in this article, and results from further simulation studies are provided in the supplementary materials available online.},
  keywords = {Bayesian shrinkage,EM algorithm,Gaussian graphical modeling,Multivariate regression,Nonconvex optimization,todo},
  annotation = {\_eprint: https://doi.org/10.1080/10618600.2019.1593179},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Deshpande_et_al_2019_Simultaneous_Variable_and_Covariance_Selection_With_the_Multivariate.pdf;/home/dede/Zotero/storage/BDIB5I7C/10618600.2019.html}
}

@article{dessein2013,
  title = {Online {{Change Detection}} in {{Exponential Families}} with {{Unknown Parameters}}},
  author = {Dessein, Arnaud and Cont, Arshia},
  year = {2013},
  month = aug,
  issn = {978-3-642-40019-3},
  doi = {10.1007/978-3-642-40020-9_70},
  abstract = {This paper studies online change detection in exponential families when both the parameters before and after change are unknown. We follow a standard statistical approach to sequential change detection with generalized likelihood ratio test statistics. We interpret these statistics within the framework of information geometry, hence providing a unified view of change detection for many common statistical models and corresponding distance functions. Using results from convex duality, we also derive an efficient scheme to compute the exact statistics sequentially, which allows their use in online settings where they are usually approximated for the sake of tractability. This is applied to real-world datasets of various natures, including onset detection in audio signals.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Dessein_Cont_2013_Online_Change_Detection_in_Exponential_Families_with_Unknown_Parameters.pdf}
}

@book{diamond1997,
  title = {Guns, {{Germs}}, and {{Steel}}: {{The Fates}} of {{Human Societies}}},
  shorttitle = {Guns, Germs, and Steel},
  author = {Diamond, Jared M.},
  year = {1997},
  edition = {1st ed},
  publisher = {{W.W. Norton \& Co}},
  address = {{New York}},
  isbn = {978-0-393-03891-0},
  langid = {english},
  lccn = {HM206 .D48 1997},
  keywords = {Civilization,Culture diffusion,doing,Effect of environment on,Ethnology,History,Human beings,Social evolution},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Diamond_1997_Guns,_Germs,_and_Steel.pdf}
}

@book{diggle2013,
  title = {{Analysis of Longitudinal Data}},
  shorttitle = {{Analysis of Longitudinal Data (Oxford Statistical Science)}},
  author = {Diggle, Peter and Heagerty, Patrick and Liang, Kung-Yee and Zeger, Scott},
  year = {2013},
  month = aug,
  publisher = {{Oxford University Press, Usa}},
  address = {{Oxford}},
  abstract = {The first edition of Analysis for Longitudinal Data has become a classic. Describing the statistical models and methods for the analysis of longitudinal data, it covers both the underlying statistical theory of each method, and its application to a range of examples from the agricultural and biomedical sciences. The main topics discussed are design issues, exploratory methods of analysis, linear models for continuous data, general linear models for discrete data, and models and methods for handling data and missing values. Under each heading, worked examples are presented in parallel with the methodological development, and sufficient detail is given to enable the reader to reproduce the author's results using the data-sets as an appendix. This second edition, published for the first time in paperback, provides a thorough and expanded revision of this important text. It includes two new chapters; the first discusses fully parametric models for discrete repeated measures data, and the second explores statistical models for time-dependent predictors.},
  isbn = {978-0-19-967675-0},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Diggle_et_al_2013_Analysis_of_Longitudinal_Data_(Oxford_Statistical_Science).pdf}
}

@article{dimakopoulou2018,
  title = {Estimation {{Considerations}} in {{Contextual Bandits}}},
  author = {Dimakopoulou, Maria and Zhou, Zhengyuan and Athey, Susan and Imbens, Guido},
  year = {2018},
  month = dec,
  journal = {arXiv:1711.07077 [cs, econ, stat]},
  eprint = {1711.07077},
  eprinttype = {arxiv},
  primaryclass = {cs, econ, stat},
  abstract = {Contextual bandit algorithms are sensitive to the estimation method of the outcome model as well as the exploration method used, particularly in the presence of rich heterogeneity or complex outcome models, which can lead to difficult estimation problems along the path of learning. We study a consideration for the exploration vs. exploitation framework that does not arise in multi-armed bandits but is crucial in contextual bandits; the way exploration and exploitation is conducted in the present affects the bias and variance in the potential outcome model estimation in subsequent stages of learning. We develop parametric and non-parametric contextual bandits that integrate balancing methods from the causal inference literature in their estimation to make it less prone to problems of estimation bias. We provide the first regret bound analyses for contextual bandits with balancing in the domain of linear contextual bandits that match the state of the art regret bounds. We demonstrate the strong practical advantage of balanced contextual bandits on a large number of supervised learning datasets and on a synthetic example that simulates model mis-specification and prejudice in the initial training data. Additionally, we develop contextual bandits with simpler assignment policies by leveraging sparse model estimation methods from the econometrics literature and demonstrate empirically that in the early stages they can improve the rate of learning and decrease regret.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,done,Economics - Econometrics,Statistics - Machine Learning},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Dimakopoulou_et_al_2018_Estimation_Considerations_in_Contextual_Bandits.pdf;/home/dede/Zotero/storage/4HLVF32P/1711.html}
}

@article{dimatteo2001,
  title = {Bayesian Curve-fitting with Free-knot Splines},
  author = {Dimatteo, Ilaria and Genovese, Christopher R. and Kass, Robert E.},
  year = {2001},
  month = dec,
  journal = {Biometrika},
  volume = {88},
  number = {4},
  pages = {1055--1071},
  issn = {0006-3444},
  doi = {10.1093/biomet/88.4.1055},
  abstract = {We describe a Bayesian method, for fitting curves to data drawn from an exponential family, that uses splines for which the number and locations of knots are free parameters. The method uses reversible-jump Markov chain Monte Carlo to change the knot configurations and a locality heuristic to speed up mixing. For nonnormal models, we approximate the integrated likelihood ratios needed to compute acceptance probabilities by using the Bayesian information criterion, BIC, under priors that make this approximation accurate. Our technique is based on a marginalised chain on the knot number and locations, but we provide methods for inference about the regression coefficients, and functions of them, in both normal and nonnormal models. Simulation results suggest that the method performs well, and we illustrate the method in two neuroscience applications.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Dimatteo_et_al_2001_Bayesian_curveâ€fitting_with_freeâ€knot_splines.pdf;/home/dede/Zotero/storage/JW55Z2SG/225982.html}
}

@article{dixon2021,
  title = {Industrial {{Forecasting}} with {{Exponentially Smoothed Recurrent Neural Networks}}},
  author = {Dixon, Matthew},
  year = {2021},
  month = apr,
  journal = {Technometrics},
  volume = {0},
  number = {0},
  pages = {1--11},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.2021.1921035},
  abstract = {Time series modeling has entered an era of unprecedented growth in the size and complexity of data which require new modeling approaches. While many new general purpose machine learning approaches have emerged, they remain poorly understand and irreconcilable with more traditional statistical modeling approaches. We present a general class of exponential smoothed recurrent neural networks (RNNs) which are well suited to modeling nonstationary dynamical systems arising in industrial applications. In particular, we analyze their capacity to characterize the nonlinear partial autocorrelation structure of time series and directly capture dynamic effects such as seasonality and trends. Application of exponentially smoothed RNNs to forecasting electricity load, weather data, and stock prices highlight the efficacy of exponential smoothing of the hidden state for multistep time series forecasting. The results also suggest that popular, but more complicated neural network architectures originally designed for speech processing are likely over-engineered for industrial forecasting and light-weight exponentially smoothed architectures, trained in a fraction of the time, capture the salient features while being superior and more robust than simple RNNs and autoregressive models. Additionally, uncertainty quantification of Bayesian exponential smoothed RNNs is shown to provide improved coverage.},
  keywords = {Exponential smoothing,Forecasting,Nonstationarity,Partial autocorrelations,Quantification,todo,Uncertainty},
  annotation = {\_eprint: https://doi.org/10.1080/00401706.2021.1921035},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Dixon_2021_Industrial_Forecasting_with_Exponentially_Smoothed_Recurrent_Neural_Networks.pdf;/home/dede/Zotero/storage/RGYFHD8K/00401706.2021.html}
}

@book{docarmo1992,
  title = {Riemannian {{Geometry}}},
  author = {{do Carmo}, Manfredo Perdigao},
  translator = {Flaherty, Francis},
  year = {1992},
  edition = {1st edition},
  publisher = {{Birkh\"auser}},
  address = {{Boston}},
  abstract = {Riemannian Geometry is an expanded edition of a highly acclaimed and successful textbook (originally published in Portuguese)~for first-year graduate students in mathematics and physics. The author's treatment goes very directly to the basic language of Riemannian geometry and immediately presents some of its most fundamental theorems. It is elementary, assuming only a modest background from readers, making it suitable for a wide variety of students and course structures. Its selection of topics has been deemed "superb" by teachers who have used the text. A significant feature of the book is its powerful and revealing structure, beginning simply with the definition of a differentiable manifold and ending with one of the most important results in Riemannian geometry, a proof of the Sphere Theorem. The text abounds with basic definitions and theorems, examples, applications, and numerous exercises to test the student's understanding and extend knowledge and insight into the subject. Instructors and students alike will find the work to be~a significant contribution to this highly applicable and stimulating subject.},
  isbn = {978-0-8176-3490-2},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/do_Carmo_1992_Riemannian_Geometry.pdf}
}

@article{dogu2021a,
  title = {Monitoring Exponentially Distributed Time between Events Data: Self-Starting Perspective},
  shorttitle = {Monitoring Exponentially Distributed Time between Events Data},
  author = {Dogu, Eralp and {Noor-ul-Amin}, Muhammad},
  year = {2021},
  month = jan,
  journal = {Communications in Statistics - Simulation and Computation},
  volume = {0},
  number = {0},
  pages = {1--13},
  publisher = {{Taylor \& Francis}},
  issn = {0361-0918},
  doi = {10.1080/03610918.2021.1874417},
  abstract = {Time between events (TBE) control charts have been widely used to monitor high yield processes. Traditionally, an estimated in-control occurrence rate from a Phase I dataset is used to calculate the control limits when the rate is unknown. However, when Phase I analysis is time consuming or costly, the traditional Phase I/Phase II approach is not feasible. A self-starting method that sequentially updates the occurrence rate can be integrated to overcome this difficulty and leverages the use of TBE chart for cases of lack of in-control data. The motivation behind this study is to compare different self-starting TBE charts to investigate the contribution of such a sequential parameter update method. Our results indicate the potential of these schemes as they provide satisfactory performance when moderate and large sizes of base period before the shift observed. Additionally, time weighted schemes provided promising performance for relatively small sample sizes in the base period. However, our results also show a significant adverse effect in performance when the base periods are contaminated with out-of-control data.},
  keywords = {Exponential data,Self-starting method,Statistical process control (SPC),Time between events (TBE)},
  annotation = {\_eprint: https://doi.org/10.1080/03610918.2021.1874417},
  file = {/home/dede/Zotero/storage/6ZEFVCWW/03610918.2021.html}
}

@article{doksum1974,
  title = {Tailfree and {{Neutral Random Probabilities}} and {{Their Posterior Distributions}}},
  author = {Doksum, Kjell},
  year = {1974},
  month = apr,
  journal = {The Annals of Probability},
  volume = {2},
  number = {2},
  pages = {183--201},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0091-1798, 2168-894X},
  doi = {10.1214/aop/1176996703},
  abstract = {The random distribution function \$F\$ and its law is said to be neutral to the right if \$F(t\_1), \textbackslash lbrack F(t\_2) - F(t\_1) \textbackslash rbrack/\textbackslash lbrack 1 - F(t\_1)\textbackslash rbrack, \textbackslash cdots, \textbackslash lbrack F(t\_k) - F(t\_\{k-1\}) \textbackslash rbrack/\textbackslash lbrack 1 - F(t\_\{k-1\}) \textbackslash rbrack\$ are independent whenever \$t\_1 {$<$} \textbackslash cdots {$<$} t\_k\$. The posterior distribution of a random distribution function neutral to the right is shown to be neutral to the right. Characterizations of these random distribution functions and connections between neutrality to the right and general concepts of neutrality and tailfreeness (tailfreedom) are given.},
  keywords = {60K99,62C10,62G99,Bayes estimates,Dirichlet process,neutral,posterior distributions,posterior mean of a process,processes,Random probabilities,tailfree,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Doksum_1974_Tailfree_and_Neutral_Random_Probabilities_and_Their_Posterior_Distributions.pdf;/home/dede/Zotero/storage/N8XFAGBE/1176996703.html}
}

@book{douc2014,
  title = {{Nonlinear Time Series: Theory, Methods and Applications with R Examples}},
  shorttitle = {{Nonlinear Time Series}},
  author = {Douc, Randal and Moulines, Eric and Stoffer, David},
  year = {2014},
  edition = {1\textdegree{} edizione},
  publisher = {{Chapman and Hall/CRC}},
  address = {{Boca Raton}},
  isbn = {978-1-4665-0225-3},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Douc_et_al_2014_Nonlinear_Time_Series.pdf}
}

@book{doucet2012,
  title = {Sequential {{Monte Carlo Methods}} in {{Practice}}},
  author = {Doucet, Arnaud and de Freitas, Nando and Smith, A.},
  year = {2012},
  month = nov,
  publisher = {{Springer}},
  isbn = {978-1-4757-3438-6},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Doucet_et_al_2012_Sequential_Monte_Carlo_Methods_in_Practice2.pdf}
}

@article{drezner2020,
  title = {Immature Leadership: {{Donald Trump}} and the {{American}} Presidency},
  shorttitle = {Immature Leadership},
  author = {Drezner, Daniel W},
  year = {2020},
  month = mar,
  journal = {International Affairs},
  volume = {96},
  number = {2},
  pages = {383--400},
  issn = {0020-5850},
  doi = {10.1093/ia/iiaa009},
  abstract = {There has been a renaissance in the study of how the backgrounds of individual leaders affect foreign policy outcomes. Donald Trump's presidency highlights the limits of this approach. Trump's psychology is so unique, and so akin to that of a small child, that studying his background alone is insufficient to explain his decision-making. The evidence for this characterization of Trump's leadership comes not from his political opponents, but his allies, staffers and subordinates. Trump's lack of impulse control, short attention span and frequent temper tantrums have all undercut his effectiveness as president as compared to his predecessors. Nonetheless, the 45th president helps to clarify ongoing debates in American politics about the relative strength of the presidency as an institution. In particular, the powers of the presidency have become so enhanced that even comparatively weak and inexperienced leaders can execute dramatic policy shifts. The formal checks on presidential power, from the legislative, judicial and executive branches have all eroded. Similarly, the informal checks on the presidency had also degraded before Trump's inauguration. This article uses Trump's presidency\textemdash and his severe limitations as a decision-maker\textemdash to highlight the ways in which even a weak leader can affect change by holding a powerful office.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Drezner_2020_Immature_leadership.pdf;/home/dede/Zotero/storage/2I62FGUX/5722298.html}
}

@incollection{drovandi2017,
  title = {Approximate {{Bayesian Computation}}},
  booktitle = {Wiley {{StatsRef}}: {{Statistics Reference Online}}},
  author = {Drovandi, Christopher C.},
  year = {2017},
  pages = {1--9},
  publisher = {{American Cancer Society}},
  doi = {10.1002/9781118445112.stat07974},
  abstract = {Bayesian statistics provides a principled framework for performing statistical inference for an unknown parameter of a stochastic model assumed to be responsible for generating some observed data. However, standard Bayesian algorithms to sample from the posterior require that the likelihood function, the probability density of the data given the parameter represented as a function of the parameter for fixed observed data, is computationally tractable. However, there are an increasing number of models across Science and Technology where the likelihood function is difficult or impossible to compute. When simulation from the model is comparatively cheaper, a class of likelihood-free methods called approximate Bayesian computation (ABC) can be used. However, ABC introduces an approximation to the posterior. This article gives an introduction to ABC, describes the approximation behavior of ABC, and provides advice on the successful implementation of ABC. Some current challenges facing ABC methods are also discussed.},
  copyright = {Copyright \textcopyright{} 2017 John Wiley \& Sons, Ltd. All rights reserved.},
  isbn = {978-1-118-44511-2},
  langid = {english},
  keywords = {approximate Bayesian computation,done,Markov chain Monte Carlo,pseudo-marginal methods,sequential Monte Carlo,synthetic likelihood},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118445112.stat07974},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Drovandi_2017_Approximate_Bayesian_Computation.pdf;/home/dede/Documents/MEGA/universita/zotero-pdf/Drovandi_2017_Approximate_Bayesian_Computation2.pdf;/home/dede/Zotero/storage/TYWWUNNV/9781118445112.html}
}

@article{du2016,
  title = {Stepwise {{Signal Extraction}} via {{Marginal Likelihood}}},
  author = {Du, Chao and Kao, Chu-Lan Michael and Kou, S. C.},
  year = {2016},
  month = jan,
  journal = {Journal of the American Statistical Association},
  volume = {111},
  number = {513},
  pages = {314--330},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.2015.1006365},
  abstract = {This article studies the estimation of a stepwise signal. To determine the number and locations of change-points of the stepwise signal, we formulate a maximum marginal likelihood estimator, which can be computed with a quadratic cost using dynamic programming. We carry out an extensive investigation on the choice of the prior distribution and study the asymptotic properties of the maximum marginal likelihood estimator. We propose to treat each possible set of change-points equally and adopt an empirical Bayes approach to specify the prior distribution of segment parameters. A detailed simulation study is performed to compare the effectiveness of this method with other existing methods. We demonstrate our method on single-molecule enzyme reaction data and on DNA array comparative genomic hybridization (CGH) data. Our study shows that this method is applicable to a wide range of models and offers appealing results in practice. Supplementary materials for this article are available online.},
  pmid = {27212739},
  keywords = {Array comparative genomic hybridization,Asymptotic consistency,Change-points,Choice of prior,Dynamic programming,Single-molecule experiment.,todo},
  annotation = {\_eprint: https://doi.org/10.1080/01621459.2015.1006365},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Du_et_al_2016_Stepwise_Signal_Extraction_via_Marginal_Likelihood.pdf;/home/dede/Zotero/storage/DD3HJVGC/01621459.2015.html}
}

@book{dudley2002,
  title = {Real {{Analysis}} and {{Probability}}},
  author = {Dudley, R. M.},
  year = {2002},
  month = oct,
  edition = {2nd edition},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge ; New York}},
  abstract = {This classic textbook, now reissued, offers a clear exposition of modern probability theory and of the interplay between the properties of metric spaces and probability measures. The new edition has been made even more self-contained than before; it now includes a foundation of the real number system and the Stone-Weierstrass theorem on uniform approximation in algebras of functions. Several other sections have been revised and improved, and the comprehensive historical notes have been further amplified. A number of new exercises have been added, together with hints for solution.},
  isbn = {978-0-521-00754-2},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Dudley_2002_Real_Analysis_and_Probability.pdf}
}

@article{dunn1996,
  title = {Randomized {{Quantile Residuals}}},
  author = {Dunn, Peter K. and Smyth, Gordon K.},
  year = {1996},
  journal = {Journal of Computational and Graphical Statistics},
  volume = {5},
  number = {3},
  pages = {236--244},
  publisher = {{[American Statistical Association, Taylor \& Francis, Ltd., Institute of Mathematical Statistics, Interface Foundation of America]}},
  issn = {1061-8600},
  doi = {10.2307/1390802},
  abstract = {In this article we give a general definition of residuals for regression models with independent responses. Our definition produces residuals that are exactly normal, apart from sampling variability in the estimated parameters, by inverting the fitted distribution function for each response value and finding the equivalent standard normal quantile. Our definition includes some randomization to achieve continuous residuals when the response variable is discrete. Quantile residuals are easily computed in computer packages such as SAS, S-Plus, GLIM, or LispStat, and allow residual analyses to be carried out in many commonly occurring situations in which the customary definitions of residuals fail. Quantile residuals are applied in this article to three example data sets.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Dunn_Smyth_1996_Randomized_Quantile_Residuals.pdf}
}

@article{dunson2018,
  title = {Statistics in the Big Data Era: {{Failures}} of the Machine},
  shorttitle = {Statistics in the Big Data Era},
  author = {Dunson, David B.},
  year = {2018},
  month = may,
  journal = {Statistics \& Probability Letters},
  series = {The Role of {{Statistics}} in the Era of Big Data},
  volume = {136},
  pages = {4--9},
  issn = {0167-7152},
  doi = {10.1016/j.spl.2018.02.028},
  abstract = {There is vast interest in automated methods for complex data analysis. However, there is a lack of consideration of (1) interpretability, (2) uncertainty quantification, (3) applications with limited training data, and (4) selection bias. Statistical methods can achieve (1)-(4) with a change in focus.},
  langid = {english},
  keywords = {Deep learning,done,High-dimensional data,Large p small n,Machine learning,Scientific inference,Selection bias,Uncertainty quantification},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Dunson_2018_Statistics_in_the_big_data_era.pdf;/home/dede/Zotero/storage/NEPTPLW6/S0167715218300737.html}
}

@article{durbin2002,
  title = {A Simple and Efficient Simulation Smoother for State Space Time Series Analysis},
  author = {Durbin, J.},
  year = {2002},
  month = aug,
  journal = {Biometrika},
  volume = {89},
  number = {3},
  pages = {603--616},
  issn = {0006-3444, 1464-3510},
  doi = {10.1093/biomet/89.3.603},
  langid = {english},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Durbin_2002_A_simple_and_efficient_simulation_smoother_for_state_space_time_series_analysis.pdf}
}

@book{durbin2012,
  title = {{Time Series Analysis by State Space Methods}},
  author = {Durbin, The late James and Koopman, Siem Jan},
  year = {2012},
  month = jan,
  edition = {2 edizione},
  publisher = {{OUP Oxford}},
  address = {{Oxford}},
  abstract = {This new edition updates Durbin \& Koopman's important text on the state space approach to time series analysis. The distinguishing feature of state space time series models is that observations are regarded as made up of distinct components such as trend, seasonal, regression elements and disturbance terms, each of which is modelled separately. The techniques that emerge from this approach are very flexible and are capable of handling a much wider range of problems than the main analytical system currently in use for time series analysis, the Box-Jenkins ARIMA system. Additions to this second edition include the filtering of nonlinear and non-Gaussian series. Part I of the book obtains the mean and variance of the state, of a variable intended to measure the effect of an interaction and of regression coefficients, in terms of the observations. Part II extends the treatment to nonlinear and non-normal models. For these, analytical solutions are not available so methods are based on simulation.},
  isbn = {978-0-19-964117-8},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Durbin_Koopman_2012_Time_Series_Analysis_by_State_Space_Methods.pdf}
}

@article{ebadi2021,
  title = {Statistical Monitoring of the Covariance Matrix in Multivariate Processes: {{A}} Literature Review},
  shorttitle = {Statistical Monitoring of the Covariance Matrix in Multivariate Processes},
  author = {Ebadi, Mohsen and Chenouri, Shojaeddin and Lin, Dennis K. J. and Steiner, Stefan H.},
  year = {2021},
  month = may,
  journal = {Journal of Quality Technology},
  volume = {0},
  number = {0},
  pages = {1--136},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2021.1889419},
  abstract = {Monitoring several correlated quality characteristics of a process is common in modern manufacturing and service industries. Although a lot of attention has been paid to monitoring the multivariate process mean, not many control charts are available for monitoring the covariance matrix. This paper presents a comprehensive overview of the literature on control charts for monitoring the covariance matrix in a multivariate statistical process monitoring (MSPM) framework. It classifies the research that has previously appeared in the literature. We highlight the challenging areas for research and provide some directions for future research.},
  keywords = {covariance matrix,multivariate control charts,Phase I and Phase II,statistical process monitoring (SPM),todo},
  annotation = {\_eprint: https://doi.org/10.1080/00224065.2021.1889419},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Ebadi_et_al_2021_Statistical_monitoring_of_the_covariance_matrix_in_multivariate_processes.pdf;/home/dede/Zotero/storage/RNT4CB4V/00224065.2021.html}
}

@article{eckle2019,
  title = {A Comparison of Deep Networks with {{ReLU}} Activation Function and Linear Spline-Type Methods},
  author = {Eckle, Konstantin and {Schmidt-Hieber}, Johannes},
  year = {2019},
  month = feb,
  journal = {Neural Networks},
  volume = {110},
  pages = {232--242},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2018.11.005},
  abstract = {Deep neural networks (DNNs) generate much richer function spaces than shallow networks. Since the function spaces induced by shallow networks have several approximation theoretic drawbacks, this explains, however, not necessarily the success of deep networks. In this article we take another route by comparing the expressive power of DNNs with ReLU activation function to linear spline methods. We show that MARS (multivariate adaptive regression splines) is improper learnable by DNNs in the sense that for any given function that can be expressed as a function in MARS with M parameters there exists a multilayer neural network with O(Mlog(M/{$\epsilon$})) parameters that approximates this function up to sup-norm error {$\epsilon$}. We show a similar result for expansions with respect to the Faber\textendash Schauder system. Based on this, we derive risk comparison inequalities that bound the statistical risk of fitting a neural network by the statistical risk of spline-based methods. This shows that deep networks perform better or only slightly worse than the considered spline methods. We provide a constructive proof for the function approximations.},
  langid = {english},
  keywords = {Deep neural networks,Faberâ€“Schauder system,MARS,Nonparametric regression,Rates of convergence,Splines,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Eckle_Schmidt-Hieber_2019_A_comparison_of_deep_networks_with_ReLU_activation_function_and_linear.pdf}
}

@article{eddelbuettel2011,
  title = {Rcpp: {{Seamless R}} and {{C}}++ {{Integration}}},
  shorttitle = {Rcpp},
  author = {Eddelbuettel, Dirk and Francois, Romain},
  year = {2011},
  month = apr,
  journal = {Journal of Statistical Software},
  volume = {40},
  number = {1},
  pages = {1--18},
  issn = {1548-7660},
  doi = {10.18637/jss.v040.i08},
  copyright = {Copyright (c) 2010 Dirk Eddelbuettel, Romain Francois},
  langid = {english},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Eddelbuettel_Francois_2011_Rcpp.pdf;/home/dede/Zotero/storage/6ZBAF9SF/v040i08.html}
}

@book{eddelbuettel2013,
  title = {Seamless {{R}} and {{C}}++ {{Integration}} with {{Rcpp}}},
  author = {Eddelbuettel, Dirk},
  year = {2013},
  publisher = {{Springer-Verlag}},
  address = {{New York}},
  doi = {10.1007/978-1-4614-6868-4},
  abstract = {Rcpp is the glue that binds the power and versatility of R with the speed and efficiency of C++. With Rcpp, the transfer of data between R and C++ is nearly seamless, and high-performance statistical computing is finally accessible to most R users. Rcpp should be part of every statistician's toolbox. -- Michael Braun, MIT Sloan School of Management "Seamless R and C++ integration with Rcpp" is simply a wonderful book. For anyone who uses C/C++ and R, it is an indispensable resource. The writing is outstanding. A huge bonus is the section on applications. This section covers the matrix packages Armadillo and Eigen and the GNU Scientific Library as well as RInside which enables you to use R inside C++. These applications are what most of us need to know to really do scientific programming with R and C++. I love this book. -- Robert McCulloch, University of Chicago Booth School of Business Rcpp is now considered an essential package for anybody doing serious computational research using R. Dirk's book is an excellent companion and takes the reader from a gentle introduction to more advanced applications via numerous examples and efficiency enhancing gems. The book is packed with all you might have ever wanted to know about Rcpp, its cousins (RcppArmadillo, RcppEigen .etc.), modules, package development and sugar. Overall, this book is a must-have on your shelf. -- Sanjog Misra, UCLA Anderson School of ManagementThe Rcpp package represents a major leap forward for scientific computations with R. With very few lines of C++ code, one has R's data structures readily at hand for further computations in C++. Hence, high-level numerical programming can be made in C++ almost as easily as in R, but often with a substantial speed gain. Dirk is a crucial person in these developments, and his book takes the reader from the first fragile steps on to using the full Rcpp machinery. A very recommended book! -- S\o ren H\o jsgaard, Department of Mathematical Sciences, Aalborg University, Denmark "Seamless R and C ++ Integration with Rcpp" provides the first comprehensive introduction to Rcpp. Rcpp has become the most widely-used language extension for R, and is deployed by over one-hundred different CRAN and BioConductor packages. Rcpp permits users to pass scalars, vectors, matrices, list or entire R objects back and forth between R and C++ with ease. This brings the depth of the R analysis framework together with the power, speed, and efficiency of C++.Dirk Eddelbuettel has been a contributor to CRAN for over a decade and maintains around twenty packages. He is the Debian/Ubuntu maintainer for R and other quantitative software, edits the CRAN Task Views for Finance and High-Performance Computing, is a co-founder of the annual R/Finance conference, and an editor of the Journal of Statistical Software. He holds a Ph.D. in Mathematical Economics from EHESS (Paris), and works in Chicago as a Senior Quantitative Analyst.},
  isbn = {978-1-4614-6867-7},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Eddelbuettel_2013_Seamless_R_and_C++_Integration_with_Rcpp.pdf;/home/dede/Zotero/storage/SNMLCZV4/9781461468677.html}
}

@article{eddelbuettel2014,
  title = {{{RcppArmadillo}}: {{Accelerating R}} with High-Performance {{C}}++~Linear Algebra},
  shorttitle = {{{RcppArmadillo}}},
  author = {Eddelbuettel, Dirk and Sanderson, Conrad},
  year = {2014},
  month = mar,
  journal = {Computational Statistics \& Data Analysis},
  volume = {71},
  pages = {1054--1063},
  issn = {0167-9473},
  doi = {10.1016/j.csda.2013.02.005},
  abstract = {The R statistical environment and language has demonstrated particular strengths for interactive development of statistical algorithms, as well as data modelling and visualisation. Its current implementation has an interpreter at its core which may result in a performance penalty in comparison to directly executing user algorithms in the native machine code of the host CPU. In contrast, the C++~language has no built-in visualisation capabilities, handling of linear algebra or even basic statistical algorithms; however, user programs are converted to high-performance machine code, ahead of execution. A new method avoids possible speed penalties in R by using the Rcpp extension package in conjunction with the Armadillo C++~matrix library. In addition to the inherent performance advantages of compiled code, Armadillo provides an easy-to-use template-based meta-programming framework, allowing the automatic pooling of several linear algebra operations into one, which in turn can lead to further speedups. With the aid of Rcpp and Armadillo, conversion of linear algebra centred algorithms from R to C++~becomes straightforward. The algorithms retain the overall structure as well as readability, all while maintaining a bidirectional link with the host R environment. Empirical timing comparisons of R and C++~implementations of a Kalman filtering algorithm indicate a speedup of several orders of magnitude.},
  langid = {english},
  keywords = {C++,Linear algebra,R,Software,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Eddelbuettel_Sanderson_2014_RcppArmadillo.pdf;/home/dede/Zotero/storage/YNIMXC8P/S0167947313000492.html}
}

@article{eddelbuettel2018,
  title = {Extending {{R}} with {{C}}++: {{A Brief Introduction}} to {{Rcpp}}},
  shorttitle = {Extending {{R}} with {{C}}++},
  author = {Eddelbuettel, Dirk and Balamuta, James Joseph},
  year = {2018},
  month = jan,
  journal = {The American Statistician},
  volume = {72},
  number = {1},
  pages = {28--36},
  publisher = {{Taylor \& Francis}},
  issn = {0003-1305},
  doi = {10.1080/00031305.2017.1375990},
  abstract = {R has always provided an application programming interface (API) for extensions. Based on the C language, it uses a number of macros and other low-level constructs to exchange data structures between the R process and any dynamically loaded component modules authors added to it. With the introduction of the Rcpp package, and its later refinements, this process has become considerably easier yet also more robust. By now, Rcpp has become the most popular extension mechanism for R. This article introduces Rcpp, and illustrates with several examples how the Rcpp Attributes mechanism in particular eases the transition of objects between R and C++ code. Supplementary materials for this article are available online.},
  keywords = {Applications and case studies,Computationally intensive methods,Simulation,Statistical computing,todo},
  annotation = {\_eprint: https://doi.org/10.1080/00031305.2017.1375990},
  file = {/home/dede/Zotero/storage/IR2T8WKG/00031305.2017.html}
}

@book{efron1982,
  title = {{The Jackknife, the Bootstrap, and Other Resampling Plans}},
  author = {Efron, Bradley},
  year = {1982},
  publisher = {{Society for Industrial and Applied Mathematics}},
  address = {{Philadelphia, Pa}},
  isbn = {978-0-89871-179-0},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Efron_1982_The_Jackknife,_the_Bootstrap,_and_Other_Resampling_Plans.pdf}
}

@article{efron1986,
  title = {Why {{Isn}}'t {{Everyone}} a {{Bayesian}}?},
  author = {Efron, B.},
  year = {1986},
  month = feb,
  journal = {The American Statistician},
  volume = {40},
  number = {1},
  pages = {1--5},
  publisher = {{Taylor \& Francis}},
  issn = {0003-1305},
  doi = {10.1080/00031305.1986.10475342},
  abstract = {Originally a talk delivered at a conference on Bayesian statistics, this article attempts to answer the following question: why is most scientific data analysis carried out in a non-Bayesian framework? The argument consists mainly of some practical examples of data analysis, in which the Bayesian approach is difficult but Fisherian/frequentist solutions are relatively easy. There is a brief discussion of objectivity in statistical analyses and of the difficulties of achieving objectivity within a Bayesian framework. The article ends with a list of practical advantages of Fisherian/frequentist methods, which so far seem to have outweighed the philosophical superiority of Bayesianism.},
  keywords = {done,Fisherian inference,Frequentist theory,Neyman-Pearson-Wald,Objectivity},
  annotation = {\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/00031305.1986.10475342},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Efron_1986_Why_Isn't_Everyone_a_Bayesian.pdf;/home/dede/Zotero/storage/V6ABFDKP/00031305.1986.html}
}

@book{efron1993,
  title = {An {{Introduction}} to the {{Bootstrap}}},
  author = {Efron, Bradley and Tibshirani, R. J.},
  year = {1993},
  month = jan,
  edition = {1 edizione},
  publisher = {{Chapman and Hall/CRC}},
  address = {{New York}},
  abstract = {Statistics is a subject of many uses and surprisingly few effective practitioners. The traditional road to statistical knowledge is blocked, for most, by a formidable wall of mathematics. The approach in An Introduction to the Bootstrap avoids that wall. It arms scientists and engineers, as well as statisticians, with the computational techniques they need to analyze and understand complicated data sets.},
  isbn = {978-0-412-04231-7},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Efron_Tibshirani_1993_An_Introduction_to_the_Bootstrap.pdf}
}

@article{efron1996,
  title = {Empirical Bayes Methods for Combining Likelihoods},
  author = {Efron, Bradley},
  year = {1996},
  journal = {Journal of the American Statistical Association},
  pages = {13},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Efron_1996_Empirical_bayes_methods_for_combining_likelihoods.pdf}
}

@article{efron1998,
  title = {R. {{A}}. {{Fisher}} in the 21st Century},
  author = {Efron, Bradley},
  year = {1998},
  month = may,
  journal = {Statistical Science},
  volume = {13},
  number = {2},
  pages = {95--122},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/ss/1028905930},
  abstract = {Fisher is the single most important figure in 20th century statistics. This talk examines his influence on modern statistical thinking, trying to predict how Fisherian we can expect the 21st century to be. Fisher's philosophy is characterized as a series of shrewd compromises between the Bayesian and frequentist viewpoints, augmented by some unique characteristics that are particularly useful in applied problems. Several current research topics are examined with an eye toward Fisherian influence, or the lack of it, and what this portends for future statistical developments. Based on the 1996 Fisher lecture, the article closely follows the text of that talk.},
  langid = {english},
  mrnumber = {MR1647499},
  zmnumber = {1074.01536},
  keywords = {Bayes,bootstrap,confidence intervals,done,empirical Bayes,fiducial,frequentist,model selection,Statistical inference},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Efron_1998_R.pdf;/home/dede/Zotero/storage/KJZFSZ3Y/1028905930.html}
}

@article{efron2004,
  title = {Least Angle Regression},
  author = {Efron, Bradley and Hastie, Trevor and Johnstone, Iain and Tibshirani, Robert},
  year = {2004},
  month = apr,
  journal = {The Annals of Statistics},
  volume = {32},
  number = {2},
  pages = {407--499},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/009053604000000067},
  abstract = {The purpose of model selection algorithms such as All Subsets, Forward Selection and Backward Elimination is to choose a linear model on the basis of the same set of data to which the model will be applied. Typically we have available a large collection of possible covariates from which we hope to select a parsimonious set for the efficient prediction of a response variable. Least Angle Regression (LARS), a new model selection algorithm, is a useful and less greedy version of traditional forward selection methods. Three main properties are derived: (1) A simple modification of the LARS algorithm implements the Lasso, an attractive version of ordinary least squares that constrains the sum of the absolute regression coefficients; the LARS modification calculates all possible Lasso estimates for a given problem, using an order of magnitude less computer time than previous methods. (2) A different LARS modification efficiently implements Forward Stagewise linear regression, another promising new model selection method; this connection explains the similar numerical results previously observed for the Lasso and Stagewise, and helps us understand the properties of both methods, which are seen as constrained versions of the simpler LARS algorithm. (3) A simple approximation for the degrees of freedom of a LARS estimate is available, from which we derive a Cp estimate of prediction error; this allows a principled choice among the range of possible LARS estimates. LARS and its variants are computationally efficient: the paper describes a publicly available algorithm that requires only the same order of magnitude of computational effort as ordinary least squares applied to the full set of covariates.},
  langid = {english},
  mrnumber = {MR2060166},
  zmnumber = {1091.62054},
  keywords = {boosting,coefficient paths,done,Lasso,linear regression,variable selection},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Efron_et_al_2004_Least_angle_regression.pdf;/home/dede/Zotero/storage/283NSS39/1083178935.html}
}

@book{efron2016,
  title = {{Computer Age Statistical Inference: Algorithms, Evidence, and Data Science}},
  shorttitle = {{Computer Age Statistical Inference}},
  author = {Efron, Bradley and Hastie, Trevor},
  year = {2016},
  month = jul,
  publisher = {{Cambridge University Press}},
  address = {{New York}},
  abstract = {The twenty-first century has seen a breathtaking expansion of statistical methodology, both in scope and in influence. 'Big data', 'data science', and 'machine learning' have become familiar terms in the news, as statistical methods are brought to bear upon the enormous data sets of modern science and commerce. How did we get here? And where are we going? This book takes us on an exhilarating journey through the revolution in data analysis following the introduction of electronic computation in the 1950s. Beginning with classical inferential theories - Bayesian, frequentist, Fisherian - individual chapters take up a series of influential topics: survival analysis, logistic regression, empirical Bayes, the jackknife and bootstrap, random forests, neural networks, Markov chain Monte Carlo, inference after model selection, and dozens more. The distinctly modern approach integrates methodology and algorithms with statistical inference. The book ends with speculation on the future direction of statistics and data science.},
  isbn = {978-1-107-14989-2},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Efron_Hastie_2016_Computer_Age_Statistical_Inference.pdf}
}

@article{efron2019,
  title = {Bayes, {{Oracle Bayes}} and {{Empirical Bayes}}},
  author = {Efron, Bradley},
  year = {2019},
  month = may,
  journal = {Statistical Science},
  volume = {34},
  number = {2},
  pages = {177--201},
  issn = {0883-4237},
  doi = {10.1214/18-STS674},
  abstract = {This article concerns the Bayes and frequentist aspects of empirical Bayes inference. Some of the ideas explored go back to Robbins in the 1950s, while others are current. Several examples are discussed, real and artificial, illustrating the two faces of empirical Bayes methodology: ``oracle Bayes'' shows empirical Bayes in its most frequentist mode, while ``finite Bayes inference'' is a fundamentally Bayesian application. In either case, modern theory and computation allow us to present a sharp finite-sample picture of what is at stake in an empirical Bayes analysis.},
  langid = {english},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Efron_2019_Bayes,_Oracle_Bayes_and_Empirical_Bayes.pdf}
}

@article{efron2020,
  title = {Prediction, {{Estimation}}, and {{Attribution}}},
  author = {Efron, Bradley},
  year = {2020},
  month = apr,
  journal = {Journal of the American Statistical Association},
  volume = {115},
  number = {530},
  pages = {636--655},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.2020.1762613},
  abstract = {The scientific needs and computational limitations of the twentieth century fashioned classical statistical methodology. Both the needs and limitations have changed in the twenty-first, and so has the methodology. Large-scale prediction algorithms\textemdash neural nets, deep learning, boosting, support vector machines, random forests\textemdash have achieved star status in the popular press. They are recognizable as heirs to the regression tradition, but ones carried out at enormous scale and on titanic datasets. How do these algorithms compare with standard regression techniques such as ordinary least squares or logistic regression? Several key discrepancies will be examined, centering on the differences between prediction and estimation or prediction and attribution (significance testing). Most of the discussion is carried out through small numerical examples.},
  keywords = {Black box,done,Ephemeral predictors,Random forests,Surface plus noise},
  annotation = {\_eprint: https://doi.org/10.1080/01621459.2020.1762613},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Efron_2020_Prediction,_Estimation,_and_Attribution.pdf;/home/dede/Zotero/storage/QYEJ37R7/01621459.2020.html}
}

@article{egeland2020,
  title = {Who Stole Disarmament? {{History}} and Nostalgia in Nuclear Abolition Discourse},
  shorttitle = {Who Stole Disarmament?},
  author = {Egeland, Kj{\o}lv},
  year = {2020},
  month = sep,
  journal = {International Affairs},
  volume = {96},
  number = {5},
  pages = {1387--1403},
  issn = {0020-5850},
  doi = {10.1093/ia/iiaa096},
  abstract = {Influential members of the disarmament community have in recent years maintained that further progress towards the international community's nominally shared goal of a world without nuclear weapons depends on recapturing the spirit and practices of cooperation that prevailed in the late 1980s and 1990s. Proponents of abolition, in this view, should focus their efforts on revitalizing the tried and tested arms control formula that was implemented following the end of the Cold War. In this article, I argue that this call to make disarmament great again reflects unwarranted nostalgia for a past that never was, fostering overconfidence in established approaches to the elimination of nuclear weapons. Far from putting the world on course to nuclear abolition, the end of the Cold War saw the legitimation of nuclear weapons as a hedge against `future uncertainties' and entrenchment of the power structures that sustain the retention of nuclear armouries. By overselling past progress towards the elimination of nuclear arms, the nostalgic narrative of a lost abolitionist consensus is used to rationalize the existing nuclear order and delegitimize the pursuit of new approaches to elimination such as the movement to stigmatize nuclear weapons and the practice of nuclear deterrence.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Egeland_2020_Who_stole_disarmament.pdf;/home/dede/Zotero/storage/62EUKIUG/5875277.html}
}

@inproceedings{egele2013,
  title = {{{COMPA}}: {{Detecting Compromised Accounts}} on {{Social Networks}}},
  shorttitle = {{{COMPA}}},
  booktitle = {{{NDSS}}},
  author = {Egele, Manuel and Stringhini, G. and Kr{\"u}gel, Christopher and Vigna, Giovanni},
  year = {2013},
  abstract = {This work has extensively studied the use of fake (Sybil) accounts that attackers set up to distribute spam messages, which typically exhibit highly anomalous behavior, and hence, are relatively easy to detect. As social networking sites have risen in popularity, cyber-criminals started to exploit these sites to spread malware and to carry out scams. Previous work has extensively studied the use of fake (Sybil) accounts that attackers set up to distribute spam messages (mostly messages that contain links to scam pages or drive-by download sites). Fake accounts typically exhibit highly anomalous behavior, and hence, are relatively easy to detect. As a response, attackers have started to compromise and abuse legitimate accounts. Compromising legitimate accounts is very effective, as attackers can leverage the trust relationships that the account owners have established in the past. Moreover, compromised accounts are more difficult to clean up because a social network provider cannot simply delete the correspond-},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Egele_et_al_2013_COMPA.pdf}
}

@article{egele2015,
  title = {Towards {{Detecting Compromised Accounts}} on {{Social Networks}}},
  author = {Egele, Manuel and Stringhini, Gianluca and Kruegel, Christopher and Vigna, Giovanni},
  year = {2015},
  month = sep,
  journal = {IEEE Transactions on Dependable and Secure Computing},
  volume = {14},
  doi = {10.1109/TDSC.2015.2479616},
  abstract = {Compromising social network accounts has become a profitable course of action for cybercriminals. By hijacking control of a popular media or business account, attackers can distribute their malicious messages or disseminate fake information to a large user base. The impacts of these incidents range from a tarnished reputation to multi-billion dollar monetary losses on financial markets. In our previous work, we demonstrated how we can detect large-scale compromises (i.e., so-called campaigns) of regular online social network users. In this work, we show how we can use similar techniques to identify compromises of individual high-profile accounts. High-profile accounts frequently have one characteristic that makes this detection reliable -- they show consistent behavior over time. We show that our system, were it deployed, would have been able to detect and prevent three real-world attacks against popular companies and news agencies. Furthermore, our system, in contrast to popular media, would not have fallen for a staged compromise instigated by a US restaurant chain for publicity reasons.},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Egele_et_al_2015_Towards_Detecting_Compromised_Accounts_on_Social_Networks.pdf}
}

@article{eilers1996,
  title = {Flexible Smoothing with {{B-splines}} and Penalties},
  author = {Eilers, Paul H. C. and Marx, Brian D.},
  year = {1996},
  month = may,
  journal = {Statistical Science},
  volume = {11},
  number = {2},
  pages = {89--121},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/ss/1038425655},
  abstract = {B-splines are attractive for nonparametric modelling, but choosing the optimal number and positions of knots is a complex task. Equidistant knots can be used, but their small and discrete number allows only limited control over smoothness and fit. We propose to use a relatively large number of knots and a difference penalty on coefficients of adjacent B-splines. We show connections to the familiar spline penalty on the integral of the squared second derivative. A short overview of B-splines, of their construction and of penalized likelihood is presented. We discuss properties of penalized B-splines and propose various criteria for the choice of an optimal penalty parameter. Nonparametric logistic regression, density estimation and scatterplot smoothing are used as examples. Some details of the computations are presented.},
  keywords = {Density estimation,generalized linear models,nonparametric models,smoothing,splines},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Eilers_Marx_1996_Flexible_smoothing_with_B-splines_and_penalties.pdf;/home/dede/Zotero/storage/XGS626PD/1038425655.html}
}

@article{eklund2016,
  title = {Cluster Failure: {{Why fMRI}} Inferences for Spatial Extent Have Inflated False-Positive Rates},
  shorttitle = {Cluster Failure},
  author = {Eklund, Anders and Nichols, Thomas E. and Knutsson, Hans},
  year = {2016},
  month = jul,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {113},
  number = {28},
  pages = {7900--7905},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1602413113},
  abstract = {The most widely used task functional magnetic resonance imaging (fMRI) analyses use parametric statistical methods that depend on a variety of assumptions. In this work, we use real resting-state data and a total of 3 million random task group analyses to compute empirical familywise error rates for the fMRI software packages SPM, FSL, and AFNI, as well as a nonparametric permutation method. For a nominal familywise error rate of 5\%, the parametric statistical methods are shown to be conservative for voxelwise inference and invalid for clusterwise inference. Our results suggest that the principal cause of the invalid cluster inferences is spatial autocorrelation functions that do not follow the assumed Gaussian shape. By comparison, the nonparametric permutation test is found to produce nominal results for voxelwise as well as clusterwise inference. These findings speak to the need of validating the statistical methods being used in the field of neuroimaging.},
  chapter = {Biological Sciences},
  copyright = {\textcopyright{}  . Freely available online through the PNAS open access option.},
  langid = {english},
  pmid = {27357684},
  keywords = {cluster inference,false positives,fMRI,permutation test,statistics,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Eklund_et_al_2016_Cluster_failure.pdf;/home/dede/Zotero/storage/FMEZGJEN/7900.html}
}

@article{elmachtoub2017,
  title = {A {{Practical Method}} for {{Solving Contextual Bandit Problems Using Decision Trees}}},
  author = {Elmachtoub, Adam and Mcnellis, Ryan and Oh, Sechan and Petrik, Marek},
  year = {2017},
  month = jun,
  abstract = {Many efficient algorithms with strong theoretical guarantees have been proposed for the contextual multi-armed bandit problem. However, applying these algorithms in practice can be difficult because they require domain expertise to build appropriate features and to tune their parameters. We propose a new method for the contextual bandit problem that is simple, practical, and can be applied with little or no domain expertise. Our algorithm relies on decision trees to model the context-reward relationship. Decision trees are non-parametric, interpretable, and work well without hand-crafted features. To guide the exploration-exploitation trade-off, we use a bootstrapping approach which abstracts Thompson sampling to non-Bayesian settings. We also discuss several computational heuristics and demonstrate the performance of our method on several datasets.},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Elmachtoub_et_al_2017_A_Practical_Method_for_Solving_Contextual_Bandit_Problems_Using_Decision_Trees.pdf}
}

@article{ernst2004,
  title = {Permutation {{Methods}}: {{A Basis}} for {{Exact Inference}}},
  shorttitle = {Permutation {{Methods}}},
  author = {Ernst, Michael D.},
  year = {2004},
  month = nov,
  journal = {Statistical Science},
  volume = {19},
  number = {4},
  pages = {676--685},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/088342304000000396},
  abstract = {The use of permutation methods for exact inference dates back to Fisher in 1935. Since then, the practicality of such methods has increased steadily with computing power. They can now easily be employed in many situations without concern for computing difficulties. We discuss the reasoning behind these methods and describe situations when they are exact and distribution-free. We illustrate their use in several examples.},
  langid = {english},
  mrnumber = {MR2185589},
  zmnumber = {1100.62563},
  keywords = {Distribution-free,done,Monte Carlo,nonparametric,permutation tests,randomization tests},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Ernst_2004_Permutation_Methods.pdf;/home/dede/Zotero/storage/BDM3KRVZ/1113832732.html}
}

@article{escobar1995,
  title = {Bayesian {{Density Estimation}} and {{Inference Using Mixtures}}},
  author = {Escobar, Michael D. and West, Mike},
  year = {1995},
  month = jun,
  journal = {Journal of the American Statistical Association},
  volume = {90},
  number = {430},
  pages = {577--588},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.1995.10476550},
  abstract = {We describe and illustrate Bayesian inference in models for density estimation using mixtures of Dirichlet processes. These models provide natural settings for density estimation and are exemplified by special cases where data are modeled as a sample from mixtures of normal distributions. Efficient simulation methods are used to approximate various prior, posterior, and predictive distributions. This allows for direct inference on a variety of practical issues, including problems of local versus global smoothing, uncertainty about density estimates, assessment of modality, and the inference on the numbers of components. Also, convergence results are established for a general class of normal mixture models.},
  keywords = {Kernel estimation,Mixtures of Dirichlet processes,Multimodality,Normal mixtures,Posterior sampling,Smoothing parameter estimation,todo},
  annotation = {\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/01621459.1995.10476550},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Escobar_West_1995_Bayesian_Density_Estimation_and_Inference_Using_Mixtures.pdf;/home/dede/Zotero/storage/6QFITQJ9/01621459.1995.html}
}

@article{esmenda2018,
  title = {Robust Estimation of a Multilevel Model with Structural Change},
  author = {Esmenda, Mary Jane and Barrios, Erniel B.},
  year = {2018},
  month = apr,
  journal = {Communications in Statistics - Simulation and Computation},
  volume = {47},
  number = {4},
  pages = {1014--1027},
  publisher = {{Taylor \& Francis}},
  issn = {0361-0918},
  doi = {10.1080/03610918.2017.1300270},
  abstract = {We postulate a spatiotemporal multilevel model and estimate using forward search algorithm and MLE imbedded into the backfitting algorithm. Forward search algorithm ensures robustness of the estimates by filtering the effect of temporary structural changes in the estimation of the group-level covariates, the individual-level covariates and spatial parameters. Backfitting algorithm provides computational efficiency of estimation procedure assuming an additive model. Simulation studies show that estimates are robust even in the presence of structural changes induced for example by epidemic outbreak. The model also produced robust estimates even for small sample and short time series common in epidemiological settings.},
  keywords = {62F35,62G35,68U20,Forward search algorithm,Multilevel model,Spatiotemporal model,Temporary structural change,todo},
  annotation = {\_eprint: https://doi.org/10.1080/03610918.2017.1300270},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Esmenda_Barrios_2018_Robust_estimation_of_a_multilevel_model_with_structural_change.pdf;/home/dede/Zotero/storage/4UWUHD9Y/03610918.2017.html}
}

@book{evans2006,
  title = {{Probability and Statistics: The Science of Uncertainty}},
  shorttitle = {{Probability and Statistics}},
  author = {Evans, Michael J. and Rosenthal, Jeffrey S.},
  year = {2006},
  publisher = {{W H Freeman \& Co}},
  address = {{New York}},
  isbn = {978-0-7167-6219-5},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Evans_Rosenthal_2006_Probability_and_Statistics.pdf}
}

@book{fahrmeir2010,
  title = {{Multivariate Statistical Modelling Based on Generalized Linear Models}},
  author = {Fahrmeir, Ludwig and Tutz, Gerhard},
  year = {2010},
  month = dec,
  publisher = {{Springer Nature}},
  address = {{New York; London}},
  abstract = {The book is aimed at applied statisticians, graduate students of statistics, and students and researchers with a strong interest in statistics and data analysis. This second edition is extensively revised, especially those sections relating with Bayesian concepts.},
  isbn = {978-1-4419-2900-6},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Fahrmeir_Tutz_2010_Multivariate_Statistical_Modelling_Based_on_Generalized_Linear_Models.pdf}
}

@book{fan2005,
  title = {Nonlinear {{Time Series}}: {{Nonparametric}} and {{Parametric Methods}}},
  shorttitle = {Nonlinear {{Time Series}}},
  author = {Fan, Jianqing and Yao, Qiwei},
  year = {2005},
  month = aug,
  publisher = {{Springer}},
  abstract = {This is the first book that integrates useful parametric and nonparametric techniques with time series modeling and prediction, the two important goals of time series analysis. Such a book will benefit researchers and practitioners in various fields such as econometricians, meteorologists, biologists, among others who wish to learn useful time series methods within a short period of time. The book also intends to serve as a reference or text book for graduate students in statistics and econometrics.},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Fan_Yao_2005_Nonlinear_Time_Series.pdf}
}

@inproceedings{fan2018,
  title = {Statistical {{Sparse Online Regression}}: {{A Diffusion Approximation Perspective}}},
  shorttitle = {Statistical {{Sparse Online Regression}}},
  booktitle = {Proceedings of the {{Twenty-First International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Fan, Jianqing and Gong, Wenyan and Li, Chris Junchi and Sun, Qiang},
  year = {2018},
  month = mar,
  pages = {1017--1026},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {In this paper, we propose to adopt the diffusion approximation techniques to study online regression. The diffusion approximation techniques allow us to characterize the exact dynamics of the online regression process. As a consequence, we obtain the optimal statistical rate of convergence up to a logarithmic factor of the streaming sample size. Using the idea of trajectory averaging, we further improve the rate of convergence by eliminating the logarithmic factor. Lastly, we propose a two-step algorithm for sparse online regression: a burn-in step using offline learning and a refinement step using a variant of truncated stochastic gradient descent. Under appropriate assumptions, we show the proposed algorithm produces near optimal sparse estimators. Numerical experiments lend further support to our obtained theory.},
  langid = {english},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Fan_et_al_2018_Statistical_Sparse_Online_Regression2.pdf}
}

@article{fanelli2010,
  title = {``{{Positive}}'' {{Results Increase Down}} the {{Hierarchy}} of the {{Sciences}}},
  author = {Fanelli, Daniele},
  year = {2010},
  month = apr,
  journal = {PLOS ONE},
  volume = {5},
  number = {4},
  pages = {e10068},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0010068},
  abstract = {The hypothesis of a Hierarchy of the Sciences with physical sciences at the top, social sciences at the bottom, and biological sciences in-between is nearly 200 years old. This order is intuitive and reflected in many features of academic life, but whether it reflects the ``hardness'' of scientific research\textemdash i.e., the extent to which research questions and results are determined by data and theories as opposed to non-cognitive factors\textemdash is controversial. This study analysed 2434 papers published in all disciplines and that declared to have tested a hypothesis. It was determined how many papers reported a ``positive'' (full or partial) or ``negative'' support for the tested hypothesis. If the hierarchy hypothesis is correct, then researchers in ``softer'' sciences should have fewer constraints to their conscious and unconscious biases, and therefore report more positive outcomes. Results confirmed the predictions at all levels considered: discipline, domain and methodology broadly defined. Controlling for observed differences between pure and applied disciplines, and between papers testing one or several hypotheses, the odds of reporting a positive result were around 5 times higher among papers in the disciplines of Psychology and Psychiatry and Economics and Business compared to Space Science, 2.3 times higher in the domain of social sciences compared to the physical sciences, and 3.4 times higher in studies applying behavioural and social methodologies on people compared to physical and chemical studies on non-biological material. In all comparisons, biological studies had intermediate values. These results suggest that the nature of hypotheses tested and the logical and methodological rigour employed to test them vary systematically across disciplines and fields, depending on the complexity of the subject matter and possibly other factors (e.g., a field's level of historical and/or intellectual development). On the other hand, these results support the scientific status of the social sciences against claims that they are completely subjective, by showing that, when they adopt a scientific approach to discovery, they differ from the natural sciences only by a matter of degree.},
  langid = {english},
  keywords = {Forecasting,Mental health and psychiatry,Physical sciences,Scientists,Social psychology,Social research,Social sciences,Sociology,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Fanelli_2010_â€œPositiveâ€_Results_Increase_Down_the_Hierarchy_of_the_Sciences.pdf;/home/dede/Zotero/storage/FVUB3QEI/article.html}
}

@article{fasano2020,
  title = {Scalable and {{Accurate Variational Bayes}} for {{High-Dimensional Binary Regression Models}}},
  author = {Fasano, Augusto and Durante, Daniele and Zanella, Giacomo},
  year = {2020},
  month = oct,
  journal = {arXiv:1911.06743 [stat]},
  eprint = {1911.06743},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Modern methods for Bayesian regression with binary responses are either computationally impractical or inaccurate in high dimensions. In fact, as discussed in recent literature, bypassing this trade-off is still an open problem which is object of intense research. To cover this gap, we develop a novel variational approximation for the posterior distribution of the coefficients in high-dimensional probit regression with Gaussian priors. Our method leverages a representation with global and local variables but, unlike for classical mean-field assumptions, it avoids a fully factorized approximation, and instead assumes a factorization only for the local variables. We prove that the resulting variational approximation belongs to a tractable class of unified skew-normal densities that crucially incorporates skewness and, unlike for state-of-the-art variational Bayes solutions, converges to the exact posterior density as the number of predictors p goes to infinity. To solve the variational optimization problem, we develop a tractable coordinate ascent variational algorithm which easily scales to p in tens of thousands, and provably requires a number of iterations converging to 1 as p goes to infinity. These findings are also illustrated in extensive simulation studies and in real-world medical applications where our methods are shown to uniformly improve classical mean-field variational Bayes in terms of inference accuracy and predictive performance. The magnitude of such gains is especially remarkable in those high-dimensional p{$>$}n settings where state-of-the-art alternative strategies are computationally impractical.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Computation,Statistics - Methodology,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Fasano_et_al_2020_Scalable_and_Accurate_Variational_Bayes_for_High-Dimensional_Binary_Regression.pdf;/home/dede/Zotero/storage/RBIAECT7/1911.html}
}

@book{fattorini1999,
  title = {{Infinite Dimensional Optimization and Control Theory}},
  author = {Fattorini, Hector O.},
  year = {1999},
  month = mar,
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  abstract = {This book is on existence and necessary conditions, such as Potryagin's maximum principle, for optimal control problems described by ordinary and partial differential equations. These necessary conditions are obtained from Kuhn\textendash Tucker theorems for nonlinear programming problems in infinite dimensional spaces. The optimal control problems include control constraints, state constraints and target conditions. Evolution partial differential equations are studied using semigroup theory, abstract differential equations in linear spaces, integral equations and interpolation theory. Existence of optimal controls is established for arbitrary control sets by means of a general theory of relaxed controls. Applications include nonlinear systems described by partial differential equations of hyperbolic and parabolic type and results on convergence of suboptimal controls.},
  isbn = {978-0-521-45125-3},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Fattorini_1999_Infinite_Dimensional_Optimization_and_Control_Theory.pdf}
}

@article{faul2007,
  title = {Gaussian Process Modeling of {{EEG}} for the Detection of Neonatal Seizures},
  author = {Faul, Stephen and Gregorcic, Gregor and Boylan, Geraldine and Marnane, William and Lightbody, Gordon and Connolly, Sean},
  year = {2007},
  month = dec,
  journal = {IEEE transactions on bio-medical engineering},
  volume = {54},
  number = {12},
  pages = {2151--2162},
  issn = {0018-9294},
  doi = {10.1109/tbme.2007.895745},
  abstract = {Gaussian process (GP) probabilistic models have attractive advantages over parametric and neural network modeling approaches. They have a small number of tuneable parameters, can be trained on relatively small training sets, and provide a measure of prediction certainty. In this paper, these properties are exploited to develop two methods of highlighting the presence of neonatal seizures from electroencephalograph (EEG) signals. In the first method, the certainty of the GP model prediction is used to indicate the presence of seizures. In the second approach, the hyperparameters of the GP model are used. Tests are carried out with a feature set of ten EEG measures developed from various signal processing techniques. Features are evaluated using a neural network classifier on 51 h of real neonatal EEG. The GP measures, in particular, the prediction certainty approach, produce a high level of performance compared to other modeling methods and methods currently in clinical use for EEG analysis, indicating that they are an important and useful tool for the real-time detection of neonatal seizures.},
  langid = {english},
  pmid = {18075031},
  keywords = {Algorithms,Brain,Computer Simulation,Diagnosis; Computer-Assisted,Electroencephalography,Epilepsy; Benign Neonatal,Humans,Infant; Newborn,Intensive Care; Neonatal,Models; Neurological,Models; Statistical,Reproducibility of Results,Sensitivity and Specificity}
}

@article{fellouris2016,
  title = {Second-{{Order Asymptotic Optimality}} in {{Multisensor Sequential Change Detection}}},
  author = {Fellouris, Georgios and Sokolov, Grigory},
  year = {2016},
  month = jun,
  journal = {IEEE Transactions on Information Theory},
  volume = {62},
  number = {6},
  pages = {3662--3675},
  issn = {1557-9654},
  doi = {10.1109/TIT.2016.2549042},
  abstract = {A generalized multisensor sequential change detection problem is considered, in which a number of (possibly correlated) sensors monitor an environment in real time, the joint distribution of their observations is determined by a global parameter vector, and at some unknown time there is a change in an unknown subset of components of this parameter vector. The goal is to detect the change as soon as possible, while controlling the rate of false alarms. We establish the second-order asymptotic optimality (with respect to Lorden's criterion) of various generalizations of the CUSUM rule; that is, we show that their additional expected worst case detection delay (relative to the one that could be achieved if the affected subset was known) remains bounded as the rate of false alarm goes to 0, for any possible subset of affected components. This general framework incorporates the traditional multisensor setup in which only an unknown subset of sensors is affected by the change. The latter problem has a special structure which we exploit in order to obtain feasible representations of the proposed schemes. We present the results of a simulation study where we compare the proposed schemes with scalable detection rules that are only first-order asymptotically optimal. Finally, in the special case that the change affects exactly one sensor, we consider the scheme that runs in parallel the local CUSUM rules and study the problem of specifying the local thresholds.},
  keywords = {Asymptotic optimality,change detection,Change detection,Conferences,Context,CUSUM,Delays,Monitoring,multichannel,Multichannel,multisensor,Multisensor,Robustness,Sensor fusion,sequential,Sequential,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Fellouris_Sokolov_2016_Second-Order_Asymptotic_Optimality_in_Multisensor_Sequential_Change_Detection.pdf;/home/dede/Zotero/storage/CZQSPB4H/7445238.html}
}

@article{feng2015,
  title = {Multivariate-{{Sign-Based High-Dimensional Tests}} for the {{Two-Sample Location Problem}}},
  author = {Feng, Long and Zou, Changliang and Wang, Zhaojun},
  year = {2015},
  month = mar,
  journal = {Journal of the American Statistical Association},
  doi = {10.1080/01621459.2015.1035380},
  abstract = {This article concerns tests for the two-sample location problem when data dimension is larger than the sample size. Existing multivariate-sign-based procedures are not robust against high dimensionality, producing tests with type I error rates far away from nominal levels. This is mainly due to the biases from estimating location parameters. We propose a novel test to overcome this issue by using the \textbackslash leave-one-out" idea. The proposed test statistic is scalar-invariant and thus is particularly useful when different components have different scales in high-dimensional data. Asymptotic properties of the test statistic are studied. Compared with other existing approaches, simulation studies show that the proposed method behaves well in terms of sizes and power.},
  keywords = {toRead},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Feng_et_al_2015_Multivariate-Sign-Based_High-Dimensional_Tests_for_the_Two-Sample_Location.pdf}
}

@article{ferguson1973,
  title = {A {{Bayesian Analysis}} of {{Some Nonparametric Problems}}},
  author = {Ferguson, Thomas S.},
  year = {1973},
  month = mar,
  journal = {The Annals of Statistics},
  volume = {1},
  number = {2},
  pages = {209--230},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/aos/1176342360},
  abstract = {The Bayesian approach to statistical problems, though fruitful in many ways, has been rather unsuccessful in treating nonparametric problems. This is due primarily to the difficulty in finding workable prior distributions on the parameter space, which in nonparametric ploblems is taken to be a set of probability distributions on a given sample space. There are two desirable properties of a prior distribution for nonparametric problems. (I) The support of the prior distribution should be large--with respect to some suitable topology on the space of probability distributions on the sample space. (II) Posterior distributions given a sample of observations from the true probability distribution should be manageable analytically. These properties are antagonistic in the sense that one may be obtained at the expense of the other. This paper presents a class of prior distributions, called Dirichlet process priors, broad in the sense of (I), for which (II) is realized, and for which treatment of many nonparametric statistical problems may be carried out, yielding results that are comparable to the classical theory. In Section 2, we review the properties of the Dirichlet distribution needed for the description of the Dirichlet process given in Section 3. Briefly, this process may be described as follows. Let \$\textbackslash mathscr\{X\}\$ be a space and \$\textbackslash mathscr\{A\}\$ a \$\textbackslash sigma\$-field of subsets, and let \$\textbackslash alpha\$ be a finite non-null measure on \$(\textbackslash mathscr\{X\}, \textbackslash mathscr\{A\})\$. Then a stochastic process \$P\$ indexed by elements \$A\$ of \$\textbackslash mathscr\{A\}\$, is said to be a Dirichlet process on \$(\textbackslash mathscr\{X\}, \textbackslash mathscr\{A\})\$ with parameter \$\textbackslash alpha\$ if for any measurable partition \$(A\_1, \textbackslash cdots, A\_k)\$ of \$\textbackslash mathscr\{X\}\$, the random vector \$(P(A\_1), \textbackslash cdots, P(A\_k))\$ has a Dirichlet distribution with parameter \$(\textbackslash alpha(A\_1), \textbackslash cdots, \textbackslash alpha(A\_k)). P\$ may be considered a random probability measure on \$(\textbackslash mathscr\{X\}, \textbackslash mathscr\{A\})\$, The main theorem states that if \$P\$ is a Dirichlet process on \$(\textbackslash mathscr\{X\}, \textbackslash mathscr\{A\})\$ with parameter \$\textbackslash alpha\$, and if \$X\_1, \textbackslash cdots, X\_n\$ is a sample from \$P\$, then the posterior distribution of \$P\$ given \$X\_1, \textbackslash cdots, X\_n\$ is also a Dirichlet process on \$(\textbackslash mathscr\{X\}, \textbackslash mathscr\{A\})\$ with a parameter \$\textbackslash alpha + \textbackslash sum\^n\_1 \textbackslash delta\_\{x\_i\}\$, where \$\textbackslash delta\_x\$ denotes the measure giving mass one to the point \$x\$. In Section 4, an alternative definition of the Dirichlet process is given. This definition exhibits a version of the Dirichlet process that gives probability one to the set of discrete probability measures on \$(\textbackslash mathscr\{X\}, \textbackslash mathscr\{A\})\$. This is in contrast to Dubins and Freedman [2], whose methods for choosing a distribution function on the interval [0, 1] lead with probability one to singular continuous distributions. Methods of choosing a distribution function on [0, 1] that with probability one is absolutely continuous have been described by Kraft [7]. The general method of choosing a distribution function on [0, 1], described in Section 2 of Kraft and van Eeden [10], can of course be used to define the Dirichlet process on [0, 1]. Special mention must be made of the papers of Freedman and Fabius. Freedman [5] defines a notion of tailfree for a distribution on the set of all probability measures on a countable space \$\textbackslash mathscr\{X\}\$. For a tailfree prior, posterior distribution given a sample from the true probability measure may be fairly easily computed. Fabius [3] extends the notion of tailfree to the case where \$\textbackslash mathscr\{X\}\$ is the unit interval [0, 1], but it is clear his extension may be made to cover quite general \$\textbackslash mathscr\{X\}\$. With such an extension, the Dirichlet process would be a special case of a tailfree distribution for which the posterior distribution has a particularly simple form. There are disadvantages to the fact that \$P\$ chosen by a Dirichlet process is discrete with probability one. These appear mainly because in sampling from a \$P\$ chosen by a Dirichlet process, we expect eventually to see one observation exactly equal to another. For example, consider the goodness-of-fit problem of testing the hypothesis \$H\_0\$ that a distribution on the interval [0, 1] is uniform. If on the alternative hypothesis we place a Dirichlet process prior with parameter \$\textbackslash alpha\$ itself a uniform measure on [0, 1], and if we are given a sample of size \$n \textbackslash geqq 2\$, the only nontrivial nonrandomized Bayes rule is to reject \$H\_0\$ if and only if two or more of the observations are exactly equal. This is really a test of the hypothesis that a distribution is continuous against the hypothesis that it is discrete. Thus, there is still a need for a prior that chooses a continuous distribution with probability one and yet satisfies properties (I) and (II). Some applications in which the possible doubling up of the values of the observations plays no essential role are presented in Section 5. These include the estimation of a distribution function, of a mean, of quantiles, of a variance and of a covariance. A two-sample problem is considered in which the Mann-Whitney statistic, equivalent to the rank-sum statistic, appears naturally. A decision theoretic upper tolerance limit for a quantile is also treated. Finally, a hypothesis testing problem concerning a quantile is shown to yield the sign test. In each of these problems, useful ways of combining prior information with the statistical observations appear. Other applications exist. In his Ph. D. dissertation [1], Charles Antoniak finds a need to consider mixtures of Dirichlet processes. He treats several problems, including the estimation of a mixing distribution, bio-assay, empirical Bayes problems, and discrimination problems.},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Ferguson_1973_A_Bayesian_Analysis_of_Some_Nonparametric_Problems.pdf;/home/dede/Zotero/storage/4IWPBQ5W/1176342360.html}
}

@incollection{ferguson1983,
  title = {Bayesian Density Estimation by Mixtures of Normal Distributions},
  booktitle = {Recent {{Advances}} in {{Statistics}}},
  author = {Ferguson, Thomas S.},
  editor = {Rizvi, M. Haseeb and Rustagi, Jagdish S. and Siegmund, David},
  year = {1983},
  month = jan,
  pages = {287--302},
  publisher = {{Academic Press}},
  doi = {10.1016/B978-0-12-589320-6.50018-6},
  abstract = {This chapter discusses Bayesian density estimation by mixtures of normal distributions and discusses the estimation of an arbitrary density f(x) on the real line. This density is modeled as a mixture of a countable number of normal distributions. Using such mixtures, any distribution on the real line can be approximated to within any preassigned accuracy in the Levy metric and any density on the real line can be approximated similarly in the L1 norm. Thus, the problem can be considered nonparametric. Asymptotic theory for kernel estimators involves the problems of letting the window size tend to zero at some rate as the sample size tends to infinity.},
  isbn = {978-0-12-589320-6},
  langid = {english},
  file = {/home/dede/Zotero/storage/B7SX8X5W/B9780125893206500186.html}
}

@book{ferguson2017,
  title = {A {{Course}} in {{Large Sample Theory}}},
  author = {Ferguson, Thomas S.},
  year = {2017},
  month = sep,
  publisher = {{Routledge}},
  doi = {10.1201/9781315136288},
  abstract = {A Course in Large Sample Theory is presented in four parts. The first treats basic probabilistic notions, the second features the basic statistical tools for},
  isbn = {978-1-315-13628-8},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Ferguson_2017_A_Course_in_Large_Sample_Theory.pdf}
}

@article{ferreira2017,
  title = {Partially Linear Models and Their Applications to Change Point Detection of Chemical Process Data},
  author = {Ferreira, Cl{\'e}cio S. and Zeller, Camila B. and Mimura, Aparecida M. S. and Silva, J{\'u}lio C. J.},
  year = {2017},
  month = sep,
  journal = {Journal of Applied Statistics},
  volume = {44},
  number = {12},
  pages = {2125--2141},
  publisher = {{Taylor \& Francis}},
  issn = {0266-4763},
  doi = {10.1080/02664763.2016.1247788},
  abstract = {In many chemical data sets, the amount of radiation absorbed (absorbance) is related to the concentration of the element in the sample by Lambert\textendash Beer's law. However, this relation changes abruptly when the variable concentration reaches an unknown threshold level, the so-called change point. In the context of analytical chemistry, there are many methods that describe the relationship between absorbance and concentration, but none of them provide inferential procedures to detect change points. In this paper, we propose partially linear models with a change point separating the parametric and nonparametric components. The Schwarz information criterion is used to locate a change point. A back-fitting algorithm is presented to obtain parameter estimates and the penalized Fisher information matrix is obtained to calculate the standard errors of the parameter estimates. To examine the proposed method, we present a simulation study. Finally, we apply the method to data sets from the chemistry area. The partially linear models with a change point developed in this paper are useful supplements to other methods of absorbance\textendash concentration analysis in chemical studies, for example, and in many other practical applications.},
  keywords = {Back-fitting,change point,chemical methods,generalized cross validation,partially linear models,Schwarz information criterion,todo},
  annotation = {\_eprint: https://doi.org/10.1080/02664763.2016.1247788},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Ferreira_et_al_2017_Partially_linear_models_and_their_applications_to_change_point_detection_of.pdf;/home/dede/Zotero/storage/L4QUQ8I8/02664763.2016.html}
}

@article{fienberg1992,
  title = {A {{Brief History}} of {{Statistics}} in {{Three}} and {{One-Half Chapters}}: {{A Review Essay}}},
  shorttitle = {A {{Brief History}} of {{Statistics}} in {{Three}} and {{One-Half Chapters}}},
  author = {Fienberg, Stephen E.},
  year = {1992},
  month = may,
  journal = {Statistical Science},
  volume = {7},
  number = {2},
  pages = {208--225},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/ss/1177011360},
  abstract = {Statistical Science},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Fienberg_1992_A_Brief_History_of_Statistics_in_Three_and_One-Half_Chapters.pdf;/home/dede/Zotero/storage/F5TFN67U/1177011360.html}
}

@article{fisher1926,
  title = {The Arrangement of Field Experiments},
  author = {Fisher, R. A.},
  year = {1926},
  journal = {Journal of the Ministry of Agriculture},
  volume = {33},
  pages = {503--515},
  publisher = {{Ministry of Agriculture and Fisheries}},
  issn = {0368-3087},
  doi = {10.23637/rothamsted.8v61q},
  abstract = {Author's note. From about 1923 onwards the Statistical Department at Rothamsted had been much concerned with the precision of field experiments in agriculture, and with modifications in their design, having the dual aim of increasing the precision and of providing a valid estimate of error. These two desiderata had been somewhat confused in the minds of the experimenters, and the present paper was the author's first attempt at setting out the rational principles on which he might proceed. The paper is a precursor to the book on the Design of Experiments published nine years later.},
  copyright = {Crown copyright},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Fisher_1926_The_arrangement_of_field_experiments.pdf;/home/dede/Zotero/storage/RRKM9IV5/the-arrangement-of-field-experiments.html}
}

@article{fisher1936,
  title = {The {{Use}} of {{Multiple Measurements}} in {{Taxonomic Problems}}},
  author = {Fisher, R. A.},
  year = {1936},
  journal = {Annals of Eugenics},
  volume = {7},
  number = {2},
  pages = {179--188},
  issn = {2050-1439},
  doi = {10.1111/j.1469-1809.1936.tb02137.x},
  abstract = {The articles published by the Annals of Eugenics (1925\textendash 1954) have been made available online as an historical archive intended for scholarly use. The work of eugenicists was often pervaded by prejudice against racial, ethnic and disabled groups. The online publication of this material for scholarly research purposes is not an endorsement of those views nor a promotion of eugenics in any way.},
  langid = {english},
  keywords = {done},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1469-1809.1936.tb02137.x},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Fisher_1936_The_Use_of_Multiple_Measurements_in_Taxonomic_Problems.pdf;/home/dede/Zotero/storage/TVE9SJ56/j.1469-1809.1936.tb02137.html}
}

@book{folland1999,
  title = {{Real Analysis: Modern Techniques and Their Applications}},
  shorttitle = {{Real Analysis}},
  author = {Folland, Gerald B.},
  year = {1999},
  month = mar,
  edition = {Subsequent edizione},
  publisher = {{John Wiley \& Sons Inc}},
  address = {{New York}},
  abstract = {An in-depth look at real analysis and its applications-now expanded and revised.  This new edition of the widely used analysis book continues to cover real analysis in greater detail and at a more advanced level than most books on the subject. Encompassing several subjects that underlie much of modern analysis, the book focuses on measure and integration theory, point set topology, and the basics of functional analysis. It illustrates the use of the general theories and introduces readers to other branches of analysis such as Fourier analysis, distribution theory, and probability theory.  This edition is bolstered in content as well as in scope-extending its usefulness to students outside of pure analysis as well as those interested in dynamical systems. The numerous exercises, extensive bibliography, and review chapter on sets and metric spaces make Real Analysis: Modern Techniques and Their Applications, Second Edition invaluable for students in graduate-level analysis courses. New features include: * Revised material on the n-dimensional Lebesgue integral. * An improved proof of Tychonoff's theorem. * Expanded material on Fourier analysis. * A newly written chapter devoted to distributions and differential equations. * Updated material on Hausdorff dimension and fractal dimension.},
  isbn = {978-0-471-31716-6},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Folland_1999_Real_Analysis.pdf}
}

@inproceedings{fouche2019,
  title = {Scaling {{Multi-Armed Bandit Algorithms}}},
  booktitle = {Proceedings of the 25th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Fouch{\'e}, Edouard and Komiyama, Junpei and B{\"o}hm, Klemens},
  year = {2019},
  month = jul,
  series = {{{KDD}} '19},
  pages = {1449--1459},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3292500.3330862},
  abstract = {The Multi-Armed Bandit (MAB) is a fundamental model capturing the dilemma between exploration and exploitation in sequential decision making. At every time step, the decision maker selects a set of arms and observes a reward from each of the chosen arms. In this paper, we present a variant of the problem, which we call the Scaling MAB (S-MAB): The goal of the decision maker is not only to maximize the cumulative rewards, i.e., choosing the arms with the highest expected reward, but also to decide how many arms to select so that, in expectation, the cost of selecting arms does not exceed the rewards. This problem is relevant to many real-world applications, e.g., online advertising, financial investments or data stream monitoring. We propose an extension of Thompson Sampling, which has strong theoretical guarantees and is reported to perform well in practice. Our extension dynamically controls the number of arms to draw. Furthermore, we combine the proposed method with ADWIN, a state-of-the-art change detector, to deal with non-static environments. We illustrate the benefits of our contribution via a real-world use case on predictive maintenance.},
  isbn = {978-1-4503-6201-6},
  keywords = {adaptive windowing,bandit algorithms,data stream monitoring,predictive maintenance,thompson sampling,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/FouchÃ©_et_al_2019_Scaling_Multi-Armed_Bandit_Algorithms.pdf}
}

@incollection{frankel2014,
  title = {Resampling {{Procedures}} for {{Sample Surveys}}},
  booktitle = {Wiley {{StatsRef}}: {{Statistics Reference Online}}},
  author = {Frankel, Martin R.},
  editor = {Balakrishnan, N. and Colton, Theodore and Everitt, Brian and Piegorsch, Walter and Ruggeri, Fabrizio and Teugels, Jozef L.},
  year = {2014},
  month = sep,
  pages = {stat05043},
  publisher = {{John Wiley \& Sons, Ltd}},
  address = {{Chichester, UK}},
  doi = {10.1002/9781118445112.stat05043},
  isbn = {978-1-118-44511-2},
  langid = {english},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Frankel_2014_Resampling_Procedures_for_Sample_Surveys.pdf}
}

@book{freeden2003,
  title = {{Ideology: A Very Short Introduction}},
  shorttitle = {{Ideology}},
  author = {Freeden, Michael},
  year = {2003},
  publisher = {{OUP Oxford}},
  address = {{Oxford ; New York}},
  isbn = {978-0-19-280281-1},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Freeden_2003_Ideology.epub}
}

@article{friedman1974,
  title = {A {{Projection Pursuit Algorithm}} for {{Exploratory Data Analysis}}},
  author = {Friedman, J.H. and Tukey, J.W.},
  year = {1974},
  month = sep,
  journal = {IEEE Transactions on Computers},
  volume = {C-23},
  number = {9},
  pages = {881--890},
  issn = {1557-9956},
  doi = {10.1109/T-C.1974.224051},
  abstract = {An algorithm for the analysis of multivariate data is presented and is discussed in terms of specific examples. The algorithm seeks to find one-and two-dimensional linear projections of multivariate data that are relatively highly revealing.},
  keywords = {Clustering; dimensionality reduction; mappings; multidimensional scaling; multivariate data analysis; nonparametric pattern recognition; statistics.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Friedman_Tukey_1974_A_Projection_Pursuit_Algorithm_for_Exploratory_Data_Analysis.pdf;/home/dede/Zotero/storage/NPMMIEYV/1672644.html}
}

@article{friedman2000,
  title = {Additive Logistic Regression: A Statistical View of Boosting},
  shorttitle = {Additive Logistic Regression},
  author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
  year = {2000},
  month = apr,
  journal = {The Annals of Statistics},
  volume = {28},
  number = {2},
  pages = {337--407},
  issn = {0090-5364},
  doi = {10.1214/aos/1016218223},
  langid = {english},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Friedman_et_al_2000_Additive_logistic_regression.pdf}
}

@article{friedman2008,
  title = {Sparse Inverse Covariance Estimation with the Graphical Lasso},
  author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
  year = {2008},
  month = jul,
  journal = {Biostatistics},
  volume = {9},
  number = {3},
  pages = {432--441},
  issn = {1465-4644},
  doi = {10.1093/biostatistics/kxm045},
  abstract = {We consider the problem of estimating sparse graphs by a lasso penalty applied to the inverse covariance matrix. Using a coordinate descent procedure for the lasso, we develop a simple algorithm\textemdash the graphical lasso\textemdash that is remarkably fast: It solves a 1000-node problem ({$\sim$}500000 parameters) in at most a minute and is 30\textendash 4000 times faster than competing methods. It also provides a conceptual link between the exact problem and the approximation suggested by Meinshausen and B\"uhlmann (2006). We illustrate the method on some cell-signaling data from proteomics.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Friedman_et_al_2008_Sparse_inverse_covariance_estimation_with_the_graphical_lasso.pdf;/home/dede/Zotero/storage/UYKUVH3M/224260.html}
}

@article{friedman2010,
  title = {Regularization {{Paths}} for {{Generalized Linear Models}} via {{Coordinate Descent}}},
  author = {Friedman, Jerome H. and Hastie, Trevor and Tibshirani, Rob},
  year = {2010},
  month = feb,
  journal = {Journal of Statistical Software},
  volume = {33},
  number = {1},
  pages = {1--22},
  issn = {1548-7660},
  doi = {10.18637/jss.v033.i01},
  copyright = {Copyright (c) 2009 Jerome H. Friedman, Trevor Hastie, Rob Tibshirani},
  langid = {english},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Friedman_et_al_2010_Regularization_Paths_for_Generalized_Linear_Models_via_Coordinate_Descent.pdf;/home/dede/Zotero/storage/WAW3XYAM/v033i01.html}
}

@techreport{frigyik2010,
  title = {Introduction to the {{Dirichlet}} Distribution and Related Processes},
  author = {Frigyik, Bela A. and Kapila, Amol and Gupta, Maya R.},
  year = {2010},
  abstract = {This tutorial covers the Dirichlet distribution, Dirichlet process, P\'olya urn (and the associated Chinese restaurant process), hierarchical Dirichlet Process, and the Indian buffet process. Apart from basic properties, we describe and contrast three methods of generating samples: stick-breaking, the P\'olya urn, and drawing gamma random variables. For the Dirichlet process we first present an informal introduction, and then a rigorous description for those more comfortable with probability theory.},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Frigyik_et_al_2010_Introduction_to_the_Dirichlet_distribution_and_related_processes.pdf}
}

@book{fristedt1996,
  title = {{A Modern Approach to Probability Theory}},
  author = {Fristedt, Bert and Gray, Lawrence},
  year = {1996},
  month = dec,
  edition = {1997\textdegree{} edizione},
  publisher = {{Birkhauser}},
  address = {{Boston}},
  abstract = {Students and teachers of mathematics and related fields will find~this book~a comprehensive and modern approach to probability theory, providing the background and techniques to go from the beginning graduate level to the point of specialization in research areas of current interest. The book is designed for a two- or three-semester course, assuming only courses in undergraduate real analysis or rigorous advanced calculus, and some elementary linear algebra. A variety of applicationsBayesian statistics, financial mathematics, information theory, tomography, and signal processingappear as threads to both enhance the understanding of the relevant mathematics and motivate students whose main interests are outside of pure areas.},
  isbn = {978-0-8176-3807-8},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Fristedt_Gray_1996_A_Modern_Approach_to_Probability_Theory.pdf}
}

@article{friston1996,
  title = {Detecting Activations in {{PET}} and {{fMRI}}: Levels of Inference and Power},
  shorttitle = {Detecting Activations in {{PET}} and {{fMRI}}},
  author = {Friston, K. J. and Holmes, A. and Poline, J. B. and Price, C. J. and Frith, C. D.},
  year = {1996},
  month = dec,
  journal = {NeuroImage},
  volume = {4},
  number = {3 Pt 1},
  pages = {223--235},
  issn = {1053-8119},
  doi = {10.1006/nimg.1996.0074},
  abstract = {This paper is about detecting activations in statistical parametric maps and considers the relative sensitivity of a nested hierarchy of tests that we have framed in terms of the level of inference (voxel level, cluster level, and set level). These tests are based on the probability of obtaining c, or more, clusters with k, or more, voxels, above a threshold u. This probability has a reasonably simple form and is derived using distributional approximations from the theory of Gaussian fields. The most important contribution of this work is the notion of set-level inference. Set-level inference refers to the statistical inference that the number of clusters comprising an observed activation profile is highly unlikely to have occurred by chance. This inference pertains to the set of activations reaching criteria and represents a new way of assigning P values to distributed effects. Cluster-level inferences are a special case of set-level inferences, which obtain when the number of clusters c = 1. Similarly voxel-level inferences are special cases of cluster-level inferences that result when the cluster can be very small (i.e., k = 0). Using a theoretical power analysis of distributed activations, we observed that set-level inferences are generally more powerful than cluster-level inferences and that cluster-level inferences are generally more powerful than voxel-level inferences. The price paid for this increased sensitivity is reduced localizing power: Voxel-level tests permit individual voxels to be identified as significant, whereas cluster-and set-level inferences only allow clusters or sets of clusters to be so identified. For all levels of inference the spatial size of the underlying signal f (relative to resolution) determines the most powerful thresholds to adopt. For set-level inferences if f is large (e.g., fMRI) then the optimum extent threshold should be greater than the expected number of voxels for each cluster. If f is small (e.g., PET) the extent threshold should be small. We envisage that set-level inferences will find a role in making statistical inferences about distributed activations, particularly in fMRI.},
  langid = {english},
  pmid = {9345513},
  keywords = {Brain,Brain Mapping,Data Interpretation; Statistical,Humans,Magnetic Resonance Imaging,Mathematical Computing,Normal Distribution,Probability,ROC Curve,Sensitivity and Specificity,todo,Tomography; Emission-Computed},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Friston_et_al_1996_Detecting_activations_in_PET_and_fMRI.pdf}
}

@article{fruhling2016,
  title = {Managing Escalation: Missile Defence, Strategy and {{US}} Alliances},
  shorttitle = {Managing Escalation},
  author = {Fr{\"u}hling, Stephan},
  year = {2016},
  month = jan,
  journal = {International Affairs},
  volume = {92},
  number = {1},
  pages = {81--95},
  issn = {0020-5850},
  doi = {10.1111/1468-2346.12501},
  abstract = {Missile defence plays an increasing role in NATO and in most US alliances in Asia, which raises the question of what impact it has on the management of extended deterrence. Extended deterrence relies on the threat of escalation. Since the costs of escalation are different for different allies, the management of extended deterrence is inherently difficult. Missile defence shifts the relative costs of conflict, and therefore also impacts on the alliance bargains that underpin agreement on extended deterrence strategy. Although increased defensive capacity is a clear net benefit, the strategic effects of its deployment and use can still be complex if, for example, missile defence increases the chances of localizing a conflict. The article discusses the role of missile defences for the US homeland, and of the territory and population of US allies, for extended deterrence credibility and the reassurance of US allies in Asia and in NATO. It argues that there is increased scope in strengthening deterrence by enmeshing the defence of the US homeland with that of its allies, and that allies need to pay closer attention to the way the deployment and use of missile defence influence pressures for escalation. In general, missile defence thus reinforces the need for the United States and its allies in Europe and Asia to negotiate an overall alliance strategy.},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/FRÃœHLING_2016_Managing_escalation.pdf;/home/dede/Zotero/storage/CIZ5CNHA/2199924.html}
}

@phdthesis{fu2013,
  title = {Statistical {{Process Control Methods}} for {{Network Monitoring Using Generalized Linear Mixed Models}}},
  author = {Fu, Yingzhuo},
  year = {2013},
  abstract = {Network surveillance algorithms are becoming increasingly important as the ability to monitor a wide variety of data is rapidly expanding. Traffic metrics are usually count data that display a non-stationary pattern in their mean structure. We propose to model traffic counts using a generalized linear mixed model to capture these features. We then develop three tracking statistics proposed for anomaly detection. Two of the statistics are derived variants of a generalized likelihood ratio approach, which itself is not computationally tractable. The first of these variants is based on an approximation to the integrated likelihood while the second is based on the concept of h-likelihood. We also consider a tracking statistic that is an exponentially weighted moving average. We investigate the properties of the three tracking statistics from the point of view of false alarm rate and detection power, and compare the proposed tracking statistics with current literature. Our comparisons show that the two generalized likelihood ratio variants are preferred choices as SPC tools for network surveillance. Computational aspects of the three procedures are also discussed.},
  langid = {english},
  school = {UC Riverside},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Fu_2013_Statistical_Process_Control_Methods_for_Network_Monitoring_Using_Generalized.pdf;/home/dede/Zotero/storage/XL6UE53J/27z9q5qp.html}
}

@article{fu2014,
  title = {{{SPC}} Methods for Nonstationary Correlated Count Data with Application to Network Surveillance},
  author = {Fu, Yingzhuo and Jeske, Daniel R.},
  year = {2014},
  journal = {Applied Stochastic Models in Business and Industry},
  volume = {30},
  number = {6},
  pages = {708--722},
  issn = {1526-4025},
  doi = {10.1002/asmb.2038},
  abstract = {AbstractNetwork surveillance methods are becoming increasingly important as the ability to monitor a wide variety of data is rapidly expanding. Network traffic metrics are usually correlated count data that display a nonstationary pattern in their mean structures. We propose to model traffic counts using a generalized linear mixed model to capture these features. We then develop three tracking statistics proposed for anomaly detection. Two of the statistics are derived variants of a Bartlett-type likelihood ratio, which itself is not computationally tractable. The first of these variants is based on an approximation to the integrated likelihood while the second is based on the concept of h-likelihood. We also consider a tracking statistic that is an exponentially weighted moving average. We compare the properties of the three tracking statistics from the point of view of FAR and detection power and contrast the proposed tracking statistics with current literature. Our comparisons show that the two generalized likelihood ratio variants are preferred choices as statistical process control tools for network surveillance. Computational aspects of the three procedures are also discussed. While our application focus is network surveillance, our proposed methods apply to other applications that have similar data characteristics. Copyright \textcopyright{} 2014 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {Bartlett likelihood ratio,count data,done,false alarm rate,sequential test},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/asmb.2038},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Fu_Jeske_2014_SPC_methods_for_nonstationary_correlated_count_data_with_application_to_network.pdf;/home/dede/Zotero/storage/54H6W2LG/asmb.html}
}

@book{fu2015,
  title = {Handbook of {{Simulation Optimization}}},
  editor = {Fu, Michael C.},
  year = {2015},
  series = {International {{Series}} in {{Operations Research}} \& {{Management Science}}},
  publisher = {{Springer-Verlag}},
  address = {{New York}},
  doi = {10.1007/978-1-4939-1384-8},
  abstract = {The Handbook of Simulation Optimization presents an overview of the state of the art of simulation optimization, providing a survey of the most well-established approaches for optimizing stochastic simulation models and a sampling of recent research advances in theory and methodology. Leading contributors cover such topics as discrete optimization via simulation, ranking and selection, efficient simulation budget allocation, random search methods, response surface methodology, stochastic gradient estimation, stochastic approximation, sample average approximation, stochastic constraints, variance reduction techniques, model-based stochastic search methods and Markov decision processes.This single volume should serve as a reference for those already in the field and as a means for those new to the field for understanding and applying the main approaches. The intended audience includes researchers, practitioners and graduate students in the business/engineering fields of operations research, management science, operations management and stochastic control, as well as in economics/finance and computer science.},
  isbn = {978-1-4939-1383-1},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Fu_2015_Handbook_of_Simulation_Optimization.pdf;/home/dede/Zotero/storage/Z3WCMYKZ/9781493913831.html}
}

@article{fuquene2015,
  title = {A Robust {{Bayesian}} Dynamic Linear Model for {{Latin-American}} Economic Time Series: ``The {{Mexico}} and {{Puerto Rico}} Cases''},
  shorttitle = {A Robust {{Bayesian}} Dynamic Linear Model for {{Latin-American}} Economic Time Series},
  author = {F{\'u}quene, Jairo and {\'A}lvarez, Marta and Ra{\'u}l Pericchi, Luis},
  year = {2015},
  month = dec,
  journal = {Latin American Economic Review},
  volume = {24},
  number = {1},
  pages = {1--17},
  publisher = {{SpringerOpen}},
  issn = {2196-436X},
  doi = {10.1007/s40503-015-0020-z},
  abstract = {The traditional time series methodology requires at least a preliminary transformation of the data to get stationarity. On the other hand, robust Bayesian dynamic models (RBDMs) do not assume a regular pattern or stability of the underlying system but can include points of statement breaks. In this paper we use RBDMs in order to account possible outliers and structural breaks in Latin-American economic time series. We work with important economic time series from Puerto Rico and Mexico. We show by using a random walk model how RBDMs can be applied for detecting historic changes in the economic inflation of Mexico. Also, we model the Consumer Price Index, the Economic Activity Index and the total number of employments economic time series in Puerto Rico using local linear trend and seasonal RBDMs with observational and states variances. The results illustrate how the model accounts the structural breaks for the historic recession periods in Puerto Rico.},
  copyright = {2015 The Author(s)},
  langid = {english},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/FÃºquene_et_al_2015_A_robust_Bayesian_dynamic_linear_model_for_Latin-American_economic_time_series.pdf;/home/dede/Zotero/storage/Y8JBM7FC/s40503-015-0020-z.html}
}

@book{gaetan2010,
  title = {Spatial {{Statistics}} and {{Modeling}}},
  author = {Gaetan, Carlo and Guyon, Xavier},
  year = {2010},
  series = {Springer {{Series}} in {{Statistics}}},
  publisher = {{Springer-Verlag}},
  address = {{New York}},
  doi = {10.1007/978-0-387-92257-7},
  abstract = {Spatial statistics are useful in subjects as diverse as climatology, ecology, economics, environmental and earth sciences, epidemiology, image analysis and more. This book covers the best-known spatial models for three types of spatial data: geostatistical data (stationarity, intrinsic models, variograms, spatial regression and space-time models), areal data (Gibbs-Markov fields and spatial auto-regression) and point pattern data (Poisson, Cox, Gibbs and Markov point processes). The level is relatively advanced, and the presentation concise but complete. The most important statistical methods and their asymptotic properties are described, including estimation in geostatistics, autocorrelation and second-order statistics, maximum likelihood methods, approximate inference using the pseudo-likelihood or Monte-Carlo simulations, statistics for point processes and Bayesian hierarchical models. A chapter is devoted to Markov Chain Monte Carlo simulation (Gibbs sampler, Metropolis-Hastings algorithms and exact simulation).A large number of real examples are studied with R, and each chapter ends with a set of theoretical and applied exercises. While a foundation in probability and mathematical statistics is assumed, three appendices introduce some necessary background. The book is accessible to senior undergraduate students with a solid math background and Ph.D. students in statistics. Furthermore, experienced statisticians and researchers in the above-mentioned fields will find the book valuable as a mathematically sound reference. This book is the English translation of Mod\'elisation et Statistique Spatiales published by Springer in the series Math\'ematiques \& Applications, a series established by Soci\'et\'e de Math\'ematiques Appliqu\'ees et Industrielles (SMAI). Carlo Gaetan is Associate Professor of Statistics in the Department of Statistics at the Ca' Foscari University of Venice. Xavier Guyon is Professor Emeritus at the University of Paris 1 Panth\'eon-Sorbonne. He is author of a Springer monograph on random fields.},
  isbn = {978-0-387-92256-0},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Gaetan_Guyon_2010_Spatial_Statistics_and_Modeling2.pdf;/home/dede/Zotero/storage/Z8RXAUTM/9780387922560.html}
}

@inproceedings{gai2010,
  title = {Learning {{Multiuser Channel Allocations}} in {{Cognitive Radio Networks}}: {{A Combinatorial Multi-Armed Bandit Formulation}}},
  shorttitle = {Learning {{Multiuser Channel Allocations}} in {{Cognitive Radio Networks}}},
  booktitle = {2010 {{IEEE Symposium}} on {{New Frontiers}} in {{Dynamic Spectrum}} ({{DySPAN}})},
  author = {Gai, Yi and Krishnamachari, Bhaskar and Jain, Rahul},
  year = {2010},
  month = apr,
  pages = {1--9},
  doi = {10.1109/DYSPAN.2010.5457857},
  abstract = {We consider the following fundamental problem in the context of channelized dynamic spectrum access. There are M secondary users and N \textquestiondown{} M orthogonal channels. Each secondary user requires a single channel for operation that does not conflict with the channels assigned to the other users. Due to geographic dispersion, each secondary user can potentially see different primary user occupancy behavior on each channel. Time is divided into discrete decision rounds. The throughput obtainable from spectrum opportunities on each user-channel combination over a decision period is modeled as an arbitrarily-distributed random variable with bounded support but unknown mean, i.i.d. over time. The objective is to search for an allocation of channels for all users that maximizes the expected sum throughput. We formulate this problem as a combinatorial multi-armed bandit (MAB), in which each arm corresponds to a matching of the users to channels. Unlike most prior work on multi-armed bandits, this combinatorial formulation results in dependent arms. Moreover, the number of arms grows super-exponentially as the permutation P(N, M). We present a novel matching-learning algorithm with polynomial storage and polynomial computation per decision period for this problem, and prove that it results in a regret (the gap between the expected sum-throughput obtained by a genie-aided perfect allocation and that obtained by this algorithm) that is uniformly upper-bounded for all time n by a function that grows as O(M4Nlogn), i.e. polynomial in the number of unknown parameters and logarithmic in time. We also discuss how our results provide a non-trivial generalization of known theoretical results on multi-armed bandits.},
  keywords = {Arm,Cognitive radio,Communications Society,Intelligent networks,Multiuser channels,Polynomials,Random variables,Stochastic processes,Throughput,todo,USA Councils},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Gai_et_al_2010_Learning_Multiuser_Channel_Allocations_in_Cognitive_Radio_Networks2.pdf;/home/dede/Zotero/storage/KHVL9JGG/5457857.html}
}

@article{gandy2013,
  title = {Guaranteed {{Conditional Performance}} of {{Control Charts}} via {{Bootstrap Methods}}},
  author = {Gandy, Axel and Kval{\o}y, Jan Terje},
  year = {2013},
  journal = {Scandinavian Journal of Statistics},
  volume = {40},
  number = {4},
  pages = {647--668},
  publisher = {{[Board of the Foundation of the Scandinavian Journal of Statistics, Wiley]}},
  issn = {0303-6898},
  abstract = {To use control charts in practice, the in-control state usually has to be estimated. This estimation has a detrimental effect on the performance of control charts, which is often measured by the false alarm probability or the average run length. We suggest an adjustment of the monitoring schemes to overcome these problems. It guarantees, with a certain probability, a conditional performance given the estimated in-control state. The suggested method is based on bootstrapping the data used to estimate the in-control state. The method applies to different types of control charts, and also works with charts based on regression models. If a non-parametric bootstrap is used, the method is robust to model errors. We show large sample properties of the adjustment. The usefulness of our approach is demonstrated through simulation studies.},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/GANDY_KVALÃ˜Y_2013_Guaranteed_Conditional_Performance_of_Control_Charts_via_Bootstrap_Methods.pdf}
}

@article{garavan2018,
  title = {Recruiting the {{ABCD}} Sample: {{Design}} Considerations and Procedures},
  shorttitle = {Recruiting the {{ABCD}} Sample},
  author = {Garavan, H. and Bartsch, H. and Conway, K. and Decastro, A. and Goldstein, R. Z. and Heeringa, S. and Jernigan, T. and Potter, A. and Thompson, W. and Zahs, D.},
  year = {2018},
  month = aug,
  journal = {Developmental Cognitive Neuroscience},
  series = {The {{Adolescent Brain Cognitive Development}} ({{ABCD}}) {{Consortium}}: {{Rationale}}, {{Aims}}, and {{Assessment Strategy}}},
  volume = {32},
  pages = {16--22},
  issn = {1878-9293},
  doi = {10.1016/j.dcn.2018.04.004},
  abstract = {The ABCD study is a new and ongoing project of very substantial size and scale involving 21 data acquisition sites. It aims to recruit 11,500 children and follow them for ten years with extensive assessments at multiple timepoints. To deliver on its potential to adequately describe adolescent development, it is essential that it adopt recruitment procedures that are efficient and effective and will yield a sample that reflects the nation's diversity in an epidemiologically informed manner. Here, we describe the sampling plans and recruitment procedures of this study. Participants are largely recruited through the school systems with school selection informed by gender, race and ethnicity, socioeconomic status, and urbanicity. Procedures for school selection designed to mitigate selection biases, dynamic monitoring of the accumulating sample to correct deviations from recruitment targets, and a description of the recruitment procedures designed to foster a collaborative attitude between the researchers, the schools and the local communities, are provided.},
  langid = {english},
  keywords = {Adolescence,Adolescent Brain Cognitive Development,Recruitment,Study design,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Garavan_et_al_2018_Recruiting_the_ABCD_sample.pdf;/home/dede/Zotero/storage/CW8579HD/Garavan et al. - 2018 - Recruiting the ABCD sample Design considerations .pdf;/home/dede/Zotero/storage/ZWQ9JS98/S1878929317301809.html}
}

@article{garivier2013,
  title = {The {{KL-UCB Algorithm}} for {{Bounded Stochastic Bandits}} and {{Beyond}}},
  author = {Garivier, Aur{\'e}lien and Capp{\'e}, Olivier},
  year = {2013},
  month = aug,
  journal = {arXiv:1102.2490 [cs, math, stat]},
  eprint = {1102.2490},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {This paper presents a finite-time analysis of the KL-UCB algorithm, an online, horizon-free index policy for stochastic bandit problems. We prove two distinct results: first, for arbitrary bounded rewards, the KL-UCB algorithm satisfies a uniformly better regret bound than UCB or UCB2; second, in the special case of Bernoulli rewards, it reaches the lower bound of Lai and Robbins. Furthermore, we show that simple adaptations of the KL-UCB algorithm are also optimal for specific classes of (possibly unbounded) rewards, including those generated from exponential families of distributions. A large-scale numerical study comparing KL-UCB with its main competitors (UCB, UCB2, UCB-Tuned, UCB-V, DMED) shows that KL-UCB is remarkably efficient and stable, including for short time horizons. KL-UCB is also the only method that always performs better than the basic UCB policy. Our regret bounds rely on deviations results of independent interest which are stated and proved in the Appendix. As a by-product, we also obtain an improved regret bound for the standard UCB algorithm.},
  archiveprefix = {arXiv},
  keywords = {93E35,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Systems and Control,Mathematics - Optimization and Control,Mathematics - Statistics Theory,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Garivier_CappÃ©_2013_The_KL-UCB_Algorithm_for_Bounded_Stochastic_Bandits_and_Beyond.pdf;/home/dede/Zotero/storage/BTTUYU4S/1102.html}
}

@article{geisser1979,
  title = {A {{Predictive Approach}} to {{Model Selection}}},
  author = {Geisser, Seymour and Eddy, William F.},
  year = {1979},
  month = mar,
  journal = {Journal of the American Statistical Association},
  volume = {74},
  number = {365},
  pages = {153--160},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.1979.10481632},
  langid = {english},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Geisser_Eddy_1979_A_Predictive_Approach_to_Model_Selection.pdf}
}

@article{gelfand1994,
  title = {Bayesian {{Model Choice}}: {{Asymptotics}} and {{Exact Calculations}}},
  shorttitle = {Bayesian {{Model Choice}}},
  author = {Gelfand, A. E. and Dey, D. K.},
  year = {1994},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  volume = {56},
  number = {3},
  pages = {501--514},
  publisher = {{[Royal Statistical Society, Wiley]}},
  issn = {0035-9246},
  abstract = {Model determination is a fundamental data analytic task. Here we consider the problem of choosing among a finite (without loss of generality we assume two) set of models. After briefly reviewing classical and Bayesian model choice strategies we present a general predictive density which includes all proposed Bayesian approaches that we are aware of. Using Laplace approximations we can conveniently assess and compare the asymptotic behaviour of these approaches. Concern regarding the accuracy of these approximations for small to moderate sample sizes encourages the use of Monte Carlo techniques to carry out exact calculations. A data set fitted with nested non-linear models enables comparisons between proposals and between exact and asymptotic values.},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Gelfand_Dey_1994_Bayesian_Model_Choice.pdf}
}

@book{gelfand2010,
  title = {{Handbook of Spatial Statistics}},
  author = {Gelfand, Alan E. and Diggle, Peter and Guttorp, Peter and Fuentes, Montserrat},
  year = {2010},
  month = mar,
  edition = {1\textdegree{} edizione},
  publisher = {{CRC Press}},
  abstract = {Assembling a collection of very prominent researchers in the field, the Handbook of Spatial Statistics presents a comprehensive treatment of both classical and state-of-the-art aspects of this maturing area. It takes a unified, integrated approach to the material, providing cross-references among chapters.The handbook begins with a historical introduction detailing the evolution of the field. It then focuses on the three main branches of spatial statistics: continuous spatial variation (point referenced data); discrete spatial variation, including lattice and areal unit data; and spatial point patterns. The book also contains a section on space\textendash time work as well as a section on important topics that build upon earlier chapters.By collecting the major work in the field in one source, along with including an extensive bibliography, this handbook will assist future research efforts. It deftly balances theory and application, strongly emphasizes modeling, and introduces many real data analysis examples.},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Gelfand_et_al_2010_Handbook_of_Spatial_Statistics.pdf}
}

@misc{gelman,
  title = {Mismatch between Scientific Theories and Statistical Models},
  author = {Gelman, Andrew},
  keywords = {todo},
  file = {/home/dede/Zotero/storage/GRBYV523/mismatch.pdf}
}

@article{gelman1992,
  title = {Inference from {{Iterative Simulation Using Multiple Sequences}}},
  author = {Gelman, Andrew and Rubin, Donald B.},
  year = {1992},
  month = nov,
  journal = {Statistical Science},
  volume = {7},
  number = {4},
  pages = {457--472},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/ss/1177011136},
  abstract = {The Gibbs sampler, the algorithm of Metropolis and similar iterative simulation methods are potentially very helpful for summarizing multivariate distributions. Used naively, however, iterative simulation can give misleading answers. Our methods are simple and generally applicable to the output of any iterative simulation; they are designed for researchers primarily interested in the science underlying the data and models they are analyzing, rather than for researchers interested in the probability theory underlying the iterative simulations themselves. Our recommended strategy is to use several independent sequences, with starting points sampled from an overdispersed distribution. At each step of the iterative simulation, we obtain, for each univariate estimand of interest, a distributional estimate and an estimate of how much sharper the distributional estimate might become if the simulations were continued indefinitely. Because our focus is on applied inference for Bayesian posterior distributions in real problems, which often tend toward normality after transformations and marginalization, we derive our results as normal-theory approximations to exact Bayesian inference, conditional on the observed simulations. The methods are illustrated on a random-effects mixture model applied to experimental measurements of reaction times of normal and schizophrenic patients.},
  keywords = {Bayesian inference,Convergence of stochastic processes,done,ECM,EM,Gibbs sampler,importance sampling,Metropolis algorithm,multiple imputation,random-effects model,SIR},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Gelman_Rubin_1992_Inference_from_Iterative_Simulation_Using_Multiple_Sequences.pdf;/home/dede/Zotero/storage/FPUEM783/1177011136.html}
}

@article{gelman2000,
  title = {Type {{S}} Error Rates for Classical and {{Bayesian}} Single and Multiple Comparison Procedures},
  author = {Gelman, Andrew and Tuerlinckx, Francis},
  year = {2000},
  month = sep,
  journal = {Computational Statistics},
  volume = {15},
  number = {3},
  pages = {373--390},
  issn = {1613-9658},
  doi = {10.1007/s001800000040},
  abstract = {In classical statistics, the significance of comparisons (e.g., \texttheta 1- \texttheta 2) is calibrated using the Type 1 error rate, relying on the assumption that the true difference is zero, which makes no sense in many applications. We set up a more relevant framework in which a true comparison can be positive or negative, and, based on the data, you can state ``\texttheta 1 {$>$} \texttheta 2 with confidence,'' ``\texttheta 2 {$>$} \texttheta 1 with confidence,'' or ``no claim with confidence.'' We focus on the Type S (for sign) error, which occurs when you claim ``\texttheta 1 {$>$} \texttheta 2 with confidence'' when \texttheta 2{$>$} \texttheta 1 (or vice-versa). We compute the Type S error rates for classical and Bayesian confidence statements and find that classical Type S error rates can be extremely high (up to 50\%). Bayesian confidence statements are conservative, in the sense that claims based on 95\% posterior intervals have Type S error rates between 0 and 2.5\%. For multiple comparison situations, the conclusions are similar.},
  langid = {english},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Gelman_Tuerlinckx_2000_Type_S_error_rates_for_classical_and_Bayesian_single_and_multiple_comparison.pdf}
}

@article{gelman2006,
  title = {Multilevel ({{Hierarchical}}) {{Modeling}}: {{What It Can}} and {{Cannot Do}}},
  shorttitle = {Multilevel ({{Hierarchical}}) {{Modeling}}},
  author = {Gelman, Andrew},
  year = {2006},
  month = aug,
  journal = {Technometrics},
  volume = {48},
  number = {3},
  pages = {432--435},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1198/004017005000000661},
  abstract = {Multilevel (hierarchical) modeling is a generalization of linear and generalized linear modeling in which regression coefficients are themselves given a model, whose parameters are also estimated from data. We illustrate the strengths and limitations of multilevel modeling through an example of the prediction of home radon levels in U.S. counties. The multilevel model is highly effective for predictions at both levels of the model, but could easily be misinterpreted for causal inference.},
  keywords = {Contextual effects,done,Hierarchical model,Multilevel regression},
  annotation = {\_eprint: https://doi.org/10.1198/004017005000000661},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Gelman_2006_Multilevel_(Hierarchical)_Modeling.pdf;/home/dede/Zotero/storage/4ILJLPGF/004017005000000661.html}
}

@book{gelman2007,
  title = {Data {{Analysis Using Regression}} and {{Multilevel}}/{{Hierarchical Models}}},
  author = {Gelman, Andrew and Hill, Jennifer},
  year = {2007},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Gelman_Hill_2007_Data_Analysis_Using_Regression_and_Multilevel-Hierarchical_Models.pdf;/home/dede/Zotero/storage/LGJKENPF/8705fd.html}
}

@article{gelman2008,
  title = {A Weakly Informative Default Prior Distribution for Logistic and Other Regression Models},
  author = {Gelman, Andrew and Jakulin, Aleks and Pittau, Maria Grazia and Su, Yu-Sung},
  year = {2008},
  month = dec,
  journal = {The Annals of Applied Statistics},
  volume = {2},
  number = {4},
  pages = {1360--1383},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {1932-6157, 1941-7330},
  doi = {10.1214/08-AOAS191},
  abstract = {We propose a new prior distribution for classical (nonhierarchical) logistic regression models, constructed by first scaling all nonbinary variables to have mean 0 and standard deviation 0.5, and then placing independent Student-t prior distributions on the coefficients. As a default choice, we recommend the Cauchy distribution with center 0 and scale 2.5, which in the simplest setting is a longer-tailed version of the distribution attained by assuming one-half additional success and one-half additional failure in a logistic regression. Cross-validation on a corpus of datasets shows the Cauchy class of prior distributions to outperform existing implementations of Gaussian and Laplace priors. We recommend this prior distribution as a default choice for routine applied use. It has the advantage of always giving answers, even when there is complete separation in logistic regression (a common problem, even when the sample size is large and the number of predictors is small), and also automatically applying more shrinkage to higher-order interactions. This can be useful in routine data analysis as well as in automated procedures such as chained equations for missing-data imputation. We implement a procedure to fit generalized linear models in R with the Student-t prior distribution by incorporating an approximate EM algorithm into the usual iteratively weighted least squares. We illustrate with several applications, including a series of logistic regressions predicting voting preferences, a small bioassay experiment, and an imputation model for a public health data set.},
  keywords = {Bayesian inference,generalized linear model,hierarchical model,least squares,Linear regression,logistic regression,multilevel model,noninformative prior distribution,todo,weakly informative prior distribution},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Gelman_et_al_2008_A_weakly_informative_default_prior_distribution_for_logistic_and_other.pdf;/home/dede/Zotero/storage/NPSC75PS/08-AOAS191.html}
}

@book{gelman2013,
  title = {Bayesian Data Analysis},
  author = {Gelman, Andrew and Carlin, John B. and Stern, Hal S. and Dunson, David B. and Vehtari, Aki and Rubin, Donald B.},
  year = {2013},
  edition = {Third},
  publisher = {{Chapman and Hall/CRC}},
  isbn = {978-1-4398-4095-5},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Gelman_et_al_2013_Bayesian_data_analysis.pdf}
}

@article{gelman2017,
  title = {Beyond Subjective and Objective in Statistics},
  author = {Gelman, Andrew and Hennig, Christian},
  year = {2017},
  journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
  volume = {180},
  number = {4},
  pages = {967--1033},
  issn = {1467-985X},
  doi = {10.1111/rssa.12276},
  abstract = {Decisions in statistical data analysis are often justified, criticized or avoided by using concepts of objectivity and subjectivity. We argue that the words `objective' and `subjective' in statistics discourse are used in a mostly unhelpful way, and we propose to replace each of them with broader collections of attributes, with objectivity replaced by transparency, consensus, impartiality and correspondence to observable reality, and subjectivity replaced by awareness of multiple perspectives and context dependence. Together with stability, these make up a collection of virtues that we think is helpful in discussions of statistical foundations and practice. The advantage of these reformulations is that the replacement terms do not oppose each other and that they give more specific guidance about what statistical science strives to achieve. Instead of debating over whether a given statistical method is subjective or objective (or normatively debating the relative merits of subjectivity and objectivity in statistical practice), we can recognize desirable attributes such as transparency and acknowledgement of multiple perspectives as complementary goals. We demonstrate the implications of our proposal with recent applied examples from pharmacology, election polling and socio-economic stratification. The aim of the paper is to push users and developers of statistical methods towards more effective use of diverse sources of information and more open acknowledgement of assumptions and goals.},
  copyright = {\textcopyright{} 2017 Royal Statistical Society},
  langid = {english},
  keywords = {Bayesian,done,Frequentist,Good practice,Philosophy of statistics,Virtues},
  annotation = {\_eprint: https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/rssa.12276},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Gelman_Hennig_2017_Beyond_subjective_and_objective_in_statistics.pdf;/home/dede/Zotero/storage/IFHJHVDC/rssa.html}
}

@article{gelman2017a,
  title = {The {{Prior Can Often Only Be Understood}} in the {{Context}} of the {{Likelihood}}},
  author = {Gelman, Andrew and Simpson, Daniel and Betancourt, Michael},
  year = {2017},
  month = oct,
  journal = {Entropy},
  volume = {19},
  number = {10},
  pages = {555},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  doi = {10.3390/e19100555},
  abstract = {A key sticking point of Bayesian analysis is the choice of prior distribution, and there is a vast literature on potential defaults including uniform priors, Jeffreys' priors, reference priors, maximum entropy priors, and weakly informative priors. These methods, however, often manifest a key conceptual tension in prior modeling: a model encoding true prior information should be chosen without reference to the model of the measurement process, but almost all common prior modeling techniques are implicitly motivated by a reference likelihood. In this paper we resolve this apparent paradox by placing the choice of prior into the context of the entire Bayesian analysis, from inference to prediction to model evaluation.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {Bayesian inference,default priors,prior distribution,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Gelman_et_al_2017_The_Prior_Can_Often_Only_Be_Understood_in_the_Context_of_the_Likelihood.pdf;/home/dede/Zotero/storage/I5JMFCZT/555.html}
}

@book{gelman2020,
  title = {{Regression and Other Stories}},
  author = {Gelman, Andrew},
  year = {2020},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  abstract = {Most textbooks on regression focus on theory and the simplest of examples. Real statistical problems, however, are complex and subtle. This is not a book about the theory of regression. It is about using regression to solve real problems of comparison, estimation, prediction, and causal inference. Unlike other books, it focuses on practical issues such as sample size and missing data and a wide range of goals and techniques. It jumps right in to methods and computer code you can use immediately. Real examples, real stories from the authors' experience demonstrate what regression can do and its limitations, with practical advice for understanding assumptions and implementing methods for experiments and observational studies. They make a smooth transition to logistic regression and GLM. The emphasis is on computation in R and Stan rather than derivations, with code available online. Graphics and presentation aid understanding of the models and model fitting.},
  isbn = {978-1-107-67651-0},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Gelman_2020_Regression_and_Other_Stories.pdf}
}

@article{gelman2020a,
  title = {Bayesian {{Workflow}}},
  author = {Gelman, Andrew and Vehtari, Aki and Simpson, Daniel and Margossian, Charles and Carpenter, Bob and Yao, Yuling and Kennedy, Lauren and Gabry, Jonah and B{\"u}rkner, Paul-Christian and Modr{\'a}k, Martin},
  year = {2020},
  month = nov,
  journal = {arXiv:2011.01808 [stat.ME]},
  eprint = {2011.01808},
  eprinttype = {arxiv},
  primaryclass = {stat.ME},
  abstract = {The Bayesian approach to data analysis provides a powerful way to handle uncertainty in all observations, model parameters, and model structure using probability theory. Probabilistic programming languages make it easier to specify and fit Bayesian models, but this still leaves us with many options regarding constructing, evaluating, and using these models, along with many remaining challenges in computation. Using Bayesian inference to solve real-world problems requires not only statistical skills, subject matter knowledge, and programming, but also awareness of the decisions made in the process of data analysis. All of these aspects can be understood as part of a tangled workflow of applied Bayesian statistics. Beyond inference, the workflow also includes iterative model building, model checking, validation and troubleshooting of computational problems, model understanding, and model comparison. We review all these aspects of workflow in the context of several examples, keeping in mind that in practice we will be fitting many models for any given problem, even if only a subset of them will ultimately be relevant for our conclusions.},
  archiveprefix = {arXiv},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Gelman_et_al_2020_Bayesian_Workflow.pdf}
}

@article{gelman2021,
  title = {What Are the {{Most Important Statistical Ideas}} of the {{Past}} 50 {{Years}}?},
  author = {Gelman, Andrew and Vehtari, Aki},
  year = {2021},
  month = jun,
  journal = {Journal of the American Statistical Association},
  volume = {0},
  number = {0},
  pages = {1--11},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.2021.1938081},
  abstract = {We review the most important statistical ideas of the past half century, which we categorize as: counterfactual causal inference, bootstrapping and simulation-based inference, overparameterized models and regularization, Bayesian multilevel models, generic computation algorithms, adaptive decision analysis, robust inference, and exploratory data analysis. We discuss key contributions in these subfields, how they relate to modern computing and big data, and how they might be developed and extended in future decades. The goal of this article is to provoke thought and discussion regarding the larger themes of research in statistics and data science.},
  keywords = {Data analysis,done,History of statistics,Statistical computing},
  annotation = {\_eprint: https://doi.org/10.1080/01621459.2021.1938081},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Gelman_Vehtari_2021_What_are_the_Most_Important_Statistical_Ideas_of_the_Past_50_Years.pdf;/home/dede/Zotero/storage/NPXGDPVL/01621459.2021.html}
}

@article{gelman2021a,
  title = {Why {{Did It Take So Many Decades}} for the {{Behavioral Sciences}} to {{Develop}} a {{Sense}} of {{Crisis Around Methodology}} and {{Replication}}?},
  author = {Gelman, Andrew and Vazire, Simine},
  year = {2021},
  month = nov,
  journal = {Journal of Methods and Measurement in the Social Sciences},
  volume = {12},
  number = {1},
  publisher = {{University of Arizona Libraries}},
  issn = {2159-7855},
  doi = {10.2458/jmmss.3062},
  abstract = {For several decades, leading behavioral scientists have offered strong criticisms of the common practice of null hypothesis significance testing as producing spurious findings without strong theoretical or empirical support. But only in the past decade has this manifested as a full-scale replication crisis. We consider some possible reasons why, on or about December 2010, the behavioral sciences changed.},
  langid = {english},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Gelman_Vazire_2021_Why_Did_It_Take_So_Many_Decades_for_the_Behavioral_Sciences_to_Develop_a_Sense.pdf;/home/dede/Zotero/storage/GZ5FEY22/3062.html}
}

@article{gelman2021b,
  title = {Failure and {{Success}} in {{Political Polling}} and {{Election Forecasting}}},
  author = {Gelman, Andrew},
  year = {2021},
  month = jan,
  journal = {Statistics and Public Policy},
  volume = {8},
  number = {1},
  pages = {67--72},
  publisher = {{Taylor \& Francis}},
  issn = {null},
  doi = {10.1080/2330443X.2021.1971126},
  abstract = {The recent successes and failures of political polling invite several questions: Why did the polls get it wrong in some high-profile races? Conversely, how is it that polls can perform so well, even given all the evident challenges of conducting and interpreting them?},
  keywords = {Political science,Polling,Public opinion,todo},
  annotation = {\_eprint: https://doi.org/10.1080/2330443X.2021.1971126},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Gelman_2021_Failure_and_Success_in_Political_Polling_and_Election_Forecasting.pdf;/home/dede/Zotero/storage/U4STTX8V/2330443X.2021.html}
}

@article{geng2019,
  title = {Probabilistic {{Community Detection With Unknown Number}} of {{Communities}}},
  author = {Geng, Junxian and Bhattacharya, Anirban and Pati, Debdeep},
  year = {2019},
  month = apr,
  journal = {Journal of the American Statistical Association},
  volume = {114},
  number = {526},
  pages = {893--905},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.2018.1458618},
  abstract = {A fundamental problem in network analysis is clustering the nodes into groups which share a similar connectivity pattern. Existing algorithms for community detection assume the knowledge of the number of clusters or estimate it a priori using various selection criteria and subsequently estimate the community structure. Ignoring the uncertainty in the first stage may lead to erroneous clustering, particularly when the community structure is vague. We instead propose a coherent probabilistic framework for simultaneous estimation of the number of communities and the community structure, adapting recently developed Bayesian nonparametric techniques to network models. An efficient Markov chain Monte Carlo (MCMC) algorithm is proposed which obviates the need to perform reversible jump MCMC on the number of clusters. The methodology is shown to outperform recently developed community detection algorithms in a variety of synthetic data examples and in benchmark real-datasets. Using an appropriate metric on the space of all configurations, we develop nonasymptotic Bayes risk bounds even when the number of clusters is unknown. Enroute, we develop concentration properties of nonlinear functions of Bernoulli random variables, which may be of independent interest in analysis of related models. Supplementary materials for this article are available online.},
  keywords = {Bayesian nonparametrics,Clustering consistency,MCMC,Mixture models,Model selection,Network analysis,todo},
  annotation = {\_eprint: https://doi.org/10.1080/01621459.2018.1458618},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Geng_et_al_2019_Probabilistic_Community_Detection_With_Unknown_Number_of_Communities.pdf;/home/dede/Zotero/storage/9RRFMNZ8/01621459.2018.html}
}

@article{genz1992,
  title = {Numerical {{Computation}} of {{Multivariate Normal Probabilities}}},
  author = {Genz, Alan},
  year = {1992},
  month = jun,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {1},
  number = {2},
  pages = {141--149},
  publisher = {{Taylor \& Francis}},
  issn = {1061-8600},
  doi = {10.1080/10618600.1992.10477010},
  abstract = {The numerical computation of a multivariate normal probability is often a difficult problem. This article describes a transformation that simplifies the problem and places it into a form that allows efficient calculation using standard numerical multiple integration algorithms. Test results are presented that compare implementations of two algorithms that use the transformation with currently available software.},
  keywords = {Adaptive integration,done,Monte Carlo,Multivariate normal distribution},
  annotation = {\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/10618600.1992.10477010},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Genz_1992_Numerical_Computation_of_Multivariate_Normal_Probabilities.pdf;/home/dede/Zotero/storage/MJ6YLEGW/10618600.1992.html}
}

@book{genz2009,
  title = {Computation of {{Multivariate Normal}} and t {{Probabilities}}},
  author = {Genz, Alan and Bretz, Frank},
  year = {2009},
  series = {Lecture {{Notes}} in {{Statistics}}},
  publisher = {{Springer-Verlag}},
  address = {{Berlin Heidelberg}},
  doi = {10.1007/978-3-642-01689-9},
  abstract = {Multivariate normal and t probabilities are needed for statistical inference in many applications. Modern statistical computation packages provide functions for the computation of these probabilities for problems with one or two variables. This book describes recently developed methods for accurate and efficient computation of the required probability values for problems with two or more variables. The book discusses methods for specialized problems as well as methods for general problems. The book includes examples that illustrate the probability computations for a variety of applications.},
  isbn = {978-3-642-01688-2},
  langid = {english},
  file = {/home/dede/Zotero/storage/5EIM4MRY/9783642016882.html}
}

@article{george1997,
  title = {Approaches for {{Bayesian Variable Selection}}},
  author = {George, Edward I. and McCulloch, Robert E.},
  year = {1997},
  journal = {Statistica Sinica},
  volume = {7},
  number = {2},
  pages = {339--373},
  publisher = {{Institute of Statistical Science, Academia Sinica}},
  issn = {1017-0405},
  abstract = {This paper describes and compares various hierarchical mixture prior formulations of variable selection uncertainty in normal linear regression models. These include the nonconjugate SSVS formulation of George and McCulloch (1993), as well as conjugate formulations which allow for analytical simplification. Hyperparameter settings which base selection on practical significance, and the implications of using mixtures with point priors are discussed. Computational methods for posterior evaluation and exploration are considered. Rapid updating methods are seen to provide feasible methods for exhaustive evaluation using Gray Code sequencing in moderately sized problems, and fast Markov Chain Monte Carlo exploration in large problems. Estimation of normalization constants is seen to provide improved posterior estimates of individual model probabilities and the total visited probability. Various procedures are illustrated on simulated sample problems and on a real problem concerning the construction of financial index tracking portfolios.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/George_McCulloch_1997_APPROACHES_FOR_BAYESIAN_VARIABLE_SELECTION.pdf}
}

@article{gerber2015,
  title = {Sequential Quasi {{Monte Carlo}}},
  author = {Gerber, Mathieu and Chopin, Nicolas},
  year = {2015},
  month = jun,
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {77},
  number = {3},
  pages = {509--579},
  issn = {1467-9868},
  doi = {10.1111/rssb.12104},
  abstract = {We derive and study sequential quasi Monte Carlo (SQMC), a class of algorithms obtained by introducing QMC point sets in particle filtering. SQMC is related to, and may be seen as an extension of, th...},
  langid = {english},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Gerber_Chopin_2015_Sequential_quasi_Monte_Carlo.pdf;/home/dede/Zotero/storage/7JITJ5VI/rssb.html}
}

@article{german2017,
  title = {{{NATO}} and the Enlargement Debate: Enhancing {{Euro-Atlantic}} Security or Inciting Confrontation?},
  shorttitle = {{{NATO}} and the Enlargement Debate},
  author = {German, Tracey},
  year = {2017},
  month = mar,
  journal = {International Affairs},
  volume = {93},
  number = {2},
  pages = {291--308},
  issn = {0020-5850},
  doi = {10.1093/ia/iix017},
  abstract = {The enlargements of the post-Cold War era have undermined NATO's stated objectives vis-\`a-vis the purpose of incorporating new members and exposed tensions within the alliance over the twin pillars of the 1967 Harmel Report, namely deterrence and dialogue. NATO's stance on enlargement has not only undermined Euro-Atlantic security and triggered new divisions between East and West, it has also exposed aspirant states, particularly those in the post-Soviet space, to sustained pressure and coercion from Moscow. This article examines the rationale for NATO enlargement in the post-Cold War era, focusing on the two post-Soviet aspirant states, Georgia and Ukraine. While these two states contribute, as partners, to the alliance's efforts to advance cooperative security, the issue of their prospective membership threatens to undermine alliance security and cohesion, and their ambitions are therefore unlikely to be realized in the foreseeable future. With Russia taking an increasingly assertive stance on the global stage and uncertainty surrounding the direction of US foreign policy under the presidency of Donald Trump, the issue of NATO enlargement is unlikely to be a priority for the alliance over the next few years. If NATO ultimately rejects any prospect of membership for states in the post-Soviet space, they could be abandoned to Russian influence, indicating that Moscow has a de facto veto over membership of the alliance and conceding spheres of influence to Russia.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/German_2017_NATO_and_the_enlargement_debate.pdf;/home/dede/Zotero/storage/D8EEKVTU/2996078.html}
}

@article{geweke2010,
  title = {Comparing and Evaluating {{Bayesian}} Predictive Distributions of Asset Returns},
  author = {Geweke, John and Amisano, Gianni},
  year = {2010},
  month = apr,
  journal = {International Journal of Forecasting},
  series = {Special {{Issue}}: {{Bayesian Forecasting}} in {{Economics}}},
  volume = {26},
  number = {2},
  pages = {216--230},
  issn = {0169-2070},
  doi = {10.1016/j.ijforecast.2009.10.007},
  abstract = {Bayesian inference in a time series model provides exact out-of-sample predictive distributions that fully and coherently incorporate parameter uncertainty. This study compares and evaluates Bayesian predictive distributions from alternative models, using as an illustration five alternative models of asset returns applied to daily S\&P 500 returns from the period 1976 through 2005. The comparison exercise uses predictive likelihoods and is inherently Bayesian. The evaluation exercise uses the probability integral transformation and is inherently frequentist. The illustration shows that the two approaches can be complementary, with each identifying strengths and weaknesses in models that are not evident using the other.},
  langid = {english},
  keywords = {Forecasting,GARCH,Inverse probability transformation,Markov mixture,Predictive likelihood,S\&P 500 returns,Stochastic volatility},
  file = {/home/dede/Zotero/storage/QADQTWGL/S0169207009001757.html}
}

@article{ghatak2021,
  title = {A {{Change-Detection-Based Thompson Sampling Framework}} for {{Non-Stationary Bandits}}},
  author = {Ghatak, Gourab},
  year = {2021},
  month = oct,
  journal = {IEEE Transactions on Computers},
  volume = {70},
  number = {10},
  pages = {1670--1676},
  issn = {1557-9956},
  doi = {10.1109/TC.2020.3022634},
  abstract = {We consider a non-stationary two-armed bandit framework and propose a change-detection-based Thompson sampling (TS) algorithm, named TS with change-detection (TS-CD), to keep track of the dynamic environment. The non-stationarity is modeled using a Poisson arrival process, which changes the mean of the rewards on each arrival. The proposed strategy compares the empirical mean of the recent rewards of an arm with the estimate of the mean of the rewards from its history. It detects a change when the empirical mean deviates from the mean estimate by a value larger than a threshold. Then, we characterize the lower bound on the duration of the time-window for which the bandit framework must remain stationary for TS-CD to successfully detect a change when it occurs. Consequently, our results highlight an upper bound on the parameter for the Poisson arrival process, for which the TS-CD achieves asymptotic regret optimality with high probability. Finally, we validate the efficacy of TS-CD by testing it for edge-control of radio access technique (RAT)-selection in a wireless network. Our results show that TS-CD not only outperforms the classical max-power RAT selection strategy but also other actively adaptive and passively adaptive bandit algorithms that are designed for non-stationary environments.},
  keywords = {5G,Bayes methods,Change detection algorithms,done,Heuristic algorithms,Microsoft Windows,millimeter-wave communication,Multi-armed bandits,non-stationary bandits,Rats,thompson sampling,Upper bound,Wireless networks},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Ghatak_2021_A_Change-Detection-Based_Thompson_Sampling_Framework_for_Non-Stationary_Bandits2.pdf;/home/dede/Zotero/storage/M2WWLQFS/9194367.html}
}

@article{ghosal2007,
  title = {Convergence Rates of Posterior Distributions for Noniid Observations},
  author = {Ghosal, Subhashis and {van der Vaart}, Aad},
  year = {2007},
  month = feb,
  journal = {The Annals of Statistics},
  volume = {35},
  number = {1},
  eprint = {0708.0491},
  eprinttype = {arxiv},
  pages = {192--223},
  issn = {0090-5364},
  doi = {10.1214/009053606000001172},
  abstract = {We consider the asymptotic behavior of posterior distributions and Bayes estimators based on observations which are required to be neither independent nor identically distributed. We give general results on the rate of convergence of the posterior measure relative to distances derived from a testing criterion. We then specialize our results to independent, nonidentically distributed observations, Markov processes, stationary Gaussian time series and the white noise model. We apply our general results to several examples of infinite-dimensional statistical models including nonparametric regression with normal errors, binary regression, Poisson regression, an interval censoring model, Whittle estimation of the spectral density of a time series and a nonlinear autoregressive model.},
  archiveprefix = {arXiv},
  keywords = {62G20 (Primary) 62G08 (Secondary),Mathematics - Statistics Theory,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Ghosal_van_der_Vaart_2007_Convergence_rates_of_posterior_distributions_for_noniid_observations.pdf;/home/dede/Zotero/storage/R4ZW4VMB/0708.html}
}

@book{ghosal2017,
  title = {Fundamentals of {{Nonparametric Bayesian Inference}}},
  author = {Ghosal, Subhashis and {van der Vaart}, Aad},
  year = {2017},
  series = {Cambridge {{Series}} in {{Statistical}} and {{Probabilistic Mathematics}}},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/9781139029834},
  abstract = {Explosive growth in computing power has made Bayesian methods for infinite-dimensional models - Bayesian nonparametrics - a nearly universal framework for inference, finding practical use in numerous subject areas. Written by leading researchers, this authoritative text draws on theoretical advances of the past twenty years to synthesize all aspects of Bayesian nonparametrics, from prior construction to computation and large sample behavior of posteriors. Because understanding the behavior of posteriors is critical to selecting priors that work, the large sample theory is developed systematically, illustrated by various examples of model and prior combinations. Precise sufficient conditions are given, with complete proofs, that ensure desirable posterior properties and behavior. Each chapter ends with historical notes and numerous exercises to deepen and consolidate the reader's understanding, making the book valuable for both graduate students and researchers in statistics and machine learning, as well as in application areas such as econometrics and biostatistics.},
  isbn = {978-0-521-87826-5},
  file = {/home/dede/Zotero/storage/378HLVQQ/C96325101025D308C9F31F4470DEA2E8.html}
}

@book{ghosh2003,
  title = {Bayesian {{Nonparametrics}}},
  author = {Ghosh, J. K. and Ramamoorthi, R. V.},
  year = {2003},
  series = {Springer {{Series}} in {{Statistics}}},
  publisher = {{Springer-Verlag}},
  address = {{New York}},
  doi = {10.1007/b97842},
  abstract = {Bayesian nonparametrics has grown tremendously in the last three decades, especially in the last few years. This book is the first systematic treatment of Bayesian nonparametric methods and the theory behind them. While the book is of special interest to Bayesians, it will also appeal to statisticians in general because Bayesian nonparametrics offers a whole continuous spectrum of robust alternatives to purely parametric and purely nonparametric methods of classical statistics. The book is primarily aimed at graduate students and can be used as the text for a graduate course in Bayesian nonparametrics. Though the emphasis of the book is on nonparametrics, there is a substantial chapter on asymptotics of classical Bayesian parametric models. Jayanta Ghosh has been Director and Jawaharlal Nehru Professor at the Indian Statistical Institute and President of the International Statistical Institute. He is currently professor of statistics at Purdue University. He has been editor of Sankhya and served on the editorial boards of several journals including the Annals of Statistics. Apart from Bayesian analysis, his interests include asymptotics, stochastic modeling, high dimensional model selection, reliability and survival analysis and bioinformatics. R.V. Ramamoorthi is professor at the Department of Statistics and Probability at Michigan State University. He has published papers in the areas of sufficiency invariance, comparison of experiments, nonparametric survival analysis and Bayesian analysis. In addition to Bayesian nonparametrics, he is currently interested in Bayesian networks and graphical models. He is on the editorial board of Sankhya.},
  isbn = {978-0-387-95537-7},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Ghosh_Ramamoorthi_2003_Bayesian_Nonparametrics.pdf;/home/dede/Zotero/storage/E8QHBU9D/9780387955377.html}
}

@article{gijbels2004,
  title = {Bootstrap Test for Change-Points in Nonparametric Regression},
  author = {Gijbels, I. and Goderniaux, A-C.},
  year = {2004},
  month = jun,
  journal = {Journal of Nonparametric Statistics},
  volume = {16},
  number = {3-4},
  pages = {591--611},
  publisher = {{Taylor \& Francis}},
  issn = {1048-5252},
  doi = {10.1080/10485250310001626088},
  abstract = {The objective of this article is to test whether or not there is an abrupt change in the regression function itself or in its first derivative at certain (prespecified or not) locations. The test does not rely on asymptotics but approximates the sample distribution of the test statistic using a bootstrap procedure. The proposed testing method involves a data-driven choice of the smoothing parameters. The performance of the testing procedures is evaluated via a simulation study. Some comparison with an asymptotic test by Hamrouni (1999) and Gr\'egoire and Hamrouni (2002b) and asymptotic tests by M\"uller and Stadtm\"uller (1999) and Dubowik and Stadtm\"uller (2000) is provided. We also demonstrate the use of the testing procedures on some real data.},
  keywords = {Bandwidth,Bootstrap,Cross-validation,Derivative,Discontinuity points,Least-squares fitting,Local polynomial approximation,todo},
  annotation = {\_eprint: https://doi.org/10.1080/10485250310001626088},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Gijbels_Goderniaux_2004_Bootstrap_test_for_change-points_in_nonparametric_regression.pdf;/home/dede/Zotero/storage/UT2ZNXRR/10485250310001626088.html}
}

@book{gill1982,
  title = {{Practical Optimization}},
  author = {Gill, Philip E. and Murray, Walter and Wright, Margaret H.},
  year = {1982},
  month = jan,
  edition = {Reprint edizione},
  publisher = {{Academic Press}},
  address = {{London ; New York}},
  abstract = {This book is designed to help problem solvers make the best use of optimization software--i.e., to use existing methods most effectively whenever possible, and to adapt and modify techniques for particular problems if necessary. The contents of this book therefore include some topics that are essential for all those who wish to solve optimization problems. In addition, certain topics are treated that should be of special interest in most practical optimization problems. For example, advice is given to users who wish to take an active role in formulating their problems so as to enhance the chances of solving them successfully, and to users who need to understand why a certain method fails to solve their problem.},
  isbn = {978-0-12-283952-8},
  langid = {Inglese}
}

@article{gilli2019,
  title = {Why {{China Has Not Caught Up Yet}}: {{Military-Technological Superiority}} and the {{Limits}} of {{Imitation}}, {{Reverse Engineering}}, and {{Cyber Espionage}}},
  shorttitle = {Why {{China Has Not Caught Up Yet}}},
  author = {Gilli, Andrea and Gilli, Mauro},
  year = {2019},
  month = feb,
  journal = {International Security},
  volume = {43},
  number = {3},
  pages = {141--189},
  issn = {0162-2889},
  doi = {10.1162/isec_a_00337},
  abstract = {Can countries easily imitate the United States' advanced weapon systems and thus erode its military-technological superiority? Scholarship in international relations theory generally assumes that rising states benefit from the ``advantage of backwardness.'' That is, by free riding on the research and technology of the most advanced countries, less developed states can allegedly close the military-technological gap with their rivals relatively easily and quickly. More recent works maintain that globalization, the emergence of dual-use components, and advances in communications have facilitated this process. This literature is built on shaky theoretical foundations, however, and its claims lack empirical support. In particular, it largely ignores one of the most important changes to have occurred in the realm of weapons development since the second industrial revolution: the exponential increase in the complexity of military technology. This increase in complexity has promoted a change in the system of production that has made the imitation and replication of the performance of state-of-the-art weapon systems harder\textemdash so much so as to offset the diffusing effects of globalization and advances in communications. An examination of the British-German naval rivalry (1890\textendash 1915) and China's efforts to imitate U.S. stealth fighters supports these findings.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Gilli_Gilli_2019_Why_China_Has_Not_Caught_Up_Yet.pdf;/home/dede/Zotero/storage/7PXFMBE9/Why-China-Has-Not-Caught-Up-Yet-Military.html}
}

@book{giordano2004,
  title = {{Storia della politica internazionale 1870 - 2001}},
  author = {Giordano, Giancarlo},
  year = {2004},
  series = {{Collana di scienza politica e relazioni internazionali}},
  edition = {Nuova ed. ampliata},
  number = {21},
  publisher = {{Angeli}},
  address = {{Milano}},
  isbn = {978-88-464-5697-7},
  langid = {italian},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Giordano_2004_Storia_della_politica_internazionale_1870_-_2001.pdf}
}

@book{giraud2021,
  title = {{Introduction to High-Dimensional Statistics}},
  author = {Giraud, Christophe},
  year = {2021},
  edition = {Second},
  publisher = {{Chapman and Hall/CRC}},
  address = {{Boca Raton}},
  isbn = {978-1-4822-3794-8},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Giraud_2021_Introduction_to_High-Dimensional_Statistics.pdf}
}

@article{gnedenko1943,
  title = {Sur {{La Distribution Limite Du Terme Maximum D}}'{{Une Serie Aleatoire}}},
  author = {Gnedenko, B.},
  year = {1943},
  journal = {Annals of Mathematics},
  volume = {44},
  number = {3},
  pages = {423--453},
  publisher = {{Annals of Mathematics}},
  issn = {0003-486X},
  doi = {10.2307/1968974}
}

@article{godambe1960,
  title = {An {{Optimum Property}} of {{Regular Maximum Likelihood Estimation}}},
  author = {Godambe, V. P.},
  year = {1960},
  month = dec,
  journal = {The Annals of Mathematical Statistics},
  volume = {31},
  number = {4},
  pages = {1208--1211},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177705693},
  abstract = {The Annals of Mathematical Statistics},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Godambe_1960_An_Optimum_Property_of_Regular_Maximum_Likelihood_Estimation.pdf;/home/dede/Zotero/storage/K9NZXTVA/1177705693.html}
}

@article{godambe1987,
  title = {Quasi-{{Likelihood}} and {{Optimal Estimation}}, {{Correspondent Paper}}},
  author = {Godambe, V. P. and Heyde, C. C.},
  year = {1987},
  journal = {International Statistical Review / Revue Internationale de Statistique},
  volume = {55},
  number = {3},
  pages = {231--244},
  publisher = {{[Wiley, International Statistical Institute (ISI)]}},
  issn = {0306-7734},
  doi = {10.2307/1403403},
  abstract = {Within the framework of estimating function theory, this paper provides a very general definition of quasi-likelihood estimating equations. Applications to stochastic processes are discussed. This work extends the previous results of Godambe (1985) and Hutton \& Nelson (1986). /// Cet article \'etudie l'estimation des param\`etres optimaux pour les processus stochastiques. Les \'equations d'estimation sont utilis\'ees. Une d\'efinition tr\`es g\'en\'erale est pr\'esent\'ee pour l'estimateur de quasi-vraisemblance. Ce travail g\'en\'eralise les r\'esultats de Godambe (1985) et de Hutton \& Nelson (1986).},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Godambe_Heyde_1987_Quasi-Likelihood_and_Optimal_Estimation,_Correspondent_Paper.pdf}
}

@article{goeman2011,
  title = {Multiple {{Testing}} for {{Exploratory Research}}},
  author = {Goeman, Jelle J. and Solari, Aldo},
  year = {2011},
  month = nov,
  journal = {Statistical Science},
  volume = {26},
  number = {4},
  issn = {0883-4237},
  doi = {10.1214/11-STS356},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Goeman_Solari_2011_Multiple_Testing_for_Exploratory_Research.pdf}
}

@incollection{goldstein2016,
  title = {Multilevel {{Models}}},
  booktitle = {Wiley {{StatsRef}}: {{Statistics Reference Online}}},
  author = {Goldstein, Harvey and Browne, William J.},
  editor = {Balakrishnan, N. and Colton, Theodore and Everitt, Brian and Piegorsch, Walter and Ruggeri, Fabrizio and Teugels, Jozef L.},
  year = {2016},
  month = jul,
  pages = {1--8},
  publisher = {{John Wiley \& Sons, Ltd}},
  address = {{Chichester, UK}},
  doi = {10.1002/9781118445112.stat05764.pub2},
  isbn = {978-1-118-44511-2},
  langid = {english},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Goldstein_Browne_2016_Multilevel_Models.pdf}
}

@book{golub2013,
  title = {Matrix {{Computations}}},
  author = {Golub, Gene H. and Van Loan, Charles F.},
  year = {2013},
  publisher = {{JHU Press}},
  abstract = {The fourth edition of Gene H. Golub and Charles F. Van Loan's classic is an essential reference for computational scientists and engineers in addition to researchers in the numerical linear algebra community. Anyone whose work requires the solution to a matrix problem and an appreciation of its mathematical properties will find this book to be an indispensible tool. This revision is a cover-to-cover expansion and renovation of the third edition. It now includes an introduction to tensor computations and brand new sections on \textbullet{} fast transforms\textbullet{} parallel LU\textbullet{} discrete Poisson solvers\textbullet{} pseudospectra\textbullet{} structured linear equation problems\textbullet{} structured eigenvalue problems\textbullet{} large-scale SVD methods\textbullet{} polynomial eigenvalue problems Matrix Computations is packed with challenging problems, insightful derivations, and pointers to the literature\textemdash everything needed to become a matrix-savvy developer of numerical methods and software. The second most cited math book of 2012 according to MathSciNet, the book has placed in the top 10 for since 2005.},
  googlebooks = {X5YfsuCWpxMC},
  isbn = {978-1-4214-0794-4},
  langid = {english},
  keywords = {Mathematics / Applied,Mathematics / General},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Golub_Loan_2013_Matrix_Computations.pdf}
}

@article{gombay2003,
  title = {Sequential {{Change-Point Detection}} and {{Estimation}}},
  author = {Gombay, Edit},
  year = {2003},
  month = jan,
  journal = {Sequential Analysis},
  volume = {22},
  number = {3},
  pages = {203--222},
  publisher = {{Taylor \& Francis}},
  issn = {0747-4946},
  doi = {10.1081/SQA-120025028},
  abstract = {Two groups of sequential testing procedures are proposed to detect an abrupt change in the distribution of a sequence of observations: truncated and open ended. They are based on large sample strong approximations of the efficient score vector under the null hypothesis of no change and under the alternative hypothesis. An estimator of the time of change is proposed and its approximate bias is analyzed. The estimation of the new parameters that describe the changed distribution naturally follows.},
  keywords = {Change detection,Efficient score vector,Exponential family,Sequential tests,Strong approximations},
  annotation = {\_eprint: https://doi.org/10.1081/SQA-120025028},
  file = {/home/dede/Zotero/storage/HXQ6ZNJQ/SQA-120025028.html}
}

@article{gombay2004,
  title = {U-{{Statistics}} in {{Sequential Tests}} and {{Change Detection}}},
  author = {Gombay, Edit},
  year = {2004},
  month = dec,
  journal = {Sequential Analysis},
  volume = {23},
  number = {2},
  pages = {257--274},
  publisher = {{Taylor \& Francis}},
  issn = {0747-4946},
  doi = {10.1081/SQA-120034111},
  abstract = {Research on U-statistics within the general framework of sequential and change-point literature is surveyed. Some recent developments are discussed and extended. New sequential testing strategies based on Wiener process approximations are proposed, and empirical studies explore the finite sample performance of these tests. It allows users to choose one that is appropriate for their application.},
  keywords = {Change-point detection,Primary 62G20,Secondary 60F05; 62G10,Sequential tests,Stochastic processes,Strong approximations,todo,U-Statistic},
  annotation = {\_eprint: https://doi.org/10.1081/SQA-120034111},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Gombay_2004_U-Statistics_in_Sequential_Tests_and_Change_Detection.pdf;/home/dede/Zotero/storage/ZNBZNLII/SQA-120034111.html}
}

@article{gomez2021,
  title = {An {{Adaptive Sampling Strategy}} for {{Online Monitoring}} and {{Diagnosis}} of {{High-Dimensional Streaming Data}}},
  author = {G{\'o}mez, Ana Mar{\'i}a Estrada and Li, Dan and Paynabar, Kamran},
  year = {2021},
  month = aug,
  journal = {Technometrics},
  volume = {0},
  number = {0},
  pages = {1--17},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.2021.1967198},
  abstract = {Statistical process control techniques have been widely used for online process monitoring and diagnosis of streaming data in various applications, including manufacturing, healthcare, and environmental engineering. In some applications, the sensing system that collects online data can only provide partial information from the process due to resource constraints. In such cases, an adaptive sampling strategy is needed to decide where to collect data while maximizing the change detection capability. This article proposes an adaptive sampling strategy for online monitoring and diagnosis with partially observed data. The proposed methodology integrates two novel ideas (i) the recursive projection of the high-dimensional streaming data onto a low-dimensional subspace to capture the spatio-temporal structure of the data while performing missing data imputation; and (ii) the development of an adaptive sampling scheme, balancing exploration and exploitation, to decide where to collect data at each acquisition time. Through simulations and two case studies, the proposed framework's performance is evaluated and compared with benchmark methods.},
  keywords = {Adaptive sampling,doing,High-dimensional streaming data,Online monitoring and diagnosis,Tensor recovery},
  annotation = {\_eprint: https://doi.org/10.1080/00401706.2021.1967198},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/GÃ³mez_et_al_2021_An_Adaptive_Sampling_Strategy_for_Online_Monitoring_and_Diagnosis_of.pdf}
}

@article{goncalves2003,
  title = {Consistency of the Stationary Bootstrap under Weak Moment Conditions},
  author = {Gon{\c c}alves, S{\'{\i}}lvia and {de Jong}, Robert},
  year = {2003},
  month = nov,
  journal = {Economics Letters},
  volume = {81},
  number = {2},
  pages = {273--278},
  issn = {0165-1765},
  doi = {10.1016/S0165-1765(03)00192-7},
  abstract = {We prove the first order asymptotic validity of the stationary bootstrap of Politis and Romano [J. Am. Statistics Assoc. 89 (1994) 1303] under the existence of only slightly more than second moments. Our results improve upon previous results in the literature, which assumed finite sixth moments.},
  langid = {english},
  keywords = {Second moments,Stationary bootstrap},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/GonÃ§alves_de_Jong_2003_Consistency_of_the_stationary_bootstrap_under_weak_moment_conditions.pdf;/home/dede/Zotero/storage/E8N9I44T/S0165176503001927.html}
}

@book{goodfellow2017,
  title = {{Deep Learning}},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2017},
  month = jan,
  publisher = {{Mit Pr}},
  address = {{Cambridge, Massachusetts}},
  abstract = {An introduction to a broad range of topics in deep learning, covering mathematical and conceptual background, deep learning techniques used in industry, and research perspectives.Â“Written by three experts in the field, Deep Learning is the only comprehensive book on the subject.Â”Â—Elon Musk, cochair of OpenAI; cofounder and CEO of Tesla and SpaceXDeep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and videogames. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models. Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors.},
  isbn = {978-0-262-03561-3},
  langid = {Inglese},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Goodfellow_et_al_2017_Deep_Learning.pdf}
}

@article{goodfellow2017a,
  title = {{{NIPS}} 2016 {{Tutorial}}: {{Generative Adversarial Networks}}},
  shorttitle = {{{NIPS}} 2016 {{Tutorial}}},
  author = {Goodfellow, Ian},
  year = {2017},
  month = apr,
  journal = {arXiv:1701.00160 [cs]},
  eprint = {1701.00160},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {This report summarizes the tutorial presented by the author at NIPS 2016 on generative adversarial networks (GANs). The tutorial describes: (1) Why generative modeling is a topic worth studying, (2) how generative models work, and how GANs compare to other generative models, (3) the details of how GANs work, (4) research frontiers in GANs, and (5) state-of-the-art image models that combine GANs with other methods. Finally, the tutorial contains three exercises for readers to complete, and the solutions to these exercises.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Goodfellow_2017_NIPS_2016_Tutorial.pdf;/home/dede/Zotero/storage/G4ESBBU6/1701.html}
}

@book{goodrich2011,
  title = {{Data Structures and Algorithms in C++}},
  author = {Goodrich, Michael T. and Tamassia, Roberto and Mount, David M.},
  year = {2011},
  month = jan,
  edition = {2\textdegree{} edizione},
  publisher = {{John Wiley \& Sons Inc}},
  address = {{Hoboken, N.J}},
  abstract = {An updated, innovative approach to data structures and algorithms Written by an author team of experts in their fields, this authoritative guide demystifies even the most difficult mathematical concepts so that you can gain a clear understanding of data structures and algorithms in C++. The unparalleled author team incorporates the object-oriented design paradigm using C++ as the implementation language, while also providing intuition and analysis of fundamental algorithms.  Offers a unique multimedia format for learning the fundamentals of data structures and algorithms Allows you to visualize key analytic concepts, learn about the most recent insights in the field, and do data structure design Provides clear approaches for developing programs Features a clear, easy-to-understand writing style that breaks down even the most difficult mathematical concepts  Building on the success of the first edition, this new version offers you an innovative approach to fundamental data structures and algorithms.},
  isbn = {978-0-470-38327-8},
  langid = {Inglese}
}

@misc{goodrich2020,
  title = {Bayesian {{Applied Regression Modeling}} via {{Stan}}},
  author = {Goodrich, B. and Gabry, J. and Ali, I. and Brilleman, S.},
  year = {2020},
  abstract = {Estimates previously compiled regression models using the rstan     package, which provides the R interface to the Stan C++ library for Bayesian     estimation. Users specify models via the customary R syntax with a formula and     data.frame plus some additional arguments for priors.},
  file = {/home/dede/Zotero/storage/VFJWU5KI/rstanarm.html}
}

@article{grady1992,
  title = {Hormone Therapy to Prevent Disease and Prolong Life in Postmenopausal Women},
  author = {Grady, D. and Rubin, S. M. and Petitti, D. B. and Fox, C. S. and Black, D. and Ettinger, B. and Ernster, V. L. and Cummings, S. R.},
  year = {1992},
  month = dec,
  journal = {Annals of Internal Medicine},
  volume = {117},
  number = {12},
  pages = {1016--1037},
  issn = {0003-4819},
  doi = {10.7326/0003-4819-117-12-1016},
  abstract = {PURPOSE: To critically review the risks and benefits of hormone therapy for asymptomatic postmenopausal women who are considering long-term hormone therapy to prevent disease or to prolong life. DATA SOURCES: Review of the English-language literature since 1970 on the effect of estrogen therapy and estrogen plus progestin therapy on endometrial cancer, breast cancer, coronary heart disease, osteoporosis, and stroke. We used standard meta-analytic statistical methods to pool estimates from studies to determine summary relative risks for these diseases in hormone users and modified lifetable methods to estimate changes in lifetime probability and life expectancy due to use of hormone regimens. RESULTS: There is evidence that estrogen therapy decreases risk for coronary heart disease and for hip fracture, but long-term estrogen therapy increases risk for endometrial cancer and may be associated with a small increase in risk for breast cancer. The increase in endometrial cancer risk can probably be avoided by adding a progestin to the estrogen regimen for women who have a uterus, but the effects of combination hormones on risk for other diseases has not been adequately studied. We present estimates for changes in lifetime probabilities of disease and life expectancy due to hormone therapy in women who have had a hysterectomy; with coronary heart disease; and at increased risk for coronary heart disease, hip fracture, and breast cancer. CONCLUSIONS: Hormone therapy should probably be recommended for women who have had a hysterectomy and for those with coronary heart disease or at high risk for coronary heart disease. For other women, the best course of action is unclear.},
  langid = {english},
  pmid = {1443971},
  keywords = {Breast Neoplasms,Cerebrovascular Disorders,Coronary Disease,Endometrial Neoplasms,Estrogen Replacement Therapy,Female,Hip Fractures,Humans,Middle Aged,Progestins,Risk Factors}
}

@article{graham2010,
  title = {A {{Phase I}} Nonparametric {{Shewhart-type}} Control Chart Based on the Median},
  author = {Graham, M. A. and Human, S. W. and Chakraborti, S.},
  year = {2010},
  month = nov,
  journal = {Journal of Applied Statistics},
  volume = {37},
  number = {11},
  pages = {1795--1813},
  publisher = {{Taylor \& Francis}},
  issn = {0266-4763},
  doi = {10.1080/02664760903164913},
  abstract = {A nonparametric Shewhart-type control chart is proposed for monitoring the location of a continuous variable in a Phase I process control setting. The chart is based on the pooled median of the available Phase I samples and the charting statistics are the counts (number of observations) in each sample that are less than the pooled median. An exact expression for the false alarm probability (FAP) is given in terms of the multivariate hypergeometric distribution and this is used to provide tables for the control limits for a specified nominal FAP value (of 0.01, 0.05 and 0.10, respectively) and for some values of the sample size (n) and the number of Phase I samples (m). Some approximations are discussed in terms of the univariate hypergeometric and the normal distributions. A simulation study shows that the proposed chart performs as well as, and in some cases better than, an existing Shewhart-type chart based on the normal distribution. Numerical examples are given to demonstrate the implementation of the new chart.},
  keywords = {distribution-free,false alarm probability,false alarm rate,multivariate hypergeometric,prospective,retrospective,todo},
  annotation = {\_eprint: https://doi.org/10.1080/02664760903164913},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Graham_et_al_2010_A_Phase_I_nonparametric_Shewhart-type_control_chart_based_on_the_median.pdf;/home/dede/Zotero/storage/BRTU4ARJ/02664760903164913.html}
}

@book{gramacy,
  title = {Chapter 1 {{Historical Perspective}} | {{Surrogates}}},
  author = {Gramacy, Robert B.},
  abstract = {Chapter 1 Historical Perspective | Surrogates: a new graduate level textbook on topics lying at the interface between machine learning, spatial statistics, computer simulation, meta-modeling (i.e., emulation), and design of experiments. Gaussian process emphasis facilitates flexible nonparametric and nonlinear modeling, with applications to uncertainty quantification, sensitivity analysis, calibration of computer models to field data, sequential design and (blackbox) optimization under uncertainty. Presentation targets numerically competent scientists in the engineering, physical, and biological sciences. Treatment includes historical perspective and canonical examples, but primarily concentrates on modern statistical methods, computation and implementation in R at modern scale. Rmarkdown facilitates a fully reproducible tour complete with motivation from, application to, and illustration with, compelling real-data examples.},
  file = {/home/dede/Zotero/storage/MS6XZE2F/chap1.html}
}

@article{gramacy2009,
  title = {Bayesian Treed {{Gaussian}} Process Models with an Application to Computer Modeling},
  author = {Gramacy, Robert B. and Lee, Herbert K. H.},
  year = {2009},
  month = mar,
  journal = {arXiv:0710.4536 [stat]},
  eprint = {0710.4536},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Motivated by a computer experiment for the design of a rocket booster, this paper explores nonstationary modeling methodologies that couple stationary Gaussian processes with treed partitioning. Partitioning is a simple but effective method for dealing with nonstationarity. The methodological developments and statistical computing details which make this approach efficient are described in detail. In addition to providing an analysis of the rocket booster simulator, our approach is demonstrated to be effective in other arenas.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Applications,Statistics - Computation,Statistics - Methodology,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Gramacy_Lee_2009_Bayesian_treed_Gaussian_process_models_with_an_application_to_computer_modeling.pdf;/home/dede/Zotero/storage/HVMRWT2E/0710.html}
}

@book{gramacy2020,
  title = {{Surrogates Gaussian Process Modeling, Design, and Optimization for the Applied Sciences}},
  shorttitle = {{Surrogates}},
  author = {Gramacy, Robert B.},
  year = {2020},
  month = jan,
  edition = {1\textdegree{} edizione},
  publisher = {{Chapman and Hall/CRC}},
  address = {{Boca Raton}},
  abstract = {Surrogates: a graduate textbook, or professional handbook, on topics at the interface between machine learning, spatial statistics, computer simulation, meta-modeling (i.e., emulation), design of experiments, and optimization. Experimentation through simulation, "human out-of-the-loop" statistical support (focusing on the science), management of dynamic processes, online and real-time analysis, automation, and practical application are at the forefront.   Topics include:    Gaussian process (GP) regression for flexible nonparametric and nonlinear modeling.   Applications to uncertainty quantification, sensitivity analysis, calibration of computer models to field data, sequential design/active learning and (blackbox/Bayesian) optimization under uncertainty.    Advanced topics include treed partitioning, local GP approximation, modeling of simulation experiments (e.g., agent-based models) with coupled nonlinear mean and variance (heteroskedastic) models.    Treatment appreciates historical response surface methodology (RSM) and canonical examples, but emphasizes contemporary methods and implementation in R at modern scale.   Rmarkdown facilitates a fully reproducible tour, complete with motivation from, application to, and illustration with, compelling real-data examples.   Presentation targets numerically competent practitioners in engineering, physical, and biological sciences. Writing is statistical in form, but the subjects are not about statistics. Rather, they're about prediction and synthesis under uncertainty; about visualization and information, design and decision making, computing and clean code.},
  isbn = {978-0-367-41542-6},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Gramacy_2020_Surrogates_Gaussian_Process_Modeling,_Design,_and_Optimization_for_the_Applied.pdf}
}

@article{green1995,
  title = {Reversible {{Jump Markov Chain Monte Carlo Computation}} and {{Bayesian Model Determination}}},
  author = {Green, Peter J.},
  year = {1995},
  journal = {Biometrika},
  volume = {82},
  number = {4},
  pages = {711--732},
  publisher = {{[Oxford University Press, Biometrika Trust]}},
  issn = {0006-3444},
  doi = {10.2307/2337340},
  abstract = {Markov chain Monte Carlo methods for Bayesian computation have until recently been restricted to problems where the joint distribution of all variables has a density with respect to some fixed standard underlying measure. They have therefore not been available for application to Bayesian model determination, where the dimensionality of the parameter vector is typically not fixed. This paper proposes a new framework for the construction of reversible Markov chain samplers that jump between parameter subspaces of differing dimensionality, which is flexible and entirely constructive. It should therefore have wide applicability in model determination problems. The methodology is illustrated with applications to multiple change-point analysis in one and two dimensions, and to a Bayesian comparison of binomial experiments.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Green_1995_Reversible_Jump_Markov_Chain_Monte_Carlo_Computation_and_Bayesian_Model.pdf}
}

@book{greene1999,
  title = {{Econometric Analysis}},
  author = {Greene, William H.},
  year = {1999},
  month = jul,
  edition = {Subsequent edizione},
  publisher = {{Prentice Hall}},
  address = {{Upper Saddle River, N.J}},
  abstract = {For graduate-level courses in Introduction to Econometrics. A standard text/reference in courses that include basic techniques in regression analysis and extensions used when linear models prove inadequate or inappropriate. Areas of application include Economics, Sociology, Political Science, Medical Research, Transport Research, and Environmental Economics. This book introduces students to the broad field of applied econometrics. An effective bridge to both on-the-job problems and to the professional literature, it features extensive applications and presents sufficient theoretical background to enable students to recognize new variants of the models that they learn about here as merely natural extensions that fit within a common body of principles.},
  isbn = {978-0-13-013297-0},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Greene_1999_Econometric_Analysis.pdf}
}

@article{griffin2006,
  title = {Order-{{Based Dependent Dirichlet Processes}}},
  author = {Griffin, J. E. and Steel, M. F. J.},
  year = {2006},
  journal = {Journal of the American Statistical Association},
  volume = {101},
  number = {473},
  pages = {179--194},
  publisher = {{[American Statistical Association, Taylor \& Francis, Ltd.]}},
  issn = {0162-1459},
  abstract = {In this article we propose a new framework for Bayesian nonparametric modeling with continuous covariates. In particular, we allow the nonparametric distribution to depend on covariates through ordering the random variables building the weights in the stick-breaking representation. We focus mostly on the class of random distributions that induces a Dirichlet process at each covariate value. We derive the correlation between distributions at different covariate values and use a point process to implement a practically useful type of ordering. Two main constructions with analytically known correlation structures are proposed. Practical and efficient computational methods are introduced. We apply our framework, through mixtures of these processes, to regression modeling, the modeling of stochastic volatility in time series data, and spatial geostatistical modeling.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Griffin_Steel_2006_Order-Based_Dependent_Dirichlet_Processes.pdf}
}

@article{griffin2021,
  title = {In Search of Lost Mixing Time: Adaptive {{Markov}} Chain {{Monte Carlo}} Schemes for {{Bayesian}} Variable Selection with Very Large p},
  shorttitle = {In Search of Lost Mixing Time},
  author = {Griffin, J E and {\L}atuszy{\'n}ski, K G and Steel, M F J},
  year = {2021},
  month = mar,
  journal = {Biometrika},
  volume = {108},
  number = {1},
  pages = {53--69},
  issn = {0006-3444, 1464-3510},
  doi = {10.1093/biomet/asaa055},
  abstract = {Summary             The availability of datasets with large numbers of variables is rapidly increasing. The effective application of Bayesian variable selection methods for regression with these datasets has proved difficult since available Markov chain Monte Carlo methods do not perform well in typical problem sizes of interest. We propose new adaptive Markov chain Monte Carlo algorithms to address this shortcoming. The adaptive design of these algorithms exploits the observation that in large-\$p\$, small-\$n\$ settings, the majority of the \$p\$ variables will be approximately uncorrelated a posteriori. The algorithms adaptively build suitable nonlocal proposals that result in moves with squared jumping distance significantly larger than standard methods. Their performance is studied empirically in high-dimensional problems and speed-ups of up to four orders of magnitude are observed.},
  langid = {english},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Griffin_et_al_2021_In_search_of_lost_mixing_time.pdf}
}

@article{gu2014,
  title = {A T-chart for {{Monitoring Multi}}-variety and {{Small Batch Production Run}}},
  author = {Gu, Kai and Jia, Xinzhang and You, Hailong and Zhang, Shihong},
  year = {2014},
  journal = {Quality \& Reliability Engineering International},
  volume = {30},
  number = {2},
  pages = {287--299},
  doi = {10.1002/qre.1496},
  abstract = {Statistical process control is an important tool to monitor and control a process. It is used to ensure that the manufacturing process operates in the in-control state. Multi-variety and small batch production runs are common in manufacturing environments like flexible manufacturing systems and Just-in-Time systems, which are characterized by a wide variety of mixed products with small volume for each kind of production. It is difficult to apply traditional control charts efficiently and effectively in such environments. The method that control charts are plotted for each individual part is not proper, since the successive state of the manufacturing process cannot be reflected. In this paper, a proper t-chart is proposed for implementation in multi-variety and small batch production runs to monitor the process mean, and its statistical properties are evaluated. The run length distribution of the proposed t-chart has been obtained by modelling the multi-variety process. The ARL performance for various shifts, number of product types, and subgroup sizes has also been obtained. The results show that the t-chart can be successfully implemented to monitor a multi-variety production run. Finally, illustrative examples show that the proposed t-chart is effective in multi-variety and small batch manufacturing environment. Copyright  2013 John Wiley \& Sons, Ltd.},
  keywords = {control chart,f-distribution,multi-variety,small batch,statistical process control},
  file = {/home/dede/Zotero/storage/GQA9FWPR/show.html}
}

@article{guo2020,
  title = {Partially {{Observable Online Change Detection}} via {{Smooth-Sparse Decomposition}}},
  author = {Guo, Jie and Yan, Hao and Zhang, Chen and Hoi, Steven},
  year = {2020},
  month = sep,
  journal = {arXiv:2009.10645 [cs, stat]},
  eprint = {2009.10645},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We consider online change detection of high dimensional data streams with sparse changes, where only a subset of data streams can be observed at each sensing time point due to limited sensing capacities. On the one hand, the detection scheme should be able to deal with partially observable data and meanwhile have efficient detection power for sparse changes. On the other, the scheme should be able to adaptively and actively select the most important variables to observe to maximize the detection power. To address these two points, in this paper, we propose a novel detection scheme called CDSSD. In particular, it describes the structure of high dimensional data with sparse changes by smooth-sparse decomposition, whose parameters can be learned via spike-slab variational Bayesian inference. Then the posterior Bayes factor, which incorporates the learned parameters and sparse change information, is formulated as a detection statistic. Finally, by formulating the statistic as the reward of a combinatorial multi-armed bandit problem, an adaptive sampling strategy based on Thompson sampling is proposed. The efficacy and applicability of our method in practice are demonstrated with numerical studies and a real case study.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,done,Statistics - Applications,Statistics - Machine Learning},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Guo_et_al_2020_Partially_Observable_Online_Change_Detection_via_Smooth-Sparse_Decomposition.pdf;/home/dede/Zotero/storage/US3LWREV/2009.html}
}

@article{guo2021,
  title = {Threshold Selection in Feature Screening for Error Rate Control},
  author = {Guo, Xu and Ren, Haojie and Zou, Changliang and Li, Runze},
  year = {2021},
  month = dec,
  journal = {Journal of the American Statistical Association},
  volume = {0},
  number = {ja},
  pages = {1--37},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.2021.2011735},
  abstract = {Hard thresholding rule is commonly adopted in feature screening procedures to screen out unimportant predictors for ultrahigh-dimensional data. However, different thresholds are required to adapt to different contexts of screening problems and an appropriate thresholding magnitude usually varies from the model and error distribution. With an ad-hoc choice, it is unclear whether all of the important predictors are selected or not, and it is very likely that the procedures would include many unimportant features. We introduce a data-adaptive threshold selection procedure with error rate control, which is applicable to most kinds of popular screening methods. The key idea is to apply the sample-splitting strategy to construct a series of statistics with marginal symmetry property and then to utilize the symmetry for obtaining an approximation to the number of false discoveries. We show that the proposed method is able to asymptotically control the false discovery rate and per family error rate under certain conditions and still retains all of the important predictors. Three important examples are presented to illustrate the merits of the new proposed procedures. Numerical experiments indicate that the proposed methodology works well for many existing screening methods.},
  keywords = {Empirical distribution,False discovery rate,Feature screening,Per family error rate,skimmed,Symmetry},
  annotation = {\_eprint: https://doi.org/10.1080/01621459.2021.2011735},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Guo_et_al_2021_Threshold_selection_in_feature_screening_for_error_rate_control.pdf}
}

@inproceedings{gupta2011,
  title = {Thompson {{Sampling}} for {{Dynamic Multi-armed Bandits}}},
  booktitle = {2011 10th {{International Conference}} on {{Machine Learning}} and {{Applications}} and {{Workshops}}},
  author = {Gupta, Neha and Granmo, Ole-Christoffer and Agrawala, Ashok},
  year = {2011},
  month = dec,
  volume = {1},
  pages = {484--489},
  doi = {10.1109/ICMLA.2011.144},
  abstract = {The importance of multi-armed bandit (MAB) problems is on the rise due to their recent application in a large variety of areas such as online advertising, news article selection, wireless networks, and medicinal trials, to name a few. The most common assumption made when solving such MAB problems is that the unknown reward probability \texttheta k of each bandit arm k is fixed. However, this assumption rarely holds in practice simply because real-life problems often involve underlying processes that are dynamically evolving. In this paper, we model problems where reward probabilities \texttheta k are drifting, and introduce a new method called Dynamic Thompson Sampling (DTS) that facilitates Order Statistics based Thompson Sampling for these dynamically evolving MABs. The DTS algorithm adapts its success probability estimates, hat \texttheta k, faster than traditional Thompson Sampling schemes and thus leads to improved performance in terms of lower regret. Extensive experiments demonstrate that DTS outperforms current state-of-the-art approaches, namely pure Thompson Sampling, UCB-Normal and UCBf, for the case of dynamic reward probabilities. Furthermore, this performance advantage increases persistently with the number of bandit arms.},
  keywords = {Bayesian methods,Bayesian Techniques,Dynamics,Educational institutions,Electronic mail,Heuristic algorithms,Learning Algorithms,Multi-Armed Bandits,Random variables,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Gupta_et_al_2011_Thompson_Sampling_for_Dynamic_Multi-armed_Bandits.pdf;/home/dede/Zotero/storage/3RBDSE9X/6147024.html}
}

@article{gupta2011a,
  title = {Theory and {{Use}} of the {{EM Algorithm}}},
  author = {Gupta, Maya R. and Chen, Yihua},
  year = {2011},
  month = apr,
  journal = {Foundations and Trends\textregistered{} in Signal Processing},
  volume = {4},
  number = {3},
  pages = {223--296},
  publisher = {{Now Publishers, Inc.}},
  issn = {1932-8346, 1932-8354},
  doi = {10.1561/2000000034},
  abstract = {Theory and Use of the EM Algorithm},
  langid = {english},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Gupta_Chen_2011_Theory_and_Use_of_the_EM_Algorithm.pdf;/home/dede/Zotero/storage/WB3HZGKD/SIG-034.html}
}

@book{gut2009,
  title = {{An Intermediate Course in Probability}},
  author = {Gut, Allan},
  year = {2009},
  edition = {2\textdegree{} edizione},
  publisher = {{Springer Nature}},
  abstract = {This is the only book that gives a rigorous and comprehensive treatment with lots of examples, exercises, remarks on this particular level between the standard first undergraduate course and the first graduate course based on measure theory.There is no competitor to this book.The book can be used in classrooms as well as for self-study.},
  isbn = {978-1-4419-0161-3},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Gut_2009_An Intermediate Course in Probability.pdf}
}

@book{haan,
  title = {Extreme {{Value Theory}}: {{An Introduction}}},
  shorttitle = {Extreme {{Value Theory}}},
  author = {de de Haan, Laurens and Ferreira, Ana},
  abstract = {Focuses on theoretical results along with applicationsAll the main topics covering the heart of the subject are introduced to the reader in a systematic fashionConcentration is on the probabilistic and statistical aspects of extreme valuesExcellent introduction to extreme value theory at the graduate level, requiring only some mathematical maturity}
}

@article{haario2001,
  title = {An Adaptive {{Metropolis}} Algorithm},
  author = {Haario, Heikki and Saksman, Eero and Tamminen, Johanna},
  year = {2001},
  month = apr,
  journal = {Bernoulli},
  volume = {7},
  number = {2},
  pages = {223--242},
  publisher = {{Bernoulli Society for Mathematical Statistics and Probability}},
  issn = {1350-7265},
  abstract = {A proper choice of a proposal distribution for Markov chain Monte Carlo methods, for example for the Metropolis-Hastings algorithm, is well known to be a crucial factor for the convergence of the algorithm. In this paper we introduce an adaptive Metropolis (AM) algorithm, where the Gaussian proposal distribution is updated along the process using the full information cumulated so far. Due to the adaptive nature of the process, the AM algorithm is non-Markovian, but we establish here that it has the correct ergodic properties. We also include the results of our numerical tests, which indicate that the AM algorithm competes well with traditional Metropolis-Hastings algorithms, and demonstrate that the AM algorithm is easy to use in practical computation.},
  keywords = {Adaptive Markov chain Monte Carlo,Comparison,convergence,ergodicity,Markov chain Monte Carlo,Metropolis-Hastings algorithm,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Haario_et_al_2001_An_adaptive_Metropolis_algorithm.pdf;/home/dede/Zotero/storage/JW7KVPSC/1080222083.html}
}

@article{hadidoust2015,
  title = {Monitoring and Change-Point Estimation for Spline-Modeled Non-Linear Profiles in Phase {{II}}},
  author = {Hadidoust, Zahra and Samimi, Yaser and Shahriari, Hamid},
  year = {2015},
  month = dec,
  journal = {Journal of Applied Statistics},
  volume = {42},
  number = {12},
  pages = {2520--2530},
  publisher = {{Taylor \& Francis}},
  issn = {0266-4763},
  doi = {10.1080/02664763.2015.1043864},
  abstract = {In some applications of statistical quality control, quality of a process or a product is best characterized by a functional relationship between a response variable and one or more explanatory variables. This relationship is referred to as a profile. In certain cases, the quality of a process or a product is better described by a non-linear profile which does not follow a specific parametric model. In these circumstances, nonparametric approaches with greater flexibility in modeling the complicated profiles are adopted. In this research, the spline smoothing method is used to model a complicated non-linear profile and the Hotelling T2 control chart based on the spline coefficients is used to monitor the process. After receiving an out-of-control signal, a maximum likelihood estimator is employed for change point estimation. The simulation studies, which include both global and local shifts, provide appropriate evaluation of the performance of the proposed estimation and monitoring procedure. The results indicate that the proposed method detects large global shifts while it is very sensitive in detecting local shifts.},
  keywords = {B-spline smoothing method,change-point estimation,Hotelling T2 chart,non-linear profile,profile monitoring,todo},
  annotation = {\_eprint: https://doi.org/10.1080/02664763.2015.1043864},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Hadidoust_et_al_2015_Monitoring_and_change-point_estimation_for_spline-modeled_non-linear_profiles.pdf;/home/dede/Zotero/storage/QCBXZM8W/02664763.2015.html}
}

@article{haizler2019,
  title = {Factorial {{Designs}} for {{Online Experiments}}},
  author = {Haizler, Tamar and Steinberg, David M.},
  year = {2019},
  journal = {Technometrics},
  volume = {63},
  number = {1},
  pages = {1--12},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.2019.1701556},
  abstract = {Online experiments and specifically A/B testing are commonly used to identify whether a proposed change to a web page is in fact an effective one. This study focuses on basic settings in which a binary outcome is obtained from each user who visits the website and the probability of a response may be affected by numerous factors. We use Bayesian probit regression to model the factor effects and combine elements from traditional two-level factorial experiments and multiarmed bandits to construct sequential designs that embed attractive features of estimation and exploitation.},
  keywords = {todo},
  annotation = {\_eprint: https://doi.org/10.1080/00401706.2019.1701556},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Haizler_Steinberg_2019_Factorial_Designs_for_Online_Experiments.pdf;/home/dede/Zotero/storage/6GG5ECBA/00401706.2019.html}
}

@book{hall1980,
  title = {Martingale {{Limit Theory}} and Its {{Application}}},
  author = {Hall, Peter and Heide, C.C.},
  year = {1980},
  publisher = {{Elsevier}},
  doi = {10.1016/C2013-0-10818-5},
  isbn = {978-0-12-319350-6},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Hall_Heide_1980_Martingale_Limit_Theory_and_its_Application.pdf}
}

@article{hall1987,
  title = {On {{Kullback-Leibler Loss}} and {{Density Estimation}}},
  author = {Hall, Peter},
  year = {1987},
  month = dec,
  journal = {The Annals of Statistics},
  volume = {15},
  number = {4},
  pages = {1491--1519},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/aos/1176350606},
  abstract = {"Discrimination information," or Kullback-Leibler loss, is an appropriate measure of distance in problems of discrimination. We examine it in the context of nonparametric kernel density estimation and show that its asymptotic properties are profoundly influenced by tail properties of the kernel and of the unknown density. We suggest ways of choosing the kernel so as to reduce loss, and describe the extent to which likelihood cross-validation asymptotically minimises loss. Likelihood cross-validation generally leads to selection of a window width of the correct order of magnitude, but not necessarily to a window with the correct first-order properties. However, if the kernel is chosen appropriately, then likelihood cross-validation does result in asymptotic minimisation of Kullback-Leibler loss.},
  keywords = {62G99,62H99,Density estimation,discrimination,kernel method,Kullback-Leibler loss,likelihood cross-validation},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Hall_1987_On_Kullback-Leibler_Loss_and_Density_Estimation.pdf;/home/dede/Zotero/storage/D2FPUKVR/1176350606.html}
}

@book{hall1992,
  title = {The {{Bootstrap}} and {{Edgeworth Expansion}}},
  author = {Hall, Peter},
  year = {1992},
  publisher = {{Springer}},
  address = {{New York}},
  isbn = {978-0-387-97720-1},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Hall_1992_The_Bootstrap_and_Edgeworth_Expansion.pdf}
}

@book{hall2012,
  title = {Macroeconomics: {{Principles}} and {{Applications}}},
  shorttitle = {Macroeconomics},
  author = {Hall, Robert and Lieberman, Marc},
  year = {2012},
  month = feb,
  edition = {6 edition},
  publisher = {{Cengage Learning}},
  address = {{Mason, OH}},
  abstract = {Discover how today's macroeconomic policy issues, decisions, and applications impact you every day with the practical, accessible presentation in MACROECONOMICS. Written by acclaimed economists Hall and Lieberman, this straightforward contemporary text offers a presentation as current as the latest headlines. Fresh new cutting-edge examples throughout this edition as well as updated mini-cases clearly illustrate core macroeconomic principles and applications in action. This edition's streamlined chapters focus on today's most important macroeconomic theories and events. The latest thinking from leading economists helps equip readers with a solid foundation in macroeconomics necessary for success, no matter what the career.},
  isbn = {978-1-111-82235-4},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Hall_Lieberman_2012_Macroeconomics.pdf}
}

@article{hallin2002,
  title = {Optimal {{Tests}} for {{Multivariate Location Based}} on {{Interdirections}} and {{Pseudo-Mahalanobis Ranks}}},
  author = {Hallin, Marc and Paindaveine, Davy},
  year = {2002},
  journal = {The Annals of Statistics},
  volume = {30},
  number = {4},
  pages = {1103--1133},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364},
  abstract = {We propose a family of tests, based on Randles' (1989) concept of interdirections and the ranks of pseudo-Mahalanobis distances computed with respect to a multivariate M-estimator of scatter due to Tyler (1987), for the multivariate one-sample problem under elliptical symmetry. These tests, which generalize the univariate signed-rank tests, are affine-invariant. Depending on the score function considered (van der Waerden, Laplace,...), they allow for locally asymptotically maximin tests at selected densities (multivariate normal, multivariate double-exponential,...). Local powers and asymptotic relative efficiencies are derived-with respect to Hotelling's test, Randles' (1989) multivariate sign test. Peters and Randles' (1990) Wilcoxon-type test, and with respect to the Oja median tests. We, moreover, extend to the multivariate setting two famous univariate results: the traditional Chernoff-Savage (1958) property, showing that Hotelling's traditional procedure is uniformly dominated, in the Pitman sense, by the van der Waerden version of our tests, and the celebrated Hodges-Lehmann (1956) ".864 result," providing, for any fixed space dimension k, the lower bound for the asymptotic relative efficiency of Wilcoxon-type tests with respect to Hotelling's. These asymptotic results are confirmed by a Monte Carlo investigation, and application to a real data set.},
  keywords = {toRead},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Hallin_Paindaveine_2002_Optimal_Tests_for_Multivariate_Location_Based_on_Interdirections_and.pdf}
}

@article{hallin2008,
  title = {Optimal Rank-Based Tests for Homogeneity of Scatter},
  author = {Hallin, Marc and Paindaveine, Davy},
  year = {2008},
  month = jun,
  journal = {The Annals of Statistics},
  volume = {36},
  number = {3},
  pages = {1261--1298},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/07-AOS508},
  abstract = {We propose a class of locally and asymptotically optimal tests, based on multivariate ranks and signs for the homogeneity of scatter matrices in m elliptical populations. Contrary to the existing parametric procedures, these tests remain valid without any moment assumptions, and thus are perfectly robust against heavy-tailed distributions (validity robustness). Nevertheless, they reach semiparametric efficiency bounds at correctly specified elliptical densities and maintain high powers under all (efficiency robustness). In particular, their normal-score version outperforms traditional Gaussian likelihood ratio tests and their pseudo-Gaussian robustifications under a very broad range of non-Gaussian densities including, for instance, all multivariate Student and power-exponential distributions.},
  keywords = {62G35,62M15,Adaptivity,Elliptic densities,local asymptotic normality,scatter matrix,Semiparametric efficiency,shape matrix,toRead},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Hallin_Paindaveine_2008_Optimal_rank-based_tests_for_homogeneity_of_scatter.pdf;/home/dede/Zotero/storage/9TTWY6YU/07-AOS508.html}
}

@article{hallin2021,
  title = {Multivariate Goodness-of-Fit Tests Based on {{Wasserstein}} Distance},
  author = {Hallin, Marc and Mordant, Gilles and Segers, Johan},
  year = {2021},
  month = jan,
  journal = {Electronic Journal of Statistics},
  volume = {15},
  number = {1},
  pages = {1328--1371},
  publisher = {{Institute of Mathematical Statistics and Bernoulli Society}},
  issn = {1935-7524, 1935-7524},
  doi = {10.1214/21-EJS1816},
  abstract = {Goodness-of-fit tests based on the empirical Wasserstein distance are proposed for simple and composite null hypotheses involving general multivariate distributions. For group families, the procedure is to be implemented after preliminary reduction of the data via invariance. This property allows for calculation of exact critical values and p-values at finite sample sizes. Applications include testing for location\textendash scale families and testing for families arising from affine transformations, such as elliptical distributions with given standard radial density and unspecified location vector and scatter matrix. A novel test for multivariate normality with unspecified mean vector and covariance matrix arises as a special case. For more general parametric families, we propose a parametric bootstrap procedure to calculate critical values. The lack of asymptotic distribution theory for the empirical Wasserstein distance means that the validity of the parametric bootstrap under the null hypothesis remains a conjecture. Nevertheless, we show that the test is consistent against fixed alternatives. To this end, we prove a uniform law of large numbers for the empirical distribution in Wasserstein distance, where the uniformity is over any class of underlying distributions satisfying a uniform integrability condition but no additional moment assumptions. The calculation of test statistics boils down to solving the well-studied semi-discrete optimal transport problem. Extensive numerical experiments demonstrate the practical feasibility and the excellent performance of the proposed tests for the Wasserstein distance of order p=1 and p=2 and for dimensions at least up to d=5. The simulations also lend support to the conjecture of the asymptotic validity of the parametric bootstrap.},
  keywords = {copula,elliptical distribution,Goodness-of-fit,group families,multivariate normality,Optimal transport,semi-discrete problem,skew-t distribution,todo,Wasserstein distance},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Hallin_et_al_2021_Multivariate_goodness-of-fit_tests_based_on_Wasserstein_distance.pdf;/home/dede/Zotero/storage/B2C95GWW/21-EJS1816.html}
}

@book{hamilton1994,
  title = {{Time Series Analysis}},
  author = {Hamilton, James D.},
  year = {1994},
  month = mar,
  publisher = {{Princeton Univ Pr}},
  address = {{Princeton, N.J}},
  abstract = {The last decade has brought dramatic changes in the way that researchers analyze economic and financial time series. This book synthesizes these recent advances and makes them accessible to first-year graduate students. James Hamilton provides the first adequate text-book treatments of important innovations such as vector autoregressions, generalized method of moments, the economic and statistical consequences of unit roots, time-varying variances, and nonlinear time series models. In addition, he presents basic tools for analyzing dynamic systems (including linear representations, autocovariance generating functions, spectral analysis, and the Kalman filter) in a way that integrates economic theory with the practical difficulties of analyzing and interpreting real-world data. Time Series Analysis fills an important need for a textbook that integrates economic theory, econometrics, and new results. The book is intended to provide students and researchers with a self-contained survey of time series analysis. It starts from first principles and should be readily accessible to any beginning graduate student, while it is also intended to serve as a reference book for researchers.},
  isbn = {978-0-691-04289-3},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Hamilton_1994_Time_Series_Analysis.pdf}
}

@inproceedings{hamilton2007,
  title = {Compact {{Hilbert Indices}} for {{Multi-Dimensional Data}}},
  booktitle = {First {{International Conference}} on {{Complex}}, {{Intelligent}} and {{Software Intensive Systems}}},
  author = {Hamilton, Chris H. and {Rau-Chaplin}, Andrew},
  year = {2007},
  month = apr,
  pages = {139--146},
  publisher = {{IEEE}},
  doi = {10.1109/CISIS.2007.16},
  isbn = {978-0-7695-2823-6},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Hamilton_Rau-Chaplin_2007_Compact_Hilbert_Indices_for_Multi-Dimensional_Data.pdf}
}

@article{hanagal2017,
  title = {Correlated Gamma Frailty Models for Bivariate Survival Data},
  author = {Hanagal, David D. and Pandey, Arvind and Ganguly, Ayon},
  year = {2017},
  month = may,
  journal = {Communications in Statistics - Simulation and Computation},
  volume = {46},
  number = {5},
  pages = {3627--3644},
  issn = {0361-0918},
  doi = {10.1080/03610918.2015.1085559},
  abstract = {Frailty models are used in the survival analysis to account for the unobserved heterogeneity in individual risks to disease and death. To analyze the bivariate data on related survival times (e.g., matched pairs experiments, twin or family data) the shared frailty models were suggested. Shared frailty models are used despite their limitations. To overcome their disadvantages correlated frailty models may be used. In this article, we introduce the gamma correlated frailty models with two different baseline distributions namely, the generalized log logistic, and the generalized Weibull. We introduce the Bayesian estimation procedure using Markov chain Monte Carlo (MCMC) technique to estimate the parameters involved in these models. We present a simulation study to compare the true values of the parameters with the estimated values. Also we apply these models to a real life bivariate survival dataset related to the kidney infection data and a better model is suggested for the data.},
  keywords = {62F15,62N01,62P10,Bayesian estimation,correlated frailty,Correlated gamma frailty,done,Generalized log-logistic distribution,Generalized Weibull distribution,survival},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Hanagal_et_al_2017_Correlated_gamma_frailty_models_for_bivariate_survival_data.pdf;/home/dede/Zotero/storage/N98JNHI3/03610918.2015.html}
}

@article{hansen1943,
  title = {On the {{Theory}} of {{Sampling}} from {{Finite Populations}}},
  author = {Hansen, Morris H. and Hurwitz, William N.},
  year = {1943},
  month = dec,
  journal = {The Annals of Mathematical Statistics},
  volume = {14},
  number = {4},
  pages = {333--362},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177731356},
  abstract = {The Annals of Mathematical Statistics},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Hansen_Hurwitz_1943_On_the_Theory_of_Sampling_from_Finite_Populations.pdf;/home/dede/Zotero/storage/ICJ2QILY/1177731356.html}
}

@book{hardle2019,
  title = {Applied {{Multivariate Statistical Analysis}}},
  author = {H{\"a}rdle, Wolfgang Karl and Simar, L{\'e}opold},
  year = {2019},
  edition = {Fifth},
  publisher = {{Springer-Verlag}},
  address = {{Berlin Heidelberg}},
  doi = {10.1007/978-3-662-45171-7},
  abstract = {Focusing on high-dimensional applications, this 4th edition presents the tools and concepts used in multivariate data analysis in a style that is also accessible for non-mathematicians and practitioners. All chapters include practical exercises that highlight applications in different multivariate data analysis fields. All of the examples involve high to ultra-high dimensions and represent a number of major fields in big data analysis.The fourth edition of this book on Applied Multivariate Statistical Analysis offers the following new features:A new chapter on Variable Selection (Lasso, SCAD and Elastic Net)All exercises are supplemented by R and MATLAB code that can be found on www.quantlet.de.The practical exercises include solutions that can be found in H\"ardle, W. and Hlavka, Z., Multivariate Statistics: Exercises and Solutions. Springer Verlag, Heidelberg.},
  isbn = {978-3-662-45171-7},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/HÃ¤rdle_Simar_2015_Applied Multivariate Statistical Analysis.pdf;/home/dede/Zotero/storage/ZIYZ6XMU/9783662451717.html}
}

@article{harrison1987,
  title = {Practical {{Bayesian Forecasting}}},
  author = {Harrison, Jeff and West, Mike},
  year = {1987},
  journal = {Journal of the Royal Statistical Society. Series D (The Statistician)},
  volume = {36},
  number = {2/3},
  pages = {115--125},
  publisher = {{[Royal Statistical Society, Wiley]}},
  issn = {0039-0526},
  doi = {10.2307/2348504},
  abstract = {We describe and review the purpose and environment of Bayesian forecasting systems, stressing foundational concepts, component models, the discount concept and intervention, and interactive analyses using a purpose-built suite of APL\vphantom\{\}\$ functions.}
}

@article{harrison1991,
  title = {Dynamic {{Linear Model Diagnostics}}},
  author = {Harrison, Jeff and West, Mike},
  year = {1991},
  journal = {Biometrika},
  volume = {78},
  number = {4},
  pages = {797--808},
  publisher = {{[Oxford University Press, Biometrika Trust]}},
  issn = {0006-3444},
  doi = {10.2307/2336931},
  abstract = {In time series analysis using dynamic linear models, retrospective analysis involves the calculation of filtered, or smoothed, distributions for state parameters in the past. We develop and illustrate novel results that are useful in retrospective assessment of the influence of individual observations on such distributions. In particular, new and computationally simple filtering equations are derived for past state parameters based on leaving out one observation at a time, providing dynamic model based versions of methods currently used in standard, static regression diagnostics.}
}

@article{harrison1994,
  title = {A Bayesian Decision Approach to Model Monitoring and Cusums},
  author = {Harrison, P. J. and Veerapen, P. P.},
  year = {1994},
  journal = {Journal of Forecasting},
  volume = {13},
  number = {1},
  pages = {29--36},
  issn = {1099-131X},
  doi = {10.1002/for.3980130105},
  abstract = {Cumulative Sum techniques are widely used in quality control and model monitoring. A single-sided cusum may be regarded essentially as a sequence of sequential tests which, in many cases, such as those for the Exponential Family, is equivalent to a Sequence of Sequential Probability Ratio Tests. The relationship between cusums and Bayesian decisions is difficult to establish using conventional methods. An alternative approach is proposed which not only reveals a relation but also offers a very simple formulation of the decision process involved in model monitoring. This is first illustrated for a Normal mean and then extended to other important practical cases including Dynamic Models. For V-mask cusum graphs a particular feature is the interpretation of the distance of the V vertex from the latest plotted point in terms of the prior precision as measured in `equivalent' observations.},
  langid = {english},
  keywords = {Bayes,Cusums,Decisions,Dynamic models,Exponential family,Forecasting,Monitoring,Quality control,Sequential tests},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/for.3980130105},
  file = {/home/dede/Zotero/storage/7QPKC76H/for.html}
}

@article{harrison1999,
  title = {Statistical Process Control and Model Monitoring},
  author = {Harrison, P. J.},
  year = {1999},
  month = feb,
  journal = {Journal of Applied Statistics},
  volume = {26},
  number = {2},
  pages = {273--292},
  publisher = {{Taylor \& Francis}},
  issn = {0266-4763},
  doi = {10.1080/02664769922601},
  abstract = {This paper is concerned with model monitoring and quality control schemes, which are founded on a decision theoretic formulation. After identifying unacceptable weaknesses associated with Wald, sequential probability ratio test (SPRT) and Cuscore monitors, the Bayes decision monitor is developed. In particular, the paper focuses on what is termed a 'popular decision scheme' (PDS) for which the monitoring run loss functions are specified simply in terms of two indiff erence qualities. For most applications, the PDS results in forward cumulative sum tests of functions of the observations. For many exponential family applications, the PDS is equivalent to well-used SPRTs and Cusums. In particular, a neat interpretation of V-mask cusum chart settings is derived when simultaneously running two symmetric PDSs. However, apart from providing a decision theoretic basis for monitoring, sensible procedures occur in applications for which SPRTs and Cuscores are particularly unsatisfactory. Average run lengths (ARLs) are given for two special cases, and the inadequacy of the Wald and similar ARL approximations is revealed. Generalizations and applications to normal and dynamic linear models are discussed. The paper concludes by deriving conditions under which sequences of forward and backward sequential or Cusum chart tests are equivalent.},
  annotation = {\_eprint: https://doi.org/10.1080/02664769922601},
  file = {/home/dede/Zotero/storage/TXUKS6ZA/02664769922601.html}
}

@book{harville2018,
  title = {{Linear Models and the Relevant Distributions and Matrix Algebra}},
  author = {Harville, David A.},
  year = {2018},
  edition = {1\textdegree{} edizione},
  publisher = {{Chapman and Hall/CRC}},
  address = {{S.l.}},
  isbn = {978-0-367-57203-7},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Harville_2018_Linear_Models_and_the_Relevant_Distributions_and_Matrix_Algebra.pdf}
}

@article{hastie1987,
  title = {Generalized {{Additive Models}}: {{Some Applications}}},
  shorttitle = {Generalized {{Additive Models}}},
  author = {Hastie, Trevor and Tibshirani, Robert},
  year = {1987},
  month = jun,
  journal = {Journal of the American Statistical Association},
  volume = {82},
  number = {398},
  pages = {371--386},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.1987.10478440},
  langid = {english},
  keywords = {done}
}

@article{hastie1989,
  title = {Principal {{Curves}}},
  author = {Hastie, Trevor and Stuetzle, Werner},
  year = {1989},
  month = jun,
  journal = {Journal of the American Statistical Association},
  volume = {84},
  number = {406},
  pages = {502--516},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.1989.10478797},
  langid = {english},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Hastie_Stuetzle_1989_Principal_Curves.pdf}
}

@book{hastie2013,
  title = {{The Elements of Statistical Learning: Data Mining, Inference, and Prediction}},
  shorttitle = {{The Elements of Statistical Learning}},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  year = {2013},
  month = jun,
  publisher = {{Springer Nature}},
  address = {{New York}},
  abstract = {This book describes the important ideas in a variety of fields such as medicine, biology, finance, and marketing in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of colour graphics. It is a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting---the first comprehensive treatment of this topic in any book. This major new edition features many topics not covered in the original, including graphical models, random forests, ensemble methods, least angle regression \& path algorithms for the lasso, non-negative matrix factorisation, and spectral clustering. There is also a chapter on methods for "wide'' data (p bigger than n), including multiple testing and false discovery rates.},
  isbn = {978-0-387-84857-0},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Hastie_et_al_2013_The_Elements_of_Statistical_Learning.pdf}
}

@book{hastie2015,
  title = {Statistical {{Learning}} with {{Sparsity}}},
  author = {Hastie, Trevor and Tibshirani, Robert and Wainwright, Martin},
  year = {2015},
  month = aug,
  edition = {1st edition},
  publisher = {{Routledge}},
  address = {{Boca Raton}},
  isbn = {978-1-4987-1216-3},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Hastie_et_al_2015_Statistical_Learning_with_Sparsity.pdf}
}

@book{hastings2012,
  title = {Inferno: {{The World}} at {{War}}, 1939-1945},
  shorttitle = {Inferno},
  author = {Hastings, Max},
  year = {2012},
  month = oct,
  edition = {Illustrated edition},
  publisher = {{Vintage}},
  address = {{New York}},
  isbn = {978-0-307-47553-4},
  langid = {english},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Hastings_2012_Inferno.epub}
}

@article{hawkins1977,
  title = {Testing a {{Sequence}} of {{Observations}} for a {{Shift}} in {{Location}}},
  author = {Hawkins, Douglas M.},
  year = {1977},
  journal = {Journal of the American Statistical Association},
  volume = {72},
  number = {357},
  pages = {180--186},
  publisher = {{[American Statistical Association, Taylor \& Francis, Ltd.]}},
  issn = {0162-1459},
  doi = {10.2307/2286934},
  abstract = {A possible alternative to the hypothesis that the sequence X\textsubscript{1}, X\textsubscript{2}, ..., X\textsubscript{n} are iid N({$\xi$}, {$\sigma$}\textsuperscript{2}) random variables is that at some unknown instant the expectation {$\xi$} shifts. The likelihood ratio test for the alternative of a location shift is studied and its distribution under the null hypothesis found. Tables of standard fractiles are given, along with asymptotic results.}
}

@article{hawkins1987,
  title = {Self-{{Starting Cusum Charts}} for {{Location}} and {{Scale}}},
  author = {Hawkins, Douglas M.},
  year = {1987},
  journal = {Journal of the Royal Statistical Society. Series D (The Statistician)},
  volume = {36},
  number = {4},
  pages = {299--316},
  publisher = {{[Royal Statistical Society, Wiley]}},
  issn = {0039-0526},
  doi = {10.2307/2348827},
  abstract = {In some quality control problems, it is not known what the exact process mean and standard deviation are under control but it is desired to determine whether there have been drifts from the conditions obtained at the process start-up. This situation is not well-covered by standard cumulative sum procedures, which generally assume known process parameters. This paper uses the running mean and standard deviation of all observations made on the process since start-up as substitutes for the unknown true values of the process mean and standard deviation. Using some theoretical properties of independence of residuals, two pairs of cusums are set up: one testing for constancy of location of the process, and the other for constancy of the spread. While the process is under control, both these cusum pairs are of approximately normal \$N(0, 1)\$ quantities (and therefore are well understood), but if the location, the spread or both change, then non-centrality is introduced into one or both of the location and scale cusum pairs, and it drifts out-of-control. It is shown that the procedure performs well in detecting changes in the process, even in comparison with the often utopian situation in which the process mean and variance are known exactly prior to the start of the cusum.},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Hawkins_1987_Self-Starting_Cusum_Charts_for_Location_and_Scale.pdf}
}

@book{hawkins1998,
  title = {Cumulative {{Sum Charts}} and {{Charting}} for {{Quality Improvement}}},
  author = {Hawkins, Douglas M. and Olwell, David H.},
  year = {1998},
  edition = {1998th edition},
  publisher = {{Springer}},
  address = {{New York}},
  isbn = {978-0-387-98365-3},
  langid = {english}
}

@article{hawkins2003,
  title = {The {{Changepoint Model}} for {{Statistical Process Control}}},
  author = {Hawkins, Douglas M. and Qiu, Peihua and Kang, Chang Wook},
  year = {2003},
  month = oct,
  journal = {Journal of Quality Technology},
  volume = {35},
  number = {4},
  pages = {355--366},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2003.11980233},
  abstract = {Statistical process control (SPC) requires statistical methodologies that detect changes in the pattern of data over time. The common methodologies, such as Shewhart, cumulative sum (cusum), and exponentially weighted moving average (EWMA) charting, require the in-control values of the process parameters, but these are rarely known accurately. Using estimated parameters, the run length behavior changes randomly from one realization to another, making it impossible to control the run length behavior of any particular chart. A suitable methodology for detecting and diagnosing step changes based on imperfect process knowledge is the unknown-parameter changepoint formulation. Long recognized as a Phase I analysis tool, we argue that it is also highly effective in allowing the user to progress seamlessly from the start of Phase I data gathering through Phase II SPC monitoring. Despite not requiring specification of the post-change process parameter values, its performance is never far short of that of the optimal cusum chart which requires this knowledge, and it is far superior for shifts away from the cusum shift for which the cusum chart is optimal. As another benefit, while changepoint methods are designed for step changes that persist, they are also competitive with the Shewhart chart, the chart of choice for isolated non-sustained special causes.},
  keywords = {Cumulative Sum Control Charts,done,Exponentially Weighted Moving Average Control Charts,Shewhart Control Charts},
  annotation = {\_eprint: https://doi.org/10.1080/00224065.2003.11980233},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Hawkins_et_al_2003_The_Changepoint_Model_for_Statistical_Process_Control.pdf;/home/dede/Zotero/storage/XN95JT6L/00224065.2003.html}
}

@article{hawkins2005,
  title = {Statistical {{Process Control}} for {{Shifts}} in {{Mean}} or {{Variance Using}} a {{Changepoint Formulation}}},
  author = {Hawkins, Douglas M and Zamba, K. D},
  year = {2005},
  month = may,
  journal = {Technometrics},
  volume = {47},
  number = {2},
  pages = {164--173},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1198/004017004000000644},
  abstract = {Statistical process control (SPC) involves ongoing checks to ensure that neither the mean nor the variability of the process readings has changed. Conventionally, this is done by pairs of charts\textemdash Shewhart X and S (or R) charts, cumulative sum charts for mean and for variance, or exponentially weighted moving average charts for mean and variance. The traditional methods of calculating the statistical properties of control charts are based on the assumption that the in-control true mean and variance were known exactly, and use these assumed true values to set center lines, control limits, and decision intervals. The reality, however, is that true parameter values are seldom if ever known exactly; rather, they are commonly estimated from a phase I sample. The random errors in the estimates lead to uncertain run length distribution of the resulting charts. An attractive alternative to the traditional charting methods is a single chart using the unknown-parameter likelihood ratio test for a change in mean and/or variance in normally distributed data. This formulation gives a single diagnostic to detect a shift in mean, in variance, or in both, rather than two separate diagnostics. Using the unknown parameter formulation recognizes the reality that at best one has reasonable estimates of parameters and not their exact values. This description implies an immediate benefit of the formulation, that the run behavior is controlled despite the lack of a large phase I sample. We demonstrate another benefit, that the changepoint formulation is competitive with the best of traditional formulations for detecting step changes in parameters.},
  keywords = {Control charts,doing,Generalized likelihood ratio,Phase i,Phase ii},
  annotation = {\_eprint: https://doi.org/10.1198/004017004000000644},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Hawkins_Zamba_2005_Statistical_Process_Control_for_Shifts_in_Mean_or_Variance_Using_a_Changepoint.pdf;/home/dede/Zotero/storage/3LAT73WR/004017004000000644.html}
}

@article{hawkins2005a,
  title = {A {{Change-Point Model}} for a {{Shift}} in {{Variance}}},
  author = {Hawkins, Douglas M. and Zamba, K. D.},
  year = {2005},
  month = jan,
  journal = {Journal of Quality Technology},
  volume = {37},
  number = {1},
  pages = {21--31},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2005.11980297},
  abstract = {A control chart for detecting shifts in the variance of a process is developed for the case where the nominal value of the variance is unknown. As our approach does not require that the in-control variance be known a priori, it avoids the need for a lengthy Phase I data-gathering step before charting can begin. The method is a variance-change-point model, based on the likelihood ratio test for a change in variance with the conventional Bartlett correction, adapted for repeated sequential use. The chart may be used alone in settings where one wishes to monitor one-degree-of-freedom chi-squared variates for departure from control; or it may be used together with a parallel change-point methodology for the mean to monitor process data for shifts in mean and/or variance. In both the solo use and as the scale portion of a combined scheme for monitoring changes in mean and/or variance, the approach has good performance across the range of possible shifts.},
  keywords = {Average Run Length,Change Point,Control Chart,CUSUM,done,EWMA,Likelihood Ratio,Phase I,Phase II,Shewhart},
  annotation = {\_eprint: https://doi.org/10.1080/00224065.2005.11980297},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Hawkins_Zamba_2005_A_Change-Point_Model_for_a_Shift_in_Variance.pdf;/home/dede/Zotero/storage/I4K8TRPI/00224065.2005.html}
}

@article{hawkins2007,
  title = {Self-{{Starting Multivariate Exponentially Weighted Moving Average Control Charting}}},
  author = {Hawkins, Douglas M and {Maboudou-Tchao}, Edgard M},
  year = {2007},
  journal = {Technometrics},
  volume = {49},
  number = {2},
  pages = {199--209},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1198/004017007000000083},
  abstract = {Multivariate control charts are valuable tools for industrial quality control. The conventional discussion of them rests on the presumption that the in-control process parameters are known a priori. The more common reality is that practitioners plug in parameter estimates gathered from a special phase I sample to establish parameter values for the charts. But no sample will establish the exact process parameters, and quite small random errors translate into serious distortions of the run behavior, particularly of sensitive charts, and can affect chart performance. So-called ``self-starting'' methods can begin the control of the process right after startup without the preliminary step of a large phase I sample. Univariate self-starting methods for converting the unknown-parameter stream of process readings into a known-parameter sequence have been available for some time now. This article develops a multivariate equivalent by providing a way to transform the process readings into a stream of vectors following an exact known-parameter distribution. Although our approach is far from being the first proposal for self-starting charting of multivariate data, we believe it is the first that does so by transforming the unknown-parameter process vectors into known-parameter vectors of the same dimensionality. This stream of vectors has many potential uses. In particular, it may be used to construct any multivariate control chart, such as Hotelling'sT2, or any of the multivariate cusum methods. We illustrate using the transformed stream to set up a multivariate exponentially weighted moving average chart. With the self-starting front end, this (or any other) chart will have the same in-control properties as if the true process mean and covariance matrix were known exactly, thereby allowing multivariate control charting to proceed without a large and costly phase I data-gathering exercise.},
  keywords = {Cholesky decomposition,done,Recursive residual,Regression adjustment},
  annotation = {\_eprint: https://doi.org/10.1198/004017007000000083},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Hawkins_Maboudou-Tchao_2007_Self-Starting_Multivariate_Exponentially_Weighted_Moving_Average_Control.pdf;/home/dede/Zotero/storage/WG9GJLTG/004017007000000083.html}
}

@article{hawkins2008,
  title = {Multivariate {{Exponentially Weighted Moving Covariance Matrix}}},
  author = {Hawkins, Douglas M. and {Maboudou-Tchao}, Edgard M.},
  year = {2008},
  journal = {Technometrics},
  volume = {50},
  number = {2},
  pages = {155--166},
  publisher = {{[Taylor \& Francis, Ltd., American Statistical Association, American Society for Quality]}},
  issn = {0040-1706},
  abstract = {Multivariate exponentially weighted moving average (MEWMA) charts are among the best control charts for detecting small changes in any direction. The well-known MEWMA is directed at changes in the mean vector. But changes can occur in either the location or the variability of the correlated multivariate quality characteristics, calling for parallel methodologies for detecting changes in the covariance matrix. This article discusses an exponentially weighted moving covariance matrix for monitoring the stability of the covariance matrix of a process. Used together with the location MEWMA, this chart provides a way to satisfy Shewhart's dictum that proper process control monitor both mean and variability. We show that the chart is competitive, generally outperforming current control charts for the covariance matrix.},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Hawkins_Maboudou-Tchao_2008_Multivariate_Exponentially_Weighted_Moving_Covariance_Matrix.pdf}
}

@book{hawkins2012,
  title = {Cumulative {{Sum Charts}} and {{Charting}} for {{Quality Improvement}}},
  author = {Hawkins, Douglas M. and Olwell, David H.},
  year = {2012},
  month = oct,
  edition = {Softcover reprint of the original 1st ed. 1998 edition},
  publisher = {{Springer}},
  address = {{New York Berlin Heidelberg Barcelona Budapest Hong Kong London Milan Paris Santa Clara Singapore Tokyo}},
  isbn = {978-1-4612-7245-8},
  langid = {english}
}

@article{haynes2017,
  title = {Computationally {{Efficient Changepoint Detection}} for a {{Range}} of {{Penalties}}},
  author = {Haynes, Kaylea and Eckley, Idris A. and Fearnhead, Paul},
  year = {2017},
  month = jan,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {26},
  number = {1},
  pages = {134--143},
  publisher = {{Taylor \& Francis}},
  issn = {1061-8600},
  doi = {10.1080/10618600.2015.1116445},
  abstract = {In the multiple changepoint setting, various search methods have been proposed, which involve optimizing either a constrained or penalized cost function over possible numbers and locations of changepoints using dynamic programming. Recent work in the penalized optimization setting has focused on developing an exact pruning-based approach that, under certain conditions, is linear in the number of data points. Such an approach naturally requires the specification of a penalty to avoid under/over-fitting. Work has been undertaken to identify the appropriate penalty choice for data-generating processes with known distributional form, but in many applications the model assumed for the data is not correct and these penalty choices are not always appropriate. To this end, we present a method that enables us to find the solution path for all choices of penalty values across a continuous range. This permits an evaluation of the various segmentations to identify a suitable penalty choice. The computational complexity of this approach can be linear in the number of data points and linear in the difference between the number of changepoints in the optimal segmentations for the smallest and largest penalty values. Supplementary materials for this article are available online.},
  keywords = {Dynamic programming,PELT,Penalized likelihood,Segmentation,Structural change,todo},
  annotation = {\_eprint: https://doi.org/10.1080/10618600.2015.1116445},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Haynes_et_al_2017_Computationally_Efficient_Changepoint_Detection_for_a_Range_of_Penalties.pdf;/home/dede/Zotero/storage/7W2IWEQ2/10618600.2015.html}
}

@article{haynes2017a,
  title = {A Computationally Efficient Nonparametric Approach for Changepoint Detection},
  author = {Haynes, Kaylea and Fearnhead, Paul and Eckley, Idris A.},
  year = {2017},
  month = sep,
  journal = {Statistics and Computing},
  volume = {27},
  number = {5},
  pages = {1293--1305},
  issn = {1573-1375},
  doi = {10.1007/s11222-016-9687-5},
  abstract = {In this paper we build on an approach proposed by Zou et al. (2014) for nonparametric changepoint detection. This approach defines the best segmentation for a data set as the one which minimises a penalised cost function, with the cost function defined in term of minus a non-parametric log-likelihood for data within each segment. Minimising this cost function is possible using dynamic programming, but their algorithm had a computational cost that is cubic in the length of the data set. To speed up computation, Zou et al. (2014) resorted to a screening procedure which means that the estimated segmentation is no longer guaranteed to be the global minimum of the cost function. We show that the screening procedure adversely affects the accuracy of the changepoint detection method, and show how a faster dynamic programming algorithm, pruned exact linear time (PELT) (Killick et al. 2012), can be used to find the optimal segmentation with a computational cost that can be close to linear in the amount of data. PELT requires a penalty to avoid under/over-fitting the model which can have a detrimental effect on the quality of the detected changepoints. To overcome this issue we use a relatively new method, changepoints over a range of penalties (Haynes et al. 2016), which finds all of the optimal segmentations for multiple penalty values over a continuous range. We apply our method to detect changes in heart-rate during physical activity.},
  langid = {english},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Haynes_et_al_2017_A_computationally_efficient_nonparametric_approach_for_changepoint_detection.pdf}
}

@article{hazan2019,
  title = {Introduction to {{Online Convex Optimization}}},
  author = {Hazan, Elad},
  year = {2019},
  month = sep,
  journal = {arXiv:1909.05207 [cs, math, stat]},
  eprint = {1909.05207},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {This manuscript portrays optimization as a process. In many practical applications the environment is so complex that it is infeasible to lay out a comprehensive theoretical model and use classical algorithmic theory and mathematical optimization. It is necessary as well as beneficial to take a robust approach, by applying an optimization method that learns as one goes along, learning from experience as more aspects of the problem are observed. This view of optimization as a process has become prominent in varied fields and has led to some spectacular success in modeling and systems that are now part of our daily lives.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Hazan_2019_Introduction_to_Online_Convex_Optimization.pdf;/home/dede/Zotero/storage/W5QAKJGW/1909.html}
}

@article{he2008,
  title = {Improved {{Self-Starting Control Charts}} for {{Short Runs}}},
  author = {He, Fangyi and Jiang, Wei and Shu, Lianjie},
  year = {2008},
  month = jan,
  journal = {Quality Technology \& Quantitative Management},
  volume = {5},
  number = {3},
  pages = {289--308},
  publisher = {{Taylor \& Francis}},
  issn = {null},
  doi = {10.1080/16843703.2008.11673402},
  abstract = {The traditional control charts are designed to minimize the out-of-control average run length (ARL), given a desired in-control ARL. When the out-of-control ARL is larger than the desired in-control ARL, the control chart is called biased. This paper investigates the bias of Shewhart Q charts proposed by Quesenberry [9] for detecting process mean shifts in start-up processes and short runs. Two improved schemes are proposed to alleviate the bias problem of the Shewhart Q charts.},
  keywords = {Average run length,bias,variance estimation},
  annotation = {\_eprint: https://doi.org/10.1080/16843703.2008.11673402},
  file = {/home/dede/Zotero/storage/4YBR6FNS/16843703.2008.html}
}

@article{he2015,
  title = {Delving {{Deep}} into {{Rectifiers}}: {{Surpassing Human-Level Performance}} on {{ImageNet Classification}}},
  shorttitle = {Delving {{Deep}} into {{Rectifiers}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2015},
  month = feb,
  journal = {arXiv:1502.01852 [cs]},
  eprint = {1502.01852},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94\% top-5 test error on the ImageNet 2012 classification dataset. This is a 26\% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66\%). To our knowledge, our result is the first to surpass human-level performance (5.1\%, Russakovsky et al.) on this visual recognition challenge.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/He_et_al_2015_Delving_Deep_into_Rectifiers.pdf;/home/dede/Zotero/storage/9SUECQ8Z/1502.html}
}

@article{he2016,
  title = {Extensible Grids: Uniform Sampling on a Space Filling Curve},
  shorttitle = {Extensible Grids},
  author = {He, Zhijian and Owen, Art B.},
  year = {2016},
  month = sep,
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {78},
  number = {4},
  pages = {917--931},
  issn = {13697412},
  doi = {10.1111/rssb.12132},
  langid = {english},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/He_Owen_2016_Extensible_grids.pdf}
}

@article{he2017,
  title = {Bayesian Multiple Change-Point Estimation for Exponential Distribution with Truncated and Censored Data},
  author = {He, Chaobing},
  year = {2017},
  month = jun,
  journal = {Communications in Statistics - Theory and Methods},
  volume = {46},
  number = {12},
  pages = {5827--5839},
  publisher = {{Taylor \& Francis}},
  issn = {0361-0926},
  doi = {10.1080/03610926.2016.1161797},
  abstract = {This paper considers the multiple change-point estimation for exponential distribution with truncated and censored data by Gibbs sampling. After all the missing data of interest is filled in by some sampling methods such as rejection sampling method, the complete-data likelihood function is obtained. The full conditional distributions of all parameters are discussed. The means of Gibbs samples are taken as Bayesian estimations of the parameters. The implementation steps of Gibbs sampling are introduced in detail. Finally random simulation test is developed, and the results show that Bayesian estimations are fairly accurate.},
  keywords = {62F15,Complete-data likelihood function,full conditional distribution,Gibbs sampling,MCMC methods,rejection sampling method,todo,truncated and censored data.},
  annotation = {\_eprint: https://doi.org/10.1080/03610926.2016.1161797},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/He_2017_Bayesian_multiple_change-point_estimation_for_exponential_distribution_with.pdf;/home/dede/Zotero/storage/94P4DU7M/03610926.2016.html}
}

@article{he2020,
  title = {The Institutionalization of the {{Indo-Pacific}}: Problems and Prospects},
  shorttitle = {The Institutionalization of the {{Indo-Pacific}}},
  author = {He, Kai and Feng, Huiyun},
  year = {2020},
  month = jan,
  journal = {International Affairs},
  volume = {96},
  number = {1},
  pages = {149--168},
  issn = {0020-5850},
  doi = {10.1093/ia/iiz194},
  abstract = {Although the term `Indo-Pacific' has become popular in the foreign policy discourse of some countries, we have yet to see any significant institution-building in the Indo-Pacific region. Borrowing insights from functional institutionalism and political leadership studies of international regimes, we introduce a `leadership\textendash institution' model to explore the problems and prospects of institutionalizing the Indo-Pacific. Through a comparative case study of the institutionalization of the Asia\textendash Pacific vs the Indo-Pacific, we argue that two crucial factors contributed to the slow institutionalization of the Indo-Pacific as a regional system in world politics: the lack of ideational leadership from an epistemic community and the weak executive leadership from a powerful state. While ideational leaders can help states identify and expand common interests in cooperation, executive leadership will facilitate states to overcome operational obstacles in cooperation, such as the `collective action' problem and the `relative gains' concern. The future of institution-building in the Indo-Pacific will depend on whether and how these two leadership roles are played by scholars and states in the region. In the conclusion, we discuss the challenges of institutionalizing the Indo-Pacific and highlight China as a wild card in the future of Indo-Pacific regionalism.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/He_Feng_2020_The_institutionalization_of_the_Indo-Pacific.pdf;/home/dede/Zotero/storage/U22PUC4Z/5697495.html}
}

@article{healy1987,
  title = {A {{Note}} on {{Multivariate CUSUM Procedures}}},
  author = {Healy, John D.},
  year = {1987},
  journal = {Technometrics},
  volume = {29},
  number = {4},
  pages = {409--412},
  publisher = {{[Taylor \& Francis, Ltd., American Statistical Association, American Society for Quality]}},
  issn = {0040-1706},
  doi = {10.2307/1269451},
  abstract = {Cumulative sum (CUSUM) procedures are among the most powerful tools for detecting a shift from a good quality distribution to a bad quality distribution. This article discusses the natural application of CUSUM procedures to the multivariate normal distribution. It discusses two cases, detecting a shift in the mean vector and detecting a shift in the covariance matrix. As an example, the procedure is applied to measurements taken on optical fibers.},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Healy_1987_A_Note_on_Multivariate_CUSUM_Procedures.pdf}
}

@article{heard2010,
  title = {Bayesian Anomaly Detection Methods for Social Networks},
  author = {Heard, Nicholas and Weston, David and Platanioti, Kiriaki and Hand, David},
  year = {2010},
  month = nov,
  journal = {The Annals of Applied Statistics},
  volume = {4},
  doi = {10.1214/10-AOAS329},
  abstract = {Learning the network structure of a large graph is computationally demanding, and dynamically monitoring the network over time for any changes in structure threatens to be more challenging still. This paper presents a two-stage method for anomaly detection in dynamic graphs: the first stage uses simple, conjugate Bayesian models for discrete time counting processes to track the pairwise links of all nodes in the graph to assess normality of behavior; the second stage applies standard network inference tools on a greatly reduced subset of potentially anomalous nodes. The utility of the method is demonstrated on simulated and real data sets.},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Heard_et_al_2010_Bayesian_anomaly_detection_methods_for_social_networks.pdf}
}

@article{heard2017,
  title = {Adaptive {{Sequential Monte Carlo}} for {{Multiple Changepoint Analysis}}},
  author = {Heard, Nicholas A. and Turcotte, Melissa J. M.},
  year = {2017},
  month = apr,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {26},
  number = {2},
  pages = {414--423},
  publisher = {{Taylor \& Francis}},
  issn = {1061-8600},
  doi = {10.1080/10618600.2016.1190281},
  abstract = {Process monitoring and control requires the detection of structural changes in a data stream in real time. This article introduces an efficient sequential Monte Carlo algorithm designed for learning unknown changepoints in continuous time. The method is intuitively simple: new changepoints for the latest window of data are proposed by conditioning only on data observed since the most recent estimated changepoint, as these observations carry most of the information about the current state of the process. The proposed method shows improved performance over the current state of the art. Another advantage of the proposed algorithm is that it can be made adaptive, varying the number of particles according to the apparent local complexity of the target changepoint probability distribution. This saves valuable computing time when changes in the changepoint distribution are negligible, and enables rebalancing of the importance weights of existing particles when a significant change in the target distribution is encountered. The plain and adaptive versions of the method are illustrated using the canonical continuous time changepoint problem of inferring the intensity of an inhomogeneous Poisson process, although the method is generally applicable to any changepoint problem. Performance is demonstrated using both conjugate and nonconjugate Bayesian models for the intensity. Appendices to the article are available online, illustrating the method on other models and applications.},
  keywords = {Adaptive sample size,Markov chain Monte Carlo methods,Online inference,Particle filters,todo},
  annotation = {\_eprint: https://doi.org/10.1080/10618600.2016.1190281},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Heard_Turcotte_2017_Adaptive_Sequential_Monte_Carlo_for_Multiple_Changepoint_Analysis.pdf;/home/dede/Zotero/storage/RDAYTWE2/Heard and Turcotte - 2017 - Adaptive Sequential Monte Carlo for Multiple Chang.pdf;/home/dede/Zotero/storage/H7YXQI3D/10618600.2016.html}
}

@book{hedayat1991,
  title = {{Design and Inference in Finite Population Sampling}},
  author = {Hedayat, A. S. and Sinha, Bikas K.},
  year = {1991},
  publisher = {{Wiley-Interscience}},
  isbn = {978-0-471-88073-8},
  langid = {Inglese}
}

@article{heil2019,
  title = {Advantages of Fuzzy K-Means over k-Means Clustering in the Classification of Diffuse Reflectance Soil Spectra: {{A}} Case Study with {{West African}} Soils},
  shorttitle = {Advantages of Fuzzy K-Means over k-Means Clustering in the Classification of Diffuse Reflectance Soil Spectra},
  author = {Heil, Jannis and H{\"a}ring, Volker and Marschner, Bernd and Stumpe, Britta},
  year = {2019},
  month = mar,
  journal = {Geoderma},
  volume = {337},
  pages = {11--21},
  issn = {00167061},
  doi = {10.1016/j.geoderma.2018.09.004},
  langid = {english},
  keywords = {done}
}

@article{heinze2018,
  title = {Variable Selection \textendash{} {{A}} Review and Recommendations for the Practicing Statistician},
  author = {Heinze, Georg and Wallisch, Christine and Dunkler, Daniela},
  year = {2018},
  journal = {Biometrical Journal},
  volume = {60},
  number = {3},
  pages = {431--449},
  issn = {1521-4036},
  doi = {10.1002/bimj.201700067},
  abstract = {Statistical models support medical research by facilitating individualized outcome prognostication conditional on independent variables or by estimating effects of risk factors adjusted for covariates. Theory of statistical models is well-established if the set of independent variables to consider is fixed and small. Hence, we can assume that effect estimates are unbiased and the usual methods for confidence interval estimation are valid. In routine work, however, it is not known a priori which covariates should be included in a model, and often we are confronted with the number of candidate variables in the range 10\textendash 30. This number is often too large to be considered in a statistical model. We provide an overview of various available variable selection methods that are based on significance or information criteria, penalized likelihood, the change-in-estimate criterion, background knowledge, or combinations thereof. These methods were usually developed in the context of a linear regression model and then transferred to more generalized linear models or models for censored survival data. Variable selection, in particular if used in explanatory modeling where effect estimates are of central interest, can compromise stability of a final model, unbiasedness of regression coefficients, and validity of p-values or confidence intervals. Therefore, we give pragmatic recommendations for the practicing statistician on application of variable selection methods in general (low-dimensional) modeling problems and on performing stability investigations and inference. We also propose some quantities based on resampling the entire variable selection process to be routinely reported by software packages offering automated variable selection algorithms.},
  copyright = {\textcopyright{} 2017 The Authors. Biometrical Journal Published by WILEY-VCH Verlag GmbH \& Co. KGaA, Weinheim},
  langid = {english},
  keywords = {change-in-estimate criterion,done,penalized likelihood,resampling,statistical model,stepwise selection},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/bimj.201700067},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Heinze_et_al_2018_Variable_selection_â€“_A_review_and_recommendations_for_the_practicing.pdf;/home/dede/Zotero/storage/LGFQR9JP/bimj.html}
}

@inproceedings{heller2008,
  title = {Statistical Models for Partial Membership},
  booktitle = {Proceedings of the 25th International Conference on {{Machine}} Learning},
  author = {Heller, Katherine A. and Williamson, Sinead and Ghahramani, Zoubin},
  year = {2008},
  month = jul,
  series = {{{ICML}} '08},
  pages = {392--399},
  publisher = {{Association for Computing Machinery}},
  address = {{Helsinki, Finland}},
  doi = {10.1145/1390156.1390206},
  abstract = {We present a principled Bayesian framework for modeling partial memberships of data points to clusters. Unlike a standard mixture model which assumes that each data point belongs to one and only one mixture component, or cluster, a partial membership model allows data points to have fractional membership in multiple clusters. Algorithms which assign data points partial memberships to clusters can be useful for tasks such as clustering genes based on microarray data (Gasch \& Eisen, 2002). Our Bayesian Partial Membership Model (BPM) uses exponential family distributions to model each cluster, and a product of these distibtutions, with weighted parameters, to model each datapoint. Here the weights correspond to the degree to which the datapoint belongs to each cluster. All parameters in the BPM are continuous, so we can use Hybrid Monte Carlo to perform inference and learning. We discuss relationships between the BPM and Latent Dirichlet Allocation, Mixed Membership models, Exponential Family PCA, and fuzzy clustering. Lastly, we show some experimental results and discuss nonparametric extensions to our model.},
  isbn = {978-1-60558-205-4},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Heller_et_al_2008_Statistical_models_for_partial_membership.pdf}
}

@article{hewitt1955,
  title = {Symmetric Measures on {{Cartesian}} Products},
  author = {Hewitt, Edwin and Savage, Leonard J.},
  year = {1955},
  journal = {Transactions of the American Mathematical Society},
  volume = {80},
  number = {2},
  pages = {470--501},
  issn = {0002-9947, 1088-6850},
  doi = {10.1090/S0002-9947-1955-0076206-8},
  abstract = {Advancing research. Creating connections.},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Hewitt_Savage_1955_Symmetric_measures_on_Cartesian_products.pdf;/home/dede/Zotero/storage/LI8W3F3Z/S0002-9947-1955-0076206-8.html}
}

@article{hilbert1891,
  title = {{\"Uber die stetige Abbildung einer Line auf ein Fl\"achenst\"uck}},
  author = {Hilbert, David},
  year = {1891},
  month = sep,
  journal = {Mathematische Annalen},
  volume = {38},
  number = {3},
  pages = {459--460},
  issn = {1432-1807},
  doi = {10.1007/BF01199431},
  langid = {german},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Hilbert_1891_Ãœber_die_stetige_Abbildung_einer_Line_auf_ein_FlÃ¤chenstÃ¼ck.pdf}
}

@article{hinton2012,
  title = {Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors},
  author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
  year = {2012},
  month = jul,
  journal = {arXiv:1207.0580 [cs]},
  eprint = {1207.0580},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This "overfitting" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random "dropout" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Hinton_et_al_2012_Improving_neural_networks_by_preventing_co-adaptation_of_feature_detectors.pdf;/home/dede/Zotero/storage/M8EVBRKR/1207.html}
}

@book{hjort2010,
  title = {Bayesian {{Nonparametrics}}},
  editor = {Hjort, Nils Lid and Holmes, Chris and M{\"u}ller, Peter and Walker, Stephen G.},
  year = {2010},
  month = apr,
  publisher = {{Cambridge University Press}},
  abstract = {Bayesian nonparametrics works - theoretically, computationally. The theory provides highly flexible models whose complexity grows appropriately with the amount of data. Computational issues, though challenging, are no longer intractable. All that is needed is an entry point: this intelligent book is the perfect guide to what can seem a forbidding landscape. Tutorial chapters by Ghosal, Lijoi and Pr\"unster, Teh and Jordan, and Dunson advance from theory, to basic models and hierarchical modeling, to applications and implementation, particularly in computer science and biostatistics. These are complemented by companion chapters by the editors and Griffin and Quintana, providing additional models, examining computational issues, identifying future growth areas, and giving links to related topics. This coherent text gives ready access both to underlying principles and to state-of-the-art practice. Specific examples are drawn from information retrieval, NLP, machine vision, computational biology, biostatistics, and bioinformatics.},
  isbn = {978-0-521-51346-3},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Hjort_et_al_2010_Bayesian_Nonparametrics.pdf}
}

@article{hlavka2016,
  title = {Bootstrap {{Procedures}} for {{Online Monitoring}} of {{Changes}} in {{Autoregressive Models}}},
  author = {Hl{\'a}vka, Z. and Hu{\v s}kov{\'a}, M. and Kirch, C. and Meintanis, S. G.},
  year = {2016},
  month = aug,
  journal = {Communications in Statistics - Simulation and Computation},
  volume = {45},
  number = {7},
  pages = {2471--2490},
  publisher = {{Taylor \& Francis}},
  issn = {0361-0918},
  doi = {10.1080/03610918.2014.904346},
  abstract = {We compare the behavior of several bootstrap procedures for monitoring changes in the error distribution of autoregressive time series. The proposed procedures are designed to control the overall significance level and include classical tests based on the empirical distribution function as well as Fourier-type methods that utilize the empirical characteristic function, both functions being computed on the basis of properly estimated residuals. The Monte Carlo study incorporates different estimators and a variety of sampling situations with and without outliers.},
  keywords = {Bootstrap,Change point analysis,Empirical distribution function,Primary 62L10,Robustness,Secondary 62M10; 62G09,Time series,todo},
  annotation = {\_eprint: https://doi.org/10.1080/03610918.2014.904346},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/HlÃ¡vka_et_al_2016_Bootstrap_Procedures_for_Online_Monitoring_of_Changes_in_Autoregressive_Models.pdf}
}

@book{hoff2009,
  title = {A First Course in Bayesian Statistical Methods},
  author = {Hoff, Peter D.},
  year = {2009},
  series = {Springer {{Texts}} in {{Statistics}}},
  publisher = {{Springer-Verlag}},
  address = {{New York}},
  abstract = {This book provides a compact self-contained introduction to the theory and application of Bayesian statistical methods. The book is accessible to readers having a basic familiarity with probability, yet allows more advanced readers to quickly grasp the principles underlying Bayesian theory and methods. The examples and computer code allow the reader to understand and implement basic Bayesian data analyses using standard statistical models and to extend the standard models to specialized data analysis situations. The book begins with fundamental notions such as probability, exchangeability and Bayes' rule, and ends with modern topics such as variable selection in regression, generalized linear mixed effects models, and semiparametric copula estimation. Numerous examples from the social, biological and physical sciences show how to implement these methodologies in practice. Monte Carlo summaries of posterior distributions play an important role in Bayesian data analysis. The open-source R statistical computing environment provides sufficient functionality to make Monte Carlo estimation very easy for a large number of statistical models and example R-code is provided throughout the text. Much of the example code can be run ``as is'' in R, and essentially all of it can be run after downloading the relevant datasets from the companion website for this book. Peter Hoff is an Associate Professor of Statistics and Biostatistics at the University of Washington. He has developed a variety of Bayesian methods for multivariate data, including covariance and copula estimation, cluster analysis, mixture modeling and social network analysis. He is on the editorial board of the Annals of Applied Statistics.},
  isbn = {978-0-387-92299-7},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Hoff_2009_A_first_course_in_bayesian_statistical_methods.pdf;/home/dede/Zotero/storage/HF3AV4V2/9780387922997.html}
}

@article{hoff2015,
  title = {Dyadic Data Analysis with Amen},
  author = {Hoff, Peter D.},
  year = {2015},
  month = jun,
  journal = {arXiv:1506.08237 [stat]},
  eprint = {1506.08237},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Dyadic data on pairs of objects, such as relational or social network data, often exhibit strong statistical dependencies. Certain types of second-order dependencies, such as degree heterogeneity and reciprocity, can be well-represented with additive random effects models. Higher-order dependencies, such as transitivity and stochastic equivalence, can often be represented with multiplicative effects. The "amen" package for the R statistical computing environment provides estimation and inference for a class of additive and multiplicative random effects models for ordinal, continuous, binary and other types of dyadic data. The package also provides methods for missing, censored and fixed-rank nomination data, as well as longitudinal dyadic data. This tutorial illustrates the "amen" package via example statistical analyses of several of these different data types.},
  archiveprefix = {arXiv},
  keywords = {62-07; 62F15,heterogeneity,network model,social,Statistics - Computation,Statistics - Methodology,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Hoff_2015_Dyadic_data_analysis_with_amen.pdf;/home/dede/Zotero/storage/M7K4QSRE/1506.html}
}

@article{hoff2018,
  title = {Additive and Multiplicative Effects Network Models},
  author = {Hoff, Peter D.},
  year = {2018},
  month = jul,
  journal = {arXiv:1807.08038 [stat]},
  eprint = {1807.08038},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Network datasets typically exhibit certain types of statistical dependencies, such as within-dyad correlation, row and column heterogeneity, and third-order dependence patterns such as transitivity and clustering. The first two of these can be well-represented statistically with a social relations model, a type of additive random effects model originally developed for continuous dyadic data. Third-order patterns can be represented with multiplicative random effects models, which are related to matrix decompositions commonly used for matrix-variate data analysis. Additionally, these multiplicative random effects models generalize other popular latent variable network models, such as the stochastic blockmodel and the latent space model. In this article we review a general regression framework for the analysis of network data that combines these two types of random effects and accommodates a variety of network data types, including continuous, binary and ordinal network relations.},
  archiveprefix = {arXiv},
  keywords = {62H25; 62F15,bayes,network models,Statistics - Methodology,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Hoff_2018_Additive_and_multiplicative_effects_network_models.pdf;/home/dede/Zotero/storage/VRK9GG8L/1807.html}
}

@article{hoffman2014,
  title = {The {{No-U-Turn Sampler}}: {{Adaptively Setting Path Lengths}} in {{Hamiltonian Monte Carlo}}},
  shorttitle = {The {{No-U-Turn Sampler}}},
  author = {Hoffman, Matthew D. and Gelman, Andrew},
  year = {2014},
  journal = {Journal of Machine Learning Research},
  volume = {15},
  number = {47},
  pages = {1593--1623},
  issn = {1533-7928},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Hoffman_Gelman_2014_The_No-U-Turn_Sampler.pdf;/home/dede/Zotero/storage/SHJTJQZT/hoffman14a.html}
}

@article{hofmann2020,
  title = {{{lmSubsets}}: {{Exact Variable-Subset Selection}} in {{Linear Regression}} for {{R}}},
  shorttitle = {{{lmSubsets}}},
  author = {Hofmann, Marc and Gatu, Cristian and Kontoghiorghes, Erricos J. and Colubi, Ana and Zeileis, Achim},
  year = {2020},
  month = apr,
  journal = {Journal of Statistical Software},
  volume = {93},
  number = {1},
  pages = {1--21},
  issn = {1548-7660},
  doi = {10.18637/jss.v093.i03},
  copyright = {Copyright (c) 2020 Marc Hofmann, Cristian Gatu, Erricos J. Kontoghiorghes, Ana Colubi, Achim Zeileis},
  langid = {english},
  keywords = {best-subset regression,linear regression,model selection,R,todo,variable selection},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Hofmann_et_al_2020_lmSubsets.pdf;/home/dede/Zotero/storage/LUNIQA8T/v093i03.html}
}

@article{hogg1956,
  title = {On the {{Distribution}} of the {{Likelihood Ratio}}},
  author = {Hogg, Robert V.},
  year = {1956},
  journal = {The Annals of Mathematical Statistics},
  volume = {27},
  number = {2},
  pages = {529--532},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851},
  abstract = {In an investigation of the distribution of the likelihood ratio \$\textbackslash lambda\$, Wilks [3] proved, under certain regularity conditions, that \$-2 \textbackslash ln \textbackslash lambda\$ is, except for terms of order \$1/\textbackslash sqrt n\$, distributed like \$\textbackslash chi\^2\$ with \$k - m\$ degrees of freedom, where \$k\$ is the dimension of the parameter space \$\textbackslash Omega\$ of admissible hypotheses and \$m\$ is the dimension of the parameter space \$\textbackslash omega\$ of null hypotheses. In this paper, we consider the nonregular densities investigated by R. C. Davis [1] and show that for certain hypotheses \$-2 \textbackslash ln \textbackslash lambda\$ has an exact \$\textbackslash chi\^2\$-distribution with \$2(k - m)\$ degrees of freedom.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Hogg_1956_On_the_Distribution_of_the_Likelihood_Ratio.pdf}
}

@article{holland1973,
  title = {Covariance {{Stabilizing Transformations}}},
  author = {Holland, Paul W.},
  year = {1973},
  journal = {The Annals of Statistics},
  volume = {1},
  number = {1},
  pages = {84--92},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364},
  abstract = {After reviewing the asymptotic variance stabilizing transformations in one dimension, a generalization of these to multivariate cases is discussed. Results are given for the uniqueness of solutions when they exist, but unlike the one-dimensional case, covariance stabilizing transformations need not exist. In the two-dimensional case, a necessary and sufficient condition is given for the existence of solutions. It takes the form of a second-order partial differential equation that the elements of any square root of the inverse of the limiting covariance matrix must satisfy. This condition is applied to three examples with the conclusion that no covariance stabilizing transformation exists for the trinomial distribution. It is conjectured that this non-existence of solutions is true for the general multinomial.},
  keywords = {todo}
}

@article{horiguchi2020,
  title = {Assessing Variable Activity for {{Bayesian}} Regression Trees},
  author = {Horiguchi, Akira and Pratola, Matthew T. and Santner, Thomas J.},
  year = {2020},
  month = sep,
  journal = {arXiv:2005.13622 [stat]},
  eprint = {2005.13622},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Bayesian Additive Regression Trees (BART) are non-parametric models that can capture complex exogenous variable effects. In any regression problem, it is often of interest to learn which variables are most active. Variable activity in BART is usually measured by counting the number of times a tree splits for each variable. Such one-way counts have the advantage of fast computations. Despite their convenience, one-way counts have several issues. They are statistically unjustified, cannot distinguish between main effects and interaction effects, and become inflated when measuring interaction effects. An alternative method well-established in the literature is Sobol' indices, a variance-based global sensitivity analysis technique. However, these indices often require Monte Carlo integration, which can be computationally expensive. This paper provides analytic expressions for Sobol' indices for BART posterior samples. These expressions are easy to interpret and are computationally feasible. Furthermore, we will show a fascinating connection between first-order (main-effects) Sobol' indices and one-way counts. We also introduce a novel ranking method, and use this to demonstrate that the proposed indices preserve the Sobol'-based rank order of variable importance. Finally, we compare these methods using analytic test functions and the En-ROADS climate impacts simulator.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Horiguchi_et_al_2020_Assessing_variable_activity_for_Bayesian_regression_trees.pdf;/home/dede/Zotero/storage/W8YZHAHA/2005.html}
}

@article{horrace2005,
  title = {Some Results on the Multivariate Truncated Normal Distribution},
  author = {Horrace, William C.},
  year = {2005},
  month = may,
  journal = {Journal of Multivariate Analysis},
  volume = {94},
  number = {1},
  pages = {209--221},
  issn = {0047-259X},
  doi = {10.1016/j.jmva.2004.10.007},
  abstract = {This note formalizes some analytical results on the n-dimensional multivariate truncated normal distribution where truncation is one-sided and at an arbitrary point. Results on linear transformations, marginal and conditional distributions, and independence are provided. Also, results on log-concavity, A-unimodality and the MTP2 property are derived.},
  langid = {english},
  keywords = {A-unimodality,Characteristic function,done,Log-concavity},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Horrace_2005_Some_results_on_the_multivariate_truncated_normal_distribution.pdf;/home/dede/Zotero/storage/R2M5X8KQ/S0047259X04002167.html}
}

@article{horvitz1952,
  title = {A {{Generalization}} of {{Sampling Without Replacement From}} a {{Finite Universe}}},
  author = {Horvitz, D. G. and Thompson, D. J.},
  year = {1952},
  journal = {Journal of the American Statistical Association},
  volume = {47},
  number = {260},
  pages = {663--685},
  publisher = {{[American Statistical Association, Taylor \& Francis, Ltd.]}},
  issn = {0162-1459},
  doi = {10.2307/2280784},
  abstract = {This paper presents a general technique for the treatment of samples drawn without replacement from finite universes when unequal selection probabilities are used. Two sampling schemes are discussed in connection with the problem of determining optimum selection probabilities according to the information available in a supplementary variable. Admittedly, these two schemes have limited application. They should prove useful, however, for the first stage of sampling with multi-stage designs, since both permit unbiased estimation of the sampling variance without resorting to additional assumptions.}
}

@article{hou2020,
  title = {A New {{Bayesian}} Scheme for Self-Starting Process Mean Monitoring},
  author = {Hou, Yuxing and He, Baosheng and Zhang, Xudong and Chen, Yong and Yang, Qingyu},
  year = {2020},
  month = nov,
  journal = {Quality Technology \& Quantitative Management},
  volume = {17},
  number = {6},
  pages = {661--684},
  publisher = {{Taylor \& Francis}},
  issn = {null},
  doi = {10.1080/16843703.2020.1726052},
  abstract = {A self-starting process mean monitoring scheme is needed in applications with short production runs or processes subject to degradation. The major challenge in implementing a self-starting monitoring scheme is that there exists little or no historical in-control data to accurately estimate in-control process parameters. In this paper, we propose a new Bayesian self-starting monitoring scheme to detect on-line whether a process mean has exceeded a pre-determined critical threshold. We assume the process is subject to various types of random drift and random jumps prior to exceeding a critical threshold. In comparison with existing self-starting Bayesian schemes in the literature, our model is more flexible in capturing various types of trends and requires less knowledge of process parameters. In addition, the proposed monitoring scheme is much more computationally efficient, rendering it much more applicable for numerous practical situations where model parameter information is limited and timely detection of a critical event is crucial. Numerical studies based on simulated signals and several real data sets are used to evaluate the performance of the proposed method and compare with existing methods in the literature. The proposed method is shown to be less sensitive to parameter misspecification, more flexible in capturing various trends in the data, and much more computationally efficient.},
  keywords = {Bayesian method,particle filter,piecewise linear model,Processing Monitoring,self-starting scheme},
  annotation = {\_eprint: https://doi.org/10.1080/16843703.2020.1726052},
  file = {/home/dede/Zotero/storage/9BPNU5ZB/16843703.2020.html}
}

@article{hou2021,
  title = {A Non-Parametric {{CUSUM}} Control Chart for Process Distribution Change Detection and Change Type Diagnosis},
  author = {Hou, Shiwang and Yu, Keming},
  year = {2021},
  month = feb,
  journal = {International Journal of Production Research},
  volume = {59},
  number = {4},
  pages = {1166--1186},
  publisher = {{Taylor \& Francis}},
  issn = {0020-7543},
  doi = {10.1080/00207543.2020.1721588},
  abstract = {Non-parametric control charts have been good alternatives to parametric control charts when little information is known about the type of process distribution or the value of parameters. Most approaches proposed by the current literature monitor either location or scale change in batch mode and their performance is discounted when monitoring distribution change in both location and scale simultaneously in a sequential pattern. This paper proposed a log-likelihood-ratio-based non-parametric cumulative sum (CUSUM) control chart to monitor arbitrary distribution change and diagnose the detailed change type simultaneously. By integrating the superiority of log-likelihood ratio test to detect any change of distribution and CUSUM chart to detect a small change, the proposed approach can detect small potential changes in location, scale and shape; and provide detailed information about change type when control chart gives a signal. Comparison results with many other non-parametric approaches were provided by numerical simulation and the results of an application case demonstrate the effectiveness of the proposed approach.},
  keywords = {change point detection,change type diagnosis,CUSUM,log-likelihood ratio},
  annotation = {\_eprint: https://doi.org/10.1080/00207543.2020.1721588},
  file = {/home/dede/Zotero/storage/IKG589WJ/00207543.2020.html}
}

@book{howard,
  title = {The {{First World War}}: {{A Very Short Introduction}}},
  shorttitle = {The {{First World War}}},
  author = {Howard, Michael},
  keywords = {done}
}

@misc{howard2020,
  title = {Time-Uniform {{Chernoff}} Bounds via Nonnegative Supermartingales},
  author = {Howard, Steven R. and Ramdas, Aaditya and McAuliffe, Jon and Sekhon, Jasjeet},
  year = {2020},
  month = may,
  number = {arXiv:1808.03204},
  eprint = {1808.03204},
  eprinttype = {arxiv},
  primaryclass = {math},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.1808.03204},
  abstract = {We develop a class of exponential bounds for the probability that a martingale sequence crosses a time-dependent linear threshold. Our key insight is that it is both natural and fruitful to formulate exponential concentration inequalities in this way. We illustrate this point by presenting a single assumption and theorem that together unify and strengthen many tail bounds for martingales, including classical inequalities (1960-80) by Bernstein, Bennett, Hoeffding, and Freedman; contemporary inequalities (1980-2000) by Shorack and Wellner, Pinelis, Blackwell, van de Geer, and de la Pe\textbackslash\textasciitilde na; and several modern inequalities (post-2000) by Khan, Tropp, Bercu and Touati, Delyon, and others. In each of these cases, we give the strongest and most general statements to date, quantifying the time-uniform concentration of scalar, matrix, and Banach-space-valued martingales, under a variety of nonparametric assumptions in discrete and continuous time. In doing so, we bridge the gap between existing line-crossing inequalities, the sequential probability ratio test, the Cram\textbackslash 'er-Chernoff method, self-normalized processes, and other parts of the literature.},
  archiveprefix = {arXiv},
  keywords = {60E15; 60G17 (Primary) 60F10; 60B20 (Secondary),Mathematics - Probability},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Howard_et_al_2020_Time-uniform_Chernoff_bounds_via_nonnegative_supermartingales.pdf;/home/dede/Zotero/storage/UNR3D77X/1808.html}
}

@article{howard2021,
  title = {Time-Uniform, Nonparametric, Nonasymptotic Confidence Sequences},
  author = {Howard, Steven R. and Ramdas, Aaditya and McAuliffe, Jon and Sekhon, Jasjeet},
  year = {2021},
  month = apr,
  journal = {The Annals of Statistics},
  volume = {49},
  number = {2},
  eprint = {1810.08240},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  issn = {0090-5364},
  doi = {10.1214/20-AOS1991},
  abstract = {A confidence sequence is a sequence of confidence intervals that is uniformly valid over an unbounded time horizon. Our work develops confidence sequences whose widths go to zero, with nonasymptotic coverage guarantees under nonparametric conditions. We draw connections between the Cram\textbackslash 'er-Chernoff method for exponential concentration, the law of the iterated logarithm (LIL), and the sequential probability ratio test -- our confidence sequences are time-uniform extensions of the first; provide tight, nonasymptotic characterizations of the second; and generalize the third to nonparametric settings, including sub-Gaussian and Bernstein conditions, self-normalized processes, and matrix martingales. We illustrate the generality of our proof techniques by deriving an empirical-Bernstein bound growing at a LIL rate, as well as a novel upper LIL for the maximum eigenvalue of a sum of random matrices. Finally, we apply our methods to covariance matrix estimation and to estimation of sample average treatment effect under the Neyman-Rubin potential outcomes model.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Probability,Mathematics - Statistics Theory,Statistics - Methodology},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Howard_et_al_2021_Time-uniform,_nonparametric,_nonasymptotic_confidence_sequences.pdf;/home/dede/Zotero/storage/VQYIFG9D/1810.html}
}

@article{howard2022,
  title = {Sequential Estimation of Quantiles with Applications to {{A}}/{{B}} Testing and Best-Arm Identification},
  author = {Howard, Steven R. and Ramdas, Aaditya},
  year = {2022},
  journal = {Bernoulli},
  volume = {28},
  number = {3},
  pages = {1704--1728},
  publisher = {{Bernoulli Society for Mathematical Statistics and Probability}},
  issn = {1350-7265},
  doi = {10.3150/21-BEJ1388},
  abstract = {We design confidence sequences\textemdash sequences of confidence intervals which are valid uniformly over time\textemdash for quantiles of any distribution over a complete, fully-ordered set, based on a stream of i.i.d. observations. We give methods both for tracking a fixed quantile and for tracking all quantiles simultaneously. Specifically, we provide explicit expressions with small constants for intervals whose widths shrink at the fastest possible t-1loglogt rate, along with a nonasymptotic concentration inequality for the empirical distribution function which holds uniformly over time with the same rate. The latter strengthens Smirnov's empirical process law of the iterated logarithm and extends the Dvoretzky-Kiefer-Wolfowitz inequality to hold uniformly over time. We give a new algorithm and sample complexity bound for selecting an arm with an approximately best quantile in a multi-armed bandit framework. In simulations, our method needs fewer samples than existing methods by a factor of five to fifty.},
  keywords = {best-arm identification,Confidence sequences,doing,Dvoretzky-Kiefer-Wolfowitz inequality,empirical process,Quantile estimation},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Howard_Ramdas_2022_Sequential_estimation_of_quantiles_with_applications_to_A-B_testing_and.pdf;/home/dede/Zotero/storage/9Q9ADHJH/21-BEJ1388.html}
}

@article{howell2020,
  title = {The Juche {{H-bomb}}? {{North Korea}}, Nuclear Weapons and Regime-State Survival},
  shorttitle = {The Juche {{H-bomb}}?},
  author = {Howell, Edward},
  year = {2020},
  month = jul,
  journal = {International Affairs},
  volume = {96},
  number = {4},
  pages = {1051--1068},
  issn = {0020-5850},
  doi = {10.1093/ia/iiz253},
  abstract = {Existing scholarship on North Korea's nuclear programme remains overwhelmingly centred around questions of containment or engagement with the North Korean regime-state, amid international calls for denuclearization. Yet, scholarship has rarely interrogated the precise value of nuclear weapons to the regime-state. This article develops a new theoretical framework of nuclear ideology to explore the puzzle of the survival of North Korea. This framework aims to show how the North Korean nuclear programme is deeply entrenched within the state ideology of juche, as one device for continued regime-state survival. Through interviews with elite North Korean defectors and textual analysis of North Korean and international sources, I show that North Korea's nuclear ideology has been constructed according to different frames of meaning, targeting referent actors of international `enemy' powers and domestic audiences. This article concludes that nuclear ideology functions primarily as a tool to arouse domestic legitimacy for the North Korean regime-state, by targeting elite actors within the highly stratified domestic population. From an international perspective, perception of North Korea's survival remains tied largely to the regime-state's physical possession of nuclear weapons. This article has extremely timely theoretical and policy implications given the current `dialogue' between US and North Korean leaders. First, it opens up fruitful avenues of inquiry surrounding questions of the legitimacy of rogue states within international relations. Secondly, this article calls for a more robust understanding of the domestic-level politics of North Korea, in order to understand the regime-state's foreign policy decisions vis-\`a-vis its nuclear programme.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Howell_2020_The_juche_H-bomb.pdf;/home/dede/Zotero/storage/H8N7K45P/5716258.html}
}

@inproceedings{howley2009,
  title = {Monitoring Clinical Indicators},
  booktitle = {3rd {{Annual ASEARC Research Conference}}},
  author = {Howley, Peter and Hancock, Stephen and Ford, Megan},
  year = {2009},
  publisher = {{Applied Statistics Education and Research Collaboration (ASEARC)}},
  isbn = {978-0-9806034-8-4},
  langid = {english},
  file = {/home/dede/Zotero/storage/HBV93SGP/uon8786.html}
}

@inproceedings{hu2011,
  title = {A Comparison of {{EWMA-type}} Charts under Linear Drifts in {{Poisson}} Means},
  booktitle = {2011 {{IEEE International Conference}} on {{Quality}} and {{Reliability}}},
  author = {Hu, Jinfei and Shu, Lianjie and Tse, Peter Wai and Tsui, Kwok-Leung},
  year = {2011},
  month = sep,
  pages = {297--301},
  doi = {10.1109/ICQR.2011.6031729},
  abstract = {Various control charts have been recommended to detect step shift in the process mean, however, relatively less attention has been paid to monitor the linear drifts in Poisson mean. In this paper, we evaluate the ability of four EWMA-type control charts in detecting increases in the Poisson rate by using the steady-state average run length, which is the most commonly used measure in statistical process control (SPC) or health surveillance. The simulation study shows that for small drift rates, both the one-sided upper EWMA chart (referred as EZ chart) and the adaptive EWMA (AEWMA) chart with large {$\gamma$} value from Huber's score function perform better than the other EWMA-type control charts; for large drift rates, the AEWMA chart with small {$\gamma$} value seems to behave the best amongst the four EWMA-type control charts for detecting Poisson mean drifts.},
  keywords = {Control charts,Process control,Public healthcare,Steady-state,Surveillance,Yttrium},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Hu_et_al_2011_A_comparison_of_EWMA-type_charts_under_linear_drifts_in_Poisson_means.pdf;/home/dede/Zotero/storage/WLR457IM/6031729.html}
}

@book{hubbard2015,
  title = {Data {{Structures}} and {{Algorithms}} with {{Python}}},
  author = {Hubbard, Kent D. Lee;Steve},
  year = {2015},
  publisher = {{Springer}},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Hubbard_2015_Data_Structures_and_Algorithms_with_Python.pdf}
}

@article{huberts2022,
  title = {Improved {{Control Chart Performance Using Cautious Parameter Learning}}},
  author = {Huberts, Leo C. E. and Goedhart, Rob and Does, Ronald J. M. M.},
  year = {2022},
  journal = {Computers \& Industrial Engineering},
  pages = {108185},
  issn = {0360-8352},
  doi = {10.1016/j.cie.2022.108185},
  abstract = {Parameter estimation is an important topic in Statistical Process Monitoring, as inaccurate estimates may lead to undesirable control chart performance. Updating the control chart limits during the monitoring period reduces estimation uncertainty. However, when out-of-control situations remain undetected, using the corresponding samples to update the parameter estimates can deteriorate the control chart performance in terms of in-control and out-of-control run lengths. For this reason, updating parameter estimates should only occur when there is sufficient evidence of an in-control process state. In this article, we study the performance of a cautious updating scheme for the Shewhart, Cumulative Sum, and Exponentially Weighted Moving Average control charts. We propose simple rules for updating parameters that improve the out-of-control performance of the control charts. We show the added value of using these updating rules in practice through a case study using data from a truck manufacturer.},
  langid = {english},
  keywords = {CUSUM,Estimation Effects,EWMA,reference only,Shewhart,Statistical Process Monitoring},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Huberts_et_al_2022_Improved_Control_Chart_Performance_Using_Cautious_Parameter_Learning.pdf}
}

@article{hullait2020,
  title = {Robust {{Function-on-Function Regression}}},
  author = {Hullait, Harjit and Leslie, David S. and Pavlidis, Nicos G. and King, Steve},
  year = {2020},
  month = jul,
  journal = {Technometrics},
  volume = {0},
  number = {0},
  pages = {1--14},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.2020.1802350},
  abstract = {Functional linear regression is a widely used approach to model functional responses with respect to functional inputs. However, classical functional linear regression models can be severely affected by outliers. We therefore introduce a Fisher-consistent robust functional linear regression model that is able to effectively fit data in the presence of outliers. The model is built using robust functional principal component and least squares regression estimators. The performance of the functional linear regression model depends on the number of principal components used. We therefore introduce a consistent robust model selection procedure to choose the number of principal components. Our robust functional linear regression model can be used alongside an outlier detection procedure to effectively identify abnormal functional responses. A simulation study shows our method is able to effectively capture the regression behavior in the presence of outliers, and is able to find the outliers with high accuracy. We demonstrate the usefulness of our method on jet engine sensor data. We identify outliers that would not be found if the functional responses were modeled independently of the functional input, or using nonrobust methods.},
  keywords = {done,Outlier detection,Robust functional data analysis,Robust model selection},
  annotation = {\_eprint: https://doi.org/10.1080/00401706.2020.1802350},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Hullait_et_al_2020_Robust_Function-on-Function_Regression.pdf;/home/dede/Zotero/storage/CQDJGLLX/00401706.2020.html}
}

@article{hyvarinen2000,
  title = {Independent Component Analysis: Algorithms and Applications},
  shorttitle = {Independent Component Analysis},
  author = {Hyv{\"a}rinen, A. and Oja, E.},
  year = {2000},
  month = jun,
  journal = {Neural Networks},
  volume = {13},
  number = {4},
  pages = {411--430},
  issn = {0893-6080},
  doi = {10.1016/S0893-6080(00)00026-5},
  abstract = {A fundamental problem in neural network research, as well as in many other disciplines, is finding a suitable representation of multivariate data, i.e. random vectors. For reasons of computational and conceptual simplicity, the representation is often sought as a linear transformation of the original data. In other words, each component of the representation is a linear combination of the original variables. Well-known linear transformation methods include principal component analysis, factor analysis, and projection pursuit. Independent component analysis (ICA) is a recently developed method in which the goal is to find a linear representation of non-Gaussian data so that the components are statistically independent, or as independent as possible. Such a representation seems to capture the essential structure of the data in many applications, including feature extraction and signal separation. In this paper, we present the basic theory and applications of ICA, and our recent work on the subject.},
  langid = {english},
  keywords = {Blind signal separation,done,Factor analysis,Independent component analysis,Projection pursuit,Representation,Source separation},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/HyvÃ¤rinen_Oja_2000_Independent_component_analysis.pdf}
}

@book{ian2018,
  title = {{Roller-Coaster: Europe, 1950-2017}},
  shorttitle = {{Roller-Coaster}},
  author = {Ian, Kershaw},
  year = {2018},
  edition = {1\textdegree{} edizione},
  publisher = {{Allen Lane}},
  address = {{London}},
  isbn = {978-0-241-36823-7},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Ian_2018_Roller-Coaster.epub}
}

@article{ide2021,
  title = {The Past and Future(s) of Environmental Peacebuilding},
  author = {Ide, Tobias and Bruch, Carl and Carius, Alexander and Conca, Ken and Dabelko, Geoffrey D and Matthew, Richard and Weinthal, Erika},
  year = {2021},
  month = jan,
  journal = {International Affairs},
  volume = {97},
  number = {1},
  pages = {1--16},
  issn = {0020-5850},
  doi = {10.1093/ia/iiaa177},
  abstract = {Environmental peacebuilding is a rapidly growing field of research and practice at the intersection of environment, conflict, peace and security. Focusing on these linkages is crucial in a time when the environment is a core issue of international politics and the number of armed conflicts remains high. This article introduces a special issue with a particular emphasis on environmental opportunities for building and sustaining peace. We first detail the definitions, theoretical assumptions and intellectual background of environmental peacebuilding. The article then provides context for the special issue by briefly reviewing core findings and debates of the first two generations of environmental peacebuilding research. Finally, we identify knowledge gaps that should be addressed in the next generation of research, and to which the articles in this special issue contribute: bottom-up approaches, gender, conflict-sensitive programming, use of big data and frontier technology, and monitoring and evaluation.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Ide_et_al_2021_The_past_and_future(s)_of_environmental_peacebuilding.pdf}
}

@book{imbens2015,
  title = {{Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction}},
  shorttitle = {{Causal Inference for Statistics, Social, and Biomedical Sciences}},
  author = {Imbens, Guido W. and Rubin, Donald B.},
  year = {2015},
  month = apr,
  publisher = {{Cambridge University Press}},
  address = {{New York}},
  abstract = {Most questions in social and biomedical sciences are causal in nature: what would happen to individuals, or to groups, if part of their environment were changed? In this groundbreaking text, two world-renowned experts present statistical methods for studying such questions. This book starts with the notion of potential outcomes, each corresponding to the outcome that would be realized if a subject were exposed to a particular treatment or regime. In this approach, causal effects are comparisons of such potential outcomes. The fundamental problem of causal inference is that we can only observe one of the potential outcomes for a particular subject. The authors discuss how randomized experiments allow us to assess causal effects and then turn to observational studies. They lay out the assumptions needed for causal inference and describe the leading analysis methods, including matching, propensity-score methods, and instrumental variables. Many detailed applications are included, with special focus on practical aspects for the empirical researcher.},
  isbn = {978-0-521-88588-1},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Imbens_Rubin_2015_Causal_Inference_for_Statistics,_Social,_and_Biomedical_Sciences.pdf}
}

@article{ing2017,
  title = {Multiple {{Testing}} in {{Regression Models With Applications}} to {{Fault Diagnosis}} in the {{Big Data Era}}},
  author = {Ing, C. and Lai, T. and Shen, M. and Tsang, K. and Yu, Shu-Hui},
  year = {2017},
  journal = {Technometrics},
  doi = {10.1080/00401706.2016.1236755},
  abstract = {A new approach to address the multiple testing problem and its advantages over existing methods is introduced and its performance in an application to semiconductor wafer fabrication is illustrated. ABSTRACT Motivated by applications to root-cause identification of faults in multistage manufacturing processes that involve a large number of tools or equipment at each stage, we consider multiple testing in regression models whose outputs represent the quality characteristics of a multistage manufacturing process. Because of the large number of input variables that correspond to the tools or equipments used, this falls in the framework of regression modeling in the modern era of big data. On the other hand, with quick fault detection and diagnosis followed by tool rectification, sparsity can be assumed in the regression model. We introduce a new approach to address the multiple testing problem and demonstrate its advantages over existing methods. We also illustrate its performance in an application to semiconductor wafer fabrication that motivated this development. Supplementary materials for this article are available online.},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Ing_et_al_2017_Multiple_Testing_in_Regression_Models_With_Applications_to_Fault_Diagnosis_in.pdf}
}

@book{irizarry,
  title = {Data {{Analysis}} for the {{Life Sciences}} with {{R}}},
  author = {Irizarry, Rafael A. and Love, Michael I.},
  abstract = {This book covers several of the statistical concepts and data analytic skills needed to succeed in data-driven life science research. The authors proceed from relatively basic concepts related to computed p-values to advanced topics related to analyzing highthroughput data. They include the R code that performs this analysis and connect the lines of code to the statistical and mathematical concepts explained.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Irizarry_Love_Data_Analysis_for_the_Life_Sciences_with_R.pdf}
}

@article{ishwaran2001,
  title = {Gibbs {{Sampling Methods}} for {{Stick-Breaking Priors}}},
  author = {Ishwaran, Hemant and James, Lancelot F.},
  year = {2001},
  month = mar,
  journal = {Journal of the American Statistical Association},
  volume = {96},
  number = {453},
  pages = {161--173},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1198/016214501750332758},
  abstract = {A rich and flexible class of random probability measures, which we call stick-breaking priors, can be constructed using a sequence of independent beta random variables. Examples of random measures that have this characterization include the Dirichlet process, its two-parameter extension, the two-parameter Poisson\textendash Dirichlet process, finite dimensional Dirichlet priors, and beta two-parameter processes. The rich nature of stick-breaking priors offers Bayesians a useful class of priors for nonparametric problems, while the similar construction used in each prior can be exploited to develop a general computational procedure for fitting them. In this article we present two general types of Gibbs samplers that can be used to fit posteriors of Bayesian hierarchical models based on stick-breaking priors. The first type of Gibbs sampler, referred to as a P\'olya urn Gibbs sampler, is a generalized version of a widely used Gibbs sampling method currently employed for Dirichlet process computing. This method applies to stick-breaking priors with a known P\'olya urn characterization, that is, priors with an explicit and simple prediction rule. Our second method, the blocked Gibbs sampler, is based on an entirely different approach that works by directly sampling values from the posterior of the random measure. The blocked Gibbs sampler can be viewed as a more general approach because it works without requiring an explicit prediction rule. We find that the blocked Gibbs avoids some of the limitations seen with the P\'olya urn approach and should be simpler for nonexperts to use.},
  keywords = {Blocked Gibbs sampler,Dirichlet process,Generalized Dirichlet distribution,Pitmanâ€“Yor process,PÃ³lya urn Gibbs sampler,Prediction rule,Random probability measure,Random weights,Stable law,todo},
  annotation = {\_eprint: https://doi.org/10.1198/016214501750332758},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Ishwaran_James_2001_Gibbs_Sampling_Methods_for_Stick-Breaking_Priors.pdf;/home/dede/Zotero/storage/JYEB9S4X/016214501750332758.html}
}

@article{ishwaran2002,
  title = {Exact and {{Approximate Sum Representations}} for the {{Dirichlet Process}}},
  author = {Ishwaran, Hemant and Zarepour, Mahmoud},
  year = {2002},
  journal = {The Canadian Journal of Statistics},
  volume = {30},
  number = {2},
  pages = {269--283},
  publisher = {{[Statistical Society of Canada, Wiley]}},
  issn = {0319-5724},
  doi = {10.2307/3315951},
  abstract = {The Dirichlet process can be regarded as a random probability measure for which the authors examine various sum representations. They consider in particular the gamma process construction of Ferguson (1973) and the "stick-breaking" construction of Sethuraman (1994). They propose a Dirichlet finite sum representation that strongly approximates the Dirichlet process. They assess the accuracy of this approximation and characterize the posterior that this new prior leads to in the context of Bayesian nonparametric hierarchical models. /// Le processus de Dirichlet constitue une mesure de probabilit\'e al\'eatoire dont les auteurs examinent diff\'erentes repr\'esentations \`a l'aide de sommes. Ils s'int\'eressent en particulier \`a la construction de Ferguson (1973) fond\'ee sur la loi gamma et \`a la construction dite \`a "b\^atons rompus" de Sethuraman (1994). Ils proposent une approximation forte du processus de Dirichlet par somme finie de type Dirichlet. Ils \'evaluent la qualit\'e de cette approximation qui conduit \`a une loi a priori dont ils caract\'erisent la loi a posteriori dans le cadre des mod\`eles bay\'esiens hi\'erarchiques non param\'etriques.},
  keywords = {todo}
}

@article{ishwaran2003,
  title = {Generalized {{Weighted Chinese Restaurant Processes}} for {{Species Sampling Mixture Models}}},
  author = {Ishwaran, Hemant and James, Lancelot F.},
  year = {2003},
  journal = {Statistica Sinica},
  volume = {13},
  number = {4},
  pages = {1211--1235},
  publisher = {{Institute of Statistical Science, Academia Sinica}},
  issn = {1017-0405},
  abstract = {The class of species sampling mixture models is introduced as an extension of semiparametric models based on the Dirichlet process to models based on the general class of species sampling priors, or equivalently the class of all exchangeable urn distributions. Using Fubini calculus in conjunction with Pitman (1995, 1996), we derive characterizations of the posterior distribution in terms of a posterior partition distribution that extend the results of Lo (1984) for the Dirichlet process. These results provide a better understanding of models and have both theoretical and practical applications. To facilitate the use of our models we generalize the work in Brunner, Chan, James and Lo (2001) by extending their weighted Chinese restaurant (WCR) Monte Carlo procedure, an i.i.d. sequential importance sampling (SIS) procedure for approximating posterior mean functionals based on the Dirichlet process, to the case of approximation of mean functionals and additionally their posterior laws in species sampling mixture models. We also discuss collapsed Gibbs sampling, P\'olya urn Gibbs sampling and a P\'olya urn SIS scheme. Our framework allows for numerous applications, including multiplicative counting process models subject to weighted gamma processes, as well as nonparametric and semiparametric hierarchical models based on the Dirichlet process, its two-parameter extension, the Pitman-Yor process and finite dimensional Dirichlet priors.},
  keywords = {todo}
}

@article{ishwaran2005,
  title = {Spike and Slab Variable Selection: {{Frequentist}} and {{Bayesian}} Strategies},
  shorttitle = {Spike and Slab Variable Selection},
  author = {Ishwaran, Hemant and Rao, J. Sunil},
  year = {2005},
  month = apr,
  journal = {The Annals of Statistics},
  volume = {33},
  number = {2},
  pages = {730--773},
  issn = {0090-5364},
  doi = {10.1214/009053604000001147},
  langid = {english},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Ishwaran_Rao_2005_Spike_and_slab_variable_selection.pdf}
}

@phdthesis{isom2009,
  title = {Exact Solution of {{Bayes}} and Minimax Change-Detection Problems},
  author = {Isom, Joshua David},
  year = {2009},
  abstract = {The challenge of detecting a change in the distribution of data is a sequential decision problem that is relevant to many engineering solutions, including quality control and machine and process monitoring. This dissertation develops techniques for exact solution of change-detection problems with discrete time and discrete observations. Change-detection problems are classified as Bayes or minimax based on the availability of information on the change-time distribution. A Bayes optimal solution uses prior information about the distribution of the change time to minimize the expected cost, whereas a minimax optimal solution minimizes the cost under the worst-case change-time distribution. Both types of problems are addressed. The most important result of the dissertation is the development of a polynomial-time algorithm for the solution of important classes of Markov Bayes change-detection problems. Existing techniques for epsilon-exact solution of partially observable Markov decision processes have complexity exponential in the number of observation symbols. A new algorithm, called constellation induction, exploits the concavity and Lipschitz continuity of the value function, and has complexity polynomial in the number of observation symbols. It is shown that change-detection problems with a geometric change-time distribution and identically- and independently-distributed observations before and after the change are solvable in polynomial time. Also, change-detection problems on hidden Markov models with a fixed number of recurrent states are solvable in polynomial time. A detailed implementation and analysis of the constellation-induction algorithm are provided. Exact solution methods are also established for several types of minimax change-detection problems. Finite-horizon problems with arbitrary observation distributions are modeled as extensive-form games and solved using linear programs. Infinite-horizon problems with linear penalty for detection delay and identically- and independently-distributed observations can be solved in polynomial time via epsilon-optimal parameterization of a cumulative-sum procedure. Finally, the properties of policies for change-detection problems are described and analyzed. Simple classes of formal languages are shown to be sufficient for epsilon-exact solution of change-detection problems, and methods for finding minimally sized policy representations are described.},
  isbn = {9781109220650},
  langid = {english},
  keywords = {todo},
  annotation = {OCLC: 551073609},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Isom_2009_Exact_solution_of_Bayes_and_minimax_change-detection_problems.pdf}
}

@book{jackman2009,
  title = {{Bayesian Analysis for the Social Sciences}},
  author = {Jackman, Simon},
  year = {2009},
  month = oct,
  publisher = {{John Wiley \& Sons Inc}},
  address = {{Chichester, U.K}},
  abstract = {Bayesian methods are increasingly being used in the social sciences, as the problems encountered lend themselves so naturally to the subjective qualities of Bayesian methodology. This book provides an accessible introduction to Bayesian methods, tailored specifically for social science students. It contains lots of real examples from political science, psychology, sociology, and economics, exercises in all chapters, and detailed descriptions of all the key concepts, without assuming any background in statistics beyond a first course. It features examples of how to implement the methods using WinBUGS \&; the most-widely used Bayesian analysis software in the world \&; and R \&; an open-source statistical software. The book is supported by a Website featuring WinBUGS and R code, and data sets.},
  isbn = {978-0-470-01154-6},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Jackman_2009_Bayesian_Analysis_for_the_Social_Sciences.pdf}
}

@inproceedings{jacob2009,
  title = {Group Lasso with Overlap and Graph Lasso},
  booktitle = {Proceedings of the 26th {{Annual International Conference}} on {{Machine Learning}}},
  author = {Jacob, Laurent and Obozinski, Guillaume and Vert, Jean-Philippe},
  year = {2009},
  month = jun,
  series = {{{ICML}} '09},
  pages = {433--440},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1553374.1553431},
  abstract = {We propose a new penalty function which, when used as regularization for empirical risk minimization procedures, leads to sparse estimators. The support of the sparse vector is typically a union of potentially overlapping groups of co-variates defined a priori, or a set of covariates which tend to be connected to each other when a graph of covariates is given. We study theoretical properties of the estimator, and illustrate its behavior on simulated and breast cancer gene expression data.},
  isbn = {978-1-60558-516-1}
}

@article{jacob2017,
  title = {Better Together? {{Statistical}} Learning in Models Made of Modules},
  shorttitle = {Better Together?},
  author = {Jacob, Pierre E. and Murray, Lawrence M. and Holmes, Chris C. and Robert, Christian P.},
  year = {2017},
  journal = {arXiv:1708.08719},
  eprint = {1708.08719},
  eprinttype = {arxiv},
  abstract = {In modern applications, statisticians are faced with integrating heterogeneous data modalities relevant for an inference, prediction, or decision problem. In such circumstances, it is convenient to use a graphical model to represent the statistical dependencies, via a set of connected "modules", each relating to a specific data modality, and drawing on specific domain expertise in their development. In principle, given data, the conventional statistical update then allows for coherent uncertainty quantification and information propagation through and across the modules. However, misspecification of any module can contaminate the estimate and update of others, often in unpredictable ways. In various settings, particularly when certain modules are trusted more than others, practitioners have preferred to avoid learning with the full model in favor of approaches that restrict the information propagation between modules, for example by restricting propagation to only particular directions along the edges of the graph. In this article, we investigate why these modular approaches might be preferable to the full model in misspecified settings. We propose principled criteria to choose between modular and full-model approaches. The question arises in many applied settings, including large stochastic dynamical systems, meta-analysis, epidemiological models, air pollution models, pharmacokinetics-pharmacodynamics, and causal inference with propensity scores.},
  archiveprefix = {arXiv},
  keywords = {done,modules statistical learning,Statistics - Methodology},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Jacob_et_al_2017_Better_together.pdf;/home/dede/Zotero/storage/J9BB8E4D/1708.html}
}

@book{james2021,
  title = {{An Introduction to Statistical Learning: With Applications in R}},
  shorttitle = {{An Introduction to Statistical Learning}},
  author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
  year = {2021},
  edition = {Second},
  publisher = {{Springer Verlag}},
  address = {{New York}},
  abstract = {An Introduction to Statistical Learning provides an accessible overview of the field of statistical learning, an essential toolset for making sense of the vast and complex data sets that have emerged in fields ranging from biology to finance to marketing to astrophysics in the past twenty years. This book presents some of the most important modeling and prediction techniques, along with relevant applications. Topics include linear regression, classification, resampling methods, shrinkage approaches, tree-based methods, support vector machines, clustering, and more. Color graphics and real-world examples are used to illustrate the methods presented. Since the goal of this textbook is to facilitate the use of these statistical learning techniques by practitioners in science, industry, and other fields, each chapter contains a tutorial on implementing the analyses and methods presented in R, an extremely popular open source statistical software platform.Two of the authors co-wrote The Elements of Statistical Learning (Hastie, Tibshirani and Friedman, 2nd edition 2009), a popular reference book for statistics and machine learning researchers. An Introduction to Statistical Learning covers many of the same topics, but at a level accessible to a much broader audience. This book is targeted at statisticians and non-statisticians alike who wish to use cutting-edge statistical learning techniques to analyze their data. The text assumes only a previous course in linear regression and no knowledge of matrix algebra.},
  isbn = {978-1-4614-7137-0},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/James_et_al_2021_An_Introduction_to_Statistical_Learning.pdf}
}

@article{jardim2019,
  title = {Chart with {{Estimated Parameters}}: {{The Conditional ARL Distribution}} and {{New Insights}}},
  shorttitle = {Chart with {{Estimated Parameters}}},
  author = {Jardim, Felipe S. and Chakraborti, Subhabrata and Epprecht, Eugenio K.},
  year = {2019},
  journal = {Production and Operations Management},
  volume = {28},
  number = {6},
  pages = {1545--1557},
  issn = {1937-5956},
  doi = {10.1111/poms.12985},
  abstract = {Performance measures of control charts with estimated parameters are random variables and vary significantly across reference samples. In this context, a recent idea has been to study the distribution of the realized (or conditional) in-control average run length (CARL0) [or, equivalently, the conditional false-alarm rate (CFAR)] for a set of estimates from a given reference sample and apply the exceedance probability criterion (EPC) to design control charts that ensure a desirable in-control performance. Under the EPC, the probability that the CARL0 (or the CFAR) is at least (or at most) equal to a specified value is guaranteed with a high probability, which helps prevent low in-control ARL's (or high false-alarm rates) from occurring. In order to apply the EPC, the c.d.f. of the CARL0 (or the CFAR) is necessary. For the two-sided Shewhart Xbar control chart, under normality, we derive the exact c.d.f. of the CARL0 and the CFAR, currently not available in the literature. Using these key results, we calculate the minimum number of Phase I samples required to guarantee a desired in-control performance in terms of the EPC. Since the required amount of data can be prohibitively large, we also provide exact formulas for adjustments to the control limits for a given amount of Phase I data; some tables are provided. Our adjustment formulas give more accurate results compared to some available methods. The impact of these adjustments on the out-of-control performance of the chart is examined in detail. A summary and some recommendations are provided.},
  langid = {english},
  keywords = {average run length,conditional run length distribution,exceedance probability criterion,false alarm rate,guaranteed in-control performance,out-of-control performance,Phase I and Phase II,todo},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/poms.12985},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Jardim_et_al_2019_Chart_with_Estimated_Parameters.pdf;/home/dede/Zotero/storage/X7ADGXGT/poms.html}
}

@article{jensen2006,
  title = {Effects of {{Parameter Estimation}} on {{Control Chart Properties}}: {{A Literature Review}}},
  shorttitle = {Effects of {{Parameter Estimation}} on {{Control Chart Properties}}},
  author = {Jensen, Willis A. and {Jones-Farmer}, L. Allison and Champ, Charles W. and Woodall, William H.},
  year = {2006},
  journal = {Journal of Quality Technology},
  volume = {38},
  number = {4},
  pages = {349--364},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2006.11918623},
  abstract = {Control charts are powerful tools used to monitor the quality of processes. In practice, control chart limits are often calculated using parameter estimates from an in-control Phase I reference sample. In Phase II of the monitoring scheme, statistics based on new samples are compared with the estimated control limits to monitor for departures from the in-control state. Many studies that evaluate control chart performance in Phase II rely on the assumption that the in-control parameters are known. Although the additional variability introduced into the monitoring scheme through parameter estimation is known to affect the chart performance, many studies do not consider the effect of estimation on the performance of the chart. This paper contains a review of the literature that explicitly considers the effect of parameter estimation on control chart properties. Some recommendations are made and future research ideas in this area are provided.},
  keywords = {ARL,Conditional Distribution,done,Marginal Distribution,Phase I,Phase II,Run-Length Performance,Sample Size,Shewhart Chart,Statistical Process Control},
  annotation = {\_eprint: https://doi.org/10.1080/00224065.2006.11918623},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Jensen_et_al_2006_Effects_of_Parameter_Estimation_on_Control_Chart_Properties.pdf;/home/dede/Zotero/storage/6KAC287L/00224065.2006.html}
}

@incollection{jensen2016,
  title = {Bayesian {{Graphical Models}}},
  booktitle = {Wiley {{StatsRef}}: {{Statistics Reference Online}}},
  author = {Jensen, Finn V. and Nielsen, Thomas D.},
  year = {2016},
  pages = {1--9},
  publisher = {{American Cancer Society}},
  doi = {10.1002/9781118445112.stat07360.pub2},
  abstract = {Mathematically, a Bayesian graphical model is a compact representation of the joint probability distribution for a set of variables. The most frequently used type of Bayesian graphical models are Bayesian networks. The structural part of a Bayesian graphical model is a graph consisting of nodes and edges. The nodes represent variables, which may be either discrete or continuous. An edge between two nodes A and B indicates a direct influence between the state of A and the state of B, which in some domains can also be interpreted as a causal relation. The widespread use of Bayesian networks is largely due to the availability of efficient inference algorithms for answering probabilistic queries about the states of the variables in the network. Furthermore, to support the construction of Bayesian network models, learning algorithms are also available. We give an overview of the Bayesian network formalism and some of the algorithmic developments in the area.},
  copyright = {Copyright \textcopyright{} 2016 John Wiley \& Sons, Ltd. All rights reserved.},
  isbn = {978-1-118-44511-2},
  langid = {english},
  keywords = {Bayesian networks,inference algorithms,learning algorithms,probabilistic graphical models,todo},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118445112.stat07360.pub2},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Jensen_Nielsen_2016_Bayesian_Graphical_Models.pdf;/home/dede/Zotero/storage/44YB4DRB/9781118445112.stat07360.html}
}

@misc{jia2019,
  title = {Parametric {{Curves}}},
  author = {Jia, Yan-Bin},
  year = {2019},
  month = aug,
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Jia_2019_Parametric_Curves.pdf}
}

@article{jiang2004,
  title = {The {{Indirect Method}}: {{Inference Based}} on {{Intermediate Statistics}}\textemdash{{A Synthesis}} and {{Examples}}},
  shorttitle = {The {{Indirect Method}}},
  author = {Jiang, Wenxin and Turnbull, Bruce},
  year = {2004},
  month = may,
  journal = {Statistical Science},
  volume = {19},
  number = {2},
  pages = {239--263},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/088342304000000152},
  abstract = {This article presents an exposition and synthesis of the theory and some applications of the so-called indirect method of inference. These ideas have been exploited in the field of econometrics, but less so in other fields such as biostatistics and epidemiology. In the indirect method, statistical inference is based on an intermediate statistic, which typically follows an asymptotic normal distribution, but is not necessarily a consistent estimator of the parameter of interest. This intermediate statistic can be a naive estimator based on a convenient but misspecified model, a sample moment or a solution to an estimating equation. We review a procedure of indirect inference based on the generalized method of moments, which involves adjusting the naive estimator to be consistent and asymptotically normal. The objective function of this procedure is shown to be interpretable as an ``indirect likelihood'' based on the intermediate statistic. Many properties of the ordinary likelihood function can be extended to this indirect likelihood. This method is often more convenient computationally than maximum likelihood estimation when handling such model complexities as random effects and measurement error, for example, and it can also serve as a basis for robust inference and model selection, with less stringent assumptions on the data generating mechanism. Many familiar estimation techniques can be viewed as examples of this approach. We describe applications to measurement error, omitted covariates and recurrent events. A dataset concerning prevention of mammary tumors in rats is analyzed using a Poisson regression model with overdispersion. A second dataset from an epidemiological study is analyzed using a logistic regression model with mismeasured covariates. A third dataset of exam scores is used to illustrate robust covariance selection in graphical models.},
  keywords = {asymptotic normality,bias correction,consistency,efficiency,estimating equations,generalized method of moments,graphical models,Indirect inference,indirect likelihood,measurement error,missing data,Model selection,naive estimators,omitted covariates,overdispersion,quasi-likelihood,random effects,robustness,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Jiang_Turnbull_2004_The_Indirect_Method.pdf;/home/dede/Zotero/storage/9T33JKBP/088342304000000152.html}
}

@article{jin2021a,
  title = {Bayesian {{Hierarchical Model}} for {{Change Point Detection}} in {{Multivariate Sequences}}},
  author = {Jin, Huaqing and Yin, Guosheng and Yuan, Binhang and Jiang, Fei},
  year = {2021},
  month = may,
  journal = {Technometrics},
  volume = {0},
  number = {0},
  pages = {1--10},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.2021.1927848},
  abstract = {Motivated by the wind turbine anomaly detection, we propose a Bayesian hierarchical model (BHM) for the mean-change detection in multivariate sequences. By combining the exchange random order distribution induced from the Poisson\textendash Dirichlet process and nonlocal priors, BHM exhibits satisfactory performance for mean-shift detection with multivariate sequences under different error distributions. In particular, BHM yields the smallest detection error compared with other competitive methods considered in the article. We use a local scan procedure to accelerate the computation, while the anomaly locations are determined by maximizing the posterior probability through dynamic programming. We establish consistency of the estimated number and locations of the change points and conduct extensive simulations to evaluate the BHM approach. Among the popular change point detection algorithms, BHM yields the best performance for most of the datasets in terms of the F1 score for the wind turbine anomaly detection.},
  keywords = {Change points,Multivariate data,Nonlocal prior,Nonmaximum suppression,Poissonâ€“Dirichlet process,todo},
  annotation = {\_eprint: https://doi.org/10.1080/00401706.2021.1927848},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Jin_et_al_2021_Bayesian_Hierarchical_Model_for_Change_Point_Detection_in_Multivariate_Sequences.pdf;/home/dede/Zotero/storage/JMUVEGEI/00401706.2021.html}
}

@article{johari2015,
  title = {Always {{Valid Inference}}: {{Bringing Sequential Analysis}} to {{A}}/{{B Testing}}},
  shorttitle = {Always {{Valid Inference}}},
  author = {Johari, R. and Pekelis, L. and Walsh, David},
  year = {2015},
  abstract = {This work defines always valid p-values and confidence intervals that let users try to take advantage of data as fast as it becomes available, providing valid statistical inference whenever they make their decision. A/B tests are typically analyzed via frequentist p-values and confidence intervals; but these inferences are wholly unreliable if users endogenously choose samples sizes by *continuously monitoring* their tests. We define *always valid* p-values and confidence intervals that let users try to take advantage of data as fast as it becomes available, providing valid statistical inference whenever they make their decision. Always valid inference can be interpreted as a natural interface for a sequential hypothesis test, which empowers users to implement a modified test tailored to them. In particular, we show in an appropriate sense that the measures we develop tradeoff sample size and power efficiently, despite a lack of prior knowledge of the user's relative preference between these two goals. We also use always valid p-values to obtain multiple hypothesis testing control in the sequential context. Our methodology has been implemented in a large scale commercial A/B testing platform to analyze hundreds of thousands of experiments to date.},
  keywords = {doing},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Johari_et_al_2015_Always_Valid_Inference.pdf}
}

@inproceedings{johari2017,
  title = {Peeking at {{A}}/{{B Tests}}: {{Why}} It Matters, and What to Do about It},
  shorttitle = {Peeking at {{A}}/{{B Tests}}},
  booktitle = {{{KDD}}},
  author = {Johari, Ramesh and Koomen, Pete and Pekelis, Leonid and Walsh, David},
  year = {2017},
  month = jan,
  abstract = {This paper reports on novel statistical methodology, which has been deployed by the commercial A/B testing platform Optimizely to communicate experimental results to their customers. Our...},
  langid = {english},
  keywords = {doing},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Johari_et_al_2017_Peeking_at_A-B_Tests.pdf;/home/dede/Zotero/storage/3YWBXYFE/forum.html}
}

@book{johnson1995,
  title = {Continuous {{Univariate Distributions}}, {{Vol}}. 2},
  author = {Johnson, Norman L. and Kotz, Samuel and Balakrishnan, Narayanaswamy},
  year = {1995},
  month = may,
  edition = {2nd edition},
  publisher = {{Wiley-Interscience}},
  address = {{New York}},
  isbn = {978-0-471-58494-0},
  langid = {english}
}

@article{jones-farmer2009,
  title = {Distribution-{{Free Phase I Control Charts}} for {{Subgroup Location}}},
  author = {{Jones-Farmer}, L. Allison and Jordan, Victoria and Champ, Charles W.},
  year = {2009},
  month = jul,
  journal = {Journal of Quality Technology},
  volume = {41},
  number = {3},
  pages = {304--316},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2009.11917784},
  abstract = {Much of the work in statistical quality control is dependent on the proper completion of a Phase I study. Many Phase I control charts are based on an implicit assumption of normally distributed process observations. In the beginning stages of process control, little information is available about the process and the normality assumption may not be reasonable. Existing robust and distribution-free control charts are concerned with the establishment of Phase II control limits that are robust to nonnormality or outliers from the Phase I sample. Our literature review revealed no purely distribution-free Phase I control-chart methods. We propose a distribution-free method for defining the in-control state of a process and identifying an in-control reference sample. The resultant reference sample can be used to estimate the process parameters for the Phase II procedure of choice. The proposed rank-based method is compared with the traditional X chart using Monte Carlo simulation. The rank-based method compares favorably to the X chart when the process is normally distributed and performs better than the X chart in many situations when the process distribution is skewed or heavy tailed.},
  keywords = {done,Nonparametric,Shewhart Chart,Statistical Process Control},
  annotation = {\_eprint: https://doi.org/10.1080/00224065.2009.11917784},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Jones-Farmer_et_al_2009_Distribution-Free_Phase_I_Control_Charts_for_Subgroup_Location.pdf;/home/dede/Zotero/storage/3C2VWHUP/00224065.2009.html}
}

@article{jones-farmer2010,
  title = {A {{Distribution-Free Phase I Control Chart}} for {{Subgroup Scale}}},
  author = {{Jones-Farmer}, L. Allison and Champ, Charles W.},
  year = {2010},
  month = oct,
  journal = {Journal of Quality Technology},
  volume = {42},
  number = {4},
  pages = {373--387},
  issn = {0022-4065, 2575-6230},
  doi = {10.1080/00224065.2010.11917834},
  langid = {english},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Jones-Farmer_Champ_2010_A_Distribution-Free_Phase_I_Control_Chart_for_Subgroup_Scale.pdf}
}

@article{jones1998,
  title = {The {{Performance}} of {{Bootstrap Control Charts}}},
  author = {Jones, L. Allison and Woodall, William H.},
  year = {1998},
  month = oct,
  journal = {Journal of Quality Technology},
  volume = {30},
  number = {4},
  pages = {362--375},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.1998.11979872},
  abstract = {The bootstrap is a statistical technique that substitutes computing-power for traditional parametric assumptions. Recently, several authors have considered the application of the bootstrap to statistical quality control charts. Simulation studies have been used to evaluate the performance of the bootstrap control charts. In most cases, the performance measures do not consider the average run lengths of the charts. In this paper, we discuss the proposed bootstrap control chart procedures. We provide extensive computer simulation results and evaluate the performance of each control chart in terms of the average run length.},
  keywords = {Bootstrap Methods,Control Limits,Quality Control,Shewhart Control Charts,Statistical Process Control,todo},
  annotation = {\_eprint: https://doi.org/10.1080/00224065.1998.11979872},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Jones_Woodall_1998_The_Performance_of_Bootstrap_Control_Charts.pdf;/home/dede/Zotero/storage/GZJN3WDT/00224065.1998.html}
}

@article{jones2001,
  title = {The {{Performance}} of {{Exponentially Weighted Moving Average Charts With Estimated Parameters}}},
  author = {Jones, L. Allison and Champ, Charles W and Rigdon, Steven E},
  year = {2001},
  journal = {Technometrics},
  volume = {43},
  number = {2},
  pages = {156--167},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1198/004017001750386279},
  abstract = {The exponentially weighted moving average (EWMA) control chart is typically designed assuming that standards are given for the process parameters. In practice, the parameters are rarely known, and control charts are constructed using estimates in place of the parameters. This practice can affect the control chart's run-length performance in both in- and out-of-control situations. Specifically, estimation can lead to substantially more frequent false alarms and yet reduce the sensitivity of the chart to detecting process changes. In this article, the run-length distribution of the EWMA chart with estimated parameters is derived. The effect of estimation on the performance of the chart is discussed in a variety of practical scenarios.},
  keywords = {Average run length,Control chart,EWMA,Integral equation,Quadrature,reference only},
  annotation = {\_eprint: https://doi.org/10.1198/004017001750386279},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Jones_et_al_2001_The_Performance_of_Exponentially_Weighted_Moving_Average_Charts_With_Estimated.pdf;/home/dede/Zotero/storage/G8YKUB8M/004017001750386279.html}
}

@article{jones2004,
  title = {The {{Run Length Distribution}} of the {{CUSUM}} with {{Estimated Parameters}}},
  author = {Jones, L. Allison and Champ, Charles W. and Rigdon, Steven E.},
  year = {2004},
  journal = {Journal of Quality Technology},
  volume = {36},
  number = {1},
  pages = {95--108},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2004.11980254},
  abstract = {The CUSUM control chart is a popular method used to monitor the performance of production processes. The performance of the CUSUM is generally evaluated with the assumption that the process parameters are known. In practice, the parameters are rarely known and are frequently replaced with estimates from an in-control reference sample. We discuss the run length distribution of the CUSUM with estimated parameters and provide a method for approximating this distribution and moments. We evaluate the performance of the CUSUM with estimated parameters in a variety of practical situations.},
  keywords = {Average Run Length,Control Charts,Cumulative Sum,reference only},
  annotation = {\_eprint: https://doi.org/10.1080/00224065.2004.11980254},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Jones_et_al_2004_The_Run_Length_Distribution_of_the_CUSUM_with_Estimated_Parameters.pdf}
}

@article{jr1975,
  title = {A {{Sequential Signed-Rank Test}} for {{Symmetry}}},
  author = {Jr, Marion R. Reynolds},
  year = {1975},
  month = mar,
  journal = {The Annals of Statistics},
  volume = {3},
  number = {2},
  pages = {382--400},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/aos/1176343064},
  abstract = {A sequential procedure for testing the hypothesis that the distribution of a sequence of i.i.d. random variables is symmetric about zero is given, where the test statistic is a function of the signs and the rank of the absolute values of the observations. Necessary and sufficient conditions that the individual signed ranks be independent are given. The critical region, power, and expected sample size of the test are determined approximately by using the fact that the test statistic behaves asymptotically like a Brownian motion process.},
  keywords = {62G10,62L12,Brownian motion process,Nonparametric test,sequential test,signed-rank statistic},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Jr_1975_A_Sequential_Signed-Rank_Test_for_Symmetry.pdf;/home/dede/Zotero/storage/UA7ZUHB5/1176343064.html}
}

@article{jr2010,
  title = {An {{Evaluation}} of a {{GLR Control Chart}} for {{Monitoring}} the {{Process Mean}}},
  author = {Jr, Marion R. Reynolds and Lou, Jianying},
  year = {2010},
  month = jul,
  journal = {Journal of Quality Technology},
  volume = {42},
  number = {3},
  pages = {287--310},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2010.11917825},
  abstract = {This paper considers the problem of monitoring the mean of a normally distributed process variable when the objective is to effectively detect both small and large shifts in this mean. The performance of a generalized likelihood ratio (GLR) control chart is evaluated, where the likelihood ratio is based on a moving window of past observations. The performance of the GLR chart is compared with the performance of other options, such as combinations of Shewhart and cumulative sum (CUSUM) charts and an adaptive CUSUM chart, that have been proposed for detecting a wide range of shift sizes. Performance is evaluated for sustained shifts, transient shifts, and drifts in the mean. It is shown that the overall performance of the GLR chart is at least as good as these other options. These other options have multiple control-chart parameters that allow for the charts to be tuned to be more sensitive to certain shifts that may be of interest. However, the GLR chart does not require users to specify the values of any control-chart parameters other than the size of the window and the control limit. We recommend a specific window size and provide a table of control limits corresponding to specified values of the false-alarm rate, so it is very easy to design the GLR chart for use in applications. Simulating the performance of the GLR chart is time consuming, but approximating the GLR chart with a set of CUSUM charts provides a much faster way of evaluating the performance of the GLR chart for research purposes.},
  keywords = {Adaptive Control Chart,Average Time to Signal,CUSUM Chart,done,Generalized Likelihood Ratio,Shewhart Chart,Statistical Process Control,Steady State,Surveillance},
  annotation = {\_eprint: https://doi.org/10.1080/00224065.2010.11917825},
  file = {/home/dede/Zotero/storage/7IKTCIMS/00224065.2010.html}
}

@book{judt2009,
  title = {{Reappraisals: Reflections on the Forgotten Twentieth Century}},
  shorttitle = {{Reappraisals}},
  author = {Judt, Tony},
  year = {2009},
  month = mar,
  edition = {Reprint edizione},
  publisher = {{Penguin Books}},
  address = {{London}},
  isbn = {978-0-14-311505-2},
  langid = {Inglese},
  keywords = {doing},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Judt_2009_Reappraisals.epub}
}

@article{kalli2011,
  title = {Slice Sampling Mixture Models},
  author = {Kalli, Maria and Griffin, Jim E. and Walker, Stephen G.},
  year = {2011},
  month = jan,
  journal = {Statistics and Computing},
  volume = {21},
  number = {1},
  pages = {93--105},
  issn = {0960-3174, 1573-1375},
  doi = {10.1007/s11222-009-9150-y},
  langid = {english},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Kalli_et_al_2011_Slice_sampling_mixture_models.pdf}
}

@article{kam2021,
  title = {Securitization, Surveillance and `de-Extremization' in {{Xinjiang}}},
  author = {Kam, Stefanie and Clarke, Michael},
  year = {2021},
  month = may,
  journal = {International Affairs},
  volume = {97},
  number = {3},
  pages = {625--642},
  issn = {0020-5850},
  doi = {10.1093/ia/iiab038},
  abstract = {Previous explanations on China's counterterrorism strategy have highlighted the results of China's strategy of repression in Xinjiang, the historical antecedents and institutional foundations of its counterterrorism policies, as well as international and domestic sources of China's counterterrorism strategy. While acknowledging the importance of all these dimensions, this article draws attention to a largely neglected feature of China's counterterrorism strategy: the Chinese party-state's social engineering of Xinjiang. Building on Maoist-era practices such as the mass line and the `friend vs. enemy' binary, the Communist party under Xi Jinping has integrated surveillance technologies as part of its strategy of preventive counterterrorism and `de-extremization'. This article argues that the Chinese party-state's embrace of modern technologies, a weak liberal tradition in China, Xi Jinping's rise to power in late 2012, and the appointment of Chen Quanguo as Xinjiang's party-secretary in 2016, provides the socio-political background for the intensification of securitization, surveillance and introduction of `re-education and training centres' in Xinjiang. Surveillance technologies now complement collective, face-to-face methods of surveillance and Maoist-era techniques of mass mobilization, enabling the Chinese party-state to govern and manage the biopolitical spaces of Uyghurs with greater intensity, according to the state's precise norms. The legalization and institutionalization of `de-extremization' has also led to the shift from mass `de-extremization' propaganda to `drip-irrigation' ideological and political re-education of individuals deemed at risk of extremism. The result is an increased capacity by the Chinese party-state to surveil and control the region, and to more effectively negate the possibility of individual resistance.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Kam_Clarke_2021_Securitization,_surveillance_and_â€˜de-extremizationâ€™_in_Xinjiang.pdf;/home/dede/Zotero/storage/UDFKZZKW/6219662.html}
}

@article{kapelner2016,
  title = {{{bartMachine}}: {{Machine Learning}} with {{Bayesian Additive Regression Trees}}},
  shorttitle = {{{bartMachine}}},
  author = {Kapelner, Adam and Bleich, Justin},
  year = {2016},
  month = apr,
  journal = {Journal of Statistical Software},
  volume = {70},
  number = {1},
  pages = {1--40},
  issn = {1548-7660},
  doi = {10.18637/jss.v070.i04},
  copyright = {Copyright (c) 2016 Adam Kapelner, Justin Bleich},
  langid = {english},
  keywords = {Bayesian,Java,machine learning,non-parametric,R,statistical learning,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Kapelner_Bleich_2016_bartMachine.pdf;/home/dede/Zotero/storage/UU5G7UAF/v070i04.html}
}

@book{karhunen1946,
  title = {\"Uber Lineare {{Methoden}} in Der {{Wahrscheinlichkeitsrechnung}}},
  author = {Karhunen, Kari},
  year = {1946},
  series = {Ann. {{Acad}}. {{Sci}}. {{Fennicae}}, {{Series A}} 1, {{Math}}. {{Phys}}},
  publisher = {{Universitat Helsinki}},
  googlebooks = {bGUUAQAAIAAJ}
}

@article{karrer2011,
  title = {Stochastic Blockmodels and Community Structure in Networks},
  author = {Karrer, Brian and Newman, M. E. J.},
  year = {2011},
  month = jan,
  journal = {Physical Review E},
  volume = {83},
  number = {1},
  eprint = {1008.3926},
  eprinttype = {arxiv},
  pages = {016107},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.83.016107},
  abstract = {Stochastic blockmodels have been proposed as a tool for detecting community structure in networks as well as for generating synthetic networks for use as benchmarks. Most blockmodels, however, ignore variation in vertex degree, making them unsuitable for applications to real-world networks, which typically display broad degree distributions that can significantly distort the results. Here we demonstrate how the generalization of blockmodels to incorporate this missing element leads to an improved objective function for community detection in complex networks. We also propose a heuristic algorithm for community detection using this objective function or its non-degree-corrected counterpart and show that the degree-corrected version dramatically outperforms the uncorrected one in both real-world and synthetic networks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Social and Information Networks,Condensed Matter - Statistical Mechanics,Physics - Data Analysis; Statistics and Probability,Physics - Physics and Society,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Karrer_Newman_2011_Stochastic_blockmodels_and_community_structure_in_networks.pdf;/home/dede/Zotero/storage/93TRQK9J/1008.html}
}

@article{katser2021,
  title = {Unsupervised {{Offline Changepoint Detection Ensembles}}},
  author = {Katser, Iurii D. and Kozitsin, Viacheslav and Lobachev, Victor and Maksimov, I.},
  year = {2021},
  doi = {10.3390/APP11094280},
  abstract = {An unsupervised CPD ensemble (CPDE) procedure with the pseudocode of the particular proposed ensemble algorithms and the link to their Python realization is proposed, showing that the proposed CPDE outperforms non-ensemble CPD procedures. Offline changepoint detection (CPD) algorithms are used for signal segmentation in an optimal way. Generally, these algorithms are based on the assumption that signal's changed statistical properties are known, and the appropriate models (metrics, cost functions) for changepoint detection are used. Otherwise, the process of proper model selection can become laborious and time-consuming with uncertain results. Although an ensemble approach is well known for increasing the robustness of the individual algorithms and dealing with mentioned challenges, it is weakly formalized and much less highlighted for CPD problems than for outlier detection or classification problems. This paper proposes an unsupervised CPD ensemble (CPDE) procedure with the pseudocode of the particular proposed ensemble algorithms and the link to their Python realization. The approach's novelty is in aggregating several cost functions before the changepoint search procedure running during the offline analysis. The numerical experiment showed that the proposed CPDE outperforms non-ensemble CPD procedures. Additionally, we focused on analyzing common CPD algorithms, scaling, and aggregation functions, comparing them during the numerical experiment. The results were obtained on the two anomaly benchmarks that contain industrial faults and failures\textemdash Tennessee Eastman Process (TEP) and Skoltech Anomaly Benchmark (SKAB). One of the possible applications of our research is the estimation of the failure time for fault identification and isolation problems of the technical diagnostics.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Katser_et_al_2021_Unsupervised_Offline_Changepoint_Detection_Ensembles.pdf}
}

@book{kay1993,
  title = {{Fundamentals of Statistical Signal Processing: Estimation Theory}},
  shorttitle = {{Fundamentals of Statistical Signal Processing}},
  author = {Kay, Steven M.},
  year = {1993},
  edition = {1\textdegree{} edizione},
  publisher = {{Pearson College Div}},
  address = {{Englewood Cliffs, N.J}},
  isbn = {978-0-13-345711-7},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Kay_1993_Fundamentals_of_Statistical_Signal_Processing.pdf}
}

@article{kazemzadeh2013,
  title = {An {{EWMA}} t Chart with Variable Sampling Intervals for Monitoring the Process Mean},
  author = {Kazemzadeh, Reza and Karbasian, Mahdi and Babakhani, Mohammad Ali},
  year = {2013},
  journal = {The International Journal of Advanced Manufacturing Technology},
  volume = {66},
  pages = {125--139},
  doi = {10.1007/s00170-012-4311-0},
  abstract = {This paper proposes a variable sampling interval (VSI) version of the fixed sampling interval (FSI) exponentially weighted moving average (EWMA) t chart developed by Zhang et al. for monitoring the changes in the process mean. An optimal design strategy based on the average time to signal (ATS) is presented. We determine the optimal parameters for the VSI EWMA t chart using a Markov chain approach so that the chart has the desired robustness property against errors in estimating the process standard deviation or changing standard deviation. Also, we explain how the various parameters of this VSI EWMA t chart can be computed and how the use of the VSI feature improves the statistical efficiency of the FSI EWMA t and FSI EWMA X-bar charts in terms of out-of-control ATS performances. Comparisons with the FSI EWMA t and FSI EWMA X-bar charts are performed.},
  keywords = {reference only},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Kazemzadeh_et_al_2013_An_EWMA_t_chart_with_variable_sampling_intervals_for_monitoring_the_process_mean.pdf}
}

@book{keegan2000,
  title = {The {{First World War}}},
  author = {Keegan, John},
  year = {2000},
  month = may,
  publisher = {{Vintage}},
  address = {{New York}},
  isbn = {978-0-375-70045-3},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Keegan_2000_The_First_World_War.epub}
}

@book{keener2010,
  title = {Theoretical {{Statistics}}: {{Topics}} for a {{Core Course}}},
  shorttitle = {Theoretical {{Statistics}}},
  author = {Keener, Robert W.},
  year = {2010},
  month = sep,
  edition = {2010 edition},
  publisher = {{Springer}},
  abstract = {Intended as the text for a sequence of advanced courses, this book covers major topics in theoretical statistics in a concise and rigorous fashion. The discussion assumes a background in advanced calculus, linear algebra, probability, and some analysis and topology. Measure theory is used, but the notation and basic results needed are presented in an initial chapter on probability, so prior knowledge of these topics is not essential.The presentation is designed to expose students to as many of the central ideas and topics in the discipline as possible, balancing various approaches to inference as well as exact, numerical, and large sample methods. Moving beyond more standard material, the book includes chapters introducing bootstrap methods, nonparametric regression, equivariant estimation, empirical Bayes, and sequential design and analysis.The book has a rich collection of exercises. Several of them illustrate how the theory developed in the book may be used in various applications. Solutions to many of the exercises are included in an appendix.},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Keener_2010_Theoretical_Statistics.pdf}
}

@article{kennedy-pipe2006,
  title = {Apocalypse Now? {{Continuities}} or Disjunctions in World Politics after 9/11},
  shorttitle = {Apocalypse Now?},
  author = {{Kennedy-Pipe}, Caroline and Rengger, Nicholas},
  year = {2006},
  month = may,
  journal = {International Affairs},
  volume = {82},
  number = {3},
  pages = {539--552},
  issn = {0020-5850},
  doi = {10.1111/j.1468-2346.2006.00550.x},
  abstract = {This article argues that the widespread assumption that the events of world politics since 9/11 represent a radical break in world politics is incorrect. It goes on to argue, first, that world politics since 9/11 displays far more continuities than disjunctions and, second, that the belief that there was a radical change signalled by 9/11 is deeply problematic not just for conceptualizations of world politics but also for practice. This argument is then discussed in the context of four specific areas, the geo-political configuration of world politics, the ideological shape of world politics, ideas and assumptions about the use of force in world politics and the relationship between civil liberty and security.},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Kennedy-Pipe_Rengger_2006_Apocalypse_now.pdf;/home/dede/Zotero/storage/EDZ5YTMQ/2435000.html}
}

@article{kennedy2020,
  title = {Know Your Population and Know Your Model: {{Using}} Model-Based Regression and Poststratification to Generalize Findings beyond the Observed Sample},
  shorttitle = {Know Your Population and Know Your Model},
  author = {Kennedy, Lauren and Gelman, Andrew},
  year = {2020},
  month = apr,
  journal = {arXiv:1906.11323 [stat]},
  eprint = {1906.11323},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Psychology research focuses on interactions, and this has deep implications for inference from non-representative samples. For the goal of estimating average treatment effects, we propose to fit a model allowing treatment to interact with background variables and then average over the distribution of these variables in the population. This can be seen as an extension of multilevel regression and poststratification (MRP), a method used in political science and other areas of survey research, where researchers wish to generalize from a sparse and possibly non-representative sample to the general population. In this paper, we discuss areas where this method can be used in the psychological sciences. We use our method to estimate the norming distribution for the Big Five Personality Scale using open source data. We argue that large open data sources like this and other collaborative data sources can be combined with MRP to help resolve current challenges of generalizability and replication in psychology.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Applications},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Kennedy_Gelman_2020_Know_your_population_and_know_your_model.pdf;/home/dede/Zotero/storage/5Z58TH9E/1906.html}
}

@article{keogh2005,
  title = {Exact Indexing of Dynamic Time Warping},
  author = {Keogh, Eamonn and Ratanamahatana, Chotirat Ann},
  year = {2005},
  month = mar,
  journal = {Knowledge and Information Systems},
  volume = {7},
  number = {3},
  pages = {358--386},
  issn = {0219-3116},
  doi = {10.1007/s10115-004-0154-9},
  abstract = {The problem of indexing time series has attracted much interest. Most algorithms used to index time series utilize the Euclidean distance or some variation thereof. However, it has been forcefully shown that the Euclidean distance is a very brittle distance measure. Dynamic time warping (DTW) is a much more robust distance measure for time series, allowing similar shapes to match even if they are out of phase in the time axis. Because of this flexibility, DTW is widely used in science, medicine, industry and finance. Unfortunately, however, DTW does not obey the triangular inequality and thus has resisted attempts at exact indexing. Instead, many researchers have introduced approximate indexing techniques or abandoned the idea of indexing and concentrated on speeding up sequential searches. In this work, we introduce a novel technique for the exact indexing of DTW. We prove that our method guarantees no false dismissals and we demonstrate its vast superiority over all competing approaches in the largest and most comprehensive set of time series indexing experiments ever undertaken.},
  langid = {english},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Keogh_Ratanamahatana_2005_Exact_indexing_of_dynamic_time_warping.pdf}
}

@article{khoo2005,
  title = {Short Runs Multivariate Control Chart for Process Dispersion},
  author = {Khoo, Michael B. C. and Quah, S. H. and Low, H. C. and Ch'ng, C. K.},
  year = {2005},
  month = apr,
  journal = {International Journal of Reliability, Quality and Safety Engineering},
  volume = {12},
  number = {02},
  pages = {127--147},
  publisher = {{World Scientific Publishing Co.}},
  issn = {0218-5393},
  doi = {10.1142/S0218539305001732},
  abstract = {The multivariate Hotelling's T2 control chart is designed to be used in a mass production for processes where data to estimate the mean vector and covariance matrix as well as the computation of control limits are available before a production run. Recent years have seen a trend in manufacturing industries to produce smaller lot sizes, a.k.a., low volume production which is a result of increased importance given to just-in-time (JIT) manufacturing techniques, synchronous manufacturing and the reduction of in-process inventory and costs. This new manufacturing environment is also referred to as short runs production or short runs. In a short runs environment, it is difficult or perhaps impossible to establish a reliable historical data set in setting valid control limits and in estimating process parameters due to the availability of insufficient data for a particular process because production runs are usually short and change frequently from one process to another. There is also a need to start charting at or very near the beginning of the run in such a case. Another problem encountered in a short runs production such as in job shops is that there are many different types of measurements so that many different control charts are needed. Standardized control charts that allow different statistics to be plotted on the same chart are extremely useful in short runs. Control charts with standard scale simplify the control charting process in a short runs environment. In this paper, we address the multivariate short runs problems for process dispersion based on individual measurements by presenting the required formulas so that the chart can be used from the start of production, whether or not prior information for estimating the chart's limit and its parameter is available. The proposed chart plots standardized statistics for multiple parts on the same chart. This paper extends the work of the authors in Ref. 13.},
  keywords = {average run length (ARL),in-control,job shops,out-of-control (o.o.c.),process dispersion,Short runs}
}

@article{khosravi2019,
  title = {Self-{{Starting}} Control Charts for Monitoring Logistic Regression Profiles},
  author = {Khosravi, Peyman and Amiri, Amirhossein},
  year = {2019},
  month = jul,
  journal = {Communications in Statistics - Simulation and Computation},
  volume = {48},
  number = {6},
  pages = {1860--1871},
  publisher = {{Taylor \& Francis}},
  issn = {0361-0918},
  doi = {10.1080/03610918.2018.1425442},
  abstract = {In some applications, quality engineers cannot monitor the processes at the beginning of the production process. Because the process parameters are unknown and there are not enough initial samples to estimate the process parameters. Self-starting control charts are applied to monitor processes at the start-up stages with no enough initial samples. In this paper, we propose three self-starting control charts to monitor a logistic regression profile which models the relationship between a binomial response variable and explanatory variables. Also, we compare the proposed control charts with each other through simulation studies in terms of average run length (ARL) criterion.},
  keywords = {Average run length (ARL),Binomial response variable,done,Logistic regression profile,Self-starting control chart,Short-run process},
  annotation = {\_eprint: https://doi.org/10.1080/03610918.2018.1425442},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Khosravi_Amiri_2019_Self-Starting_control_charts_for_monitoring_logistic_regression_profiles.pdf;/home/dede/Zotero/storage/ETH66QVI/03610918.2018.html}
}

@article{kiefer1952,
  title = {Stochastic {{Estimation}} of the {{Maximum}} of a {{Regression Function}}},
  author = {Kiefer, J. and Wolfowitz, J.},
  year = {1952},
  month = sep,
  journal = {The Annals of Mathematical Statistics},
  volume = {23},
  number = {3},
  pages = {462--466},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177729392},
  abstract = {Let \$M(x)\$ be a regression function which has a maximum at the unknown point \$\textbackslash theta. M(x)\$ is itself unknown to the statistician who, however, can take observations at any level \$x\$. This paper gives a scheme whereby, starting from an arbitrary point \$x\_1\$, one obtains successively \$x\_2, x\_3, \textbackslash cdots\$ such that \$x\_n\$ converges to \$\textbackslash theta\$ in probability as \$n \textbackslash rightarrow \textbackslash infty\$.},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Kiefer_Wolfowitz_1952_Stochastic_Estimation_of_the_Maximum_of_a_Regression_Function.pdf;/home/dede/Zotero/storage/XAB52NVX/1177729392.html}
}

@article{kim2019,
  title = {A Penalized Likelihood-Based Quality Monitoring via {{L2-norm}} Regularization for High-Dimensional Processes},
  author = {Kim, Sangahn and Jeong, M. and Elsayed, E.},
  year = {2019},
  journal = {Journal of Quality Technology},
  doi = {10.1080/00224065.2019.1571348},
  abstract = {A new penalized likelihood-based approach via norm regularization is proposed, which does not select variables but rather ``shrinks'' all process mean estimates toward zero, which makes the proposed chart significantly efficient to monitor high-dimensional processes. Abstract Technological advances have resulted in the introduction of new products and manufacturing processes that have a large number of characteristics and variables to be monitored to ensure the product quality. Simultaneous monitoring of variables under such a high-dimensional environment is quite challenging. Traditional approaches are less sensitive to the out-of-control signals in high-dimensional processes, especially when only a few variables are responsible for abnormal changes in the process output. Recently, variable selection-based charts are proposed to overcome the drawbacks of the traditional approaches. These approaches adopt diagnosis procedures to identify a small subset of potentially changed variables. However, the detection capability of these charts may be low in cases when the size of the shift is relatively small in a highly correlated data structure, which is critical in modern industry. Moreover, the complexity of computation would dramatically increase as the dimension of the process parameters increases due to the diagnosis procedure, which is inappropriate for online monitoring. This article proposes a new penalized likelihood-based approach via norm regularization, which does not select variables but rather ``shrinks'' all process mean estimates toward zero. A closed-form solution of the proposed approach along with probability distributions of the monitoring statistic under null and alternative hypotheses are obtained, which makes the proposed chart significantly efficient to monitor high-dimensional processes. In addition, we explore theoretical properties of the approach and present several extensions of the proposed chart and its integration with other existing charts. Finally, we compare the performance of the approach with existing methods and present a case study of a high-speed milling process.},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Kim_et_al_2019_A_penalized_likelihood-based_quality_monitoring_via_L2-norm_regularization_for.pdf;/home/dede/Zotero/storage/FENIIDB4/kim2019.pdf}
}

@article{kim2020,
  title = {Bayesian {{Synthetic Control Methods}}},
  author = {Kim, Sungjin and Lee, Clarence and Gupta, Sachin},
  year = {2020},
  month = oct,
  journal = {Journal of Marketing Research},
  volume = {57},
  number = {5},
  pages = {831--852},
  publisher = {{SAGE Publications Inc}},
  issn = {0022-2437},
  doi = {10.1177/0022243720936230},
  abstract = {The authors propose a new Bayesian synthetic control framework to overcome limitations of extant synthetic control methods (SCMs). The proposed Bayesian synthetic control methods (BSCMs) do not impose any restrictive constraints on the parameter space a priori. Moreover, they provide statistical inference in a straightforward manner as well as a natural mechanism to deal with the ``large p, small n'' and sparsity problems through Markov chain Monte Carlo procedures. Using simulations, the authors find that for a variety of data-generating processes, the proposed BSCMs almost always provide better predictive accuracy and parameter precision than extant SCMs. They demonstrate an application of the proposed BSCMs to a real-world context of a tax imposed on soda sales in Washington state in 2010. As in the simulations, the proposed models outperform extant models, as measured by predictive accuracy in the posttreatment periods. The authors find that the tax led to an increase of 5.7\% in retail price and a decrease of 5.5\%{$\sim$}5.8\% in sales. They also find that retailers in Washington overshifted the tax to consumers, leading to a pass-through rate of approximately 121\%.},
  langid = {english},
  keywords = {Bayesian estimation,done,soda tax,synthetic control,treatment effect},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Kim_et_al_2020_Bayesian_Synthetic_Control_Methods.pdf}
}

@article{kingma2017,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2017},
  month = jan,
  journal = {arXiv:1412.6980},
  eprint = {1412.6980},
  eprinttype = {arxiv},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Kingma_Ba_2017_Adam.pdf;/home/dede/Zotero/storage/5WL3GCBN/1412.html}
}

@book{kissinger2012,
  title = {{On China}},
  author = {Kissinger, Henry},
  year = {2012},
  month = apr,
  edition = {Reprint edizione},
  publisher = {{Penguin Books}},
  address = {{New York, NY}},
  isbn = {978-0-14-312131-2},
  langid = {Inglese},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Kissinger_2012_On_China.epub}
}

@book{kissinger2015,
  title = {World {{Order}}},
  author = {Kissinger, Henry},
  year = {2015},
  month = sep,
  edition = {Reprint edition},
  publisher = {{Penguin Books}},
  address = {{New York}},
  isbn = {978-0-14-312771-0},
  langid = {english},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Kissinger_2015_World_Order.epub}
}

@book{knight2008,
  title = {{Institutions and Social Conflict}},
  author = {Knight},
  year = {2008},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge England ; New York, N.Y}},
  isbn = {978-0-521-42189-8},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Knight_2008_Institutions_and_Social_Conflict.pdf}
}

@book{knoth2015,
  title = {Frontiers in {{Statistical Quality Control}} 11},
  editor = {Knoth, Sven and Schmid, Wolfgang},
  year = {2015},
  series = {Frontiers in {{Statistical Quality Control}}},
  publisher = {{Springer International Publishing}},
  doi = {10.1007/978-3-319-12355-4},
  abstract = {The main focus of this edited volume is on three major areas of statistical quality control: statistical process control (SPC), acceptance sampling and design of experiments. The majority of the papers deal with statistical process control, while acceptance sampling and design of experiments are also treated to a lesser extent.The book is organized into four thematic parts, with Part I addressing statistical process control. Part II is devoted to acceptance sampling. Part III covers the design of experiments, while Part IV discusses related fields.The twenty-three papers in this volume stem from The 11th International Workshop on Intelligent Statistical Quality Control, which was held in Sydney, Australia from August 20 to August 23, 2013. The event was hosted by Professor Ross Sparks, CSIRO Mathematics, Informatics and Statistics, North Ryde, Australia and was jointly organized by Professors S. Knoth, W. Schmid and Ross Sparks. The papers presented here were carefully selected and reviewed by the scientific program committee, before being revised and adapted for this volume.},
  isbn = {978-3-319-12354-7},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Knoth_Schmid_2015_Frontiers_in_Statistical_Quality_Control_11.pdf;/home/dede/Zotero/storage/68N29CAN/9783319123547.html}
}

@book{knoth2018,
  title = {Frontiers in {{Statistical Quality Control}} 12},
  editor = {Knoth, Sven and Schmid, Wolfgang},
  year = {2018},
  series = {Frontiers in {{Statistical Quality Control}}},
  publisher = {{Springer International Publishing}},
  doi = {10.1007/978-3-319-75295-2},
  abstract = {This book provides insights into important new developments in the area of statistical quality control and critically discusses methods used in on-line and off-line statistical quality control.The book is divided into three parts: Part I covers statistical process control, Part II deals with design of experiments, while Part III focuses on fields such as reliability theory and data quality. The 12th International Workshop on Intelligent Statistical Quality Control (Hamburg, Germany, August 16 \textendash{} 19, 2016) was jointly organized by Professors Sven Knoth and Wolfgang Schmid. The contributions presented in this volume were carefully selected and reviewed by the conference's scientific program committee. Taken together, they bridge the gap between theory and practice, making the book of interest to both practitioners and researchers in the field of quality control.},
  isbn = {978-3-319-75294-5},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Knoth_Schmid_2018_Frontiers_in_Statistical_Quality_Control_12.pdf}
}

@book{knoth2021,
  title = {Frontiers in {{Statistical Quality Control}} 13},
  editor = {Knoth, Sven and Schmid, Wolfgang},
  year = {2021},
  series = {Frontiers in {{Statistical Quality Control}}},
  publisher = {{Springer International Publishing}},
  doi = {10.1007/978-3-030-67856-2},
  abstract = {This contributed book focuses on major aspects of statistical quality control, shares insights into important new developments in the field, and adapts established statistical quality control methods for use in e.g. big data, network analysis and medical applications. The content is divided into two parts, the first of which mainly addresses statistical process control, also known as statistical process monitoring. In turn, the second part explores selected topics in statistical quality control, including measurement uncertainty analysis and data quality.The peer-reviewed contributions gathered here were originally presented at the 13th International Workshop on Intelligent Statistical Quality Control, ISQC 2019, held in Hong Kong on August 12-14, 2019. Taken together, they bridge the gap between theory and practice, making the book of interest to both practitioners and researchers in the field of statistical quality control.},
  isbn = {978-3-030-67855-5},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Knoth_Schmid_2021_Frontiers_in_Statistical_Quality_Control_13.pdf;/home/dede/Zotero/storage/I5CGASFH/9783030678555.html}
}

@article{knoth2021a,
  title = {The Impracticality of Homogeneously Weighted Moving Average and Progressive Mean Control Chart Approaches},
  author = {Knoth, Sven and {Tercero-G{\'o}mez}, V{\'i}ctor G. and Khakifirooz, Marzieh and Woodall, William H.},
  year = {2021},
  journal = {Quality and Reliability Engineering International},
  volume = {37},
  number = {8},
  pages = {3779--3794},
  issn = {1099-1638},
  doi = {10.1002/qre.2950},
  abstract = {There is growing literature on new versions of ``memory-type'' control charts, where deceptively good zero-state average run-length (ARL) performance is misleading. Using steady-state run-length analysis in combination with the conditional expected delay (CED) metric, we show that the increasingly discussed progressive mean (PM) and homogeneously weighted moving average (HWMA) control charts should not be used in practice. Previously reported performance of methods based on these two approaches is misleading, as we found that performance is good only when a process change occurs at the very start of monitoring. Traditional alternatives, such as exponentially weighted moving average (EWMA) and cumulative sum (CUSUM) charts, not only have more consistent detection behavior over a range of different change points, they can also lead to better out-of-control zero-state ARL performance when properly designed.},
  langid = {english},
  keywords = {average run length,control chart,done,statistical process monitoring,steady-state average run length},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/qre.2950},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Knoth_et_al_2021_The_impracticality_of_homogeneously_weighted_moving_average_and_progressive.pdf;/home/dede/Zotero/storage/UA63L5ZU/qre.html}
}

@article{knoth2021b,
  title = {Steady-State Average Run Length(s): {{Methodology}}, Formulas, and Numerics},
  shorttitle = {Steady-State Average Run Length(s)},
  author = {Knoth, Sven},
  year = {2021},
  month = jul,
  journal = {Sequential Analysis},
  volume = {40},
  number = {3},
  pages = {405--426},
  publisher = {{Taylor \& Francis}},
  issn = {0747-4946},
  doi = {10.1080/07474946.2021.1940501},
  abstract = {The average run length (ARL), with its various phenotypes, is the prevailing performance measure for evaluating control charts, or change-point detection schemes. Essentially, the ARL counts the number of observations until the corresponding procedure flags a change. To enable a fair comparison between competing designs, one frequently deploys the steady-state ARL. Differing from the older concept of the zero-state ARL (which assumes that the to-be-detected change occurs immediately at startup or never), the former measure postulates this change's appearance after reaching some steady state. Considering different notions (primarily conditional and cyclical ones) of the measure, we recapitulate its historical development; provide a critical discussion of its often-careless exploitation, including a few misconceptions; and derive some new mathematical characterizations that permit its easy calculation.},
  keywords = {62L10,62P30,65R20,ARL measures,change point,doing,integral equation,Markov chain,quasistationary distribution},
  annotation = {\_eprint: https://doi.org/10.1080/07474946.2021.1940501},
  file = {/home/dede/Zotero/storage/ZTMGWTIE/07474946.2021.html}
}

@book{kochenderfer2019,
  title = {Algorithms for {{Optimization}}},
  author = {Kochenderfer, Mykel J. and Wheeler, Tim A.},
  year = {2019},
  month = mar,
  publisher = {{MIT Press}},
  address = {{Cambridge, MA, USA}},
  abstract = {A comprehensive introduction to optimization with a focus on practical algorithms for the design of engineering systems.},
  isbn = {978-0-262-03942-0},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Kochenderfer_Wheeler_2019_Algorithms_for_Optimization.pdf}
}

@book{kolmogorov2012,
  title = {{Elements of the Theory of Functions and Functional Analysis}},
  author = {Kolmogorov, A. N. and Fomin, S. V.},
  year = {2012},
  publisher = {{Martino Fine Books}},
  address = {{Mansfield Centre, Conn}},
  abstract = {2012 Reprint of Volumes One and Two, 1957-1961. Exact facsimile of the original edition, not reproduced with Optical Recognition Software. A. N. Kolmogorov was a Soviet mathematician, preeminent in the 20th century, who advanced various scientific fields, among them probability theory, topology, logic, turbulence, classical mechanics and computational complexity. Later in life Kolmogorov changed his research interests to the area of turbulence, where his publications beginning in 1941 had a significant influence on the field. In classical mechanics, he is best known for the Kolmogorov-Arnold-Moser theorem. In 1957 he solved a particular interpretation of Hilbert's thirteenth problem (a joint work with his student V. I. Arnold). He was a founder of algorithmic complexity theory, often referred to as Kolmogorov complexity theory, which he began to develop around this time. Based on the authors' courses and lectures, this two-part advanced-level text is now available in a single volume. Topics include metric and normed spaces, continuous curves in metric spaces, measure theory, Lebesque intervals, Hilbert space, and more. Each section contains exercises. Lists of symbols, definitions, and theorems.},
  isbn = {978-1-61427-304-2},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Kolmogorov_Fomin_2012_Elements_of_the_Theory_of_Functions_and_Functional_Analysis2.pdf}
}

@article{kolouri2017,
  title = {Optimal {{Mass Transport}}: {{Signal}} Processing and Machine-Learning Applications},
  shorttitle = {Optimal {{Mass Transport}}},
  author = {Kolouri, Soheil and Park, Se Rim and Thorpe, Matthew and Slepcev, Dejan and Rohde, Gustavo K.},
  year = {2017},
  month = jul,
  journal = {IEEE Signal Processing Magazine},
  volume = {34},
  number = {4},
  pages = {43--59},
  issn = {1558-0792},
  doi = {10.1109/MSP.2017.2695801},
  abstract = {Transport-based techniques for signal and data analysis have recently received increased interest. Given their ability to provide accurate generative models for signal intensities and other data distributions, they have been used in a variety of applications, including content-based retrieval, cancer detection, image superresolution, and statistical machine learning, to name a few, and they have been shown to produce state-of-the-art results. Moreover, the geometric characteristics of transport-related metrics have inspired new kinds of algorithms for interpreting the meaning of data distributions. Here, we provide a practical overview of the mathematical underpinnings of mass transport-related methods, including numerical implementation, as well as a review, with demonstrations, of several applications. Software accompanying this article is available from [43].},
  keywords = {Analytical models,data analysis,data distributions,Data models,Estimation,generative models,geometric characteristics,image processing,learning (artificial intelligence),Linear programming,machine-learning applications,mass transport-related methods,metric space,Morphology,optimal transport,Probability density function,signal analysis,signal intensities,signal processing,todo,transport-based techniques,transport-related metrics,Transportation,wasserstein distance},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Kolouri_et_al_2017_Optimal_Mass_Transport.pdf;/home/dede/Zotero/storage/ZCJX4RK2/7974883.html}
}

@inproceedings{komiyama2015,
  title = {Optimal {{Regret Analysis}} of {{Thompson Sampling}} in {{Stochastic Multi-armed Bandit Problem}} with {{Multiple Plays}}},
  booktitle = {{{ICML}}},
  author = {Komiyama, Junpei and Honda, J. and Nakagawa, H.},
  year = {2015},
  abstract = {It is proved that MP-TS for binary rewards has the optimal regret upper bound that matches the regret lower bound provided by Anantharam et al. (1987) and is the first computationally efficient algorithm with optimal regret. We discuss a multiple-play multi-armed bandit (MAB) problem in which several arms are selected at each round. Recently, Thompson sampling (TS), a randomized algorithm with a Bayesian spirit, has attracted much attention for its empirically excellent performance, and it is revealed to have an optimal regret bound in the standard single-play MAB problem. In this paper, we propose the multiple-play Thompson sampling (MP-TS) algorithm, an extension of TS to the multiple-play MAB problem, and discuss its regret analysis. We prove that MP-TS for binary rewards has the optimal regret upper bound that matches the regret lower bound provided by Anantharam et al. (1987). Therefore, MP-TS is the first computationally efficient algorithm with optimal regret. A set of computer simulations was also conducted, which compared MP-TS with state-of-the-art algorithms. We also propose a modification of MP-TS, which is shown to have better empirical performance.},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Komiyama_et_al_2015_Optimal_Regret_Analysis_of_Thompson_Sampling_in_Stochastic_Multi-armed_Bandit.pdf}
}

@article{korzenowski2015,
  title = {Control Charts for Flexible and Multi-Variety Production Systems},
  author = {Korzenowski, A.L. and Vidor, G. and Vaccaro, G.L.R. and Ten Caten, C.S.},
  year = {2015},
  month = oct,
  journal = {Computers and Industrial Engineering},
  volume = {88},
  number = {C},
  pages = {284--292},
  issn = {0360-8352},
  doi = {10.1016/j.cie.2015.07.016},
  abstract = {Paper proposes control chart procedures for highly customized scenarios.Proposed methods are self-start and do not need Phase I.Extensive simulation with several scenarios were provide to evaluate performance.CC tie in performance with traditional approach that need Phase I.Analysis of the results discussed approaches which are suitable for different levels of personalization. This study presents two new proposals of self-start statistical process control procedures for implementing quality control charts in mass customized production environments. Mass customization is characterized by high variety and/or low production volumes, or by current production strategies that increase the necessity of flexibility. The proposed approaches consider that the quality characteristic under monitoring is the same among the items, allowing the possibility of different averages and standard deviations for different products into the series. Analysis of the proposed approaches include assumption violations and the comparison of the new approaches to a Phase I-Phase II residual control chart, as benchmark. Simulation results show that one of the proposed approaches ties in performance with the benchmark, and the other can produce faster detection of deviations from the process mean, both without the need of a Phase I - retrospective analysis, thus increasing suitability for application in real environments. Also, the proposed approaches are made to handle with unitary production lots as well. Analysis performed in an extensive simulation procedure presents that normality assumption violation (with higher asymmetry) has the most degenerative effect on all tested control charts. The study also shown decreasing in performance on scenarios that violated the independence of the observations assumption.},
  keywords = {Mass customization,Quality control for flexible production,Statistical process control}
}

@inproceedings{kotecha1999,
  title = {Gibbs Sampling Approach for Generation of Truncated Multivariate {{Gaussian}} Random Variables},
  booktitle = {1999 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}.},
  author = {Kotecha, J.H. and Djuric, P.M.},
  year = {1999},
  pages = {1757-1760 vol.3},
  publisher = {{IEEE}},
  doi = {10.1109/ICASSP.1999.756335},
  isbn = {978-0-7803-5041-0},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Kotecha_Djuric_1999_Gibbs_sampling_approach_for_generation_of_truncated_multivariate_Gaussian2.pdf}
}

@book{kotz1992,
  title = {Breakthroughs in Statistics},
  editor = {Kotz, Samuel and Johnson, Norman Lloyd},
  year = {1992},
  series = {Springer Series in Statistics},
  publisher = {{Springer-Verlag}},
  address = {{New York}},
  isbn = {978-0-387-97566-5 978-3-540-97566-3 978-0-387-94037-3 978-3-540-94037-1 978-0-387-97572-6 978-3-540-97572-4 978-0-387-94039-7 978-3-540-94039-5 978-0-387-94988-8 978-0-387-94989-5},
  lccn = {QA276 .B68465 1992},
  keywords = {Mathematical statistics},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Kotz_Johnson_1992_Breakthroughs_in_statistics.pdf}
}

@book{kotz1997,
  title = {Breakthroughs in {{Statistics}}: {{Volume III}}},
  shorttitle = {Breakthroughs in {{Statistics}}},
  editor = {Kotz, Samuel and Johnson, Norman L.},
  year = {1997},
  month = aug,
  edition = {1st edition},
  publisher = {{Springer}},
  address = {{New York}},
  isbn = {978-0-387-94988-8},
  langid = {english}
}

@article{kowal2020,
  title = {Simultaneous Transformation and Rounding ({{STAR}}) Models for Integer-Valued Data},
  author = {Kowal, Daniel R. and Canale, Antonio},
  year = {2020},
  journal = {Electronic Journal of Statistics},
  volume = {14},
  number = {1},
  pages = {1744--1772},
  publisher = {{The Institute of Mathematical Statistics and the Bernoulli Society}},
  issn = {1935-7524},
  doi = {10.1214/20-EJS1707},
  abstract = {We propose a simple yet powerful framework for modeling integer-valued data, such as counts, scores, and rounded data. The data-generating process is defined by Simultaneously Transforming and Rounding (STAR) a continuous-valued process, which produces a flexible family of integer-valued distributions capable of modeling zero-inflation, bounded or censored data, and over- or underdispersion. The transformation is modeled as unknown for greater distributional flexibility, while the rounding operation ensures a coherent integer-valued data-generating process. An efficient MCMC algorithm is developed for posterior inference and provides a mechanism for adaptation of successful Bayesian models and algorithms for continuous data to the integer-valued data setting. Using the STAR framework, we design a new Bayesian Additive Regression Tree model for integer-valued data, which demonstrates impressive predictive distribution accuracy for both synthetic data and a large healthcare utilization dataset. For interpretable regression-based inference, we develop a STAR additive model, which offers greater flexibility and scalability than existing integer-valued models. The STAR additive model is applied to study the recent decline in Amazon river dolphins.},
  langid = {english},
  mrnumber = {MR4083734},
  zmnumber = {07200242},
  keywords = {Additive models,BART,count data,done,nonparametric regression,prediction},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Kowal_Canale_2020_Simultaneous_transformation_and_rounding_(STAR)_models_for_integer-valued_data.pdf;/home/dede/Zotero/storage/8NGGW3M8/1586937696.html}
}

@article{kraus2017,
  title = {Classification of Functional Fragments by Regularized Linear Classifiers with Domain Selection},
  author = {Kraus, David and Stefanucci, Marco},
  year = {2017},
  month = aug,
  journal = {arXiv:1708.08257 [stat]},
  eprint = {1708.08257},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {We consider the problem of classification of functional data into two groups by linear classifiers based on one-dimensional projections of functions. We reformulate the task to find the best classifier as an optimization problem and solve it by regularization techniques, namely the conjugate gradient method with early stopping, the principal component method and the ridge method. We study the empirical version with finite training samples consisting of incomplete functions observed on different subsets of the domain and show that the optimal, possibly zero, misclassification probability can be achieved in the limit along a possibly non-convergent empirical regularization path. Being able to work with fragmentary training data we propose a domain extension and selection procedure that finds the best domain beyond the common observation domain of all curves. In a simulation study we compare the different regularization methods and investigate the performance of domain selection. Our methodology is illustrated on a medical data set, where we observe a substantial improvement of classification accuracy due to domain extension.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Kraus_Stefanucci_2017_Classification_of_functional_fragments_by_regularized_linear_classifiers_with.pdf;/home/dede/Zotero/storage/XF2FLCTT/1708.html}
}

@article{kreiss2011,
  title = {Bootstrap Methods for Dependent Data: {{A}} Review},
  shorttitle = {Bootstrap Methods for Dependent Data},
  author = {Kreiss, Jens-Peter and Paparoditis, Efstathios},
  year = {2011},
  month = dec,
  journal = {Journal of the Korean Statistical Society},
  volume = {40},
  number = {4},
  pages = {357--378},
  issn = {1226-3192},
  doi = {10.1016/j.jkss.2011.08.009},
  abstract = {This paper gives a review on a variety of bootstrap methods for dependent data. The main focus is not on an exhaustive listing and description of bootstrap procedures but on general principles which should be taken into account when selecting a particular bootstrap procedure in order to approximate the (properly standardized) distribution of a statistic of interest. Questions are considered related to which dependence properties of the underlying data generating process asymptotically influence the distribution of the statistic of interest and which dependence properties (or even which process) a particular bootstrap method really mimics. For answering these questions we introduce the concept of a companion stochastic process. As statistics we consider generalized means, and integrated periodogram statistics (including ratio statistics) as well as nonparametric estimators.},
  langid = {english},
  keywords = {Bootstrap methods,doing,Stochastic processes,Time series},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Kreiss_Paparoditis_2011_Bootstrap_methods_for_dependent_data.pdf;/home/dede/Zotero/storage/II8RR8DT/S1226319211000780.html}
}

@book{kreyszig1989,
  title = {Introductory {{Functional Analysis}} with {{Applications}}},
  author = {Kreyszig, Erwin},
  year = {1989},
  month = feb,
  edition = {1 edition},
  publisher = {{Wiley}},
  address = {{New York}},
  abstract = {Provides avenues for applying functional analysis to the practical study of natural sciences as well as mathematics. Contains worked problems on Hilbert space theory and on Banach spaces and emphasizes concepts, principles, methods and major applications of functional analysis.},
  isbn = {978-0-471-50459-7},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Kreyszig_1989_Introductory_Functional_Analysis_with_Applications.pdf}
}

@book{kroese2013,
  title = {{Statistical Modeling and Computation}},
  author = {Kroese, Dirk P. and Chan, Joshua C. C.},
  year = {2013},
  month = nov,
  publisher = {{Springer-Nature New York Inc}},
  address = {{New York}},
  abstract = {This textbook on statistical modeling and statistical inference will assist advanced undergraduate and graduate students. Statistical Modeling and Computation\&nbsp;provides a unique introduction to modern Statistics from both classical and Bayesian perspectives. It also offers\&nbsp;an integrated treatment of Mathematical Statistics and modern statistical computation, emphasizing statistical modeling, computational techniques, and applications. Each of the three parts will cover topics essential to university courses. Part I covers the fundamentals of probability theory. In Part II, the authors introduce a wide variety of classical models that include, among others, linear regression and ANOVA models. In Part III,\&nbsp;the authors\&nbsp;address the statistical analysis and computation of various advanced models, such as generalized linear, state-space and Gaussian models. Particular attention is paid to fast Monte Carlo techniques for Bayesian inference on these models. Throughout the book the authors\&nbsp;include a large number of illustrative examples and solved problems. The book also features a section with solutions, an appendix that serves as a MATLAB primer, and a mathematical supplement.?},
  isbn = {978-1-4614-8774-6},
  langid = {Inglese}
}

@article{kuchibhotla2020,
  title = {Valid Post-Selection Inference in Model-Free Linear Regression},
  author = {Kuchibhotla, Arun K. and Brown, Lawrence D. and Buja, Andreas and Cai, Junhui and George, Edward I. and Zhao, Linda H.},
  year = {2020},
  month = oct,
  journal = {The Annals of Statistics},
  volume = {48},
  number = {5},
  pages = {2953--2981},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/19-AOS1917},
  abstract = {Modern data-driven approaches to modeling make extensive use of covariate/model selection. Such selection incurs a cost: it invalidates classical statistical inference. A conservative remedy to the problem was proposed by Berk et al. (Ann. Statist. 41 (2013) 802\textendash 837) and further extended by Bachoc, Preinerstorfer and Steinberger (2016). These proposals, labeled ``PoSI methods,'' provide valid inference after arbitrary model selection. They are computationally NP-hard and have limitations in their theoretical justifications. We therefore propose computationally efficient confidence regions, named ``UPoSI' (``U'' is for ``uniform'' or ``universal.'') and prove large-\$p\$ asymptotics for them. We do this for linear OLS regression allowing misspecification of the normal linear model, for both fixed and random covariates, and for independent as well as some types of dependent data. We start by proving a general equivalence result for the post-selection inference problem and a simultaneous inference problem in a setting that strips inessential features still present in a related result of Berk et al. (Ann. Statist. 41 (2013) 802\textendash 837). We then construct valid PoSI confidence regions that are the first to have vastly improved computational efficiency in that the required computation times grow only quadratically rather than exponentially with the total number \$p\$ of covariates. These are also the first PoSI confidence regions with guaranteed asymptotic validity when the total number of covariates \$p\$ diverges (almost exponentially) with the sample size \$n\$. Under standard tail assumptions, we only require \$(\textbackslash log p)\^\{7\}=o(n)\$ and \$k=o(\textbackslash sqrt\{n/\textbackslash log p\})\$ where \$k\$ (\$\textbackslash le p\$) is the largest number of covariates (model size) considered for selection. We study various properties of these confidence regions, including their Lebesgue measures, and compare them theoretically with those proposed previously.},
  keywords = {62F12,62F25,62F40,62J05,Concentration inequalities,done,high-dimensional linear regression,Model selection,multiplier bootstrap,Orlicz norms,simultaneous inference,uniform consistency},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Kuchibhotla_et_al_2020_Valid_post-selection_inference_in_model-free_linear_regression.pdf;/home/dede/Zotero/storage/6IC5CMMK/19-AOS1917.html}
}

@article{kucukelbir2015,
  title = {Automatic {{Variational Inference}} in {{Stan}}},
  author = {Kucukelbir, Alp and Ranganath, Rajesh and Gelman, Andrew and Blei, David M.},
  year = {2015},
  month = jun,
  journal = {arXiv:1506.03431 [stat]},
  eprint = {1506.03431},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Variational inference is a scalable technique for approximate Bayesian inference. Deriving variational inference algorithms requires tedious model-specific calculations; this makes it difficult to automate. We propose an automatic variational inference algorithm, automatic differentiation variational inference (ADVI). The user only provides a Bayesian model and a dataset; nothing else. We make no conjugacy assumptions and support a broad class of models. The algorithm automatically determines an appropriate variational family and optimizes the variational objective. We implement ADVI in Stan (code available now), a probabilistic programming framework. We compare ADVI to MCMC sampling across hierarchical generalized linear models, nonconjugate matrix factorization, and a mixture model. We train the mixture model on a quarter million images. With ADVI we can use variational inference on any model we write in Stan.},
  archiveprefix = {arXiv},
  keywords = {done,Statistics - Machine Learning},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Kucukelbir_et_al_2015_Automatic_Variational_Inference_in_Stan.pdf;/home/dede/Zotero/storage/K2IA66PK/1506.html}
}

@article{kumar2017,
  title = {Bayesian {{Monitoring}} of {{Times Between Events}}: {{The Shewhart}} \$t\_r\$-{{Chart}}},
  shorttitle = {Bayesian {{Monitoring}} of {{Times Between Events}}},
  author = {Kumar, Nirpeksh and Chakraborti, Subha},
  year = {2017},
  month = apr,
  journal = {Journal of Quality Technology},
  volume = {49},
  number = {2},
  pages = {136--154},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2017.11917985},
  abstract = {The traditional (frequentist) tr-chart is a Shewhart-type chart useful for monitoring times between events (interarrival times) following an exponential distribution. This problem often arises in high-yield processes where the defect rate is low and hence the conventional attribute charts such as the c-chart and the u-chart are often ineffective. We consider this problem under the Bayesian framework and propose a Bayesian tr-chart when the exponential rate parameter is unknown. The Bayesian tr-chart is a Shewhart-type chart that incorporates parameter uncertainty via a prior and a posterior distribution, unlike the traditional tr-chart. The control limits are constructed from the predictive distribution of a plotting statistic. The performance of the proposed chart is evaluated and comparisons are made with the traditional tr-chart. The Bayesian chart is seen to be advantageous in certain situations. An illustrative example is given and a summary and conclusions are offered.},
  keywords = {Bayesian and Frequentist Control Charts,Exponential Distribution,Performance Metrics,Predictive Distribution,Prior and Posterior Distributions},
  annotation = {\_eprint: https://doi.org/10.1080/00224065.2017.11917985},
  file = {/home/dede/Zotero/storage/DZFJFHME/00224065.2017.html}
}

@article{kunsch1989,
  title = {The {{Jackknife}} and the {{Bootstrap}} for {{General Stationary Observations}}},
  author = {Kunsch, Hans R.},
  year = {1989},
  month = sep,
  journal = {The Annals of Statistics},
  volume = {17},
  number = {3},
  pages = {1217--1241},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/aos/1176347265},
  abstract = {We extend the jackknife and the bootstrap method of estimating standard errors to the case where the observations form a general stationary sequence. We do not attempt a reduction to i.i.d. values. The jackknife calculates the sample variance of replicates of the statistic obtained by omitting each block of \$l\$ consecutive data once. In the case of the arithmetic mean this is shown to be equivalent to a weighted covariance estimate of the spectral density of the observations at zero. Under appropriate conditions consistency is obtained if \$l = l(n) \textbackslash rightarrow \textbackslash infty\$ and \$l(n)/n \textbackslash rightarrow 0\$. General statistics are approximated by an arithmetic mean. In regular cases this approximation determines the asymptotic behavior. Bootstrap replicates are constructed by selecting blocks of length \$l\$ randomly with replacement among the blocks of observations. The procedures are illustrated by using the sunspot numbers and some simulated data.},
  keywords = {62G05,62G15,62M10,bootstrap,influence function,jackknife,statistics defined by functionals,time series,variance estimation},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Kunsch_1989_The_Jackknife_and_the_Bootstrap_for_General_Stationary_Observations.pdf;/home/dede/Zotero/storage/QIMTY66R/1176347265.html}
}

@book{kushner2003,
  title = {Stochastic {{Approximation}} and {{Recursive Algorithms}} and {{Applications}}},
  author = {Kushner, Harold and Yin, G. George},
  year = {2003},
  edition = {Second},
  publisher = {{Springer}},
  address = {{New York}},
  isbn = {978-1-4419-1847-5},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Kushner_Yin_2010_Stochastic_Approximation_and_Recursive_Algorithms_and_Applications.pdf}
}

@book{kwon2016,
  title = {{Julia Programming for Operations Research: A Primer on Computing}},
  shorttitle = {{Julia Programming for Operations Research}},
  author = {Kwon, Changhyun},
  year = {2016},
  month = may,
  publisher = {{CreateSpace Independent Publishing Platform}},
  address = {{Charleston, SC}},
  abstract = {Last Updated: May 2018The main motivation of writing this book was to help the author himself. He is a professor in the field of operations research, and his daily activities involve building models of mathematical optimization, developing algorithms for solving the problems, implementing those algorithms using computer programming languages, experimenting with data, etc. Three languages are involved: human language, mathematical language, and computer language. His team of students need to go over three different languages, which requires "translation" among the three languages. As this book was written to teach his research group how to translate, this book will also be useful for anyone who needs to learn how to translate in a similar situation.The Julia Language is as fast as C, as convenient as MATLAB, and as general as Python with a flexible algebraic modeling language for mathematical optimization problems. With the great support from Julia developers, especially the developers of the JuMP\textemdash Julia for Mathematical Programming\textemdash package, Julia makes a perfect tool for students and professionals in operations research and related areas such as industrial engineering, management science, transportation engineering, economics, and regional science.For more information, visit: http://www.chkwon.net/julia},
  isbn = {978-1-5333-2879-3},
  langid = {Inglese}
}

@book{kwong2020,
  title = {{Hands-On Design Patterns and Best Practices with Julia: Proven solutions to common problems in software design for Julia 1.x}},
  shorttitle = {{Hands-On Design Patterns and Best Practices with Julia}},
  author = {Kwong, Tom and Karpinski, Stefan},
  year = {2020},
  month = jan,
  edition = {1 edizione},
  publisher = {{Packt Publishing}},
  abstract = {Design and develop high-performance, reusable, and maintainable applications using traditional and modern Julia patterns with this comprehensive guideKey FeaturesExplore useful design patterns along with object-oriented programming in Julia 1.0Implement macros and metaprogramming techniques to make your code faster, concise, and efficientDevelop the skills necessary to implement design patterns for creating robust and maintainable applicationsBook DescriptionDesign patterns are fundamental techniques for developing reusable and maintainable code. They provide a set of proven solutions that allow developers to solve problems in software development quickly. This book will demonstrate how to leverage design patterns with real-world applications.Starting with an overview of design patterns and best practices in application design, you'll learn about some of the most fundamental Julia features such as modules, data types, functions/interfaces, and metaprogramming. You'll then get to grips with the modern Julia design patterns for building large-scale applications with a focus on performance, reusability, robustness, and maintainability. The book also covers anti-patterns and how to avoid common mistakes and pitfalls in development. You'll see how traditional object-oriented patterns can be implemented differently and more effectively in Julia. Finally, you'll explore various use cases and examples, such as how expert Julia developers use design patterns in their open source packages.By the end of this Julia programming book, you'll have learned methods to improve software design, extensibility, and reusability, and be able to use design patterns efficiently to overcome common challenges in software development.What you will learnMaster the Julia language features that are key to developing large-scale software applicationsDiscover design patterns to improve overall application architecture and designDevelop reusable programs that are modular, extendable, performant, and easy to maintainWeigh up the pros and cons of using different design patterns for use casesExplore methods for transitioning from object-oriented programming to using equivalent or more advanced Julia techniquesWho this book is forThis book is for beginner to intermediate-level Julia programmers who want to enhance their skills in designing and developing large-scale applications. Table of ContentsDesign Patterns and Related PrinciplesModules, Packages, and Data Type ConceptsDesigning Functions and InterfacesMacros and Meta Programming TechniquesReusability PatternsPerformance PatternsMaintainability PatternsRobustness PatternsMiscellaneous PatternsAnti-PatternsObject Oriented Traditional PatternsInheritance and Variance},
  langid = {Inglese},
  keywords = {julia,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Kwong_Karpinski_2020_Hands-On_Design_Patterns_and_Best_Practices_with_Julia.pdf}
}

@incollection{lad2016,
  title = {Stable {{Estimation}}},
  booktitle = {Wiley {{StatsRef}}: {{Statistics Reference Online}}},
  author = {Lad, Frank},
  editor = {Balakrishnan, N. and Colton, Theodore and Everitt, Brian and Piegorsch, Walter and Ruggeri, Fabrizio and Teugels, Jozef L.},
  year = {2016},
  month = may,
  pages = {1--2},
  publisher = {{John Wiley \& Sons, Ltd}},
  address = {{Chichester, UK}},
  doi = {10.1002/9781118445112.stat00248.pub2},
  isbn = {978-1-118-44511-2},
  langid = {english},
  keywords = {todo},
  file = {/home/dede/Zotero/storage/VZ7HTKJU/Lad - 2016 - Stable Estimation.pdf}
}

@book{lahiri2003,
  title = {Resampling {{Methods}} for {{Dependent Data}}},
  author = {Lahiri, S. N.},
  year = {2003},
  month = aug,
  edition = {2003rd edition},
  publisher = {{Springer}},
  address = {{New York}},
  isbn = {978-0-387-00928-5},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Lahiri_2003_Resampling_Methods_for_Dependent_Data.pdf}
}

@article{lai1976,
  title = {On {{Confidence Sequences}}},
  author = {Lai, Tze Leung},
  year = {1976},
  month = mar,
  journal = {The Annals of Statistics},
  volume = {4},
  number = {2},
  pages = {265--280},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/aos/1176343406},
  abstract = {This paper is concerned with confidence sequences, i.e., sequences of confidence regions which contain the true parameter for every sample size simultaneously at a prescribed level of confidence. By making use of generalized likelihood ratio martingales, confidence sequences are constructed for the unknown parameters of the binomial, Poisson, uniform, gamma and other distributions. It is proved that for the exponential family of distributions, the method of using generalized likelihood ratio martingales leads to a sequence of intervals which have the desirable property of eventually shrinking to the population parameter. The problem of nuisance parameters is considered, and in this connection, boundary crossing probabilities are obtained for the sequence of Student's \$t\$-statistics, and a limit theorem relating to the boundary crossing probabilities for the Wiener process is proved.},
  keywords = {62F25,62L10,boundary crossing probabilities,Confidence sequences,Invariance,likelihood ratio martingales,nuisance parameters,Student's $t$-statistics,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Lai_1976_On_Confidence_Sequences.pdf;/home/dede/Zotero/storage/ARN56A7C/1176343406.html}
}

@article{lai1985,
  title = {Asymptotically Efficient Adaptive Allocation Rules},
  author = {Lai, T. L and Robbins, Herbert},
  year = {1985},
  month = mar,
  journal = {Advances in Applied Mathematics},
  volume = {6},
  number = {1},
  pages = {4--22},
  issn = {0196-8858},
  doi = {10.1016/0196-8858(85)90002-8},
  abstract = {Animals, humans, and organizations are known to adjust how (much) they explore complex environments that exceed their information processing capacity, rather than relentlessly search for the optimal action. The adjusted depth of exploration is supposed to depend on the aspiration level internal to the agent. This action selection tendency is known as satisficing. The Risk-sensitive Satisficing (RS) model implements satisficing in the reinforcement learning framework through conversion of action values into gains (or losses) relative to the aspiration level. The risk-sensitive evaluation of action values by RS has been shown to be effective in reinforcement learning. In this paper, first we analyze RS in comparison with UCB and Thompson sampling algorithms. We also show that RS shows differential risk-attitudes considering the risks. Then we propose the Softsatisficing policy that is a stochastic equivalent of RS and further analyze the exploratory behavior of risk-sensitive satisficing that RS and Softsatisficing implement. We emphasize that Softsatisficing has the potential of modeling risk-sensitive foraging and other decision-making behaviors by humans, animals, and organizations. In reinforcement learning (RL), the intrinsic reward estimation is necessary for policy learning when the extrinsic reward is sparse or absent. To this end, Unified Curiosity-driven Learning with Smoothed intrinsic reward Estimation (UCLSE) is proposed to address the sparse extrinsic reward problem from the perspective of completeness of intrinsic reward estimation. We further propose state distribution-aware weighting method and policy-aware weighting method to dynamically unify two mainstream intrinsic reward estimation methods. In this way, the agent can explore the environment more effectively and efficiently. Under this framework, we propose to employ an attention module to extract task-relevant features for a more precise estimation of intrinsic reward. Moreover, we propose to improve the robustness of policy learning by smoothing the intrinsic reward with a batch of transitions close to the current transition. Extensive experimental results on Atari games demonstrate that our method outperforms the state-of-the-art approaches in terms of both score and training efficiency.},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Lai_Robbins_1985_Asymptotically_efficient_adaptive_allocation_rules.pdf;/home/dede/Zotero/storage/WN6QN9VF/0196885885900028.html}
}

@article{lai2001,
  title = {Sequential {{Analysis}}: {{Some Classical Problems}} and {{New Challenges}}},
  shorttitle = {{{SEQUENTIAL ANALYSIS}}},
  author = {Lai, Tze Leung},
  year = {2001},
  journal = {Statistica Sinica},
  volume = {11},
  number = {2},
  pages = {303--351},
  publisher = {{Institute of Statistical Science, Academia Sinica}},
  issn = {1017-0405},
  abstract = {We give a brief review of the developments in several classical problems of sequential analysis and their applications to biomedicine, economics and engineering. Even though it can only focus on a limited number of topics, the review shows that sequential analysis is still a vibrant subject after six decades of continual development, with fresh ideas brought in from various fields of application and through interactions with other branches of statistics and probability. We conclude with some remarks on the opportunities and challenges ahead.}
}

@article{lai2010,
  title = {Sequential {{Change-Point Detection When}} the {{Pre-}} and {{Post-Change Parameters}} Are {{Unknown}}},
  author = {Lai, Tze Leung and Xing, Haipeng},
  year = {2010},
  month = apr,
  journal = {Sequential Analysis},
  volume = {29},
  number = {2},
  pages = {162--175},
  publisher = {{Taylor \& Francis}},
  issn = {0747-4946},
  doi = {10.1080/07474941003741078},
  abstract = {We describe asymptotically optimal Bayesian and frequentist solutions to the problem of sequential change-point detection in multiparameter exponential families when the pre- and post-change parameters are unknown. In this connection we also address certain issues recently raised by Mei (2008) concerning performance criteria for detection rules in this setting.},
  keywords = {62F15,62L12,62M05,Asymptotic optimality,Bayes,Detection delay,False alarm rate,Generalized likelihood ratio},
  annotation = {\_eprint: https://doi.org/10.1080/07474941003741078},
  file = {/home/dede/Zotero/storage/MHXZYS66/07474941003741078.html}
}

@article{lakatos1968,
  title = {Criticism and the {{Methodology}} of {{Scientific Research Programmes}}},
  author = {Lakatos, Imre},
  year = {1968},
  journal = {Proceedings of the Aristotelian Society},
  volume = {69},
  pages = {149--186},
  publisher = {{[Aristotelian Society, Wiley]}},
  issn = {0066-7374},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Lakatos_1968_Criticism and the Methodology of Scientific Research Programmes.pdf}
}

@inproceedings{lakshminarayanan2015,
  title = {Particle {{Gibbs}} for {{Bayesian Additive Regression Trees}}},
  booktitle = {Artificial {{Intelligence}} and {{Statistics}}},
  author = {Lakshminarayanan, Balaji and Roy, Daniel and Teh, Yee Whye},
  year = {2015},
  month = feb,
  pages = {553--561},
  publisher = {{PMLR}},
  issn = {1938-7228},
  abstract = {Additive regression trees are flexible non-parametric models and popular off-the-shelf tools for real-world non-linear regression. In application domains, such as bioinformatics, where there is als...},
  langid = {english},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Lakshminarayanan_et_al_2015_Particle_Gibbs_for_Bayesian_Additive_Regression_Trees.pdf;/home/dede/Zotero/storage/GDI5G5X7/lakshminarayanan15.html}
}

@article{lall2021,
  title = {The {{MIDAS Touch}}: {{Accurate}} and {{Scalable Missing-Data Imputation}} with {{Deep Learning}}},
  shorttitle = {The {{MIDAS Touch}}},
  author = {Lall, Ranjit and Robinson, Thomas},
  year = {2021},
  month = feb,
  journal = {Political Analysis},
  pages = {1--18},
  publisher = {{Cambridge University Press}},
  issn = {1047-1987, 1476-4989},
  doi = {10.1017/pan.2020.49},
  abstract = {Principled methods for analyzing missing values, based chiefly on multiple imputation, have become increasingly popular yet can struggle to handle the kinds of large and complex data that are also becoming common. We propose an accurate, fast, and scalable approach to multiple imputation, which we call MIDAS (Multiple Imputation with Denoising Autoencoders). MIDAS employs a class of unsupervised neural networks known as denoising autoencoders, which are designed to reduce dimensionality by corrupting and attempting to reconstruct a subset of data. We repurpose denoising autoencoders for multiple imputation by treating missing values as an additional portion of corrupted data and drawing imputations from a model trained to minimize the reconstruction error on the originally observed portion. Systematic tests on simulated as well as real social science data, together with an applied example involving a large-scale electoral survey, illustrate MIDAS's accuracy and efficiency across a range of settings. We provide open-source software for implementing MIDAS.},
  langid = {english},
  keywords = {done,imputation methods,machine learning,missing data,multiple imputation},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Lall_Robinson_2021_The_MIDAS_Touch.pdf;/home/dede/Zotero/storage/I2SZ8WTU/5007854F57E88AF16D69BCCA4C5AF1FF.html}
}

@misc{lall2021,
  title = {{{MIDASverse}}/{{rMIDAS}}},
  author = {Lall, Ranjit and Robinson, Thomas},
  year = {2021},
  month = mar,
  abstract = {R package for missing-data imputation with deep learning},
  copyright = {View license         ,                 View license},
  howpublished = {MIDASverse},
  keywords = {deep-learning,done,imputation-methods,neural-network,r,reticulate,tensorflow}
}

@article{lange2014,
  title = {Nonconvex {{Optimization}} via {{MM Algorithms}}: {{Convergence Theory}}},
  shorttitle = {Nonconvex {{Optimization}} via {{MM Algorithms}}},
  author = {Lange, Kenneth and Won, Joong-Ho and Landeros, Alfonso and Zhou, Hua},
  year = {2014},
  month = apr,
  journal = {arXiv:2106.02805 [math, stat]},
  eprint = {2106.02805},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  doi = {10.1002/9781118445112.stat08295},
  abstract = {The majorization-minimization (MM) principle is an extremely general framework for deriving optimization algorithms. It includes the expectation-maximization (EM) algorithm, proximal gradient algorithm, concave-convex procedure, quadratic lower bound algorithm, and proximal distance algorithm as special cases. Besides numerous applications in statistics, optimization, and imaging, the MM principle finds wide applications in large scale machine learning problems such as matrix completion, discriminant analysis, and nonnegative matrix factorizations. When applied to nonconvex optimization problems, MM algorithms enjoy the advantages of convexifying the objective function, separating variables, numerical stability, and ease of implementation. However, compared to the large body of literature on other optimization algorithms, the convergence analysis of MM algorithms is scattered and problem specific. This survey presents a unified treatment of the convergence of MM algorithms. With modern applications in mind, the results encompass non-smooth objective functions and non-asymptotic analysis.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Optimization and Control,Statistics - Computation,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Lange_et_al_2014_Nonconvex_Optimization_via_MM_Algorithms.pdf;/home/dede/Zotero/storage/D92FAF6Q/2106.html}
}

@book{lange2015,
  title = {Optimization},
  author = {Lange, Kenneth},
  year = {2015},
  month = apr,
  edition = {2nd ed. 2013 edition},
  publisher = {{Springer}},
  address = {{New York}},
  isbn = {978-1-4899-9270-3},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Lange_2015_Optimization.pdf}
}

@book{lauritzen1996,
  title = {Graphical {{Models}}},
  author = {Lauritzen, Steffen L.},
  year = {1996},
  month = may,
  publisher = {{Clarendon Press}},
  abstract = {The idea of modelling systems using graph theory has its origin in several scientific areas: in statistical physics (the study of large particle systems), in genetics (studying inheritable properties of natural species), and in interactions in contingency tables. The use of graphical models in statistics has increased considerably over recent years and the theory has been greatly developed and extended. This book provides the first comprehensive and authoritative account of the theory of graphical models and is written by a leading expert in the field. It contains the fundamental graph theory required and a thorough study of Markov properties associated with various type of graphs. The statistical theory of log-linear and graphical models for contingency tables, covariance selection models, and graphical models with mixed discrete-continous variables in developed detail. Special topics, such as the application of graphical models to probabilistic expert systems, are described briefly, and appendices give details of the multivarate normal distribution and of the theory of regular exponential families. The author has recently been awarded the RSS Guy Medal in Silver 1996 for his innovative contributions to statistical theory and practice, and especially for his work on graphical models.},
  googlebooks = {mGQWkx4guhAC},
  isbn = {978-0-19-159122-8},
  langid = {english},
  keywords = {Mathematics / Graphic Methods,Mathematics / Probability \& Statistics / General}
}

@article{lavine1994,
  title = {More {{Aspects}} of {{Polya Tree Distributions}} for {{Statistical Modelling}}},
  author = {Lavine, Michael},
  year = {1994},
  month = sep,
  journal = {The Annals of Statistics},
  volume = {22},
  number = {3},
  pages = {1161--1176},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/aos/1176325623},
  abstract = {The definition and elementary properties of Polya tree distributions are reviewed. Two theorems are presented showing that Polya trees can be constructed to concentrate arbitrarily closely about any desired pdf, and that Polya tree priors can put positive mass in every relative entropy neighborhood of every positive density with finite entropy, thereby satisfying a consistency condition. Such theorems are false for Dirichlet processes. Models are constructed combining partially specified Polya trees with other information such as monotonicity or unimodality. It is shown how to compute bounds on posterior expectations over the class of all priors with the given specifications. A numerical example is given. A theorem of Diaconis and Freedman about Dirichlet processes is generalized to Polya trees, allowing Polya trees to be the models for errors in regression problems. Finally, empirical Bayes models using Dirichlet processes are generalized to Polya trees. An example from Berry and Christensen is reanalyzed with a Polya tree model.},
  keywords = {62A15,62G07,62G99,Dirichlet processes,Nonparametric regression,Robust Bayes,tail-free processes},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Lavine_1994_More_Aspects_of_Polya_Tree_Distributions_for_Statistical_Modelling.pdf;/home/dede/Zotero/storage/WDM5FPJZ/1176325623.html}
}

@book{lawler2006,
  title = {Introduction to {{Stochastic Processes}}},
  author = {Lawler, Gregory},
  year = {2006},
  series = {Chapman \& {{Hall}}/{{CRC Probability Series}}},
  edition = {Second},
  publisher = {{Chapman and Hall/CRC}},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Lawler_2006_Introduction_to_Stochastic_Processes.pdf}
}

@article{lawless2012,
  title = {Monitoring {{Warranty Claims With Cusums}}},
  author = {Lawless, J. F. and Crowder, M. J. and Lee, K.-A.},
  year = {2012},
  journal = {Technometrics},
  volume = {54},
  number = {3},
  pages = {269--278},
  publisher = {{Taylor \& Francis, Ltd.}},
  issn = {0040-1706},
  abstract = {Manufacturers monitor products in field use with respect to the quality and reliability of their performance, and warranty claims data are one valuable source of information. Updated warranty claims data are examined periodically by manufacturers, and formal monitoring procedures are a useful adjunct that can help identify emerging problems. This article presents cusum procedures for monitoring claims, designed to allow changes in claim rates to be detected in as timely a manner as possible. The determination of plans with given signal probabilities under nominal and increased rates is based on Markov chain calculations which are easy to implement. The procedures are motivated by and illustrated on warranty claims for North American automobiles. This article has online supplementary material.},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Lawless_et_al_2012_Monitoring_Warranty_Claims_With_Cusums.pdf}
}

@book{lawson2021,
  title = {{An Introduction to Acceptance Sampling and SPC with R}},
  author = {Lawson, John},
  year = {2021},
  month = feb,
  edition = {1\textdegree{} edizione},
  publisher = {{Chapman and Hall/CRC}},
  address = {{Boca Raton}},
  isbn = {978-0-367-56995-2},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Lawson_2021_An_Introduction_to_Acceptance_Sampling_and_SPC_with_R.pdf}
}

@book{lax2002,
  title = {{Functional Analysis}},
  author = {Lax, Peter D.},
  year = {2002},
  month = mar,
  edition = {1. edizione},
  publisher = {{Wiley-Interscience}},
  address = {{New York}},
  abstract = {Includes sections on the spectral resolution and spectral representation of self adjoint operators, invariant subspaces, strongly continuous one-parameter semigroups, the index of operators, the trace formula of Lidskii, the Fredholm determinant, and more. * Assumes prior knowledge of Naive set theory, linear algebra, point set topology, basic complex variable, and real variables. * Includes an appendix on the Riesz representation theorem.},
  isbn = {978-0-471-55604-6},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Lax_2002_Functional_Analysis.pdf}
}

@book{lazar2010,
  title = {{The Statistical Analysis of Functional MRI Data}},
  author = {Lazar, Nicole A.},
  year = {2010},
  month = nov,
  publisher = {{Springer}},
  abstract = {This book is a primer for readers interested in learning more about this fascinating subject and the many statistical challenges inherent in functional neuroimaging data. It presents the basics of technique and surveys popular statistical approaches.},
  isbn = {978-1-4419-2679-1},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Lazar_2010_The_Statistical_Analysis_of_Functional_MRI_Data.pdf}
}

@article{lazaro-gredilla2010,
  title = {Sparse {{Spectrum Gaussian Process Regression}}},
  author = {{L{\'a}zaro-Gredilla}, Miguel and {Qui{\~n}nero-Candela}, Joaquin and Rasmussen, Carl Edward and An\&\#237 and {Figueiras-Vidal}, bal R.},
  year = {2010},
  journal = {Journal of Machine Learning Research},
  volume = {11},
  number = {63},
  pages = {1865--1881},
  issn = {1533-7928},
  abstract = {We present a new sparse Gaussian Process (GP) model for regression. The key novel idea is to sparsify the spectral representation of the GP. This leads to a simple, practical algorithm for regression tasks. We compare the achievable trade-offs between predictive accuracy and computational requirements, and show that these are typically superior to existing state-of-the-art sparse approximations. We discuss both the weight space and function space representations, and note that the new construction implies priors over functions which are always stationary, and can approximate any covariance function in this class.},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/LÃ¡zaro-Gredilla_et_al_2010_Sparse_Spectrum_Gaussian_Process_Regression.pdf}
}

@incollection{lecun2012,
  title = {Efficient {{BackProp}}},
  booktitle = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}: {{Second Edition}}},
  author = {LeCun, Yann A. and Bottou, L{\'e}on and Orr, Genevieve B. and M{\"u}ller, Klaus-Robert},
  editor = {Montavon, Gr{\'e}goire and Orr, Genevi{\`e}ve B. and M{\"u}ller, Klaus-Robert},
  year = {2012},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {9--48},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-35289-8_3},
  abstract = {The convergence of back-propagation learning is analyzed so as to explain common phenomenon observed by practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposed in serious technical publications. This paper gives some of those tricks, and offers explanations of why they work.Many authors have suggested that second-order optimization methods are advantageous for neural net training. It is shown that most ``classical'' second-order methods are impractical for large neural networks. A few methods are proposed that do not have these limitations.},
  isbn = {978-3-642-35289-8},
  langid = {english},
  keywords = {Conjugate Gradient,done,Gradient Descent,Handwritten Digit,Neural Information Processing System,Newton Algorithm},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/LeCun_et_al_2012_Efficient_BackProp.pdf}
}

@book{lederer2021,
  title = {Fundamentals of {{High-Dimensional Statistics}}: {{With Exercises}} and {{R Labs}}},
  shorttitle = {Fundamentals of {{High-Dimensional Statistics}}},
  author = {Lederer, Johannes},
  year = {2021},
  month = nov,
  publisher = {{Springer Nature}},
  abstract = {This textbook provides a step-by-step introduction to the tools and principles of high-dimensional statistics. Each chapter is complemented by numerous exercises, many of them with detailed solutions, and computer labs in R that convey valuable practical insights. The book covers the theory and practice of high-dimensional linear regression, graphical models, and inference, ensuring readers have a smooth start in the field. It also offers suggestions for further reading. Given its scope, the textbook is intended for beginning graduate and advanced undergraduate students in statistics, biostatistics, and bioinformatics, though it will be equally useful to a broader audience.},
  googlebooks = {fTNPEAAAQBAJ},
  isbn = {978-3-030-73792-4},
  langid = {english},
  keywords = {Computers / Artificial Intelligence / General,Computers / Database Administration \& Management,Computers / Information Theory,Computers / Mathematical \& Statistical Software,Computers / Programming / Algorithms,Mathematics / Probability \& Statistics / General,Mathematics / Probability \& Statistics / Stochastic Processes},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Lederer_2021_Fundamentals_of_High-Dimensional_Statistics.pdf}
}

@article{lee2016,
  title = {Exact Post-Selection Inference, with Application to the Lasso},
  author = {Lee, Jason D. and Sun, Dennis L. and Sun, Yuekai and Taylor, Jonathan E.},
  year = {2016},
  month = jun,
  journal = {The Annals of Statistics},
  volume = {44},
  number = {3},
  eprint = {1311.6238},
  eprinttype = {arxiv},
  issn = {0090-5364},
  doi = {10.1214/15-AOS1371},
  abstract = {We develop a general approach to valid inference after model selection. At the core of our framework is a result that characterizes the distribution of a post-selection estimator conditioned on the selection event. We specialize the approach to model selection by the lasso to form valid confidence intervals for the selected coefficients and test whether all relevant variables have been included in the model.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory,Statistics - Machine Learning,Statistics - Methodology,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Lee_et_al_2016_Exact_post-selection_inference,_with_application_to_the_lasso.pdf;/home/dede/Zotero/storage/E9M6XTMH/1311.html}
}

@article{lee2019,
  title = {A Review of Stochastic Block Models and Extensions for Graph Clustering},
  author = {Lee, Clement and Wilkinson, Darren J.},
  year = {2019},
  month = dec,
  journal = {Applied Network Science},
  volume = {4},
  number = {1},
  pages = {122},
  issn = {2364-8228},
  doi = {10.1007/s41109-019-0232-2},
  abstract = {Abstract             There have been rapid developments in model-based clustering of graphs, also known as block modelling, over the last ten years or so. We review different approaches and extensions proposed for different aspects in this area, such as the type of the graph, the clustering approach, the inference approach, and whether the number of groups is selected or estimated. We also review models that combine block modelling with topic modelling and/or longitudinal modelling, regarding how these models deal with multiple types of data. How different approaches cope with various issues will be summarised and compared, to facilitate the demand of practitioners for a concise overview of the current status of these areas of literature.},
  langid = {english},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Lee_Wilkinson_2019_A_review_of_stochastic_block_models_and_extensions_for_graph_clustering.pdf}
}

@article{lefkimmiatis2009,
  title = {Bayesian Inference on Multiscale Models for {{Poisson}} Intensity Estimation: Applications to Photon-Limited Image Denoising},
  shorttitle = {Bayesian Inference on Multiscale Models for Poisson Intensity Estimation},
  author = {Lefkimmiatis, S. and Maragos, P. and Papandreou, G.},
  year = {2009},
  journal = {IEEE Transactions on Image Processing},
  volume = {18},
  number = {8},
  pages = {1724--1741},
  issn = {1057-7149},
  doi = {10.1109/TIP.2009.2022008},
  abstract = {We present an improved statistical model for analyzing Poisson processes, with applications to photon-limited imaging. We build on previous work, adopting a multiscale representation of the Poisson process in which the ratios of the underlying Poisson intensities (rates) in adjacent scales are modeled as mixtures of conjugate parametric distributions. Our main contributions include: 1) a rigorous and robust regularized expectation-maximization (EM) algorithm for maximum-likelihood estimation of the rate-ratio density parameters directly from the noisy observed Poisson data (counts); 2) extension of the method to work under a multiscale hidden Markov tree model (HMT) which couples the mixture label assignments in consecutive scales, thus modeling interscale coefficient dependencies in the vicinity of image edges; 3) exploration of a 2-D recursive quad-tree image representation, involving Dirichlet-mixture rate-ratio densities, instead of the conventional separable binary-tree image representation involving beta-mixture rate-ratio densities; and 4) a novel multiscale image representation, which we term Poisson-Haar decomposition, that better models the image edge structure, thus yielding improved performance. Experimental results on standard images with artificially simulated Poisson noise and on real photon-limited images demonstrate the effectiveness of the proposed techniques.},
  keywords = {2D recursive quad-tree image,Additive noise,Algorithms,artificially simulated Poisson noise,Bayes methods,Bayes Theorem,Bayesian inference,Bayesian methods,conjugate parametric distribution,Degradation,Dirichlet-mixture rate-ratio densities,done,expectation-maximisation algorithm,expectation-maximization (EM) algorithm,hidden Markov models,Hidden Markov models,hidden Markov tree (HMT),Image analysis,image denoising,Image denoising,image edge structure,image edges,Image Processing; Computer-Assisted,image representation,Image representation,interscale coefficient,Markov Chains,Maximum likelihood estimation,maximum-likelihood estimation,mixture label assignment,Models; Statistical,multiscale hidden Markov tree model,multiscale image representation,Noise level,Optics and Photonics,photon-limited image denoising,photon-limited imaging,Poisson Distribution,Poisson intensity estimation,Poisson processes,Poisson-Haar decomposition,quadtrees,rate-ratio density parameters,recursive estimation,regularized expectation-maximization algorithm,Robustness,statistical model,stochastic processes},
  file = {/home/dede/Zotero/storage/VWT2I957/4907062.html}
}

@article{lehmann1990,
  title = {Model {{Specification}}: {{The Views}} of {{Fisher}} and {{Neyman}}, and {{Later Developments}}},
  shorttitle = {Model {{Specification}}},
  author = {Lehmann, E. L.},
  year = {1990},
  journal = {Statistical Science},
  volume = {5},
  number = {2},
  pages = {160--168},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0883-4237},
  abstract = {Since Fisher's formulation in 1922 of a framework for theoretical statistics, statistical theory has been concerned primarily with the derivation and properties of suitable statistical procedures on the basis of an assumed statistical model (including sensitivity to deviations from this model). Until relatively recently, the theory has paid little attention to the question of how such a model should be chosen. In the present paper, we consider first what Fisher and Neyman had to say about this problem and in Section 2 survey some contributions statistical theory has made to it. In Section 3 we study a distinction between two types of models (empirical and explanatory) which has been discussed by Neyman, Box, and others. A concluding section considers some lines of further work.},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Lehmann_1990_Model_Specification.pdf}
}

@book{lehmann2003,
  title = {{Theory of Point Estimation}},
  author = {Lehmann, E. L. and Casella, George},
  year = {2003},
  month = sep,
  publisher = {{Springer Verlag}},
  address = {{New York}},
  abstract = {This second, much enlarged edition by Lehmann and Casella of Lehmann's classic text on point estimation maintains the outlook and general style of the first edition. All of the topics are updated, while an entirely new chapter on Bayesian and hierarchical Bayesian approaches is provided, and there is much new material on simultaneous estimation. Each chapter concludes with a Notes section which contains suggestions for further study. This is a companion volume to the second edition of Lehmann's "Testing Statistical Hypotheses".},
  isbn = {978-0-387-98502-2},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Lehmann_Casella_2003_Theory_of_Point_Estimation.pdf}
}

@book{lehmann2005,
  title = {Testing {{Statistical Hypotheses}}},
  author = {Lehmann, Erich L. and Romano, Joseph P.},
  year = {2005},
  series = {Springer {{Texts}} in {{Statistics}}},
  edition = {Third},
  publisher = {{Springer-Verlag}},
  address = {{New York}},
  doi = {10.1007/0-387-27605-X},
  abstract = {The third edition of Testing Statistical Hypotheses updates and expands upon the classic graduate text, emphasizing optimality theory for hypothesis testing and confidence sets. The principal additions include a rigorous treatment of large sample optimality, together with the requisite tools. In addition, an introduction to the theory of resampling methods such as the bootstrap is developed. The sections on multiple testing and goodness of fit testing are expanded. The text is suitable for Ph.D. students in statistics and includes over 300 new problems out of a total of more than 760. E.L. Lehmann is Professor of Statistics Emeritus at the University of California, Berkeley. He is a member of the National Academy of Sciences and the American Academy of Arts and Sciences, and the recipient of honorary degrees from the University of Leiden, The Netherlands and the University of Chicago. He is the author of Elements of Large-Sample Theory and (with George Casella) he is also the author of Theory of Point Estimation, Second Edition. Joseph P. Romano is Professor of Statistics at Stanford University. He is a recipient of a Presidential Young Investigator Award and a Fellow of the Institute of Mathematical Statistics. He has coauthored two other books, Subsampling with Dimitris Politis and Michael Wolf, and Counterexamples in Probability and Statistics with Andrew Siegel.},
  isbn = {978-0-387-98864-1},
  langid = {english},
  file = {/home/dede/Zotero/storage/RQ7VJKGN/9780387988641.html}
}

@book{lehmann2011,
  title = {Fisher, {{Neyman}}, and the {{Creation}} of {{Classical Statistics}}},
  author = {Lehmann, Erich L.},
  year = {2011},
  month = jul,
  edition = {2011th edition},
  publisher = {{Springer}},
  address = {{New York, NY}},
  isbn = {978-1-4419-9499-8},
  langid = {english},
  file = {/home/dede/Zotero/storage/U5IA5U2Y/Lehmann - 2011 - Fisher, Neyman, and the Creation of Classical Stat.pdf}
}

@article{leone1961,
  title = {The {{Folded Normal Distribution}}},
  author = {Leone, F. C. and Nelson, L. S. and Nottingham, R. B.},
  year = {1961},
  journal = {Technometrics},
  volume = {3},
  number = {4},
  pages = {543--550},
  publisher = {{[Taylor \& Francis, Ltd., American Statistical Association, American Society for Quality]}},
  issn = {0040-1706},
  doi = {10.2307/1266560},
  abstract = {Measurements are frequently recorded without their algebraic sign. As a consequence, the underlying distribution of measurements is replaced by a distribution of absolute measurements. When the underlying distribution is normal, the resulting distribution is called the "folded normal distribution". The authors describe methods for estimating the mean and standard deviation of the normal distribution based on estimates of the mean and standard deviation determined from the folded normal. Tables are provided to assist in the estimation procedure and an example included.}
}

@book{lepore2018,
  title = {{These Truths: A History of the United States}},
  shorttitle = {{These Truths}},
  author = {Lepore, Jill},
  year = {2018},
  edition = {1\textdegree{} edizione},
  publisher = {{W W Norton \& Co Inc}},
  address = {{New York ; London}},
  isbn = {978-0-393-63524-9},
  langid = {Inglese},
  keywords = {doing},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Lepore_2018_These_Truths.epub}
}

@book{leskovic2020,
  title = {{Mining of Massive Datasets}},
  author = {Leskovic, Jure and Anand, Rajaraman and Ullman, Jeffrey David},
  year = {2020},
  publisher = {{John Wiley \& Sons}},
  address = {{New Delhi}},
  isbn = {978-1-108-47634-8},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Leskovic_et_al_2020_Mining_of_Massive_Datasets.pdf}
}

@book{levi2006,
  title = {Decisions and {{Revisions}}: {{Philosophical Essays}} on {{Knowledge}} and {{Value}}},
  shorttitle = {Decisions and Revisions},
  author = {Levi, Isaac},
  year = {2006},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge ; New York}},
  isbn = {978-0-521-02762-5 978-0-521-25457-1},
  langid = {english},
  lccn = {BD161 .L377 2006},
  keywords = {Knowledge; Theory of,Methodology,Science,Values},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Levi_2006_Decisions_and_revisions.pdf}
}

@article{lewis2013,
  title = {A {{Middle East}} Free of Nuclear Weapons: Possible, Probable or Pipe-Dream?},
  shorttitle = {A {{Middle East}} Free of Nuclear Weapons},
  author = {Lewis, Patricia M.},
  year = {2013},
  month = mar,
  journal = {International Affairs},
  volume = {89},
  number = {2},
  pages = {433--450},
  issn = {0020-5850},
  doi = {10.1111/1468-2346.12026},
  abstract = {This article provides an overview of the attempts to address a Weapons of Mass Destruction (WMD)-free zone in the Middle East as mandated by the 2010 nuclear Non-Proliferation Treaty (NPT) Review Conference. The article outlines regional nuclear programmes in Egypt, Israel, Iran, Iraq and Syria, and the evolution of nuclear weapon-free zones and NPT politics. The article further proposes a set of recommendations for what might constitute a nuclear weapon-free zone treaty, including interim measures that would support the establishment of the Middle East WMD-free zone, drawing on historical precedent from relevant cases.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/LEWIS_2013_A_Middle_East_free_of_nuclear_weapons.pdf;/home/dede/Zotero/storage/M6Q3IAAH/2535111.html}
}

@article{li2010,
  title = {Self-Starting Control Chart for Simultaneously Monitoring Process Mean and Variance},
  author = {Li, Zhonghua and Zhang, Jiujun and Wang, Zhaojun},
  year = {2010},
  month = aug,
  journal = {International Journal of Production Research},
  volume = {48},
  number = {15},
  pages = {4537--4553},
  publisher = {{Taylor \& Francis}},
  issn = {0020-7543},
  doi = {10.1080/00207540903051692},
  abstract = {A self-starting control chart, based on the likelihood ratio test and the exponentially weighted moving average procedure, is proposed for monitoring the process mean and variance simultaneously when the process parameters are unknown. A table is presented to assist in the design of the control chart with different parameters. Its in-control average run length can be evaluated by a two-dimensional Markov chain model. Moreover, the diagnostic aids of the proposed chart are given. Monte Carlo simulation results compared with some competing methods in the literature show that the proposed approach has quite satisfactory charting performance across a range of possible shifts when the process parameters are unknown, even including the detection of a decrease in variability. A real data example from industrial manufacturing is used to demonstrate its implementation.},
  keywords = {quality control,reliability engineering,SPC},
  annotation = {\_eprint: https://doi.org/10.1080/00207540903051692},
  file = {/home/dede/Zotero/storage/3N5I8BKT/00207540903051692.html}
}

@article{li2010a,
  title = {Adaptive {{CUSUM}} of the {{Q}} Chart},
  author = {Li, Zhonghua and Wang, Zhaojun},
  year = {2010},
  month = mar,
  journal = {International Journal of Production Research},
  volume = {48},
  number = {5},
  pages = {1287--1301},
  publisher = {{Taylor \& Francis}},
  issn = {0020-7543},
  doi = {10.1080/00207540802484937},
  abstract = {The CUSUM of the Q chart, as a self-starting approach, was previously proposed for detecting process shifts when the in-control process parameters are unknown. Motivated by the recent development of the adaptive chart, this paper proposes a new self-starting approach that integrates the CUSUM of the Q chart with the feature of adaptively varying the reference value, to better detect a range of shifts with unknown process parameters. The choice of the chart parameters and the masking effect are also studied. Simulation results show that our proposed chart offers balanced protection against shifts of different magnitude, and has performance comparable to the dynamic change-point control scheme. A real example from industrial manufacturing is used to demonstrate its implementation.},
  keywords = {quality control,reliability engineering,SPC},
  annotation = {\_eprint: https://doi.org/10.1080/00207540802484937},
  file = {/home/dede/Zotero/storage/VGU5SJSE/00207540802484937.html}
}

@article{li2010b,
  title = {Cusum of {{Q}} Chart with Variable Sampling Intervals for Monitoring the Process Mean},
  author = {Li, Zhonghua and Luo, Yunzhao and Wang, Zhaojun},
  year = {2010},
  month = aug,
  journal = {International Journal of Production Research},
  volume = {48},
  number = {16},
  pages = {4861--4876},
  publisher = {{Taylor \& Francis}},
  issn = {0020-7543},
  doi = {10.1080/00207540903074983},
  abstract = {Recently, adaptive control charts (that is, with variable sample sizes and/or sampling intervals) for univariate or multivariate quality characteristics have received considerable attention in Phase II analysis in the literature. Due to insufficient samples to obtain good knowledge of the parameters in the start-up process, adding adaptive features to self-starting control charts remains an open problem. In this paper, we propose an adaptive Cusum of Q chart with variable sampling intervals for monitoring the process mean of normally distributed variables. A Fortran program is available to assist in the design of the control chart with different parameters. The effect of the control chart parameters on the performance is studied in detail. The control chart is further enhanced by finding adaptive reference values. Due to the powerful properties of the proposed control chart, the Monte Carlo simulation results show that it provides quite satisfactory performance in various cases. The proposed control chart is applied to a real-life data example to illustrate its implementation.},
  keywords = {quality control,statistics},
  annotation = {\_eprint: https://doi.org/10.1080/00207540903074983},
  file = {/home/dede/Zotero/storage/KAK7DU7Q/00207540903074983.html}
}

@article{li2014,
  title = {Statistical {{Process Control Using}} a {{Dynamic Sampling Scheme}}},
  author = {Li, Zhonghua and Qiu, Peihua},
  year = {2014},
  month = jul,
  journal = {Technometrics},
  volume = {56},
  number = {3},
  pages = {325--335},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.2013.844731},
  abstract = {This article considers statistical process control (SPC) of univariate processes, and tries to make two contributions to the univariate SPC problem. First, we propose a continuously variable sampling scheme, based on a quantitative measure of the likelihood of a process distributional shift at each observation time point, provided by the p-value of the conventional cumulative sum (CUSUM) charting statistic. For convenience of the design and implementation, the variable sampling scheme is described by a parametric function in the flexible Box\textendash Cox transformation family. Second, the resulting CUSUM chart using the variable sampling scheme is combined with an adaptive estimation procedure for determining its reference value, to effectively protect against a range of unknown shifts. Numerical studies show that it performs well in various cases. A real data example from a chemical process illustrates the application and implementation of our proposed method. This article has supplementary materials online.},
  keywords = {Adaptive estimation,Bootstrap,Monte Carlo simulation,todo,Variable sampling},
  annotation = {\_eprint: https://doi.org/10.1080/00401706.2013.844731},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Li_Qiu_2014_Statistical_Process_Control_Using_a_Dynamic_Sampling_Scheme.pdf;/home/dede/Zotero/storage/JVXRN5JB/00401706.2013.html}
}

@article{li2014a,
  title = {A Self-Starting Control Chart for High-Dimensional Short-Run Processes},
  author = {Li, Yanting and Liu, Yukun and Zou, Changliang and Jiang, Wei},
  year = {2014},
  month = jan,
  journal = {International Journal of Production Research},
  volume = {52},
  number = {2},
  pages = {445--461},
  publisher = {{Taylor \& Francis}},
  issn = {0020-7543},
  doi = {10.1080/00207543.2013.832001},
  abstract = {A key challenge in using a traditional Hotelling's chart with high-dimensionality measurements is that monitoring cannot begin until after the number of observations exceeds the dimensionality of the measurements, and the detection sensitivity to early shifts is reduced after that point until a substantial amount of observations has been accumulated. This is especially important with short-run processes where the measurements have high dimensionality. This article proposes a chart that allows monitoring with the second observation regardless of the dimensionality and reduces the average run length in detecting early shifts in high-dimensionality measurements. The proposed control chart can start monitoring quite early before considerable reference data are collected during the initial stage of production. A change point estimate is also available from our procedure, which is shown consistent for locating the correct change point. Both simulation results and an industry example show the effectiveness of the proposed control chart for monitoring short-run processes with high dimensionality.},
  keywords = {average run length,control chart,high-dimensional observations,mean shift,short-run processes},
  annotation = {\_eprint: https://doi.org/10.1080/00207543.2013.832001},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Li_et_al_2014_A_self-starting_control_chart_for_high-dimensional_short-run_processes.pdf;/home/dede/Zotero/storage/2BMRISM7/00207543.2013.html}
}

@incollection{li2016,
  title = {Model {{Selection}}},
  booktitle = {Wiley {{StatsRef}}: {{Statistics Reference Online}}},
  author = {Li, Ang and Pericchi, Luis},
  editor = {Balakrishnan, N. and Colton, Theodore and Everitt, Brian and Piegorsch, Walter and Ruggeri, Fabrizio and Teugels, Jozef L.},
  year = {2016},
  month = aug,
  pages = {1--10},
  publisher = {{John Wiley \& Sons, Ltd}},
  address = {{Chichester, UK}},
  doi = {10.1002/9781118445112.stat06424.pub2},
  isbn = {978-1-118-44511-2},
  langid = {english},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Li_Pericchi_2016_Model_Selection.pdf}
}

@article{li2017,
  title = {Simple, Scalable and Accurate Posterior Interval Estimation},
  author = {Li, Cheng and Srivastava, Sanvesh and Dunson, David B.},
  year = {2017},
  month = sep,
  journal = {Biometrika},
  volume = {104},
  number = {3},
  pages = {665--680},
  issn = {0006-3444},
  doi = {10.1093/biomet/asx033},
  abstract = {Standard posterior sampling algorithms, such as Markov chain Monte Carlo procedures, face major challenges in scaling up to massive datasets. We propose a simple and general posterior interval estimation algorithm to rapidly and accurately estimate quantiles of the posterior distributions for one-dimensional functionals. Our algorithm runs Markov chain Monte Carlo in parallel for subsets of the data, and then averages quantiles estimated from each subset. We provide strong theoretical guarantees and show that the credible intervals from our algorithm asymptotically approximate those from the full posterior in the leading parametric order. Our algorithm has a better balance of accuracy and efficiency than its competitors across a variety of simulations and a real-data example.},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Li_et_al_2017_Simple,_scalable_and_accurate_posterior_interval_estimation.pdf;/home/dede/Zotero/storage/KNXJ9UHL/3891479.html}
}

@article{li2017a,
  title = {A Robust Self-Starting Spatial Rank Multivariate {{EWMA}} Chart Based on Forward Variable Selection},
  author = {Li, Wendong and Pu, Xiaolong and Tsung, Fugee and Xiang, Dongdong},
  year = {2017},
  month = jan,
  journal = {Computers and Industrial Engineering},
  volume = {103},
  number = {C},
  pages = {116--130},
  issn = {0360-8352},
  doi = {10.1016/j.cie.2016.11.024},
  abstract = {A robust self-starting control chart based on forward variable selection is proposed.The proposed chart does not need prior knowledge of the IC distribution and is robust to non-normally distributed data.The need to gather extensive data before monitoring is overcome.The sensitivity to small and moderate sparse shifts in mean vectors is remarkable. Shifts in one or a few components of process mean vectors, called sparse shifts, are monitored in many applications. To monitor sparse shifts, several control charts have recently been proposed based on the variable selection technique. These charts assume either that the in-control (IC) distribution is completely known or that a sufficiently large reference dataset is available. However, this assumption is not always valid in practice. This paper develops a self-starting control chart that integrates a multivariate spatial rank test with the EWMA charting scheme based on forward variable selection for monitoring sparse mean shifts. Both the theoretical and numerical results show that the proposed chart is robust to non-normally distributed data, fast to compute, easy to construct, and can efficiently detect sparse shifts, especially when the process distribution is heavy-tailed or skewed. The proposed control chart does not need prior knowledge of the IC distribution and can start monitoring even before considerable reference data have been collected. A real-data example from a white wine production process illustrates the effectiveness of the proposed control chart.},
  keywords = {Forward variable selection,Robust,Self-starting,Statistical process control},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Li_et_al_2017_A_robust_self-starting_spatial_rank_multivariate_EWMA_chart_based_on_forward.pdf}
}

@article{li2019,
  title = {A Two-Stage Online Monitoring Procedure for High-Dimensional Data Streams},
  author = {Li, Jun},
  year = {2019},
  month = oct,
  journal = {Journal of Quality Technology},
  volume = {51},
  number = {4},
  pages = {392--406},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2018.1507562},
  abstract = {Advanced computing and data acquisition technologies have made possible the collection of high-dimensional data streams in many fields. Efficient online monitoring tools that can correctly identify any abnormal data stream for such data are highly sought. However, most of the existing monitoring procedures directly apply the false discovery rate (FDR) controlling procedure to the data at each time point, and the FDR at each time point (the pointwise FDR) is either specified by users or determined by the in-control (IC) average run length (ARL). If the pointwise FDR is specified by users, the resulting procedure lacks control of the global FDR and keeps users in the dark in terms of the IC ARL. If the pointwise FDR is determined by the IC ARL, the resulting procedure does not give users the flexibility to choose the number of false alarms (Type-I errors) they can tolerate when identifying abnormal data streams, which often makes the procedure too conservative. To address those limitations, a two-stage monitoring procedure is proposed to control both the IC-ARL and Type-I errors at the levels specified by users. As a result, the proposed procedure allows users to choose not only how often they expect any false alarms when all data streams are IC but also how many false alarms they can tolerate when identifying abnormal data streams. Due to this extra flexibility, the proposed two-stage monitoring procedure is shown to outperform the exiting methods in the simulation study and real data analysis.},
  keywords = {CUSUM,done,false discovery rate,multiple hypothesis testing,per-comparison error rate,statistical process control},
  annotation = {\_eprint: https://doi.org/10.1080/00224065.2018.1507562},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Li_2019_A_two-stage_online_monitoring_procedure_for_high-dimensional_data_streams2.pdf;/home/dede/Zotero/storage/SITR3736/00224065.2018.html}
}

@article{li2019a,
  title = {Spatial {{Bayesian}} Modeling of {{GLCM}} with Application to Malignant Lesion Characterization},
  author = {Li, Xiao and Guindani, Michele and Ng, Chaan S. and Hobbs, Brian P.},
  year = {2019},
  month = jan,
  journal = {Journal of Applied Statistics},
  volume = {46},
  number = {2},
  pages = {230--246},
  publisher = {{Taylor \& Francis}},
  issn = {0266-4763, 1360-0532},
  doi = {10.1080/02664763.2018.1473348},
  abstract = {The emerging field of cancer radiomics endeavors to characterize intrinsic patterns of tumor phenotypes and surrogate markers of response by transforming medical images into objects that yield quantifiable summary statistics to which regression and machine learning algorithms may be applied for statistical interrogation. Recent literature has identified clinicopathological association based on textural features deriving from gray-level co-occurrence matrices (GLCM) which facilitate evaluations of gray-level spatial dependence within a delineated region of interest. GLCM-derived features, however, tend to contribute highly redundant information. Moreover, when reporting selected feature sets, investigators often fail to adjust for multiplicities and commonly fail to convey the predictive power of their findings. This article presents a Bayesian probabilistic modeling framework for the GLCM as a multivariate object as well as describes its application within a cancer detection context based on computed tomography. The methodology, which circumvents processing steps and avoids evaluations of reductive and highly correlated feature sets, uses latent Gaussian Markov random field structure to characterize spatial dependencies among GLCM cells and facilitates classification via predictive probability. Correctly predicting the underlying pathology of 81\% of the adrenal lesions in our case study, the proposed method outperformed current practices which achieved a maximum accuracy of only 59\%. Simulations and theory are presented to further elucidate this comparison as well as ascertain the utility of applying multivariate Gaussian spatial processes to GLCM objects.},
  langid = {english},
  keywords = {Bayesian prediction,cancer detection,done,gray-level co-occurrence matrix,Markov random field,radiomics,texture analysis},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Li_et_al_2019_Spatial_Bayesian_modeling_of_GLCM_with_application_to_malignant_lesion.pdf;/home/dede/Documents/MEGA/universita/zotero-pdf/Li_et_al_2019_Spatial_Bayesian_modeling_of_GLCM_with_application_to_malignant_lesion2.pdf;/home/dede/Zotero/storage/4JDKR4HD/02664763.2018.html}
}

@article{li2020,
  title = {A {{Bayesian Nonparametric}} Model for Textural Pattern Heterogeneity},
  author = {Li, Xiao and Guindani, Michele and Ng, Chaan S. and Hobbs, Brian P.},
  year = {2020},
  month = nov,
  journal = {arXiv:2011.05548 [stat]},
  eprint = {2011.05548},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Cancer radiomics is an emerging discipline promising to elucidate lesion phenotypes and tumor heterogeneity through patterns of enhancement, texture, morphology, and shape. The prevailing technique for image texture analysis relies on the construction and synthesis of Gray-Level Co-occurrence Matrices (GLCM). Practice currently reduces the structured count data of a GLCM to reductive and redundant summary statistics for which analysis requires variable selection and multiple comparisons for each application, thus limiting reproducibility. In this article, we develop a Bayesian multivariate probabilistic framework for the analysis and unsupervised clustering of a sample of GLCM objects. By appropriately accounting for skewness and zero-inflation of the observed counts and simultaneously adjusting for existing spatial autocorrelation at nearby cells, the methodology facilitates estimation of texture pattern distributions within the GLCM lattice itself. The techniques are applied to cluster images of adrenal lesions obtained from CT scans with and without administration of contrast. We further assess whether the resultant subtypes are clinically oriented by investigating their correspondence with pathological diagnoses. Additionally, we compare performance to a class of machine-learning approaches currently used in cancer radiomics with simulation studies.},
  archiveprefix = {arXiv},
  keywords = {done,Statistics - Applications},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Li_et_al_2020_A_Bayesian_Nonparametric_model_for_textural_pattern_heterogeneity.pdf;/home/dede/Zotero/storage/B2K99QAG/2011.html}
}

@article{li2020a,
  title = {A {{Diagnostic Procedure}} for {{High-Dimensional Data Streams}} via {{Missed Discovery Rate Control}}},
  author = {Li, Wendong and Xiang, D. and Tsung, F. and Pu, X.},
  year = {2020},
  journal = {Technometrics},
  doi = {10.1080/00401706.2019.1575284},
  abstract = {The proposed procedure overcomes the limitations of conventional diagnostic procedures by controlling the wMDR and minimizing the expected number of false positives as well and it is shown theoretically that the proposed procedure is asymptotically valid and optimal in a certain sense. Abstract Monitoring complex systems involving high-dimensional data streams (HDS) provides quick real-time detection of abnormal changes of system performance, but accurate and efficient diagnosis of the streams responsible has also become increasingly important in many data-rich statistical process control applications. Existing diagnostic procedures, designed for low/moderate dimensional multivariate process, may miss too much important information in the out-of-control streams with a high signal-to-noise ratio (SNR) or waste too many resources finding useless in-control streams with a low SNR. In addition, these procedures do not differentiate between streams according to their severity. In this article, we formulate the diagnosis problem of HDS as a multiple testing problem and provide a computationally fast diagnostic procedure to control the weighted missed discovery rate (wMDR) at some satisfactory level. The proposed procedure overcomes the limitations of conventional diagnostic procedures by controlling the wMDR and minimizing the expected number of false positives as well. We show theoretically that the proposed procedure is asymptotically valid and optimal in a certain sense. Simulation studies and a real data analysis from a semiconductor manufacturing process show that the proposed procedure works very well in practice.},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Li_et_al_2020_A_Diagnostic_Procedure_for_High-Dimensional_Data_Streams_via_Missed_Discovery.pdf}
}

@article{li2021,
  title = {Nonparametric Monitoring of Multivariate Data via {{KNN}} Learning},
  author = {Li, Wendong and Zhang, Chi and Tsung, Fugee and Mei, Yajun},
  year = {2021},
  month = oct,
  journal = {International Journal of Production Research},
  volume = {59},
  number = {20},
  pages = {6311--6326},
  publisher = {{Taylor \& Francis}},
  issn = {0020-7543},
  doi = {10.1080/00207543.2020.1812750},
  abstract = {Process monitoring of multivariate quality attributes is important in many industrial applications, in which rich historical data are often available thanks to modern sensing technologies. While multivariate statistical process control (SPC) has been receiving increasing attention, existing methods are often inadequate as they are sensitive to the parametric model assumptions of multivariate data. In this paper, we propose a novel, nonparametric k-nearest neighbours empirical cumulative sum (KNN-ECUSUM) control chart that is a machine-learning-based black-box control chart for monitoring multivariate data by utilising extensive historical data under both in-control and out-of-control scenarios. Our proposed method utilises the k-nearest neighbours (KNN) algorithm for dimension reduction to transform multivariate data into univariate data and then applies the CUSUM procedure to monitor the change on the empirical distribution of the transformed univariate data. Extensive simulation studies and a real industrial example based on a disk monitoring system demonstrate the robustness and effectiveness of our proposed method.},
  keywords = {categorical variable,CUSUM,done,empirical probability mass function,KNN algorithm,machine learning,Multivariate statistical process control},
  annotation = {\_eprint: https://doi.org/10.1080/00207543.2020.1812750},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Li_et_al_2021_Nonparametric_monitoring_of_multivariate_data_via_KNN_learning.pdf;/home/dede/Zotero/storage/B4SSCF9E/00207543.2020.html}
}

@article{li2021a,
  title = {Multi-Sensor Based Landslide Monitoring via Transfer Learning},
  author = {Li, Wendong and Tsung, Fugee and Song, Zhenli and Zhang, Ke and Xiang, Dongdong},
  year = {2021},
  month = aug,
  journal = {Journal of Quality Technology},
  volume = {0},
  number = {0},
  pages = {1--14},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2021.1960936},
  abstract = {Landslides are severe geographical activities that result in large quantities of rock and debris flowing down hill-slopes, leading to thousands of casualties and billions of dollars in infrastructure damage every year worldwide. For detecting landslides, on-site sensor systems are widely applied for data collection and many existing statistical process control methods can be adopted for modeling and monitoring. However, the conventional methods may perform poorly or even inapplicable when the sensors have different set-up times and end times, especially when the system includes newly deployed sensors with limited data collected. To make effective use of such new sensors immediately after deployment, we propose a novel multi-sensor based charting scheme for dynamic landslide modeling and monitoring by using transfer learning. A regularized parameter-based transfer learning approach integrated with the ordered LASSO is first proposed to effectively transfer information from old sensors with sufficient historical data to new ones with limited data. The approach considers the similarities not only between the autoregressive coefficients of different sensors, but also between the temporal correlation patterns. A control chart is then proposed for monitoring the newly deployed sensors sequentially based on the generalized likelihood ratio. Extensive simulation results and a real data example of landslide monitoring demonstrate the effectiveness of our proposed method.},
  keywords = {Dynamic landslide monitoring,generalized likelihood ratio,ordered Lasso,transfer learning},
  annotation = {\_eprint: https://doi.org/10.1080/00224065.2021.1960936},
  file = {/home/dede/Zotero/storage/IU7ADL86/00224065.2021.html}
}

@article{li2021b,
  title = {Nonparametric Adaptive {{CUSUM}} Chart for Detecting Arbitrary Distributional Changes},
  author = {Li, Jun},
  year = {2021},
  month = mar,
  journal = {Journal of Quality Technology},
  volume = {53},
  number = {2},
  pages = {154--172},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2019.1694398},
  abstract = {Nonparametric control charts that can detect arbitrary distributional changes are highly desirable due to their flexibility to adapt to different distributional assumptions and changes. However, most of the nonparametric control charts in the literature either can only detect location changes, or involve intensive computation. In this article, we propose a new nonparametric adaptive CUSUM chart. The proposed control chart can detect arbitrary distributional changes and is computationally efficient. Its self-starting nature makes the proposed control chart applicable to situations where no sufficiently large reference data are available. The proposed control chart also has a built-in post-signal diagnostics function that can identify what kind of distributional changes have occurred after an alarm. Our simulation study and real data analysis show that the proposed control chart performs well across a broad range of settings, and compares favorably with existing nonparametric control charts.},
  keywords = {adaptive CUSUM,categorization,nonparametric procedure,self-starting,statistical process control},
  annotation = {\_eprint: https://doi.org/10.1080/00224065.2019.1694398},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Li_2021_Nonparametric_adaptive_CUSUM_chart_for_detecting_arbitrary_distributional.pdf;/home/dede/Zotero/storage/KKUAYLJY/00224065.2019.html}
}

@article{li2022,
  title = {Adaptive {{CUSUM}} Chart with Cautious Parameter Learning},
  author = {Li, Jun},
  year = {2022},
  journal = {Quality and Reliability Engineering International},
  volume = {n/a},
  number = {n/a},
  issn = {1099-1638},
  doi = {10.1002/qre.3116},
  abstract = {The classic CUSUM chart assumes that the in-control (IC) mean and variance are known. In practice, these parameters are usually estimated from an IC Phase I sample. Recently, Capizzi and Masarotto proposed a cautious parameter learning scheme to incorporate Phase II IC observations in the estimation of the IC mean and variance to reduce the variation of conditional average run lengths (ARLs). In this paper, we develop a new cautious parameter learning scheme that can distinguish IC observations from out-of-control (OC) observations in the Phase II sample more effectively than Capizzi and Masarotto's scheme. As a result, our cautious parameter learning scheme can provide better estimation of the IC mean and variance. Combining the new cautious parameter learning scheme with an adaptive CUSUM chart, our proposed monitoring procedure is easy to implement, and is shown to have less variability in conditional ARLs and better overall performance for detecting different mean shifts than existing methods.},
  langid = {english},
  keywords = {adaptive CUSUM,estimation effects,maximum likelihood estimator,reference only,statistical process monitoring},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/qre.3116},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Li_Adaptive_CUSUM_chart_with_cautious_parameter_learning.pdf;/home/dede/Zotero/storage/PIEZBL3D/qre.html}
}

@article{liang2008,
  title = {Mixtures of g {{Priors}} for {{Bayesian Variable Selection}}},
  author = {Liang, Feng and Paulo, Rui and Molina, German and Clyde, Merlise A. and Berger, Jim O.},
  year = {2008},
  month = mar,
  journal = {Journal of the American Statistical Association},
  volume = {103},
  number = {481},
  pages = {410--423},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1198/016214507000001337},
  abstract = {Zellner's g prior remains a popular conventional prior for use in Bayesian variable selection, despite several undesirable consistency issues. In this article we study mixtures of g priors as an alternative to default g priors that resolve many of the problems with the original formulation while maintaining the computational tractability that has made the g prior so popular. We present theoretical properties of the mixture g priors and provide real and simulated examples to compare the mixture formulation with fixed g priors, empirical Bayes approaches, and other default procedures. Please see Arnold Zellner's letter and the author's response.},
  keywords = {AIC,Bayesian model averaging,BIC,Cauchy,Empirical Bayes,Gaussian hypergeometric functions,Model selection,todo,Zellnerâ€“Siow priors},
  annotation = {\_eprint: https://doi.org/10.1198/016214507000001337},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Liang_et_al_2008_Mixtures_of_g_Priors_for_Bayesian_Variable_Selection.pdf;/home/dede/Zotero/storage/VCDZG8RY/016214507000001337.html}
}

@article{liao2020,
  title = {Connecting and {{Contrasting}} the {{Bayes Factor}} and a {{Modified ROPE Procedure}} for {{Testing Interval Null Hypotheses}}},
  author = {Liao, J. G. and Midya, Vishal and Berg, Arthur},
  year = {2020},
  month = jan,
  journal = {The American Statistician},
  pages = {1--19},
  issn = {0003-1305, 1537-2731},
  doi = {10.1080/00031305.2019.1701550},
  langid = {english},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Liao_et_al_2020_Connecting_and_Contrasting_the_Bayes_Factor_and_a_Modified_ROPE_Procedure_for.pdf}
}

@article{libman2018,
  title = {Regional International Organizations as a Strategy of Autocracy: The {{Eurasian Economic Union}} and {{Russian}} Foreign Policy},
  shorttitle = {Regional International Organizations as a Strategy of Autocracy},
  author = {Libman, Alexander and Obydenkova, Anastassia V.},
  year = {2018},
  month = sep,
  journal = {International Affairs},
  volume = {94},
  number = {5},
  pages = {1037--1058},
  issn = {0020-5850},
  doi = {10.1093/ia/iiy147},
  abstract = {This article investigates whether and how regional international organizations created by authoritarian countries can aid the regime survival of their member states. Numerous regions of the world have witnessed a proliferation of regional organizations established by powerful authoritarian states. We argue that the external influence of these organizations can affect regime survival and reinforce non-democratic regime trajectories, but in a nuanced manner. The article argues that, in addition to examining the impact of the regional organization itself, one must examine how the existence of these regional organizations changes the strategy of autocratic leading states, which\textemdash in bilateral relations with other countries\textemdash could become more eager to support authoritarian regimes of geopolitical importance. We use the case of the Eurasian Economic Union to explore various strategies of Russia, the leading state, vis-\`a-vis post-Soviet Eurasian countries. These strategies, however, appear only to matter for authoritarian consolidation when countries, from the Russian point of view, are on the `front line' of geopolitical competition with the EU, and which are, therefore, important in stabilizing Russian influence. The article identifies five strategic foreign policy models of leading states which are determined by the existence of regional organizations and evaluates the benefits of these strategies for both leading and targeted states.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Libman_Obydenkova_2018_Regional_international_organizations_as_a_strategy_of_autocracy.pdf;/home/dede/Zotero/storage/QBKQKY5G/5092105.html}
}

@book{liese2010,
  title = {{Statistical Decision Theory: Estimation, Testing, and Selection}},
  shorttitle = {{Statistical Decision Theory}},
  author = {Liese, F. and Miescke, Klaus-J.},
  year = {2010},
  edition = {Softcover reprint of hardcover 1st ed. 2008 edizione},
  publisher = {{Springer}},
  address = {{New York, NY}},
  isbn = {978-1-4419-2513-8},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Liese_Miescke_2010_Statistical_Decision_Theory.pdf}
}

@article{light2000,
  title = {A {{Wider Europe}}: The {{View}} from {{Moscow}} and {{Kyiv}}},
  shorttitle = {A {{Wider Europe}}},
  author = {Light, Margot and White, Stephen and L{\"o}wenhardt, John},
  year = {2000},
  month = jan,
  journal = {International Affairs},
  volume = {76},
  number = {1},
  pages = {77--88},
  issn = {0020-5850},
  doi = {10.1111/1468-2346.00121},
  abstract = {The expansion of NATO and the enlargement of the EU will produce outside states in which perceptions and policies will be influenced by feelings of exclusion and isolation. Russia and Ukraine are two important examples. In Russia the sense of exclusion results from NATO expansion and it was exacerbated by the air strikes against Serbia. Although Ukraine also responded negatively to NATO's attack on Serbia, Ukrainian perceptions of exclusion are caused primarily by disappointment that EU membership is proving so difficult to attain. Based on elite interviews, opinion surveys and the analysis of focus group discussions, this article compares and contrasts the attitudes towards NATO and the EU in the two countries.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Light_et_al_2000_A_Wider_Europe.pdf;/home/dede/Zotero/storage/NPEDJ8TL/2434600.html}
}

@article{lin,
  title = {Data-{{Intensive Text Processing}} with {{MapReduce}}},
  author = {Lin, Jimmy and Dyer, Chris},
  pages = {175},
  langid = {english},
  keywords = {done},
  file = {/home/dede/Zotero/storage/6J5QTYH6/Lin and Dyer - Data-Intensive Text Processing with MapReduce.pdf}
}

@article{lindquist2008,
  title = {The {{Statistical Analysis}} of {{fMRI Data}}},
  author = {Lindquist, Martin A.},
  year = {2008},
  month = nov,
  journal = {Statistical Science},
  volume = {23},
  number = {4},
  pages = {439--464},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/09-STS282},
  abstract = {In recent years there has been explosive growth in the number of neuroimaging studies performed using functional Magnetic Resonance Imaging (fMRI). The field that has grown around the acquisition and analysis of fMRI data is intrinsically interdisciplinary in nature and involves contributions from researchers in neuroscience, psychology, physics and statistics, among others. A standard fMRI study gives rise to massive amounts of noisy data with a complicated spatio-temporal correlation structure. Statistics plays a crucial role in understanding the nature of the data and obtaining relevant results that can be used and interpreted by neuroscientists. In this paper we discuss the analysis of fMRI data, from the initial acquisition of the raw data to its use in locating brain activity, making inference about brain connectivity and predictions about psychological or disease states. Along the way, we illustrate interesting and important issues where statistics already plays a crucial role. We also seek to illustrate areas where statistics has perhaps been underutilized and will have an increased role in the future.},
  keywords = {brain imaging,challenges,fMRI,statistical analysis,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Lindquist_2008_The_Statistical_Analysis_of_fMRI_Data.pdf;/home/dede/Zotero/storage/8BWAB2FC/09-STS282.html}
}

@article{linero2018,
  title = {Bayesian Regression Tree Ensembles That Adapt to Smoothness and Sparsity},
  author = {Linero, Antonio R. and Yang, Yun},
  year = {2018},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {80},
  number = {5},
  pages = {1087--1110},
  issn = {1467-9868},
  doi = {10.1111/rssb.12293},
  abstract = {Ensembles of decision trees are a useful tool for obtaining flexible estimates of regression functions. Examples of these methods include gradient-boosted decision trees, random forests and Bayesian classification and regression trees. Two potential shortcomings of tree ensembles are their lack of smoothness and their vulnerability to the curse of dimensionality. We show that these issues can be overcome by instead considering sparsity inducing soft decision trees in which the decisions are treated as probabilistic. We implement this in the context of the Bayesian additive regression trees framework and illustrate its promising performance through testing on benchmark data sets. We provide strong theoretical support for our methodology by showing that the posterior distribution concentrates at the minimax rate (up to a logarithmic factor) for sparse functions and functions with additive structures in the high dimensional regime where the dimensionality of the covariate space is allowed to grow nearly exponentially in the sample size. Our method also adapts to the unknown smoothness and sparsity levels, and can be implemented by making minimal modifications to existing Bayesian additive regression tree algorithms.},
  copyright = {\textcopyright{} 2018 Royal Statistical Society},
  langid = {english},
  keywords = {Bayesian additive regression trees,Bayesian non-parametrics,High dimensional regimes,Model averaging,Posterior consistency,todo},
  annotation = {\_eprint: https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/rssb.12293},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Linero_Yang_2018_Bayesian_regression_tree_ensembles_that_adapt_to_smoothness_and_sparsity.pdf;/home/dede/Zotero/storage/ZB9IFS7U/rssb.html}
}

@article{linero2020,
  title = {Semiparametric Mixed-Scale Models Using Shared {{Bayesian}} Forests},
  author = {Linero, Antonio R. and Sinha, Debajyoti and Lipsitz, Stuart R.},
  year = {2020},
  journal = {Biometrics},
  volume = {76},
  number = {1},
  pages = {131--144},
  issn = {1541-0420},
  doi = {10.1111/biom.13107},
  abstract = {This paper demonstrates the advantages of sharing information about unknown features of covariates across multiple model components in various nonparametric regression problems including multivariate, heteroscedastic, and semicontinuous responses. In this paper, we present a methodology which allows for information to be shared nonparametrically across various model components using Bayesian sum-of-tree models. Our simulation results demonstrate that sharing of information across related model components is often very beneficial, particularly in sparse high-dimensional problems in which variable selection must be conducted. We illustrate our methodology by analyzing medical expenditure data from the Medical Expenditure Panel Survey (MEPS). To facilitate the Bayesian nonparametric regression analysis, we develop two novel models for analyzing the MEPS data using Bayesian additive regression trees\textemdash a heteroskedastic log-normal hurdle model with a ``shrink-toward-homoskedasticity'' prior and a gamma hurdle model.},
  copyright = {\textcopyright{} 2019 The International Biometric Society},
  langid = {english},
  keywords = {Bayesian additive regression trees,heteroskedastic errors,hurdle models,nonparametric Bayes,todo,variable selection},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/biom.13107},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Linero_et_al_2020_Semiparametric_mixed-scale_models_using_shared_Bayesian_forests.pdf;/home/dede/Zotero/storage/VCGFZ9NS/biom.html}
}

@unpublished{liseo2010,
  title = {Introduzione Alla Statistica Bayesiana},
  author = {Liseo, Brunero},
  year = {2010},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Liseo_2010_Introduzione_alla_statistica_bayesiana.pdf}
}

@article{little1993,
  title = {Post-{{Stratification}}: {{A Modeler}}'s {{Perspective}}},
  shorttitle = {Post-{{Stratification}}},
  author = {Little, R. J. A.},
  year = {1993},
  month = sep,
  journal = {Journal of the American Statistical Association},
  volume = {88},
  number = {423},
  pages = {1001--1012},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.1993.10476368},
  abstract = {Post-stratification is a common technique in survey analysis for incorporating population distributions of variables into survey estimates. The basic technique divides the sample into post-strata, and computes a post-stratification weight w ih = rP h /r h for each sample case in post-stratum h, where r h is the number of survey respondents in post-stratum h, P h is the population proportion from a census, and r is the respondent sample size. Survey estimates, such as functions of means and totals, then weight cases by w h . Variants and extensions of the method include truncation of the weights to avoid excessive variability and raking to a set of two or more univariate marginal distributions. Literature on post-stratification is limited and has mainly taken the randomization (or design-based) perspective, where inference is based on the sampling distribution with population values held fixed. This article develops Bayesian model-based theory for the method. A basic normal post-stratification model is introduced which yields the post-stratified mean as the posterior mean, and a posterior variance that incorporates adjustments for estimating variances. Modifications are then proposed for small sample inference, based on (a) changing the Jeffreys prior for the post-stratum parameters to borrow strength across post-strata, and (b) ignoring partial information about the post-strata. In particular, practical rules for collapsing post-strata to reduce posterior variance are developed and compared with frequentist approaches. Methods for two post-stratifying variables are also considered. Raking sample counts and respondent counts is shown to provide approximate Bayesian inferences when the margins of the two post-stratifiers are available but their joint distribution is not. When the joint distribution is available, raking effectively ignores the information it contains, and hence can be compared with other techniques that ignore information such as collapsing. For inference about means, it is suggested that raking is most appropriate when post-stratum means have an additive or near-additive structure, whereas collapsing is indicated when interactions are present.},
  keywords = {Collapsing strata,Raking,Stratification,Superpopulation models,Survey inference,todo,Weighting},
  annotation = {\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/01621459.1993.10476368},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Little_1993_Post-Stratification.pdf;/home/dede/Zotero/storage/HNC8WCBA/01621459.1993.html}
}

@book{little2019,
  title = {Statistical {{Analysis}} with {{Missing Data}}},
  author = {Little, Roderick J. A. and Rubin, Donald B.},
  year = {2019},
  month = apr,
  edition = {3rd edition},
  publisher = {{Wiley}},
  address = {{Hoboken, NJ}},
  abstract = {An up-to-date, comprehensive treatment of a classic text on missing data in statisticsThe topic of missing data has gained considerable attention in recent decades. This new edition by two acknowledged experts on the subject offers an up-to-date account of practical methodology for handling missing data problems. Blending theory and application, authors Roderick Little and Donald Rubin review historical approaches to the subject and describe simple methods for multivariate analysis with missing values. They then provide a coherent theory for analysis of problems based on likelihoods derived from statistical models for the data and the missing data mechanism, and then they apply the theory to a wide range of important missing data problems.Statistical Analysis with Missing Data, Third Edition starts by introducing readers to the subject and approaches toward solving it. It looks at the patterns and mechanisms that create the missing data, as well as a taxonomy of missing data. It then goes on to examine missing data in experiments, before discussing complete-case and available-case analysis, including weighting methods. The new edition expands its coverage to include recent work on topics such as nonresponse in sample surveys, causal inference, diagnostic methods, and sensitivity analysis, among a host of other topics.  An updated ``classic'' written by renowned authorities on the subject Features over 150 exercises (including many new ones) Covers recent work on important methods like multiple imputation, robust alternatives to weighting, and Bayesian methods Revises previous topics based on past student feedback and class experience Contains an updated and expanded bibliography  The authors were awarded The Karl Pearson Prize in 2017 by the International Statistical Institute, for a research contribution that has had profound influence on statistical theory, methodology or applications. Their work "has been no less than defining and transforming." (ISI)Statistical Analysis with Missing Data, Third Edition is an ideal textbook for upper undergraduate and/or beginning graduate level students of the subject. It is also an excellent source of information for applied statisticians and practitioners in government and industry.},
  isbn = {978-0-470-52679-8},
  langid = {english},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Little_Rubin_2019_Statistical_Analysis_with_Missing_Data.pdf}
}

@inproceedings{liu2010,
  title = {Decentralized Multi-Armed Bandit with Multiple Distributed Players},
  booktitle = {2010 {{Information Theory}} and {{Applications Workshop}} ({{ITA}})},
  author = {Liu, Keqin and Zhao, Qing},
  year = {2010},
  month = jan,
  pages = {1--10},
  doi = {10.1109/ITA.2010.5454071},
  abstract = {We formulate and study a decentralized multi-armed bandit (MAB) problem, where M distributed players compete for N independent arms with unknown reward statistics. At each time, each player chooses one arm to play without exchanging information with other players. Players choosing the same arm collide, and, depending on the collision model, either no one receives reward or the colliding players share the reward in an arbitrary way. We show that the minimum system regret of the decentralized MAB grows with time at the same logarithmic order as in the centralized counterpart where players act collectively as a single entity by exchanging observations and making decisions jointly. A general framework of constructing fair and order-optimal decentralized policies is established based on a Time Division Fair Sharing (TDFS) of the M best arms. A lower bound on the system regret growth rate is established for a general class of decentralized polices, to which all TDFS policies belong. We further develop several fair and order-optimal decentralized polices within the TDFS framework and study their performance in different applications including cognitive radio networks, multi-channel communications in unknown fading environment, target collecting in multi-agent systems, and web search and advertising.},
  keywords = {Advertising,Arm,Cognitive radio,Fading,History,Loss measurement,Multiagent systems,Performance loss,Statistical distributions,todo,Web search},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Liu_Zhao_2010_Decentralized_multi-armed_bandit_with_multiple_distributed_players.pdf;/home/dede/Zotero/storage/U9NPV3PA/5454071.html}
}

@article{liu2013,
  title = {Objective-Oriented Optimal Sensor Allocation Strategy for Process Monitoring and Diagnosis by Multivariate Analysis in a {{Bayesian}} Network},
  author = {Liu, Kaibo and Shi, Jianjun},
  year = {2013},
  month = jun,
  journal = {IIE Transactions},
  volume = {45},
  number = {6},
  pages = {630--643},
  publisher = {{Taylor \& Francis}},
  issn = {0740-817X},
  doi = {10.1080/0740817X.2012.725505},
  abstract = {Measurement strategy and sensor allocation have a direct impact on the product quality, productivity, and cost. This article studies the couplings or interactions between the optimal design of a sensor system and quality management in a manufacturing system, which can improve cost-effectiveness and production yield by considering sensor cost, process change detection speed, and fault diagnosis accuracy. Based on an established definition of sensor allocation in a Bayesian network, an algorithm named ``Best Allocation Subsets by Intelligent Search'' (BASIS) is developed in this article to obtain the optimal sensor allocation design at minimum cost under different specified Average Run Length (ARL) requirements. Unlike previous approaches reported in the literature, the BASIS algorithm is developed based on investigating a multivariate T 2 control chart when only partial observations are available. After implementing the derived optimal sensor solution, a diagnosis ranking method is proposed to find the root cause variables by ranking all of the identified potential faults. Two case studies are conducted on a hot forming process and a cap alignment process to illustrate and evaluate the developed methods.},
  keywords = {Bayesian network,diagnosis ranking,multiple mean shifts,optimal sensor allocation,partial observation,todo},
  annotation = {\_eprint: https://doi.org/10.1080/0740817X.2012.725505},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Liu_Shi_2013_Objective-oriented_optimal_sensor_allocation_strategy_for_process_monitoring.pdf;/home/dede/Zotero/storage/TSXM6EHN/0740817X.2012.html}
}

@article{liu2013a,
  title = {A {{Sequential Rank-Based Nonparametric Adaptive EWMA Control Chart}}},
  author = {Liu, Liu and Zi, Xuemin and Zhang, Jian and Wang, Zhaojun},
  year = {2013},
  month = apr,
  journal = {Communications in Statistics - Simulation and Computation},
  volume = {42},
  number = {4},
  pages = {841--859},
  publisher = {{Taylor \& Francis}},
  issn = {0361-0918},
  doi = {10.1080/03610918.2012.655829},
  abstract = {Nonparametric control chart is useful when the underlying distribution is unknown, or is not likely to be normal. In this article, we provide a sequential rank-based nonparametric adaptive EWMA (NAE) control chart for detecting the persistent shift in the location parameter. This NAE chart is a self-starting scheme and thus can be used to monitor processes at the start-up stages rather than waiting for the accumulation of sufficiently large calibration samples. Moreover, we do not require any prior knowledge of the underlying distribution, and to prespecify any tuning parameter either. A Markov chain model is suggested to calibrate the run-length distribution of NAE, which is shown to have approximate tail probability as a geometric distribution. A simulation study demonstrates that the proposed control chart not only performs robustly for different distributions, but also is efficient in detecting various magnitude of shifts. A real-data example from manufacturing shows that it performs quite well in practical applications.},
  keywords = {Adaptive control chart,EWMA,Nonparametric scheme,Primary 62P05,Run-length distribution,Secondary 62G09,Sequential rank},
  annotation = {\_eprint: https://doi.org/10.1080/03610918.2012.655829},
  file = {/home/dede/Zotero/storage/DCL4VTNZ/03610918.2012.html}
}

@article{liu2014,
  title = {Adaptive {{Sensor Allocation Strategy}} for {{Process Monitoring}} and {{Diagnosis}} in a {{Bayesian Network}}},
  author = {Liu, Kaibo and Zhang, Xi and Shi, Jianjun},
  year = {2014},
  month = apr,
  journal = {IEEE Transactions on Automation Science and Engineering},
  volume = {11},
  number = {2},
  pages = {452--462},
  issn = {1558-3783},
  doi = {10.1109/TASE.2013.2287101},
  abstract = {Multivariate process control in Distributed Sensor Networks (DSNs) is an important and challenging topic. Although a fully deployed sensor network will minimize information loss, the associated sensing cost can be overwhelming. Many efforts have been made to investigate the optimal sensor allocation strategy for different process control applications; however, most of them assume that the sensor layout is fixed once sensors are deployed in the system. This paper proposes a novel approach to adaptively reallocate sensor resources based on online observations, which can enhance both monitoring and diagnosis capabilities. The proposed adaptive sensor allocation strategy addresses two fundamental issues: when to reallocate sensors and how to update sensor layout. A max-min criterion is developed to manage sensor reallocation and process change detection in an integrated manner. To investigate the adaptive strategy, a Bayesian Network (BN) model is assumed available to represent the causal relationships among a set of variables. Case studies are performed on a hot forming process and a cap alignment process to illustrate the procedure and evaluate the performance of the proposed method under different fault scenarios.},
  keywords = {Adaptive sensor allocation strategy,Bayesian network (BN),Delays,Fault diagnosis,Layout,maxâ€“min criterion,Monitoring,Process control,process monitoring and diagnosis,Resource management,Sensor systems,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Liu_et_al_2014_Adaptive_Sensor_Allocation_Strategy_for_Process_Monitoring_and_Diagnosis_in_a.pdf;/home/dede/Zotero/storage/IQZWEXQP/6676856.html}
}

@article{liu2014a,
  title = {Adaptive Nonparametric {{CUSUM}} Scheme for Detecting Unknown Shifts in Location},
  author = {Liu, Liu and Tsung, Fugee and Zhang, Jian},
  year = {2014},
  month = mar,
  journal = {International Journal of Production Research},
  volume = {52},
  number = {6},
  pages = {1592--1606},
  publisher = {{Taylor \& Francis}},
  issn = {0020-7543},
  doi = {10.1080/00207543.2013.812260},
  abstract = {In this paper, we provide a sequential rank-based adaptive nonparametric cumulative sum control chart for detecting a range of shifts in the location parameter. This chart is a self-starting scheme, and thus can be used to monitor processes at the start-up stages rather than having to wait for the accumulation of sufficiently large calibration samples. It does not require any prior knowledge of the underlying distribution. The choice of the chart parameters is studied and a simulation study demonstrates that the proposed control chart not only performs robustly for different distributions, but also efficiently detects various magnitudes of shifts. An illustrative example is also given to introduce the implementation of this proposed control chart.},
  keywords = {adaptive control chart,CUSUM,nonparametric scheme,sequential rank},
  annotation = {\_eprint: https://doi.org/10.1080/00207543.2013.812260},
  file = {/home/dede/Zotero/storage/L4LFJBSZ/00207543.2013.html}
}

@article{liu2015a,
  title = {An {{Adaptive Sampling Strategy}} for {{Online High-Dimensional Process Monitoring}}},
  author = {Liu, Kaibo and Mei, Yajun and Shi, Jianjun},
  year = {2015},
  month = jul,
  journal = {Technometrics},
  volume = {57},
  number = {3},
  pages = {305--319},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.2014.947005},
  abstract = {Temporally and spatially dense data-rich environments provide unprecedented opportunities and challenges for effective process control. In this article, we propose a systematic and scalable adaptive sampling strategy for online high-dimensional process monitoring in the context of limited resources with only partial information available at each acquisition time. The proposed adaptive sampling strategy includes a broad range of applications: (1) when only a limited number of sensors is available; (2) when only a limited number of sensors can be in ``ON'' state in a fully deployed sensor network; and (3) when only partial data streams can be analyzed at the fusion center due to limited transmission and processing capabilities even though the full data streams have been acquired remotely. A monitoring scheme of using the sum of top-r local CUSUM statistics is developed and named as ``TRAS'' (top-r based adaptive sampling), which is scalable and robust in detecting a wide range of possible mean shifts in all directions, when each data stream follows a univariate normal distribution. Two properties of this proposed method are also investigated. Case studies are performed on a hot-forming process and a real solar flare process to illustrate and evaluate the performance of the proposed method.},
  keywords = {done,Multiple data streams,Partial information over the spatial domain,Sensor redeployment,Shift detection,Sum of the top-r local statistics},
  annotation = {\_eprint: https://doi.org/10.1080/00401706.2014.947005},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Liu_et_al_2015_An_Adaptive_Sampling_Strategy_for_Online_High-Dimensional_Process_Monitoring.pdf;/home/dede/Zotero/storage/YM8TVDT6/00401706.2014.html}
}

@article{liu2015b,
  title = {Dual {{Nonparametric CUSUM Control Chart Based}} on {{Ranks}}},
  author = {Liu, Liu and Zhang, Jian and Zi, Xuemin},
  year = {2015},
  month = mar,
  journal = {Communications in Statistics - Simulation and Computation},
  volume = {44},
  number = {3},
  pages = {756--772},
  publisher = {{Taylor \& Francis}},
  issn = {0361-0918},
  doi = {10.1080/03610918.2013.784985},
  abstract = {In this article, we provide a sequential rank-based dual nonparametric CUSUM (DNC) control chart for detecting arbitrary magnitude of shifts in the location parameter. It is a self-starting scheme and thus can be used to monitor processes at the start-up stages. Moreover, we do not require any prior knowledge of the underlying distribution. A simulation study demonstrates that the proposed control chart not only performs robustly for different distributions, but also is efficient in detecting various magnitudes of shifts. An illustrative example is given to introduce the implementation of our proposed DNC control chart. It is easy to construct and fast to compute.},
  keywords = {62G10,62K05,Arbitrary shift,CUSUM,Dual control chart,Nonparametric scheme,Sequential rank},
  annotation = {\_eprint: https://doi.org/10.1080/03610918.2013.784985},
  file = {/home/dede/Zotero/storage/VRQI7AXL/03610918.2013.html}
}

@inproceedings{liu2018,
  title = {A {{Change-Detection}} Based {{Framework}} for {{Piecewise-stationary Multi-Armed Bandit Problem}}},
  booktitle = {{{AAAI}}},
  author = {Liu, F. and Lee, Joohyung and Shroff, N.},
  year = {2018},
  abstract = {This paper develops and studies a class of change-detection based UCB (Upper Confidence Bound) policies, CD-UCB, that actively detects change points and restarts the UCB indices, and shows that CUSUM- UCB obtains the best known regret upper bound under mild assumptions. The multi-armed bandit problem has been extensively studied under the stationary assumption. However in reality, this assumption often does not hold because the distributions of rewards themselves may change over time. In this paper, we propose a change-detection (CD) based framework for multi-armed bandit problems under the piecewise-stationary setting, and study a class of change-detection based UCB (Upper Confidence Bound) policies, CD-UCB, that actively detects change points and restarts the UCB indices. We then develop CUSUM-UCB and PHT-UCB, that belong to the CD-UCB class and use cumulative sum (CUSUM) and Page-Hinkley Test (PHT) to detect changes. We show that CUSUM-UCB obtains the best known regret upper bound under mild assumptions. We also demonstrate the regret reduction of the CD-UCB policies over arbitrary Bernoulli rewards and Yahoo! datasets of webpage click-through rates.},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Liu_et_al_2018_A_Change-Detection_based_Framework_for_Piecewise-stationary_Multi-Armed_Bandit2.pdf}
}

@article{liu2021,
  title = {Variable {{Selection Via Thompson Sampling}}},
  author = {Liu, Yi and Ro{\v c}kov{\'a}, Veronika},
  year = {2021},
  month = may,
  journal = {Journal of the American Statistical Association},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.2021.1928514},
  abstract = {\textendash Thompson sampling is a heuristic algorithm for the multi-armed bandit problem which has a long tradition in machine learning. The algorithm has a Bayesian spirit in the sense that it selects arms based on posterior samples of reward probabilities of each arm. By forging a connection between combinatorial binary bandits and spike-and-slab variable selection, we propose a stochastic optimization approach to subset selection called Thompson variable selection (TVS). TVS is a framework for interpretable machine learning which does not rely on the underlying model to be linear. TVS brings together Bayesian reinforcement and machine learning in order to extend the reach of Bayesian subset selection to nonparametric models and large datasets with very many predictors and/or very many observations. Depending on the choice of a reward, TVS can be deployed in offline as well as online setups with streaming data batches. Tailoring multiplay bandits to variable selection, we provide regret bounds without necessarily assuming that the arm mean rewards be unrelated. We show a very strong empirical performance on both simulated and real data. Unlike deterministic optimization methods for spike-and-slab variable selection, the stochastic nature makes TVS less prone to local convergence and thereby more robust.},
  keywords = {BART,Combinatorial bandits,done,Interpretable machine learning,Spike-and-slab,Thompson sampling,Variable selection},
  annotation = {\_eprint: https://doi.org/10.1080/01621459.2021.1928514},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Liu_RoÄkovÃ¡_2021_Variable_Selection_Via_Thompson_Sampling.pdf;/home/dede/Documents/MEGA/universita/zotero-pdf/Liu_RoÄkovÃ¡_2021_Variable_Selection_Via_Thompson_Sampling2.pdf;/home/dede/Zotero/storage/YNIRYNUL/01621459.2021.html}
}

@article{lockhart2014,
  title = {A Significance Test for the Lasso},
  author = {Lockhart, Richard and Taylor, Jonathan and Tibshirani, Ryan J. and Tibshirani, Robert},
  year = {2014},
  month = apr,
  journal = {The Annals of Statistics},
  volume = {42},
  number = {2},
  pages = {413--468},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/13-AOS1175},
  abstract = {In the sparse linear regression setting, we consider testing the significance of the predictor variable that enters the current lasso model, in the sequence of models visited along the lasso solution path. We propose a simple test statistic based on lasso fitted values, called the covariance test statistic, and show that when the true model is linear, this statistic has an \$\textbackslash operatorname\{Exp\}(1)\$ asymptotic distribution under the null hypothesis (the null being that all truly active variables are contained in the current lasso model). Our proof of this result for the special case of the first predictor to enter the model (i.e., testing for a single significant predictor variable against the global null) requires only weak assumptions on the predictor matrix \$X\$. On the other hand, our proof for a general step in the lasso path places further technical assumptions on \$X\$ and the generative model, but still allows for the important high-dimensional case \$p{$>$}n\$, and does not necessarily require that the current lasso model achieves perfect recovery of the truly active variables. Of course, for testing the significance of an additional variable between two nested linear models, one typically uses the chi-squared test, comparing the drop in residual sum of squares (RSS) to a \$\textbackslash chi\^\{2\}\_\{1\}\$ distribution. But when this additional variable is not fixed, and has been chosen adaptively or greedily, this test is no longer appropriate: adaptivity makes the drop in RSS stochastically much larger than \$\textbackslash chi\^\{2\}\_\{1\}\$ under the null hypothesis. Our analysis explicitly accounts for adaptivity, as it must, since the lasso builds an adaptive sequence of linear models as the tuning parameter \$\textbackslash lambda\$ decreases. In this analysis, shrinkage plays a key role: though additional variables are chosen adaptively, the coefficients of lasso active variables are shrunken due to the \$\textbackslash ell\_\{1\}\$ penalty. Therefore, the test statistic (which is based on lasso fitted values) is in a sense balanced by these two opposing properties\textemdash adaptivity and shrinkage\textemdash and its null distribution is tractable and asymptotically \$\textbackslash operatorname\{Exp\}(1)\$.},
  keywords = {$p$-value,62F03,62J05,62J07,Lasso,least angle regression,significance test},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Lockhart_et_al_2014_A_significance_test_for_the_lasso.pdf;/home/dede/Zotero/storage/5WXMMFTI/13-AOS1175.html}
}

@article{lombard2017,
  title = {Signed {{Sequential Rank CUSUMs}}},
  author = {Lombard, F. and Van Zyl, C.},
  year = {2017},
  month = jun,
  journal = {arXiv:1706.03901 [stat]},
  eprint = {1706.03901},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {CUSUMs based on the signed sequential ranks of observations are developed for detecting location and scale changes in symmetric distributions. The CUSUMs are distribution free and fully self-starting: given a specified in-control median and nominal in-control average run length, no parametric specification of the underlying distribution is required in order to find the correct control limits. If the underlying distribution is normal with unknown variance, a CUSUM based on the Van der Waerden signed rank score produces out-of-control average run lengths that are commensurate with those produced by the standard CUSUM for a normal distribution with known variance. For heavier tailed distributions, use of a CUSUM based on the Wilcoxon signed rank score is indicated. The methodology is illustrated by application to real data from an industrial environment.},
  archiveprefix = {arXiv},
  keywords = {done,Statistics - Applications,Statistics - Methodology},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Lombard_Van_Zyl_2017_Signed_Sequential_Rank_CUSUMs.pdf;/home/dede/Zotero/storage/MJ3SE9QZ/1706.html}
}

@incollection{lopes2011,
  title = {Particle {{Learning}} for {{Sequential Bayesian Computation}}*},
  booktitle = {Bayesian {{Statistics}} 9},
  author = {Lopes, Hedibert and Johannes, Michael and Carvalho, Carlos and Polson, Nicholas},
  year = {2011},
  month = oct,
  pages = {317--360},
  doi = {10.1093/acprof:oso/9780199694587.003.0011},
  abstract = {Particle learning provides a simulation-based approach to sequential Bayesian computation. To sample from a posterior distribution of interest we use an essential state vector together with a predictive distribution and propagation rule to build a resampling-sampling framework. Predictive inference and sequential Bayes factors are a direct by-product. Our approach provides a simple yet powerful framework for the construction of sequential posterior sampling strategies for a variety of commonly used models.},
  isbn = {978-0-19-969458-7}
}

@book{lovell,
  title = {The {{Soviet Union}}: {{A Very Short Introduction}}},
  shorttitle = {The {{Soviet Union}}},
  author = {Lovell, Stephen},
  journal = {The Soviet Union: A Very Short Introduction},
  publisher = {{Oxford University Press}},
  abstract = {"The Soviet Union: A Very Short Introduction" published on  by Oxford University Press.},
  isbn = {978-0-19-177716-5},
  langid = {american},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Lovell_The_Soviet_Union.epub;/home/dede/Zotero/storage/DL9AWDAC/actrade-9780199238484.html}
}

@article{lowry1992,
  title = {A {{Multivariate Exponentially Weighted Moving Average Control Chart}}},
  author = {Lowry, Cynthia A. and Woodall, William H. and Champ, Charles W. and Rigdon, Steven E.},
  year = {1992},
  month = feb,
  journal = {Technometrics},
  volume = {34},
  number = {1},
  pages = {46--53},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.1992.10485232},
  abstract = {A multivariate extension of the exponentially weighted moving average (EWMA) control chart is presented, and guidelines given for designing this easy-to-implement multivariate procedure. A comparison shows that the average run length (ARL) performance of this chart is similar to that of multivariate cumulative sum (CUSUM) control charts in detecting a shift in the mean vector of a multivariate normal distribution. As with the Hotelling's {$\chi$}2 and multivariate CUSUM charts, the ARL performance of the multivariate EWMA chart depends on the underlying mean vector and covariance matrix only through the value of the noncentrality parameter. Worst-case scenarios show that Hotelling's {$\chi$}2 charts should always be used in conjunction with multivariate CUSUM and EWMA charts to avoid potential inertia problems. Examples are given to illustrate the use of the proposed procedure.},
  keywords = {Average run length,done,Hotelling's T 2 chart,Multivariate CUSUM,Statistical process control},
  annotation = {\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/00401706.1992.10485232},
  file = {/home/dede/Zotero/storage/KR2GWAU5/00401706.1992.html}
}

@article{lu2019,
  title = {On Adaptive Estimation for Dynamic {{Bernoulli}} Bandits},
  author = {Lu, Xue and Adams, Niall and Kantas, Nikolas},
  year = {2019},
  journal = {Foundations of Data Science},
  volume = {1},
  number = {2},
  pages = {197},
  publisher = {{American Institute of Mathematical Sciences}},
  doi = {10.3934/fods.2019009},
  abstract = {{$<$}p style='text-indent:20px;'{$>$}The multi-armed bandit (MAB) problem is a classic example of the exploration-exploitation dilemma. It is concerned with maximising the total rewards for a gambler by sequentially pulling an arm from a multi-armed slot machine where each arm is associated with a reward distribution. In static MABs, the reward distributions do not change over time, while in dynamic MABs, each arm's reward distribution can change, and the optimal arm can switch over time. Motivated by many real applications where rewards are binary, we focus on dynamic Bernoulli bandits. Standard methods like {$<$}inline-formula{$><$}tex-math id="M1"{$>\backslash$}begin\{document\}\$ \textbackslash epsilon \$\textbackslash end\{document\}{$<$}/tex-math{$><$}/inline-formula{$>$}-Greedy and Upper Confidence Bound (UCB), which rely on the sample mean estimator, often fail to track changes in the underlying reward for dynamic problems. In this paper, we overcome the shortcoming of slow response to change by deploying adaptive estimation in the standard methods and propose a new family of algorithms, which are adaptive versions of {$<$}inline-formula{$><$}tex-math id="M2"{$>\backslash$}begin\{document\}\$ \textbackslash epsilon \$\textbackslash end\{document\}{$<$}/tex-math{$><$}/inline-formula{$>$}-Greedy, UCB, and Thompson sampling. These new methods are simple and easy to implement. Moreover, they do not require any prior knowledge about the dynamic reward process, which is important for real applications. We examine the new algorithms numerically in different scenarios and the results show solid improvements of our algorithms in dynamic environments.{$<$}/p{$>$}},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Lu_et_al_2019_On_adaptive_estimation_for_dynamic_Bernoulli_bandits.pdf;/home/dede/Zotero/storage/FVP27Z9P/fods.html}
}

@book{luck2014,
  title = {{An Introduction to the Event-Related Potential Technique}},
  author = {Luck, Steven J.},
  year = {2014},
  month = may,
  edition = {Seconda edizione},
  publisher = {{A Bradford Book}},
  abstract = {An essential guide to designing, conducting, and analyzing event-related potential (ERP) experiments, completely updated for this edition.The event-related potential (ERP) technique, in which neural responses to specific events are extracted from the EEG, provides a powerful noninvasive tool for exploring the human brain. This volume describes practical methods for ERP research along with the underlying theoretical rationale. It offers researchers and students an essential guide to designing, conducting, and analyzing ERP experiments. This second edition has been completely updated, with additional material, new chapters, and more accessible explanations. Freely available supplementary material, including several online-only chapters, offer expanded or advanced treatment of selected topics.The first half of the book presents essential background information, describing the origins of ERPs, the nature of ERP components, and the design of ERP experiments. The second half of the book offers a detailed treatment of the main steps involved in conducting ERP experiments, covering such topics as recording the EEG, filtering the EEG and ERP waveforms, and quantifying amplitudes and latencies. Throughout, the emphasis is on rigorous experimental design and relatively simple analyses. New material in the second edition includes entire chapters devoted to components, artifacts, measuring amplitudes and latencies, and statistical analysis; updated coverage of recording technologies; concrete examples of experimental design; and many more figures. Online chapters cover such topics as overlap, localization, writing and reviewing ERP papers, and setting up and running an ERP lab.},
  isbn = {978-0-262-52585-5},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Luck_2014_An_Introduction_to_the_Event-Related_Potential_Technique.pdf}
}

@article{ludwig2015,
  title = {{{MapReduce-based}} Fuzzy c-Means Clustering Algorithm: Implementation and Scalability},
  shorttitle = {{{MapReduce-based}} Fuzzy c-Means Clustering Algorithm},
  author = {Ludwig, Simone},
  year = {2015},
  journal = {International Journal of Machine Learning and Cybernetics},
  volume = {6},
  abstract = {The management and analysis of big data has been identified as one of the most important emerging needs in recent years. This is because of the sheer volume and increasing complexity of data being created or collected. Current clustering algorithms can not handle big data, and therefore, scalable solutions are necessary. Since fuzzy clustering algorithms have shown to outperform hard clustering approaches in terms of accuracy, this paper investigates the parallelization and scalability of a common and effective fuzzy clustering algorithm named fuzzy c-means (FCM) algorithm. The algorithm is parallelized using the MapReduce paradigm outlining how the Map and Reduce primitives are implemented. A validity analysis is conducted in order to show that the implementation works correctly achieving competitive purity results compared to state-of-the art clustering algorithms. Furthermore, a scalability analysis is conducted to demonstrate the performance of the parallel FCM implementation with increasing number of computing nodes used.},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Ludwig_2015_MapReduce-based_fuzzy_c-means_clustering_algorithm.pdf}
}

@incollection{lukas2014,
  title = {Regularization {{Methods}}},
  booktitle = {Wiley {{StatsRef}}: {{Statistics Reference Online}}},
  author = {Lukas, Mark A.},
  editor = {Balakrishnan, N. and Colton, Theodore and Everitt, Brian and Piegorsch, Walter and Ruggeri, Fabrizio and Teugels, Jozef L.},
  year = {2014},
  month = sep,
  pages = {stat07415},
  publisher = {{John Wiley \& Sons, Ltd}},
  address = {{Chichester, UK}},
  doi = {10.1002/9781118445112.stat07415},
  isbn = {978-1-118-44511-2},
  langid = {english},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Lukas_2014_Regularization_Methods.pdf}
}

@book{lutkepohl2005,
  title = {New {{Introduction}} to {{Multiple Time Series Analysis}}},
  author = {L{\"u}tkepohl, Helmut},
  year = {2005},
  publisher = {{Springer-Verlag}},
  address = {{Berlin Heidelberg}},
  doi = {10.1007/978-3-540-27752-1},
  abstract = {This reference work and graduate level textbook considers a wide range of models and methods for analyzing and forecasting multiple time series. The models covered include vector autoregressive, cointegrated,vector autoregressive moving average, multivariate ARCH and periodic processes as well as dynamic simultaneous equations and state space models. Least squares, maximum likelihood and Bayesian methods are considered for estimating these models. Different procedures for model selection and model specification are treated and a wide range of tests and criteria for model checking are introduced. Causality analysis, impulse response analysis and innovation accounting are presented as tools for structural analysis. The book is accessible to graduate students in business and economics. In addition, multiple time series courses in other fields such as statistics and engineering may be based on it. Applied researchers involved in analyzing multiple time series may benefit from the book as it provides the background and tools for their tasks. It bridges the gap to the difficult technical literature on the topic.},
  isbn = {978-3-540-40172-8},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/LÃ¼tkepohl_2005_New_Introduction_to_Multiple_Time_Series_Analysis.pdf;/home/dede/Zotero/storage/GP2NNSUV/9783540401728.html}
}

@article{ma2021,
  title = {A {{General Framework}} of {{Online Updating Variable Selection}} for {{Generalized Linear Models}} with {{Streaming Datasets}}},
  author = {Ma, Xiaoyu and Lin, Lu and Gai, Yujie},
  year = {2021},
  month = jan,
  journal = {arXiv:2101.08639 [stat]},
  eprint = {2101.08639},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {In the research field of big data, one of important issues is how to recover the sequentially changing sets of true features when the data sets arrive sequentially. The paper presents a general framework for online updating variable selection and parameter estimation in generalized linear models with streaming datasets. This is a type of online updating penalty likelihoods with differentiable or non-differentiable penalty function. The online updating coordinate descent algorithm is proposed to solve the online updating optimization problem. Moreover, a tuning parameter selection is suggested in an online updating way. The selection and estimation consistencies, and the oracle property are established, theoretically. Our methods are further examined and illustrated by various numerical examples from both simulation experiments and a real data analysis.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Ma_et_al_2021_A_General_Framework_of_Online_Updating_Variable_Selection_for_Generalized.pdf;/home/dede/Zotero/storage/3VDQNFPD/2101.html}
}

@article{maaten2008,
  title = {Visualizing {{Data}} Using T-{{SNE}}},
  author = {van der Maaten, Laurens and Hinton, Geoffrey},
  year = {2008},
  journal = {Journal of Machine Learning Research},
  volume = {9},
  number = {Nov},
  pages = {2579--2605},
  issn = {ISSN 1533-7928},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Maaten_Hinton_2008_Visualizing_Data_using_t-SNE.pdf;/home/dede/Zotero/storage/ZTLT2UFS/vandermaaten08a.html}
}

@article{maboudou-tchao2011,
  title = {Self-{{Starting Multivariate Control Charts}} for {{Location}} and {{Scale}}},
  author = {{Maboudou-Tchao}, Edgard M. and Hawkins, Douglas M.},
  year = {2011},
  month = apr,
  journal = {Journal of Quality Technology},
  volume = {43},
  number = {2},
  pages = {113--126},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2011.11917850},
  abstract = {Multivariate control charts are advisable when monitoring several correlated characteristics. The multivariate exponentially weighted moving average (MEWMA) is ideal for monitoring the mean vector, and the multivariate exponentially weighted moving covariance matrix (MEWMC) detects changes in the covariance matrix. Both charts were established under the assumption that the parameters are known a priori. This is seldom the case, and Phase I data sets are commonly used to estimate the chart's in-control parameter values. Plugging in parameter estimates, however, fundamentally changes the run-length distribution from those assumed in the known-parameter theory and diminishes chart performance, even for large calibration samples. Self-starting methods, which correctly studentize the incoming stream of process readings, provide exact control right from start up. We extend the existing multivariate self-starting methodology to a combination chart for both the mean vector and the covariance matrix. This approach is shown to have good performance.},
  keywords = {Average Run Length (ARL),Cholesky Decomposition,done,Multistandardization,Recursive Residual,Regression Adjustment},
  annotation = {\_eprint: https://doi.org/10.1080/00224065.2011.11917850},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Maboudou-Tchao_Hawkins_2011_Self-Starting_Multivariate_Control_Charts_for_Location_and_Scale.pdf}
}

@article{maboudou-tchao2013,
  title = {Monitoring the Covariance Matrix with Fewer Observations than Variables},
  author = {{Maboudou-Tchao}, Edgard M. and Agboto, Vincent},
  year = {2013},
  journal = {Computational Statistics \& Data Analysis},
  volume = {64},
  number = {C},
  pages = {99--112},
  publisher = {{Elsevier}},
  issn = {0167-9473},
  abstract = {Multivariate control charts are essential tools in multivariate statistical process control. In real applications, when a multivariate process shifts, it occurs in either location or scale. Several methods have been proposed recently to monitor the covariance matrix. Most of these methods deal with a full rank covariance matrix, i.e., in a situation where the number of rational subgroups is larger than the number of variables. When the number of features is nearly as large as, or larger than, the number of observations, existing Shewhart-type charts do not provide a satisfactory solution because the estimated covariance matrix is singular. A new Shewhart-type chart for monitoring changes in the covariance matrix of a multivariate process when the number of observations available is less than the number of variables is proposed. This chart can be used to monitor the covariance matrix with only one observation. The new control chart is based on using the graphical LASSO estimator of the covariance matrix instead of the traditional sample covariance matrix. The LASSO estimator is used here because of desirable properties such as being non-singular and positive definite even when the number of observations is less than the number of variables. The performance of this new chart is compared to that of several Shewhart control charts for monitoring the covariance matrix.},
  keywords = {Average run length (ARL),Cholesky decomposition,Covariance matrix,Multistandardization,Penalized likelihood function,toRead},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Maboudou-Tchao_Agboto_2013_Monitoring_the_covariance_matrix_with_fewer_observations_than_variables.pdf;/home/dede/Zotero/storage/ENX9RHFQ/v_3a64_3ay_3a2013_3ai_3ac_3ap_3a99-112.html}
}

@article{maceachern1998,
  title = {Estimating {{Mixture}} of {{Dirichlet Process Models}}},
  author = {MacEachern, Steven N. and M{\"u}ller, Peter},
  year = {1998},
  journal = {Journal of Computational and Graphical Statistics},
  volume = {7},
  number = {2},
  pages = {223--238},
  publisher = {{[American Statistical Association, Taylor \& Francis, Ltd., Institute of Mathematical Statistics, Interface Foundation of America]}},
  issn = {1061-8600},
  doi = {10.2307/1390815},
  abstract = {Current Gibbs sampling schemes in mixture of Dirichlet process (MDP) models are restricted to using "conjugate" base measures that allow analytic evaluation of the transition probabilities when resampling configurations, or alternatively need to rely on approximate numeric evaluations of some transition probabilities. Implementation of Gibbs sampling in more general MDP models is an open and important problem because most applications call for the use of nonconjugate base measures. In this article we propose a conceptual framework for computational strategies. This framework provides a perspective on current methods, facilitates comparisons between them, and leads to several new methods that expand the scope of MDP models to nonconjugate situations. We discuss one in detail. The basic strategy is based on expanding the parameter vector, and is applicable for MDP models with arbitrary base measure and likelihood. Strategies are also presented for the important class of normal-normal MDP models and for problems with fixed or few hyperparameters. The proposed algorithms are easily implemented and illustrated with an application.},
  keywords = {todo}
}

@inproceedings{madry2018,
  title = {Towards {{Deep Learning Models Resistant}} to {{Adversarial Attacks}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  year = {2018},
  month = feb,
  abstract = {We provide a principled, optimization-based re-look at the notion of adversarial examples, and develop methods that produce models that are adversarially robust against a wide range of adversaries.},
  langid = {english},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Madry_et_al_2018_Towards_Deep_Learning_Models_Resistant_to_Adversarial_Attacks.pdf;/home/dede/Zotero/storage/93YRZT9H/forum.html}
}

@article{magan-carrion2020,
  title = {Multivariate {{Statistical Network Monitoring}}\textendash{{Sensor}}: {{An}} Effective Tool for Real-Time Monitoring and Anomaly Detection in Complex Networks and Systems},
  shorttitle = {Multivariate {{Statistical Network Monitoring}}\textendash{{Sensor}}},
  author = {{Mag{\'a}n-Carri{\'o}n}, Roberto and Camacho, Jos{\'e} and {Maci{\'a}-Fern{\'a}ndez}, Gabriel and {Ru{\'i}z-Zafra}, {\'A}ngel},
  year = {2020},
  month = may,
  journal = {International Journal of Distributed Sensor Networks},
  volume = {16},
  number = {5},
  pages = {155014772092130},
  issn = {1550-1477, 1550-1477},
  doi = {10.1177/1550147720921309},
  abstract = {Technology evolves quickly. Low-cost and ready-to-connect devices are designed to provide new services and applications. Smart grids or smart health care systems are some examples of these applications. In this totally connected scenario, some security issues arise due to the large number of devices and communications. In this way, new solutions for monitoring and detecting security events are needed to address new challenges brought about by this scenario, among others, the real-time requirement allowing quick security event detection and, consequently, quick response to attacks. In this sense, Intrusion Detection Systems are widely used though their evaluation often relies on the use of predefined network datasets that limit their application in real environments. In this work, a real-time and ready-to-use tool for monitoring and detecting security events is introduced. The Multivariate Statistical Network Monitoring\textendash Sensor is based on the Multivariate Statistical Network Monitoring methodology and provides an alternative way for evaluating Multivariate Statistical Network Monitoring\textendash based Intrusion Detection System solutions. Experimental results based on the detection of well-known attacks in hierarchical network systems prove the suitability of this tool for complex scenarios, such as those found in smart cities or Internet of Things ecosystems.},
  langid = {english},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/MagÃ¡n-CarriÃ³n_et_al_2020_Multivariate_Statistical_Network_Monitoringâ€“Sensor.pdf}
}

@book{magnus2019,
  title = {{Matrix Differential Calculus with Applications in Statistics and Econometrics}},
  author = {Magnus, Jan R. and Neudecker, Heinz},
  year = {2019},
  month = mar,
  edition = {3\textdegree{} edizione},
  publisher = {{Wiley}},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Magnus_Neudecker_2019_Matrix_Differential_Calculus_with_Applications_in_Statistics_and_Econometrics.pdf}
}

@article{malik2020,
  title = {A {{Hierarchy}} of {{Limitations}} in {{Machine Learning}}},
  author = {Malik, Momin M.},
  year = {2020},
  month = feb,
  journal = {arXiv:2002.05193 [cs, econ, math, stat]},
  eprint = {2002.05193},
  eprinttype = {arxiv},
  primaryclass = {cs, econ, math, stat},
  abstract = {"All models are wrong, but some are useful", wrote George E. P. Box (1979). Machine learning has focused on the usefulness of probability models for prediction in social systems, but is only now coming to grips with the ways in which these models are wrong---and the consequences of those shortcomings. This paper attempts a comprehensive, structured overview of the specific conceptual, procedural, and statistical limitations of models in machine learning when applied to society. Machine learning modelers themselves can use the described hierarchy to identify possible failure points and think through how to address them, and consumers of machine learning models can know what to question when confronted with the decision about if, where, and how to apply machine learning. The limitations go from commitments inherent in quantification itself, through to showing how unmodeled dependencies can lead to cross-validation being overly optimistic as a way of assessing model performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society,Computer Science - Machine Learning,Economics - Econometrics,G.3,I.6.4,J.4,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Malik_2020_A_Hierarchy_of_Limitations_in_Machine_Learning.pdf;/home/dede/Zotero/storage/4SMQUKMA/2002.html}
}

@article{marchant2021,
  title = {D-Blink: {{Distributed End-to-End Bayesian Entity Resolution}}},
  shorttitle = {D-Blink},
  author = {Marchant, Neil G. and Kaplan, Andee and Elazar, Daniel N. and Rubinstein, Benjamin I. P. and Steorts, Rebecca C.},
  year = {2021},
  month = apr,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {30},
  number = {2},
  pages = {406--421},
  publisher = {{Taylor \& Francis}},
  issn = {1061-8600},
  doi = {10.1080/10618600.2020.1825451},
  abstract = {Entity resolution (ER; also known as record linkage or de-duplication) is the process of merging noisy databases, often in the absence of unique identifiers. A major advancement in ER methodology has been the application of Bayesian generative models, which provide a natural framework for inferring latent entities with rigorous quantification of uncertainty. Despite these advantages, existing models are severely limited in practice, as standard inference algorithms scale quadratically in the number of records. While scaling can be managed by fitting the model on separate blocks of the data, such a na\"ive approach may induce significant error in the posterior. In this article, we propose a principled model for scalable Bayesian ER, called ``distributed Bayesian linkage'' or d-blink, which jointly performs blocking and ER without compromising posterior correctness. Our approach relies on several key ideas, including: (i) an auxiliary variable representation that induces a partition of the entities and records into blocks; (ii) a method for constructing well-balanced blocks based on k-d trees; (iii) a distributed partially collapsed Gibbs sampler with improved mixing; and (iv) fast algorithms for performing Gibbs updates. Empirical studies on six datasets\textemdash including a case study on the 2010 Decennial Census\textemdash demonstrate the scalability and effectiveness of our approach. Supplementary materials for this article are available online.},
  keywords = {Auxiliary variable,Distributed computing,Markov chain Monte Carlo,Partially collapsed Gibbs sampling,Record linkage,todo},
  annotation = {\_eprint: https://doi.org/10.1080/10618600.2020.1825451},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Marchant_et_al_2021_d-blink.pdf;/home/dede/Zotero/storage/WGY83H2R/10618600.2020.html}
}

@article{marcus1976,
  title = {On {{Closed Testing Procedures}} with {{Special Reference}} to {{Ordered Analysis}} of {{Variance}}},
  author = {Marcus, Ruth and Peritz, Eric and Gabriel, K. R.},
  year = {1976},
  journal = {Biometrika},
  volume = {63},
  number = {3},
  pages = {655--660},
  publisher = {{[Oxford University Press, Biometrika Trust]}},
  issn = {0006-3444},
  doi = {10.2307/2335748},
  abstract = {A method of devising stepwise multiple testing procedures with fixed experimentwise error is presented. The method requires the set of hypotheses tested to be closed under intersection. The method is applied to the problem of comparing many treatments to one control and to ordered analysis of variance.},
  keywords = {todo}
}

@book{mardia1979,
  title = {Multivariate {{Analysis}}},
  author = {Mardia, K. V. and Kent, John T. and Kent, J. T. and Bibby, John M.},
  year = {1979},
  publisher = {{Academic Press}},
  abstract = {Multivariate Analysis deals with observations on more than one variable where there is some inherent interdependence between the variables. With several texts already available in this area, one may very well enquire of the authors as to the need for yet another book. Most of the available books fall into two categories, either theoretical or data analytic. The present book not only combines the two approaches but it also has been guided by the need to give suitable matter for the beginner as well as illustrating some deeper aspects of the subject for the research worker. Practical examples are kept to the forefront and, wherever feasible, each technique is motivated by such an example.},
  googlebooks = {bxjvAAAAMAAJ},
  isbn = {978-0-12-471250-8},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Mardia_et_al_1979_Multivariate_Analysis.pdf}
}

@book{marin2007,
  title = {Bayesian {{Core}}: {{A Practical Approach}} to {{Computational Bayesian Statistics}}},
  shorttitle = {Bayesian {{Core}}},
  author = {Marin, Jean-Michel and Robert, Christian},
  year = {2007},
  series = {Springer {{Texts}} in {{Statistics}}},
  publisher = {{Springer-Verlag}},
  address = {{New York}},
  doi = {10.1007/978-0-387-38983-7},
  abstract = {This Bayesian modeling book is intended for practitioners and applied statisticians looking for a self-contained entry to computational Bayesian statistics. Focusing on standard statistical models and backed up by discussed real datasets available from the book website, it provides an operational methodology for conducting Bayesian inference, rather than focusing on its theoretical justifications. Special attention is paid to the derivation of prior distributions in each case and specific reference solutions are given for each of the models. Similarly, computational details are worked out to lead the reader towards an effective programming of the methods given in the book. While R programs are provided on the book website and R hints are given in the computational sections of the book, The Bayesian Core requires no knowledge of the R language and it can be read and used with any other programming language. The Bayesian Core can be used as a textbook at both undergraduate and graduate levels, as exemplified by courses given at Universit\'e Paris Dauphine (France), University of Canterbury (New Zealand), and University of British Columbia (Canada). It serves as a unique textbook for a service course for scientists aiming at analyzing data the Bayesian way as well as an introductory course on Bayesian statistics. The prerequisites for the book are a basic knowledge of probability theory and of statistics. Methodological and data-based exercises are included within the main text and students are expected to solve them as they read the book. Those exercises can obviously serve as assignments, as was done in the above courses. Datasets, R codes and course slides all are available on the book website. Jean-Michel Marin is currently senior researcher at INRIA, the French Computer Science research institute, and located at Universit\'e Paris-Sud, Orsay. He has previously been Assistant Professor at Universit\'e Paris Dauphine for four years. He has written numerous papers on Bayesian methodology and computing, and is currently a member of the council of the French Statistical Society. Christian Robert is Professor of Statistics at Universit\'e Paris Dauphine and Head of the Statistics Research Laboratory at CREST-INSEE, Paris. He has written over a hundred papers on Bayesian Statistics and computational methods and is the author or co-author of seven books on those topics, including The Bayesian Choice (Springer, 2001), winner of the ISBA DeGroot Prize in 2004. He is a Fellow and member of the council of the Institute of Mathematical Statistics, and a Fellow and member of the research committee of the Royal Statistical Society. He is currently co-editor of the Journal of the Royal Statistical Society, Series B, after taking part in the editorial boards of the Journal of the American Statistical Society, the Annals of Statistics, Statistical Science, and Bayesian Analysis. He is also the winner of the Young Statistician prize of the Paris Statistical Society in 1996 and a recipient of an Erskine Fellowship from the University of Canterbury (NZ) in 2006.},
  isbn = {978-0-387-38983-7},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Marin_Robert_2007_Bayesian_Core.pdf;/home/dede/Zotero/storage/KGYA3X3E/9780387389837.html}
}

@book{marshall2009,
  title = {The {{Reformation}}: A Very Short Introduction},
  shorttitle = {The {{Reformation}}},
  author = {Marshall and Peter},
  year = {2009},
  series = {Very {{Short Introductions}} 213},
  publisher = {{Oxford University Press}},
  isbn = {978-0-19-923131-7},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Marshall_Peter_2009_The_Reformation.epub}
}

@article{matias2017,
  title = {Statistical Clustering of Temporal Networks through a Dynamic Stochastic Block Model},
  author = {Matias, Catherine and Miele, Vincent},
  year = {2017},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {79},
  number = {4},
  pages = {1119--1141},
  issn = {1467-9868},
  doi = {10.1111/rssb.12200},
  abstract = {Statistical node clustering in discrete time dynamic networks is an emerging field that raises many challenges. Here, we explore statistical properties and frequentist inference in a model that combines a stochastic block model for its static part with independent Markov chains for the evolution of the nodes groups through time. We model binary data as well as weighted dynamic random graphs (with discrete or continuous edges values). Our approach, motivated by the importance of controlling for label switching issues across the different time steps, focuses on detecting groups characterized by a stable within-group connectivity behaviour. We study identifiability of the model parameters and propose an inference procedure based on a variational expectation\textendash maximization algorithm as well as a model selection criterion to select the number of groups. We carefully discuss our initialization strategy which plays an important role in the method and we compare our procedure with existing procedures on synthetic data sets. We also illustrate our approach on dynamic contact networks: one of encounters between high school students and two others on animal interactions. An implementation of the method is available as an R package called dynsbm.},
  langid = {english},
  keywords = {Contact network,Dynamic random graph,Graph clustering,Stochastic block model,todo,Variational expectationâ€“maximization},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/rssb.12200},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Matias_Miele_2017_Statistical_clustering_of_temporal_networks_through_a_dynamic_stochastic_block.pdf;/home/dede/Zotero/storage/GYLKETQT/rssb.html}
}

@article{mattingly2002,
  title = {Ergodicity for {{SDEs}} and Approximations: Locally {{Lipschitz}} Vector Fields and Degenerate Noise},
  shorttitle = {Ergodicity for {{SDEs}} and Approximations},
  author = {Mattingly, J. C. and Stuart, A. M. and Higham, D. J.},
  year = {2002},
  month = oct,
  journal = {Stochastic Processes and their Applications},
  volume = {101},
  number = {2},
  pages = {185--232},
  issn = {0304-4149},
  doi = {10.1016/S0304-4149(02)00150-3},
  abstract = {The ergodic properties of SDEs, and various time discretizations for SDEs, are studied. The ergodicity of SDEs is established by using techniques from the theory of Markov chains on general state spaces, such as that expounded by Meyn\textendash Tweedie. Application of these Markov chain results leads to straightforward proofs of geometric ergodicity for a variety of SDEs, including problems with degenerate noise and for problems with locally Lipschitz vector fields. Applications where this theory can be usefully applied include damped-driven Hamiltonian problems (the Langevin equation), the Lorenz equation with degenerate noise and gradient systems. The same Markov chain theory is then used to study time-discrete approximations of these SDEs. The two primary ingredients for ergodicity are a minorization condition and a Lyapunov condition. It is shown that the minorization condition is robust under approximation. For globally Lipschitz vector fields this is also true of the Lyapunov condition. However in the locally Lipschitz case the Lyapunov condition fails for explicit methods such as Euler\textendash Maruyama; for pathwise approximations it is, in general, only inherited by specially constructed implicit discretizations. Examples of such discretization based on backward Euler methods are given, and approximation of the Langevin equation studied in some detail.},
  langid = {english},
  keywords = {Additive noise,Dissipative and gradient systems,Geometric ergodicity,Hypoelliptic and degenerate diffusions,Langevin equation,Monotone,Stochastic differential equations,Time-discretization,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Mattingly_et_al_2002_Ergodicity_for_SDEs_and_approximations.pdf;/home/dede/Zotero/storage/VEPHQIC9/S0304414902001503.html}
}

@article{mauldin1992,
  title = {Polya {{Trees}} and {{Random Distributions}}},
  author = {Mauldin, R. Daniel and Sudderth, William D. and Williams, S. C.},
  year = {1992},
  month = sep,
  journal = {The Annals of Statistics},
  volume = {20},
  number = {3},
  pages = {1203--1221},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/aos/1176348766},
  abstract = {Trees of Polya urns are used to generate sequences of exchangeable random variables. By a theorem of de Finetti each such sequence is a mixture of independent, identically distributed variables and the mixing measure can be viewed as a prior on distribution functions. The collection of these Polya tree priors forms a convenient conjugate family which was mentioned by Ferguson and includes the Dirichlet processes of Ferguson. Unlike Dirichlet processes, Polya tree priors can assign probability 1 to the class of continuous distributions. This property and a few others are investigated.},
  keywords = {60G09,60G57,62A15,62G99,Derechlet distributions,Polya urns,Prior distributions,Random measures},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Mauldin_et_al_1992_Polya_Trees_and_Random_Distributions.pdf;/home/dede/Zotero/storage/LU3IT58A/1176348766.html}
}

@article{may2012,
  title = {Optimistic {{Bayesian}} Sampling in Contextual-Bandit Problems},
  author = {May, Benedict C. and Korda, Nathan and Lee, Anthony and Leslie, David S.},
  year = {2012},
  month = jun,
  journal = {The Journal of Machine Learning Research},
  volume = {13},
  number = {null},
  pages = {2069--2106},
  issn = {1532-4435},
  abstract = {In sequential decision problems in an unknown environment, the decision maker often faces a dilemma over whether to explore to discover more about the environment, or to exploit current knowledge. We address the exploration-exploitation dilemma in a general setting encompassing both standard and contextualised bandit problems. The contextual bandit problem has recently resurfaced in attempts to maximise click-through rates in web based applications, a task with significant commercial interest. In this article we consider an approach of Thompson (1933) which makes use of samples from the posterior distributions for the instantaneous value of each action. We extend the approach by introducing a new algorithm, Optimistic Bayesian Sampling (OBS), in which the probability of playing an action increases with the uncertainty in the estimate of the action value. This results in better directed exploratory behaviour. We prove that, under unrestrictive assumptions, both approaches result in optimal behaviour with respect to the average reward criterion of Yang and Zhu (2002). We implement OBS and measure its performance in simulated Bernoulli bandit and linear regression domains, and also when tested with the task of personalised news article recommendation on a Yahoo! Front Page Today Module data set. We find that OBS performs competitively when compared to recently proposed benchmark algorithms and outperforms Thompson's method throughout.},
  keywords = {contextual bandits,exploration-exploitation,multi-armed bandits,sequential allocation,Thompson sampling,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/May_et_al_2012_Optimistic_Bayesian_sampling_in_contextual-bandit_problems.pdf}
}

@article{mccrisken2019,
  title = {`{{Peace}} through Strength': {{Europe}} and {{NATO}} Deterrence beyond the {{US Nuclear Posture Review}}},
  shorttitle = {`{{Peace}} through Strength'},
  author = {McCrisken, Trevor and Downman, Maxwell},
  year = {2019},
  month = mar,
  journal = {International Affairs},
  volume = {95},
  number = {2},
  pages = {277--295},
  issn = {0020-5850},
  doi = {10.1093/ia/iiz002},
  abstract = {With its 2018 Nuclear Posture Review, the Trump administration expanded the scope of US nuclear deterrence, re-emphasizing the importance of non-strategic nuclear weapons, perceptively lowering the threshold for nuclear use and casting doubt on the future of arms control. The authors argue that these changes are consistent with the administration's wider `peace through strength' approach that draws on traditional Republican thinking on security policy. While designed to demonstrate credibility and resolve to both allies and adversaries, however, this assertive approach to security policy and specifically nuclear policy as a necessary precursor to renewed engagement in strategic negotiations may have unintended consequences. This article focuses on European reactions to the strategy and argues that the Trump administration's nuclear posture challenges common European understandings in three principal areas. First, changes to US declaratory policy contest European assumptions on the role of nuclear weapons in defending NATO. Second, US modernization plans and their implications for intra-alliance relations risk accentuating controversial debates about the US commitment to Europe. Third, the apparent US rejection of arms control widens the scope for discord with European leaders. If European leaders assert a clear and credible alternative vision advocating nuclear restraint, risk reduction and arms control they could rebuild trust and confidence between the United States, NATO and Russia, demonstrating real strength and ultimately leading to more genuine opportunities for peace and sustainable European security.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/McCrisken_Downman_2019_â€˜Peace_through_strengthâ€™.pdf}
}

@book{mccullagh1989,
  title = {{Generalized Linear Models}},
  shorttitle = {{Generalized Linear Models}},
  author = {McCullagh, P. and Nelder, John A.},
  year = {1989},
  edition = {2\textdegree{} edizione},
  publisher = {{Chapman and Hall/CRC}},
  address = {{Boca Raton}},
  abstract = {The success of the first edition of Generalized Linear Models led to the updated Second Edition, which continues to provide a definitive unified, treatment of methods for the analysis of diverse types of data. Today, it remains popular for its clarity, richness of content and direct relevance to agricultural, biological, health, engineering, and other applications.The authors focus on examining the way a response variable depends on a combination of explanatory variables, treatment, and classification variables. They give particular emphasis to the important case where the dependence occurs through some unknown, linear combination of the explanatory variables.The Second Edition includes topics added to the core of the first edition, including conditional and marginal likelihood methods, estimating equations, and models for dispersion effects and components of dispersion. The discussion of other topics-log-linear and related models, log odds-ratio regression models, multinomial response models, inverse linear and related models, quasi-likelihood functions, and model checking-was expanded and incorporates significant revisions.Comprehension of the material requires simply a knowledge of matrix theory and the basic ideas of probability theory, but for the most part, the book is self-contained. Therefore, with its worked examples, plentiful exercises, and topics of direct use to researchers in many disciplines, Generalized Linear Models serves as ideal text, self-study guide, and reference.},
  isbn = {978-0-412-31760-6},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/McCullagh_Nelder_1989_Generalized_Linear_Models.pdf}
}

@book{mccullagh2018,
  title = {Tensor {{Methods}} in {{Statistics}}: {{Second Edition}}},
  shorttitle = {Tensor {{Methods}} in {{Statistics}}},
  author = {McCullagh, Peter},
  year = {2018},
  month = jul,
  edition = {Revised, Updated edition},
  publisher = {{Dover Publications}},
  address = {{Mineola, New York}},
  isbn = {978-0-486-82378-2},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/McCullagh_2018_Tensor_Methods_in_Statistics.pdf}
}

@techreport{mcculloh2008,
  type = {{{SSRN Scholarly Paper}}},
  title = {Social {{Network Change Detection}}},
  author = {McCulloh, Ian and Carley, Kathleen M.},
  year = {2008},
  month = mar,
  number = {ID 2726799},
  address = {{Rochester, NY}},
  institution = {{Social Science Research Network}},
  doi = {10.2139/ssrn.2726799},
  abstract = {Changes in observed social networks may signal an underlying change within an organization, and may even predict significant events or behaviors. The breakdown of a team's effectiveness, the emergence of informal leaders, or the preparation of an attack by a clandestine network may all be associated with changes in the patterns of interactions between group members. The ability to systematically, statistically, effectively and efficiently detect these changes has the potential to enable the anticipation of change, provide early warning of change, and enable faster response to change. By applying statistical process control techniques to social networks we can detect changes in these networks. Herein we describe this methodology and then illustrate it using three data sets. The first deals with the email communications among graduate students. The second is the perceived connections among members of al Qaeda based on open source data. The results indicate that this approach is able to detect change even with the high levels of uncertainty inherent in these data.},
  langid = {english},
  keywords = {Al-Qaeda,Change Detection,CUSUM,IkeNet,Social Networks,Statistical Process Control,Terrorism,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/McCulloh_Carley_2008_Social_Network_Change_Detection.pdf;/home/dede/Zotero/storage/9UHFM3H4/papers.html}
}

@article{mcculloh2008a,
  title = {Detecting {{Change}} in {{Longitudinal Social Networks}}},
  author = {Mcculloh, Ian and Carley, Kathleen},
  year = {2008},
  month = jan,
  journal = {Journal of Social Structure},
  volume = {12},
  doi = {10.21307/joss-2019-031},
  abstract = {Changes in observed social networks may signal an underlying change within an organization, and may even predict significant events or behaviors. The breakdown of a team's effectiveness, the emergence of informal leaders, or the preparation of an attack by a clandestine network may all be associated with changes in the patterns of interactions between group members. The ability to systematically, statistically, effectively and efficiently detect these changes has the potential to enable the anticipation, early warning, and faster response to both positive and negative organizational activities. By applying statistical process control techniques to social networks we can rapidly detect changes in these networks. Herein we describe this methodology and then illustrate it using four data sets, of which the first is the Newcomb fraternity data, the second set of data is collected on a group of mid-career U.S. Army officers in a week long training exercise, the third is the perceived connections among members of al Qaeda based on open source, and the fourth data set is simulated using multi-agent simulation. The results indicate that this approach is able to detect change even with the high levels of uncertainty inherent in these data.},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Mcculloh_Carley_2008_Detecting_Change_in_Longitudinal_Social_Networks.pdf}
}

@book{mcmahon2021,
  title = {{The Cold War: A Very Short Introduction}},
  shorttitle = {{The Cold War}},
  author = {McMahon, Robert J.},
  year = {2021},
  month = feb,
  edition = {2\textdegree{} edizione},
  publisher = {{OUP Oxford}},
  address = {{Oxford, United Kingdom}},
  isbn = {978-0-19-885954-3},
  langid = {Inglese},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/McMahon_2021_The_Cold_War.epub}
}

@article{mearsheimer2019,
  title = {Bound to {{Fail}}: {{The Rise}} and {{Fall}} of the {{Liberal International Order}}},
  shorttitle = {Bound to {{Fail}}},
  author = {Mearsheimer, John J.},
  year = {2019},
  month = apr,
  journal = {International Security},
  volume = {43},
  number = {4},
  pages = {7--50},
  issn = {0162-2889},
  doi = {10.1162/isec_a_00342},
  abstract = {The liberal international order, erected after the Cold War, was crumbling by 2019. It was flawed from the start and thus destined to fail. The spread of liberal democracy around the globe\textemdash essential for building that order\textemdash faced strong resistance because of nationalism, which emphasizes self-determination. Some targeted states also resisted U.S. efforts to promote liberal democracy for security-related reasons. Additionally, problems arose because a liberal order calls for states to delegate substantial decisionmaking authority to international institutions and to allow refugees and immigrants to move easily across borders. Modern nation-states privilege sovereignty and national identity, however, which guarantees trouble when institutions become powerful and borders porous. Furthermore, the hyperglobalization that is integral to the liberal order creates economic problems among the lower and middle classes within the liberal democracies, fueling a backlash against that order. Finally, the liberal order accelerated China's rise, which helped transform the system from unipolar to multipolar. A liberal international order is possible only in unipolarity. The new multipolar world will feature three realist orders: a thin international order that facilitates cooperation, and two bounded orders\textemdash one dominated by China, the other by the United States\textemdash poised for waging security competition between them.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Mearsheimer_2019_Bound_to_Fail.pdf}
}

@article{mechelli2005,
  title = {Voxel-{{Based Morphometry}} of the {{Human Brain}}: {{Methods}} and {{Applications}}},
  shorttitle = {Voxel-{{Based Morphometry}} of the {{Human Brain}}},
  author = {Mechelli, Andrea and Price, Cathy J. and Ashburner, John and Friston, Karl J.},
  year = {2005},
  month = may,
  journal = {Current Medical Imaging},
  volume = {1},
  number = {2},
  pages = {105--113},
  abstract = {In recent years, a whole-brain unbiased objective technique, known as voxel-based morphometry (VBM), has been developed to characterise brain differences i...},
  langid = {english},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Mechelli_et_al_2005_Voxel-Based_Morphometry_of_the_Human_Brain.pdf;/home/dede/Zotero/storage/Z4WX2ECR/voxel-based-morphometry-human-brain-methods-and-applications.html}
}

@book{medema2011,
  title = {The {{Hesitant Hand}}: {{Taming Self-Interest}} in the {{History}} of {{Economic Ideas}}},
  shorttitle = {The {{Hesitant Hand}}},
  author = {Medema, Steven G.},
  year = {2011},
  month = mar,
  edition = {Reprint edition},
  publisher = {{Princeton University Press}},
  address = {{Princeton, New Jersey Oxford}},
  abstract = {Adam Smith turned economic theory on its head in 1776 when he declared that the pursuit of self-interest mediated by the market itself--not by government--led, via an invisible hand, to the greatest possible welfare for society as a whole. The Hesitant Hand examines how subsequent economic thinkers have challenged or reaffirmed Smith's doctrine, some contending that society needs government to intervene on its behalf when the marketplace falters, others arguing that government interference ultimately benefits neither the market nor society. Steven Medema explores what has been perhaps the central controversy in modern economics from Smith to today. He traces the theory of market failure from the 1840s through the 1950s and subsequent attacks on this view by the Chicago and Virginia schools. Medema follows the debate from John Stuart Mill through the Cambridge welfare tradition of Henry Sidgwick, Alfred Marshall, and A. C. Pigou, and looks at Ronald Coase's challenge to the Cambridge approach and the rise of critiques affirming Smith's doctrine anew. He shows how, following the marginal revolution, neoclassical economists, like the preclassical theorists before Smith, believed government can mitigate the adverse consequences of self-interested behavior, yet how the backlash against this view, led by the Chicago and Virginia schools, demonstrated that self-interest can also impact government, leaving society with a choice among imperfect alternatives. The Hesitant Hand demonstrates how government's economic role continues to be bound up in questions about the effects of self-interest on the greater good.},
  isbn = {978-0-691-15000-0},
  langid = {english},
  file = {/home/dede/Zotero/storage/9AGWIZZ2/Medema - 2009 - The hesitant hand taming self-interest in the his.pdf}
}

@article{mei2006,
  title = {Sequential Change-Point Detection When Unknown Parameters Are Present in the Pre-Change Distribution},
  author = {Mei, Yajun},
  year = {2006},
  month = feb,
  journal = {The Annals of Statistics},
  volume = {34},
  number = {1},
  pages = {92--122},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/009053605000000859},
  abstract = {In the sequential change-point detection literature, most research specifies a required frequency of false alarms at a given pre-change distribution f\texttheta{} and tries to minimize the detection delay for every possible post-change distribution g{$\lambda$}. In this paper, motivated by a number of practical examples, we first consider the reverse question by specifying a required detection delay at a given post-change distribution and trying to minimize the frequency of false alarms for every possible pre-change distribution f\texttheta. We present asymptotically optimal procedures for one-parameter exponential families. Next, we develop a general theory for change-point problems when both the pre-change distribution f\texttheta{} and the post-change distribution g{$\lambda$} involve unknown parameters. We also apply our approach to the special case of detecting shifts in the mean of independent normal observations.},
  keywords = {62F05,62L10,62L15,asymptotic optimality,Change-point,optimizer,Power one tests,quality control,statistical process control,surveillance},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Mei_2006_Sequential_change-point_detection_when_unknown_parameters_are_present_in_the.pdf;/home/dede/Zotero/storage/LWTGIBTL/009053605000000859.html}
}

@inproceedings{mei2011,
  title = {Quickest Detection in Censoring Sensor Networks},
  booktitle = {2011 {{IEEE International Symposium}} on {{Information Theory Proceedings}}},
  author = {Mei, Yajun},
  year = {2011},
  month = jul,
  pages = {2148--2152},
  issn = {2157-8117},
  doi = {10.1109/ISIT.2011.6034390},
  abstract = {The quickest change detection problem is studied in a general context of monitoring a large number of data streams in sensor networks when the ``trigger event'' may affect different sensors differently. In particular, the occurring event could have an immediate or delayed impact on some unknown, but not necessarily all, sensors. Motivated by censoring sensor networks, scalable detection schemes are developed based on the sum of those local CUSUM statistics that are ``large'' under either hard thresholding or top-r thresholding rules or both. The proposed schemes are shown to possess certain asymptotic optimality properties.},
  keywords = {Biomedical monitoring,Context,Delay,Delay effects,Monitoring,Numerical simulation,Reliability,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Mei_2011_Quickest_detection_in_censoring_sensor_networks.pdf;/home/dede/Zotero/storage/6RVRQXRF/6034390.html}
}

@article{mei2019,
  title = {The {{Generalization Error}} of {{Random Features Regression}}: {{Precise Asymptotics}} and the {{Double Descent Curve}}},
  shorttitle = {The {{Generalization Error}} of {{Random Features Regression}}},
  author = {Mei, Song and Montanari, A.},
  year = {2019},
  doi = {10.1002/CPA.22008},
  abstract = {Deep learning methods operate in regimes that defy the traditional statistical mindset. The neural network architectures often contain more parameters than training samples, and are so rich that they can interpolate the observed labels, even if the latter are replaced by pure noise. Despite their huge complexity, the same architectures achieve small generalization error on real data.  This phenomenon has been rationalized in terms of a so-called `double descent' curve. As the model complexity increases, the generalization error follows the usual U-shaped curve at the beginning, first decreasing and then peaking around the interpolation threshold (when the model achieves vanishing training error). However, it descends again as model complexity exceeds this threshold. The global minimum of the generalization error is found in this overparametrized regime, often when the number of parameters is much larger than the number of samples. Far from being a peculiar property of deep neural networks, elements of this behavior have been demonstrated in much simpler settings, including linear regression with random covariates.  In this paper we consider the problem of learning an unknown function over the \$d\$-dimensional sphere \$\textbackslash mathbb S\^\{d-1\}\$, from \$n\$ i.i.d. samples \$(\textbackslash boldsymbol x\_i, y\_i) \textbackslash in \textbackslash mathbb S\^\{d-1\} \textbackslash times \textbackslash mathbb R\$, \$i \textbackslash le n\$. We perform ridge regression on \$N\$ random features of the form \$\textbackslash sigma(\textbackslash boldsymbol w\_a\^\{\textbackslash mathsf T\}\textbackslash boldsymbol x)\$, \$a \textbackslash le N\$. This can be equivalently described as a two-layers neural network with random first-layer weights. We compute the precise asymptotics of the generalization error, in the limit \$N, n, d \textbackslash to \textbackslash infty\$ with \$N/d\$ and \$n/d\$ fixed. This provides the first analytically tractable model that captures all the features of the double descent phenomenon without assuming ad hoc misspecification structures.},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Mei_Montanari_2019_The_Generalization_Error_of_Random_Features_Regression.pdf}
}

@article{meinshausen2006,
  title = {High-Dimensional Graphs and Variable Selection with the {{Lasso}}},
  author = {Meinshausen, Nicolai and B{\"u}hlmann, Peter},
  year = {2006},
  month = jun,
  journal = {The Annals of Statistics},
  volume = {34},
  number = {3},
  pages = {1436--1462},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/009053606000000281},
  abstract = {The pattern of zero entries in the inverse covariance matrix of a multivariate normal distribution corresponds to conditional independence restrictions between variables. Covariance selection aims at estimating those structural zeros from data. We show that neighborhood selection with the Lasso is a computationally attractive alternative to standard covariance selection for sparse high-dimensional graphs. Neighborhood selection estimates the conditional independence restrictions separately for each node in the graph and is hence equivalent to variable selection for Gaussian linear models. We show that the proposed neighborhood selection scheme is consistent for sparse high-dimensional graphs. Consistency hinges on the choice of the penalty parameter. The oracle value for optimal prediction does not lead to a consistent neighborhood estimate. Controlling instead the probability of falsely joining some distinct connectivity components of the graph, consistent estimation for sparse graphs is achieved (with exponential rates), even when the number of variables grows as the number of observations raised to an arbitrary power.},
  keywords = {62F12,62H20,62J07,covariance selection,Gaussian graphical models,Linear regression,penalized regression},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Meinshausen_Buhlmann_2006_High-dimensional_graphs_and_variable_selection_with_the_Lasso.pdf;/home/dede/Zotero/storage/FQ8S3LR2/009053606000000281.html}
}

@article{mellor2013,
  title = {Thompson {{Sampling}} in {{Switching Environments}} with {{Bayesian Online Change Point Detection}}},
  author = {Mellor, Joseph and Shapiro, Jonathan},
  year = {2013},
  month = feb,
  journal = {arXiv:1302.3721 [cs]},
  eprint = {1302.3721},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Thompson Sampling has recently been shown to be optimal in the Bernoulli Multi-Armed Bandit setting[Kaufmann et al., 2012]. This bandit problem assumes stationary distributions for the rewards. It is often unrealistic to model the real world as a stationary distribution. In this paper we derive and evaluate algorithms using Thompson Sampling for a Switching Multi-Armed Bandit Problem.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Mellor_Shapiro_2013_Thompson_Sampling_in_Switching_Environments_with_Bayesian_Online_Change_Point3.pdf}
}

@article{menardi2014,
  title = {Training and Assessing Classification Rules with Imbalanced Data},
  author = {Menardi, Giovanna and Torelli, Nicola},
  year = {2014},
  month = jan,
  journal = {Data Mining and Knowledge Discovery},
  volume = {28},
  number = {1},
  pages = {92--122},
  issn = {1573-756X},
  doi = {10.1007/s10618-012-0295-5},
  abstract = {The problem of modeling binary responses by using cross-sectional data has been addressed with a number of satisfying solutions that draw on both parametric and nonparametric methods. However, there exist many real situations where one of the two responses (usually the most interesting for the analysis) is rare. It has been largely reported that this class imbalance heavily compromises the process of learning, because the model tends to focus on the prevalent class and to ignore the rare events. However, not only the estimation of the classification model is affected by a skewed distribution of the classes, but also the evaluation of its accuracy is jeopardized, because the scarcity of data leads to poor estimates of the model's accuracy. In this work, the effects of class imbalance on model training and model assessing are discussed. Moreover, a unified and systematic framework for dealing with the problem of imbalanced classification is proposed, based on a smoothed bootstrap re-sampling technique. The proposed technique is founded on a sound theoretical basis and an extensive empirical study shows that it outperforms the main other remedies to face imbalanced learning problems.},
  langid = {english},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Menardi_Torelli_2014_Training_and_assessing_classification_rules_with_imbalanced_data.pdf}
}

@article{meng2018,
  title = {Statistical Paradises and Paradoxes in Big Data ({{I}}): {{Law}} of Large Populations, Big Data Paradox, and the 2016 {{US}} Presidential Election},
  shorttitle = {Statistical Paradises and Paradoxes in Big Data ({{I}})},
  author = {Meng, Xiao-Li},
  year = {2018},
  month = jun,
  journal = {The Annals of Applied Statistics},
  volume = {12},
  number = {2},
  pages = {685--726},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {1932-6157, 1941-7330},
  doi = {10.1214/18-AOAS1161SF},
  abstract = {Statisticians are increasingly posed with thought-provoking and even paradoxical questions, challenging our qualifications for entering the statistical paradises created by Big Data. By developing measures for data quality, this article suggests a framework to address such a question: ``Which one should I trust more: a 1\% survey with 60\% response rate or a self-reported administrative dataset covering 80\% of the population?'' A 5-element Euler-formula-like identity shows that for any dataset of size nnn, probabilistic or not, the difference between the sample average X{$\harrowextender\harrowextender\harrowextender\harrowextender$}nX\textasciimacron n\textbackslash overline\{X\}\_\{n\} and the population average X{$\harrowextender\harrowextender\harrowextender\harrowextender$}NX\textasciimacron N\textbackslash overline\{X\}\_\{N\} is the product of three terms: (1) a data quality measure, {$\rho$}R,X{$\rho$}R,X\textbackslash rho\_\{\{R,X\}\}, the correlation between XjXjX\_\{j\} and the response/recording indicator RjRjR\_\{j\}; (2) a data quantity measure, (N-n)/n---------{$\surd$}(N-n)/n\textbackslash sqrt\{(N-n)/n\}, where NNN is the population size; and (3) a problem difficulty measure, {$\sigma$}X{$\sigma$}X\textbackslash sigma\_\{X\}, the standard deviation of XXX. This decomposition provides multiple insights: (I) Probabilistic sampling ensures high data quality by controlling {$\rho$}R,X{$\rho$}R,X\textbackslash rho\_\{\{R,X\}\} at the level of N-1/2N-1/2N\^\{-1/2\}; (II) When we lose this control, the impact of NNN is no longer canceled by {$\rho$}R,X{$\rho$}R,X\textbackslash rho\_\{\{R,X\}\}, leading to a Law of Large Populations (LLP), that is, our estimation error, relative to the benchmarking rate 1/n-{$\surd$}1/n1/\textbackslash sqrt\{n\}, increases with N--{$\surd$}N\textbackslash sqrt\{N\}; and (III) the ``bigness'' of such Big Data (for population inferences) should be measured by the relative size f=n/Nf=n/Nf=n/N, not the absolute size nnn; (IV) When combining data sources for population inferences, those relatively tiny but higher quality ones should be given far more weights than suggested by their sizes. Estimates obtained from the Cooperative Congressional Election Study (CCES) of the 2016 US presidential election suggest a {$\rho$}R,X{$\approx-$}0.005{$\rho$}R,X{$\approx-$}0.005\textbackslash rho\_\{\{R,X\}\}\textbackslash approx-0.005 for self-reporting to vote for Donald Trump. Because of LLP, this seemingly minuscule data defect correlation implies that the simple sample proportion of the self-reported voting preference for Trump from 1\%1\%1\textbackslash\% of the US eligible voters, that is, n{$\approx$}2,300,000n{$\approx$}2,300,000n\textbackslash approx2\textbackslash mbox\{,\}300\textbackslash mbox\{,\}000, has the same mean squared error as the corresponding sample proportion from a genuine simple random sample of size n{$\approx$}400n{$\approx$}400n\textbackslash approx400, a 99.98\%99.98\%99.98\textbackslash\% reduction of sample size (and hence our confidence). The CCES data demonstrate LLP vividly: on average, the larger the state's voter populations, the further away the actual Trump vote shares from the usual 95\%95\%95\textbackslash\% confidence intervals based on the sample proportions. This should remind us that, without taking data quality into account, population inferences with Big Data are subject to a Big Data Paradox: the more the data, the surer we fool ourselves.},
  langid = {english},
  mrnumber = {MR3834282},
  zmnumber = {06980472},
  keywords = {Bias-variance tradeoff,data confidentiality and privacy,data defect correlation,data defect index (d.d.i.),data quality-quantity tradeoff,done,Euler identity,Monte Carlo and Quasi Monte Carlo (MCQMC),non-response bias},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Meng_2018_Statistical_paradises_and_paradoxes_in_big_data_(I).pdf;/home/dede/Zotero/storage/J8NGF46H/1532743473.html}
}

@book{mengersen2020,
  title = {{Case Studies in Applied Bayesian Data Science: CIRM Jean-Morlet Chair, Fall 2018}},
  shorttitle = {{Case Studies in Applied Bayesian Data Science}},
  author = {Mengersen, Kerrie L. and Pudlo, Pierre and Robert, Christian P.},
  year = {2020},
  edition = {1st ed. 2020 edizione},
  publisher = {{Springer}},
  abstract = {Presenting a range of substantive applied problems within Bayesian Statistics along with their Bayesian solutions, this book arises from a research program at CIRM in France in the second semester of 2018, which supported Kerrie Mengersen as a visiting Jean-Morlet Chair and Pierre Pudlo as the local Research Professor. The field of Bayesian statistics has exploded over the past thirty years and is now an established field of research in mathematical statistics and computer science, a key component of data science, and an underpinning methodology in many domains of science, business and social science. Moreover, while remaining naturally entwined, the three arms of Bayesian statistics, namely modelling, computation and inference, have grown into independent research fields. While the research arms of Bayesian statistics continue to grow in many directions, they are harnessed when attention turns to solving substantive applied problems. Each such problem set has its own challenges and hence draws from the suite of research a bespoke solution.The book will be useful for both theoretical and applied statisticians, as well as practitioners, to inspect these solutions in the context of the problems, in order to draw further understanding, awareness and inspiration.},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Mengersen_et_al_2020_Case_Studies_in_Applied_Bayesian_Data_Science.pdf}
}

@article{menzefricke2002,
  title = {On the Evaluation of Control Chart Limits Based on Predictive Distributions},
  author = {Menzefricke, Ulrich},
  year = {2002},
  month = jul,
  journal = {COMMUN. STATIST.\textemdash THEORY METH.},
  volume = {31(8)},
  pages = {1423--1440},
  doi = {10.1081/STA-120006077},
  abstract = {We propose a Bayesian approach to obtaining control charts when there is parameter uncertainty. Our approach consists of two stages, (i) construction of the control chart where we use a predictive distribution based on a Bayesian approach to derive the rejection region, and (ii) evaluation of the control chart where we use a sampling theory approach to examine the performance of the control chart under various hypothetical specifications for the data generation model.},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Menzefricke_2002_On_the_evaluation_of_control_chart_limits_based_on_predictive_distributions.pdf}
}

@article{menzefricke2007,
  title = {Control {{Charts}} for the {{Generalized Variance Based}} on {{Its Predictive Distribution}}},
  author = {Menzefricke, Ulrich},
  year = {2007},
  month = apr,
  journal = {Communications in Statistics\textemdash Theory and Methods},
  volume = {36},
  pages = {1031--1038},
  doi = {10.1080/03610920601036176},
  abstract = {This article develops a control chart for the generalized variance. A Bayesian approach is used to incorporate parameter uncertainty. Our approach has two stages, (i) construction of the control chart where we use a predictive distribution based on a Bayesian approach to derive the rejection region, and (ii) evaluation of the control chart where we use a sampling theory approach to examine the performance of the control chart under various hypothetical specifications for the data generation model.},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Menzefricke_2007_Control_Charts_for_the_Generalized_Variance_Based_on_Its_Predictive_Distribution.pdf}
}

@article{menzefricke2010,
  title = {Multivariate {{Exponentially Weighted Moving Average Charts}} for a {{Mean Based}} on {{Its Prediction Distribution}}},
  author = {Menzefricke, Ulrich},
  year = {2010},
  month = oct,
  journal = {Communications in Statistics\textemdash Theory and Methods},
  volume = {39},
  pages = {2942--2960},
  doi = {10.1080/03610920903168628},
  abstract = {This article develops a control chart for a mean vector when it is monitored by a quadratic form in the exponentially weighted observation vector. A Bayesian approach is used to incorporate parameter uncertainty. We first use a Bayesian predictive distribution to construct the control chart, and we then use a sampling theory approach to evaluate it under various hypothetical specifications for the data generation model.},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Menzefricke_2010_Multivariate_Exponentially_Weighted_Moving_Average_Charts_for_a_Mean_Based_on2.pdf}
}

@article{menzefricke2010a,
  title = {Control {{Charts}} for the {{Variance}} and {{Coefficient}} of {{Variation Based}} on {{Their Predictive Distribution}}},
  author = {Menzefricke, Ulrich},
  year = {2010},
  month = aug,
  journal = {Communications in Statistics-theory and Methods - COMMUN STATIST-THEOR METHOD},
  volume = {39},
  pages = {2930--2941},
  doi = {10.1080/03610920903168610},
  abstract = {This article develops a control chart for the variance of a normal distribution and, equivalently, the coefficient of variation of a log-normal distribution. A Bayesian approach is used to incorporate parameter uncertainty, and the control limits are obtained from the predictive distribution for the variance. We evaluate this control chart by examining its performance for various values of the process variance.},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Menzefricke_2010_Control_Charts_for_the_Variance_and_Coefficient_of_Variation_Based_on_Their.pdf}
}

@article{menzefricke2013,
  title = {Control {{Charts}} for the {{Mean}} and {{Variance}} Based on {{Changepoint Methodology}}},
  author = {Menzefricke, Ulrich},
  year = {2013},
  month = mar,
  journal = {Communications in Statistics: Theory and Methods},
  volume = {42},
  doi = {10.1080/03610926.2011.592252},
  abstract = {This article develops a control chart for the mean and variance of a normal distribution based on changepoint methodology. A Bayesian approach is used to incorporate parameter uncertainty. The resulting control chart plots the probabilities of ``no change'' as samples become available at the monitoring stage. Average run length considerations are used to set the control limits. Simulations are used to compare the proposed chart with a more traditional Shewhart-type combined control chart for the mean and variance.},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Menzefricke_2013_Control_Charts_for_the_Mean_and_Variance_based_on_Changepoint_Methodology.pdf}
}

@article{menzefricke2013a,
  title = {Combined {{Exponentially Weighted Moving Average Charts}} for the {{Mean}} and {{Variance Based}} on the {{Predictive Distribution}}},
  author = {Menzefricke, Ulrich},
  year = {2013},
  month = nov,
  journal = {Communications in Statistics: Theory and Methods},
  volume = {42},
  doi = {10.1080/03610926.2011.638429},
  abstract = {This article develops combined exponentially weighted moving average (EWMA) charts for the mean and variance of a normal distribution. A Bayesian approach is used to incorporate parameter uncertainty. We first use a Bayesian predictive distribution to construct the control chart, and we then use a sampling theory approach to evaluate it under various hypothetical specifications for the data generation model. Simulations are used to compare the proposed charts for different values of both the weighing constant for the exponentially weighted moving averages and for the size of the calibration sample that is used to estimate the in-statistical-control process parameters. We also examine the separate performance of the EWMA chart for the variance.},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Menzefricke_2013_Combined_Exponentially_Weighted_Moving_Average_Charts_for_the_Mean_and_Variance.pdf}
}

@article{merhav1998,
  title = {Universal Prediction},
  author = {Merhav, N. and Feder, M.},
  year = {1998},
  month = oct,
  journal = {IEEE Transactions on Information Theory},
  volume = {44},
  number = {6},
  pages = {2124--2147},
  issn = {1557-9654},
  doi = {10.1109/18.720534},
  abstract = {This paper consists of an overview on universal prediction from an information-theoretic perspective. Special attention is given to the notion of probability assignment under the self-information loss function, which is directly related to the theory of universal data compression. Both the probabilistic setting and the deterministic setting of the universal prediction problem are described with emphasis on the analogy and the differences between results in the two settings.},
  keywords = {Control theory,Data compression,Entropy,Information theory,Machine learning,Natural languages,Operations research,Predictive models,Statistics,Stochastic processes},
  file = {/home/dede/Zotero/storage/MKKF8QVK/720534.html}
}

@book{meyers2005,
  title = {{Effective C++: 55 Specific Ways to Improve Your Programs and Designs}},
  shorttitle = {{Effective C++}},
  author = {Meyers, Scott},
  year = {2005},
  month = may,
  edition = {3 edizione},
  publisher = {{Addison-Wesley Professional}},
  address = {{Upper Saddle River, NJ}},
  abstract = {Presents a collection of tips for programmers on ways to improve programming skills.},
  isbn = {978-0-321-33487-9},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Meyers_2005_Effective_C++.pdf}
}

@article{miller1976,
  title = {{{ILLIES}}, {{D}}. {{A}}.: "{{An Objective Theory}} of {{Probability}}"},
  shorttitle = {{{ILLIES}}, {{D}}. {{A}}.},
  author = {Miller, David},
  year = {1976},
  journal = {British Journal for the Philosophy of Science},
  volume = {27},
  number = {n/a},
  pages = {292},
  publisher = {{Oxford University Press}},
  doi = {10.1093/bjps/27.3.292},
  file = {/home/dede/Zotero/storage/Z8R25TZ4/MILIDA-3.html}
}

@article{miller2013,
  title = {Efficient Anomaly Detection in Dynamic, Attributed Graphs: {{Emerging}} Phenomena and Big Data},
  shorttitle = {Efficient Anomaly Detection in Dynamic, Attributed Graphs},
  author = {Miller, B. A. and Arcolano, N. and Bliss, N.},
  year = {2013},
  journal = {2013 IEEE International Conference on Intelligence and Security Informatics},
  doi = {10.1109/ISI.2013.6578815},
  abstract = {This paper uses a generalized linear model for random attributed graphs to model connection probabilities using vertex metadata to show that an approximation to the exact model yields an exploitable structure in the edge probabilities, allowing for efficient scaling of a spectral framework for anomaly detection through analysis of graph residuals. When working with large-scale network data, the interconnected entities often have additional descriptive information. This additional metadata may provide insight that can be exploited for detection of anomalous events. In this paper, we use a generalized linear model for random attributed graphs to model connection probabilities using vertex metadata. For a class of such models, we show that an approximation to the exact model yields an exploitable structure in the edge probabilities, allowing for efficient scaling of a spectral framework for anomaly detection through analysis of graph residuals, and a fast and simple procedure for estimating the model parameters. In simulation, we demonstrate that taking into account both attributes and dynamics in this analysis has a much more significant impact on the detection of an emerging anomaly than accounting for either dynamics or attributes alone. We also present an analysis of a large, dynamic citation graph, demonstrating that taking additional document metadata into account emphasizes parts of the graph that would not be considered significant otherwise.},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Miller_et_al_2013_Efficient_anomaly_detection_in_dynamic,_attributed_graphs.pdf}
}

@article{mitchell1988,
  title = {Bayesian {{Variable Selection}} in {{Linear Regression}}},
  author = {Mitchell, T. J. and Beauchamp, J. J.},
  year = {1988},
  month = dec,
  journal = {Journal of the American Statistical Association},
  volume = {83},
  number = {404},
  pages = {1023--1032},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.1988.10478694},
  abstract = {This article is concerned with the selection of subsets of predictor variables in a linear regression model for the prediction of a dependent variable. It is based on a Bayesian approach, intended to be as objective as possible. A probability distribution is first assigned to the dependent variable through the specification of a family of prior distributions for the unknown parameters in the regression model. The method is not fully Bayesian, however, because the ultimate choice of prior distribution from this family is affected by the data. It is assumed that the predictors represent distinct observables; the corresponding regression coefficients are assigned independent prior distributions. For each regression coefficient subject to deletion from the model, the prior distribution is a mixture of a point mass at 0 and a diffuse uniform distribution elsewhere, that is, a ``spike and slab'' distribution. The random error component is assigned a normal distribution with mean 0 and standard deviation {$\sigma$}, where ln({$\sigma$}) has a locally uniform noninformative prior distribution. The appropriate posterior probabilities are derived for each submodel. If the regression coefficients have identical priors, the posterior distribution depends only on the data and the parameter {$\gamma$}, which is the height of the spike divided by the height of the slab for the common prior distribution. This parameter is not assigned a probability distribution; instead, it is considered a parameter that indexes the members of a class of Bayesian methods. Graphical methods are proposed as informal guides for choosing {$\gamma$}, assessing the complexity of the response function and the strength of the individual predictor variables, and assessing the degree of uncertainty about the best submodel. The following plots against {$\gamma$} are suggested: (a) posterior probability that a particular regression coefficient is 0; (b) posterior expected number of terms in the model; (c) posterior entropy of the submodel distribution; (d) posterior predictive error; and (e) posterior probability of goodness of fit. Plots (d) and (e) are suggested as ways to choose y. The predictive error is determined using a Bayesian cross-validation approach that generates a predictive density for each observation, given all of the data except that observation, that is, a type of ``leave one out'' approach. The goodness-of-fit measure is the sum of the posterior probabilities of all submodels that pass a standard F test for goodness of fit relative to the full model, at a specified level of significance. The dependence of the results on the scaling of the variables is discussed, and some ways to choose the scaling constants are suggested. Examples based on a large data set arising from an energy-conservation study are given to demonstrate the application of the methods.},
  keywords = {Cross-validation,Linear models,Subset selection,todo},
  annotation = {\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/01621459.1988.10478694},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Mitchell_Beauchamp_1988_Bayesian_Variable_Selection_in_Linear_Regression.pdf;/home/dede/Zotero/storage/YIYK8WH7/01621459.1988.html}
}

@article{mitra2010,
  title = {Two-{{Level Stochastic Search Variable Selection}} in {{GLMs}} with {{Missing Predictors}}},
  author = {Mitra, Robin and Dunson, David},
  year = {2010},
  month = jan,
  journal = {The International Journal of Biostatistics},
  volume = {6},
  number = {1},
  issn = {1557-4679},
  doi = {10.2202/1557-4679.1173},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Mitra_Dunson_2010_Two-Level_Stochastic_Search_Variable_Selection_in_GLMs_with_Missing_Predictors.pdf}
}

@article{mockus1975,
  title = {On the {{Bayes Methods}} for {{Seeking}} the {{Extremal Point}}},
  author = {Mockus, J.},
  year = {1975},
  month = aug,
  journal = {IFAC Proceedings Volumes},
  volume = {8},
  number = {1},
  pages = {428--431},
  issn = {14746670},
  doi = {10.1016/S1474-6670(17)67769-3},
  langid = {english},
  keywords = {todo}
}

@book{mockus1989,
  title = {Bayesian {{Approach}} to {{Global Optimization}}: {{Theory}} and {{Applications}}},
  shorttitle = {Bayesian {{Approach}} to {{Global Optimization}}},
  author = {Mockus, Jonas},
  year = {1989},
  series = {Mathematics and Its {{Applications}}},
  publisher = {{Springer Netherlands}},
  doi = {10.1007/978-94-009-0909-0},
  abstract = {`Bayesian Approach to Global Optimization is an excellent reference book in the field. As a text it is probably most appropriate in a mathematics or computer science department or at an advanced graduate level in engineering departments ...' A. Belegundu, Applied Mechanics Review, Vol. 43, no. 4, April 1990},
  isbn = {978-94-010-6898-7},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Mockus_1989_Bayesian_Approach_to_Global_Optimization.pdf;/home/dede/Zotero/storage/NUIFS7RC/9789401068987.html}
}

@book{mockus2000,
  title = {A {{Set}} of {{Examples}} of {{Global}} and {{Discrete Optimization}}: {{Applications}} of {{Bayesian Heuristic Approach}}},
  shorttitle = {A {{Set}} of {{Examples}} of {{Global}} and {{Discrete Optimization}}},
  author = {Mockus, Jonas},
  year = {2000},
  series = {Applied {{Optimization}}},
  publisher = {{Springer US}},
  doi = {10.1007/978-1-4615-4671-9},
  abstract = {This book shows how the Bayesian Approach (BA) improves well\- known heuristics by randomizing and optimizing their parameters. That is the Bayesian Heuristic Approach (BHA). The ten in-depth examples are designed to teach Operations Research using Internet. Each example is a simple representation of some impor\- tant family of real-life problems. The accompanying software can be run by remote Internet users. The supporting web-sites include software for Java, C++, and other lan\- guages. A theoretical setting is described in which one can discuss a Bayesian adaptive choice of heuristics for discrete and global optimization prob\- lems. The techniques are evaluated in the spirit of the average rather than the worst case analysis. In this context, "heuristics" are understood to be an expert opinion defining how to solve a family of problems of dis\- crete or global optimization. The term "Bayesian Heuristic Approach" means that one defines a set of heuristics and fixes some prior distribu\- tion on the results obtained. By applying BHA one is looking for the heuristic that reduces the average deviation from the global optimum. The theoretical discussions serve as an introduction to examples that are the main part of the book. All the examples are interconnected. Dif\- ferent examples illustrate different points of the general subject. How\- ever, one can consider each example separately, too.},
  isbn = {978-0-7923-6359-0},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Mockus_2000_A_Set_of_Examples_of_Global_and_Discrete_Optimization.pdf;/home/dede/Zotero/storage/8EJ8DVTL/9780792363590.html}
}

@article{mokbel2003,
  title = {Analysis of {{Multi-Dimensional Space-Filling Curves}}},
  author = {Mokbel, Mohamed F. and Aref, Walid G. and Kamel, Ibrahim},
  year = {2003},
  journal = {GeoInformatica},
  volume = {7},
  number = {3},
  pages = {179--209},
  issn = {13846175},
  doi = {10.1023/A:1025196714293},
  keywords = {todo}
}

@incollection{montgomery1972,
  title = {Some {{Techniques}} for {{Multivariate Quality Control Applications}}},
  author = {Montgomery, D.C. and Wadsworth, H.M.},
  year = {1972},
  series = {Transactions of the {{ASQC}}},
  address = {{Washington, DC}}
}

@article{montgomery1994,
  title = {Integrating {{Statistical Process Control}} and {{Engineering Process Control}}},
  author = {Montgomery, Douglas C. and Keats, J. Bert and Runger, George C. and Messina, William S.},
  year = {1994},
  month = apr,
  journal = {Journal of Quality Technology},
  volume = {26},
  number = {2},
  pages = {79--87},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.1994.11979508},
  abstract = {Statistical process control (SPC) is traditionally applied to processes that vary about a fixed mean, and where successive observations are viewed as statistically independent. Engineering process control (EPC) is usually applied to processes in which successive observations are related over time, and where the mean drifts dynamically. Thus, EPC seeks to minimize variability by transferring it from the output variable to a related process input (controllable) variable, while SPC seeks to reduce variability by detecting and eliminating assignable causes of variation. This paper shows through simulation that when using EPC it is always better to have an SPC system in place that monitors and acts properly on the root cause of the assignable change.},
  keywords = {Autoregressive Integrated Moving Average Process,Engineering Process Control,Minimum Mean Square Error Controller,Statistical Process Control,todo},
  annotation = {\_eprint: https://doi.org/10.1080/00224065.1994.11979508},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Montgomery_et_al_1994_Integrating_Statistical_Process_Control_and_Engineering_Process_Control.pdf;/home/dede/Zotero/storage/XV3BE7FS/00224065.1994.html}
}

@article{moon2001,
  title = {Analysis of the Clustering Properties of the {{Hilbert}} Space-Filling Curve},
  author = {Moon, B. and Jagadish, H.V. and Faloutsos, C. and Saltz, J.H.},
  year = {2001},
  month = jan,
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {13},
  number = {1},
  pages = {124--141},
  issn = {1558-2191},
  doi = {10.1109/69.908985},
  abstract = {Several schemes for the linear mapping of a multidimensional space have been proposed for various applications, such as access methods for spatio-temporal databases and image compression. In these applications, one of the most desired properties from such linear mappings is clustering, which means the locality between objects in the multidimensional space being preserved in the linear space. It is widely believed that the Hilbert space-filling curve achieves the best clustering (Abel and Mark, 1990; Jagadish, 1990). We analyze the clustering property of the Hilbert space-filling curve by deriving closed-form formulas for the number of clusters in a given query region of an arbitrary shape (e.g., polygons and polyhedra). Both the asymptotic solution for the general case and the exact solution for a special case generalize previous work. They agree with the empirical results that the number of clusters depends on the hypersurface area of the query region and not on its hypervolume. We also show that the Hilbert curve achieves better clustering than the z curve. From a practical point of view, the formulas given provide a simple measure that can be used to predict the required disk access behaviors and, hence, the total access time.},
  keywords = {Fractals,Geographic Information Systems,Helium,Hilbert space,Image coding,Image databases,Moon,Multidimensional systems,Shape,Time measurement,todo},
  file = {/home/dede/Zotero/storage/NFSCCAFE/Moon et al_2001_Analysis of the clustering properties of the Hilbert space-filling curve.pdf;/home/dede/Zotero/storage/958ZTR2I/908985.html}
}

@article{motsepa2021,
  title = {Double Sampling Monitoring Schemes: A Literature Review and Some Future Research Ideas},
  shorttitle = {Double Sampling Monitoring Schemes},
  author = {Motsepa, Collen M. and {Malela-Majika}, Jean-Claude and Castagliola, Philippe and Shongwe, Sandile C.},
  year = {2021},
  month = jun,
  journal = {Communications in Statistics - Simulation and Computation},
  volume = {0},
  number = {0},
  pages = {1--29},
  publisher = {{Taylor \& Francis}},
  issn = {0361-0918},
  doi = {10.1080/03610918.2021.1939375},
  abstract = {Adapted from the acceptance sampling field, the double sampling monitoring schemes implement a two-stage strategy to decide whether the process being monitored is in-control or out-of-control. That is, a master sample is split into two separate subgroup samples, with the first subgroup sample used in the first stage and, depending on which type of double sampling method is used, either only the second or the combined first and second, subgroup sample(s) are used in the second stage. This strategy has been proven to effectively decrease the sampling effort and, at the same time, to decrease the time to detect potential out-of-control situations. For these reasons, it has received some attention in the statistical process monitoring (SPM) literature and, in this review paper, all 92 existing publications on the basic double sampling monitoring schemes and other different schemes that are integrated with the basic double sampling schemes are reviewed. The double sampling schemes are categorized and summarized so that any research gaps in the SPM literature can easily be identified. Finally, concluding remarks and some directions for future research ideas are given.},
  keywords = {Control chart,Double sampling,Monitoring scheme,Phase I,Phase II,Run-length,skimmed,Statistical process monitoring (SPM)},
  annotation = {\_eprint: https://doi.org/10.1080/03610918.2021.1939375},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Motsepa_et_al_2021_Double_sampling_monitoring_schemes.pdf;/home/dede/Zotero/storage/35ETNXEY/03610918.2021.html}
}

@article{mukherjee2020,
  title = {Nonparametric {{Phase-II}} Control Charts for Monitoring High-Dimensional Processes with Unknown Parameters},
  author = {Mukherjee, Amitava and Marozzi, Marco},
  year = {2020},
  month = sep,
  journal = {Journal of Quality Technology},
  volume = {0},
  number = {0},
  pages = {1--21},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2020.1805378},
  abstract = {Monitoring multivariate and high-dimensional data streams is often an essential requirement for quality management in manufacturing and service sectors in the Industry 4.0 era. Identifying a suitable distribution for a multivariate data set, especially when the number of variables is much larger than the sample size, is often challenging. Consequently, in a high-dimensional set-up, that is, when the number of variables under investigation exceeds sample size, parametric methods are generally not reliable in practice. There are various nonparametric schemes based on data depth for multivariate process monitoring, which are applicable only when the sample size is reasonably larger than the number of variables in the process but not in a high-dimensional set-up. We discuss that most of these charts are not robust when the true process parameters are unknown. There are, however, some nonparametric schemes for a high-dimensional process, when true process parameters are known. Nevertheless, when process parameters are unknown, a highly robust nonparametric scheme for monitoring high-dimensional processes is not yet available. In this paper, we propose some Shewhart-type nonparametric monitoring schemes based on specific distance metrics for surveillance of multivariate as well as high-dimensional processes. Our proposed charts are easy to implement, interpret and also advantageous in simultaneous monitoring of multiple aspects. We discuss the design and implementation issues in details. We carry out a performance study using Monte Carlo simulations and illustrate the proposed methods using a dataset related to industrial production.},
  keywords = {high-dimensional process,interpoint distance,Lepage statistic,multivariate data,nonparametric,process monitoring,todo},
  annotation = {\_eprint: https://doi.org/10.1080/00224065.2020.1805378},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Mukherjee_Marozzi_2020_Nonparametric_Phase-II_control_charts_for_monitoring_high-dimensional_processes.pdf;/home/dede/Zotero/storage/HVF6MPUB/00224065.2020.html}
}

@article{mukhopadhyay2019,
  title = {Nonparametric {{Universal Copula Modeling}}},
  author = {Mukhopadhyay, Subhadeep and Parzen, Emanuel},
  year = {2019},
  month = dec,
  journal = {arXiv:1912.05503 [stat]},
  eprint = {1912.05503},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {To handle the ubiquitous problem of "dependence learning," copulas are quickly becoming a pervasive tool across a wide range of data-driven disciplines encompassing neuroscience, finance, econometrics, genomics, social science, machine learning, healthcare and many more. Copula (or connection) functions were invented in 1959 by Abe Sklar in response to a query of Maurice Frechet. After 60 years, where do we stand now? This article provides a history of the key developments and offers a unified perspective.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Applications,Statistics - Computation,Statistics - Methodology,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Mukhopadhyay_Parzen_2019_Nonparametric_Universal_Copula_Modeling.pdf;/home/dede/Zotero/storage/MABYMHWA/1912.html}
}

@book{muller2015,
  title = {Bayesian {{Nonparametric Data Analysis}}},
  author = {M{\"u}ller, Peter and Quintana, Fernando Andr{\'e}s and Jara, Alejandro and Hanson, Tim},
  year = {2015},
  series = {Springer {{Series}} in {{Statistics}}},
  publisher = {{Springer International Publishing}},
  doi = {10.1007/978-3-319-18968-0},
  abstract = {This book reviews nonparametric Bayesian methods and models that have proven useful in the context of data analysis. Rather than providing an encyclopedic review of probability models, the book's structure follows a data analysis perspective. As such, the chapters are organized by traditional data analysis problems. In selecting specific nonparametric models, simpler and more traditional models are favored over specialized ones. The discussed methods are illustrated with a wealth of examples, including applications ranging from stylized examples to case studies from recent literature. The book also includes an extensive discussion of computational methods and details on their implementation. R code for many examples is included in online software pages.},
  isbn = {978-3-319-18967-3},
  langid = {english},
  file = {/home/dede/Zotero/storage/AE3T8C5W/9783319189673.html}
}

@article{murdoch2008,
  title = {P-{{Values}} Are {{Random Variables}}},
  author = {Murdoch, Duncan J and Tsai, Yu-Ling and Adcock, James},
  year = {2008},
  month = aug,
  journal = {The American Statistician},
  volume = {62},
  number = {3},
  pages = {242--245},
  publisher = {{Taylor \& Francis}},
  issn = {0003-1305},
  doi = {10.1198/000313008X332421},
  abstract = {P-values are taught in introductory statistics classes in a way that confuses many of the students, leading to common misconceptions about their meaning. In this article, we argue that p-values should be taught through simulation, emphasizing that p-values are random variables. By means of elementary examples we illustrate how to teach students valid interpretations of p-values and give them a deeper understanding of hypothesis testing.},
  keywords = {Empirical cumulative distribution function (ECDF),Histograms,Hypothesis testing,Teaching statistics,todo},
  annotation = {\_eprint: https://doi.org/10.1198/000313008X332421},
  file = {/home/dede/Zotero/storage/38W7SE8G/000313008X332421.html}
}

@book{murphy2012,
  title = {{Machine Learning: A Probabilistic Perspective}},
  shorttitle = {{Machine Learning}},
  author = {Murphy, Kevin P.},
  year = {2012},
  month = sep,
  edition = {1 edizione},
  publisher = {{The MIT Press}},
  abstract = {A comprehensive introduction to machine learning that uses probabilistic models and inference as a unifying approach.Today's Web-enabled deluge of electronic data calls for automated methods of data analysis. Machine learning provides these, developing methods that can automatically detect patterns in data and then use the uncovered patterns to predict future data. This textbook offers a comprehensive and self-contained introduction to the field of machine learning, based on a unified, probabilistic approach.The coverage combines breadth and depth, offering necessary background material on such topics as probability, optimization, and linear algebra as well as discussion of recent developments in the field, including conditional random fields, L1 regularization, and deep learning. The book is written in an informal, accessible style, complete with pseudo-code for the most important algorithms. All topics are copiously illustrated with color images and worked examples drawn from such application domains as biology, text processing, computer vision, and robotics. Rather than providing a cookbook of different heuristic methods, the book stresses a principled model-based approach, often using the language of graphical models to specify models in a concise and intuitive way. Almost all the models described have been implemented in a MATLAB software package\textemdash PMTK (probabilistic modeling toolkit)\textemdash that is freely available online. The book is suitable for upper-level undergraduates with an introductory-level college math background and beginning graduate students.},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Murphy_2012_Machine_Learning.pdf}
}

@book{murray1993,
  title = {Differential {{Geometry}} and {{Statistics}}},
  author = {Murray, M.K. and Rice, J.W.},
  year = {1993},
  month = jan,
  edition = {1 edition},
  publisher = {{Chapman and Hall/CRC}},
  address = {{London ; New York}},
  abstract = {This book explains why geometry should enter into parametric statistics and how the theory of asymptotic expansions involves a form of higher-order differential geometry. It gives some new explanations of geometric ideas from a first principles point of view as far as geometry is concerned.},
  isbn = {978-0-412-39860-5},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Murray_Rice_1993_Differential_Geometry_and_Statistics.pdf}
}

@article{nadaraya1964,
  title = {On {{Estimating Regression}}},
  author = {Nadaraya, E. A.},
  year = {1964},
  month = jan,
  journal = {Theory of Probability \& Its Applications},
  volume = {9},
  number = {1},
  pages = {141--142},
  issn = {0040-585X, 1095-7219},
  doi = {10.1137/1109020},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Nadaraya_1964_On_Estimating_Regression.pdf}
}

@book{nazarathy2021,
  title = {{Statistics With Julia: Fundamentals for Data Science, Machine Learning and Artificial Intelligence}},
  shorttitle = {{Statistics With Julia}},
  author = {Nazarathy, Yoni and Klok, Hayden},
  year = {2021},
  edition = {1st ed. 2021 edizione},
  publisher = {{Springer-Nature New York Inc}},
  address = {{Cham Springer}},
  isbn = {978-3-030-70900-6},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Nazarathy_Klok_2021_Statistics_With_Julia.pdf}
}

@article{neal2000,
  title = {Markov {{Chain Sampling Methods}} for {{Dirichlet Process Mixture Models}}},
  author = {Neal, Radford M.},
  year = {2000},
  journal = {Journal of Computational and Graphical Statistics},
  langid = {english},
  keywords = {todo},
  file = {/home/dede/Zotero/storage/5B3ILZGA/1390653.html}
}

@article{neal2003,
  title = {Slice Sampling},
  author = {Neal, Radford M.},
  year = {2003},
  month = jun,
  journal = {The Annals of Statistics},
  volume = {31},
  number = {3},
  pages = {705--767},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/aos/1056562461},
  abstract = {Markov chain sampling methods that adapt to characteristics of the distribution being sampled can be constructed using the principle that one can ample from a distribution by sampling uniformly from the region under the plot of its density function. A Markov chain that converges to this uniform distribution can be constructed by alternating uniform sampling in the vertical direction with uniform sampling from the horizontal "slice" defined by the current vertical position, or more generally, with some update that leaves the uniform distribution over this slice invariant. Such "slice sampling" methods are easily implemented for univariate distributions, and can be used to sample from a multivariate distribution by updating each variable in turn. This approach is often easier to implement than Gibbs sampling and more efficient than simple Metropolis updates, due to the ability of slice sampling to adaptively choose the magnitude of changes made. It is therefore attractive for routine and automated use. Slice sampling methods that update all variables simultaneously are also possible. These methods can adaptively choose the magnitudes of changes made to each variable, based on the local properties of the density function. More ambitiously, such methods could potentially adapt to the dependencies between variables by constructing local quadratic approximations. Another approach is to improve sampling efficiency by suppressing random walks. This can be done for univariate slice sampling by "overrelaxation," and for multivariate slice sampling by "reflection" from the edges of the slice.},
  keywords = {65C05,65C60,Adaptive methods,auxiliary variables,done,dynamical methods,Gibbs sampling,Markov chain Monte Carlo,Metropolis algorithm,overrelaxation},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Neal_2003_Slice_sampling.pdf;/home/dede/Zotero/storage/D57X83V9/1056562461.html}
}

@article{negiz1997,
  title = {Statistical Monitoring of Multivariable Dynamic Processes with State-Space Models},
  author = {Negiz, Antoine and {\c C}linar, Ali},
  year = {1997},
  journal = {AIChE Journal},
  volume = {43},
  number = {8},
  pages = {2002--2020},
  issn = {1547-5905},
  doi = {10.1002/aic.690430810},
  abstract = {Industrial continuous processes may have a large number of process variables and are usually operated for extended periods at fixed operating points under closed-loop control, yielding process measurements that are autocorrelated, cross-correlated, and collinear. A statistical process monitoring (SPM) method based on multivariate statistics and system theory is introduced to monitor the variability of such processes. The statistical model that describes the in-control variability is based on a canonical-variate (CV) state-space model that is an equivalent representation of a vector autoregressive moving-average time-series model. The CV state variables obtained from the state-space model are linear combinations of the past process measurements that explain the variability of the future measurements the most. Because of this distinctive feature, the CV state variables are regarded as the principal dynamic directions A T2 statistic based on the CV state variables is used for developing an SPM procedure. Simple examples based on simulated data and an experimental application based on a high-temperature short-time milk pasteurization process illustrate advantages of the proposed SPM method.},
  langid = {english},
  keywords = {skimmed},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/aic.690430810},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Negiz_Ã‡linar_1997_Statistical_monitoring_of_multivariable_dynamic_processes_with_state-space.pdf;/home/dede/Zotero/storage/CZ332KA5/aic.html}
}

@book{nesterov2004,
  title = {Introductory {{Lectures}} on {{Convex Optimization}}: {{A Basic Course}}},
  shorttitle = {Introductory {{Lectures}} on {{Convex Optimization}}},
  author = {Nesterov, Yurii},
  year = {2004},
  series = {Applied {{Optimization}}},
  publisher = {{Springer US}},
  doi = {10.1007/978-1-4419-8853-9},
  abstract = {It was in the middle of the 1980s, when the seminal paper by Kar\- markar opened a new epoch in nonlinear optimization. The importance of this paper, containing a new polynomial-time algorithm for linear op\- timization problems, was not only in its complexity bound. At that time, the most surprising feature of this algorithm was that the theoretical pre\- diction of its high efficiency was supported by excellent computational results. This unusual fact dramatically changed the style and direc\- tions of the research in nonlinear optimization. Thereafter it became more and more common that the new methods were provided with a complexity analysis, which was considered a better justification of their efficiency than computational experiments. In a new rapidly develop\- ing field, which got the name "polynomial-time interior-point methods", such a justification was obligatory. Afteralmost fifteen years of intensive research, the main results of this development started to appear in monographs [12, 14, 16, 17, 18, 19]. Approximately at that time the author was asked to prepare a new course on nonlinear optimization for graduate students. The idea was to create a course which would reflect the new developments in the field. Actually, this was a major challenge. At the time only the theory of interior-point methods for linear optimization was polished enough to be explained to students. The general theory of self-concordant functions had appeared in print only once in the form of research monograph [12].},
  isbn = {978-1-4020-7553-7},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Nesterov_2004_Introductory_Lectures_on_Convex_Optimization.pdf;/home/dede/Zotero/storage/7XT8KBSK/9781402075537.html}
}

@book{nesterov2018,
  title = {Lectures on {{Convex Optimization}}},
  author = {Nesterov, Yurii},
  year = {2018},
  series = {Springer {{Optimization}} and {{Its Applications}}},
  edition = {Second},
  publisher = {{Springer International Publishing}},
  doi = {10.1007/978-3-319-91578-4},
  abstract = {This book provides a comprehensive, modern introduction to convex optimization, a field that is becoming increasingly important in applied mathematics, economics and finance, engineering, and computer science, notably in data science and machine learning. Written by a leading expert in the field, this book includes recent advances in the algorithmic theory of convex optimization, naturally complementing the existing literature. It contains a unified and rigorous presentation of the acceleration techniques for minimization schemes of first- and second-order. It provides readers with a full treatment of the smoothing technique, which has tremendously extended the abilities of gradient-type methods. Several powerful approaches in structural optimization, including optimization in relative scale and polynomial-time interior-point methods, are also discussed in detail. Researchers in theoretical optimization as well as professionals working on optimization problems will find this book very useful. It presents many successful examples of how to develop very fast specialized minimization algorithms. Based on the author's lectures, it can naturally serve as the basis for introductory and advanced courses in convex optimization for students in engineering, economics, computer science and mathematics.},
  isbn = {978-3-319-91577-7},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Nesterov_2018_Lectures_on_Convex_Optimization.pdf;/home/dede/Zotero/storage/JS8CTT3X/9783319915777.html}
}

@book{newman2010,
  title = {Networks: {{An Introduction}}},
  shorttitle = {Networks},
  author = {Newman, Mark},
  year = {2010},
  month = mar,
  publisher = {{Oxford University Press}},
  abstract = {The scientific study of networks, including computer networks, social networks, and biological networks, has received an enormous amount of interest in the last few years. The rise of the Internet and the wide availability of inexpensive computers have made it possible to gather and analyze network data on a large scale, and the development of a variety of new theoretical tools has allowed us to extract new knowledge from many different kinds of networks. The study of networks is broadly interdisciplinary and important developments have occurred in many fields, including mathematics, physics, computer and information sciences, biology, and the social sciences. This book brings together the most important breakthroughs in each of these fields and presents them in a coherent fashion, highlighting the strong interconnections between work in different areas. Subjects covered include the measurement and structure of networks in many branches of science, methods for analyzing network data, including methods developed in physics, statistics, and sociology, the fundamentals of graph theory, computer algorithms, and spectral methods, mathematical models of networks, including random graph models and generative models, and theories of dynamical processes taking place on networks.},
  isbn = {978-0-19-159417-5},
  langid = {american},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Newman_2010_Networks.pdf;/home/dede/Zotero/storage/BU58USJB/acprof-9780199206650.html}
}

@book{newman2018,
  title = {Networks},
  author = {Newman, Mark},
  year = {2018},
  month = jul,
  edition = {Second Edition},
  publisher = {{Oxford University Press}},
  address = {{Oxford, New York}},
  abstract = {The study of networks, including computer networks, social networks, and biological networks, has attracted enormous interest in the last few years. The rise of the Internet and the wide availability of inexpensive computers have made it possible to gather and analyze network data on an unprecedented scale, and the development of new theoretical tools has allowed us to extract knowledge from networks of many different kinds. The study of networks is broadly interdisciplinary and central developments have occurred in many fields, including mathematics, physics, computer and information sciences, biology, and the social sciences. This book brings together the most important breakthroughs in each of these fields and presents them in a coherent fashion, highlighting the strong interconnections between work in different areas.Topics covered include the measurement of networks; methods for analyzing network data, including methods developed in physics, statistics, and sociology; fundamentals of graph theory; computer algorithms; mathematical models of networks, including random graph models and generative models; and theories of dynamical processes taking place on networks.},
  isbn = {978-0-19-880509-0},
  keywords = {networks},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Newman_2018_Networks.pdf;/home/dede/Zotero/storage/ZKXKXWFN/networks-9780198805090.html}
}

@book{newman2020,
  title = {Socialism: {{A Very Short Introduction}}},
  shorttitle = {Socialism},
  author = {Newman, Michael},
  year = {2020},
  series = {Very {{Short Introductions}}},
  edition = {Second},
  publisher = {{Oxford University Press}},
  isbn = {978-0-19-257351-3},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Newman_2020_Socialism.epub}
}

@article{newton2004,
  title = {Detecting Differential Gene Expression with a Semiparametric Hierarchical Mixture Method},
  author = {Newton, Michael A. and Noueiry, Amine and Sarkar, Deepayan and Ahlquist, Paul},
  year = {2004},
  month = apr,
  journal = {Biostatistics (Oxford, England)},
  volume = {5},
  number = {2},
  pages = {155--176},
  issn = {1465-4644},
  doi = {10.1093/biostatistics/5.2.155},
  abstract = {Mixture modeling provides an effective approach to the differential expression problem in microarray data analysis. Methods based on fully parametric mixture models are available, but lack of fit in some examples indicates that more flexible models may be beneficial. Existing, more flexible, mixture models work at the level of one-dimensional gene-specific summary statistics, and so when there are relatively few measurements per gene these methods may not provide sensitive detectors of differential expression. We propose a hierarchical mixture model to provide methodology that is both sensitive in detecting differential expression and sufficiently flexible to account for the complex variability of normalized microarray data. EM-based algorithms are used to fit both parametric and semiparametric versions of the model. We restrict attention to the two-sample comparison problem; an experiment involving Affymetrix microarrays and yeast translation provides the motivating case study. Gene-specific posterior probabilities of differential expression form the basis of statistical inference; they define short gene lists and false discovery rates. Compared to several competing methodologies, the proposed methodology exhibits good operating characteristics in a simulation study, on the analysis of spike-in data, and in a cross-validation calculation.},
  langid = {english},
  pmid = {15054023},
  keywords = {Algorithms,Cell Cycle Proteins,Computer Simulation,Data Interpretation; Statistical,DEAD-box RNA Helicases,Fungal Proteins,Gene Expression Profiling,Gene Expression Regulation; Fungal,Models; Genetic,Models; Statistical,Mutation,Oligonucleotide Array Sequence Analysis,Protein Biosynthesis,RNA Helicases,RNA; Fungal,Saccharomyces cerevisiae,Saccharomyces cerevisiae Proteins,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Newton_et_al_2004_Detecting_differential_gene_expression_with_a_semiparametric_hierarchical.pdf}
}

@article{neyman1933,
  title = {On the Problem of the Most Efficient Tests of Statistical Hypotheses},
  author = {Neyman, Jerzy and Pearson, Egon},
  year = {1933},
  month = feb,
  journal = {Philosophical Transactions of the Royal Society of London Series A},
  volume = {231},
  pages = {289--337},
  issn = {0264-3952, 2053-9258},
  doi = {10.1098/rsta.1933.0009},
  langid = {english},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Neyman_Pearson_1933_On_the_problem_of_the_most_efficient_tests_of_statistical_hypotheses.pdf}
}

@inproceedings{nie2010,
  title = {Statistical {{Process Control}} Based on State Space Model},
  booktitle = {The 2nd {{International Conference}} on {{Information Science}} and {{Engineering}}},
  author = {Nie, Bin and Qiang, Liu and Dan, Liao and Jing, Ding},
  year = {2010},
  month = dec,
  pages = {1--8},
  issn = {2160-1291},
  doi = {10.1109/ICISE.2010.5691018},
  abstract = {It is difficult to apply traditional Statistical Process Control (SPC) technology as state variables are unmeasurable in small batch production process. A modified individual control chart based on state space model was proposed in this paper. The process state was estimated through particle filter technology. The median of state particles group was monitored by Average Moving Range (AMR) control chart to detect variation of normal distribution process. Results showed that the proposed chart was an effective tool to monitor small batch production process.},
  keywords = {Atmospheric measurements,average moving range control chart,Control charts,Mathematical model,median,Monitoring,particle filter,Particle filters,Particle measurements,Process control,state space model,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Nie_et_al_2010_Statistical_Process_Control_based_on_state_space_model.pdf;/home/dede/Zotero/storage/DQ8JDP47/5691018.html}
}

@article{nielsen2018,
  title = {An Elementary Introduction to Information Geometry},
  author = {Nielsen, Frank},
  year = {2018},
  month = aug,
  journal = {arXiv:1808.08271 [cs, math, stat]},
  eprint = {1808.08271},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {We describe the fundamental differential-geometric structures of information manifolds, state the fundamental theorem of information geometry, and illustrate some uses of these information manifolds in information sciences. The exposition is self-contained by concisely introducing the necessary concepts of differential geometry with proofs omitted for brevity.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning,Statistics - Machine Learning,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Nielsen_2018_An_elementary_introduction_to_information_geometry.pdf;/home/dede/Zotero/storage/TN8CUY9I/1808.html}
}

@article{noor-ul-amin2021,
  title = {An Adaptive {{EWMA}} Control Chart for Monitoring the Process Mean in {{Bayesian}} Theory under Different Loss Functions},
  author = {{Noor-ul-Amin}, Muhammad and Noor, Surria},
  year = {2021},
  journal = {Quality and Reliability Engineering International},
  volume = {37},
  number = {2},
  pages = {804--819},
  issn = {1099-1638},
  doi = {10.1002/qre.2764},
  abstract = {The Shewhart control chart is used for detecting the large shift and an exponentially weighted moving average (EWMA) control chart is used for detecting the small/moderate shift in the process mean. A scheme that combines both the Shewhart control chart and the EWMA control chart in a smooth way is called the adaptive EWMA (AEWMA) control chart. In this paper, we proposed a new AEWMA control chart for monitoring the process mean in Bayesian theory under different loss functions (LFs). We used informative (conjugate prior) under two different LFs: (1) squared error loss function and (2) linex loss function for posterior and posterior predictive distributions. We used the average run length and standard deviation of run length to measure the performance of the AEWMA control chart in the Bayesian theory. A comparative study is conducted for comparing the proposed AEWMA control chart in Bayesian theory with the existing Bayesian EWMA control chart. We conducted a Monte Carlo simulation study to evaluate the proposed AEWMA control chart. For the implementation purposes, we presented a real-data example.},
  langid = {english},
  keywords = {Bayesian,control chart,EWMA,loss function,simulation study},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/qre.2764},
  file = {/home/dede/Zotero/storage/RXH8L283/qre.html}
}

@article{noor2021,
  title = {Bayesian {{EWMA}} Control Charts Based on {{Exponential}} and Transformed {{Exponential}} Distributions},
  author = {Noor, Surria and {Noor-ul-Amin}, Muhammad and Abbasi, Saddam Akber},
  year = {2021},
  journal = {Quality and Reliability Engineering International},
  volume = {37},
  number = {4},
  pages = {1678--1698},
  issn = {1099-1638},
  doi = {10.1002/qre.2820},
  abstract = {In this paper, we proposed the Bayesian exponentially weighted moving average (EWMA) control charts for mean under the nonnormal life time distributions. We used the time between events data which follow the Exponential distribution and proposed the Bayesian EWMA control charts for Exponential distribution and transformed Exponential distributions into Inverse Rayleigh and Weibull distributions. In order to develop the control charts, we used a uniform prior under five different symmetric and asymmetric loss functions (LFs), namely, squared error loss function (SELF), precautionary loss function (PLF), general entropy loss function (GELF), entropy loss function (ELF), and weighted balance loss function (WBLF). The average run length (ARL) and the standard deviation of run length (SDRL) are used to check the performance of the proposed Bayesian EWMA control charts for Exponential and transformed Exponential distributions. An extensive simulation study is conducted to evaluate the proposed Bayesian EWMA control chart for nonnormal distributions. It is observed from the results that the proposed control chart with the Weibull distribution produces the best results among the considered distributions under different LFs. A real data example is presented for implementation purposes.},
  langid = {english},
  keywords = {average run length,Bayesian approach,control charts,EWMA,loss function},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/qre.2820},
  file = {/home/dede/Zotero/storage/X6374HQC/qre.html}
}

@article{nordman2009,
  title = {A {{Note}} on the {{Stationary Bootstrap}}'s {{Variance}}},
  author = {Nordman, Daniel J.},
  year = {2009},
  journal = {The Annals of Statistics},
  volume = {37},
  number = {1},
  pages = {359--370},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364},
  abstract = {Because the stationary bootstrap resamples data blocks of random length, this method has been thought to have the largest asymptotic variance among block bootstraps Lahiri [Ann. statist. 27 (1999) 386-404]. It is shown here that the variance of the stationary bootstrap surprisingly matches that of a block bootstrap based on nonrandom, nonoverlapping blocks. This argument translates the variance expansion into the frequency domain and provides a unified way of determining variances for other block bootstraps. Some previous results on the stationary bootstrap, related to asymptotic relative efficiency and optimal block size, are also updated.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Nordman_2009_A_Note_on_the_Stationary_Bootstrap's_Variance.pdf}
}

@book{norris1997,
  title = {{Markov Chains}},
  author = {Norris, J. R.},
  year = {1997},
  month = apr,
  publisher = {{Cambridge University Press}},
  address = {{Cambridge, UK ; New York}},
  isbn = {978-0-521-48181-6},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Norris_1997_Markov_Chains.pdf}
}

@article{nowicki2001,
  title = {Estimation and {{Prediction}} for {{Stochastic Blockstructures}}},
  author = {Nowicki, Krzysztof and Snijders, Tom A. B},
  year = {2001},
  month = sep,
  journal = {Journal of the American Statistical Association},
  volume = {96},
  number = {455},
  pages = {1077--1087},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1198/016214501753208735},
  abstract = {A statistical approach to a posteriori blockmodeling for digraphs and valued digraphs is proposed. The probability model assumes that the vertices of the digraph are partitioned into several unobserved (latent) classes and that the probability distribution of the relation between two vertices depends only on the classes to which they belong. A Bayesian estimator based on Gibbs sampling is proposed. The basic model is not identified, because class labels are arbitrary. The resulting identifiability problems are solved by restricting inference to the posterior distributions of invariant functions of the parameters and the vertex class membership. In addition, models are considered where class labels are identified by prior distributions for the class membership of some of the vertices. The model is illustrated by an example from the social networks literature (Kapferer's tailor shop).},
  keywords = {Cluster analysis,Colored graph,Gibbs sampling,Latent class model,Mixture model,Social network,todo},
  annotation = {\_eprint: https://doi.org/10.1198/016214501753208735},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Nowicki_Snijders_2001_Estimation_and_Prediction_for_Stochastic_Blockstructures.pdf;/home/dede/Zotero/storage/WW3U92NU/016214501753208735.html}
}

@book{oja2010,
  title = {Multivariate {{Nonparametric Methods}} with {{R}}: {{An}} Approach Based on Spatial Signs and Ranks},
  shorttitle = {Multivariate {{Nonparametric Methods}} with {{R}}},
  author = {Oja, Hannu},
  year = {2010},
  series = {Lecture {{Notes}} in {{Statistics}}},
  publisher = {{Springer-Verlag}},
  address = {{New York}},
  doi = {10.1007/978-1-4419-0468-3},
  abstract = {This book offers a new, fairly efficient, and robust alternative to analyzing multivariate data. The analysis of data based on multivariate spatial signs and ranks proceeds very much as does a traditional multivariate analysis relying on the assumption of multivariate normality; the regular L2 norm is just replaced by different L1 norms, observation vectors are replaced by spatial signs and ranks, and so on. A unified methodology starting with the simple one-sample multivariate location problem and proceeding to the general multivariate multiple linear regression case is presented. Companion estimates and tests for scatter matrices are considered as well. The R package MNM is available for computation of the procedures. This monograph provides an up-to-date overview of the theory of multivariate nonparametric methods based on spatial signs and ranks. The classical book by Puri and Sen (1971) uses marginal signs and ranks and different type of L1 norm. The book may serve as a textbook and a general reference for the latest developments in the area. Readers are assumed to have a good knowledge of basic statistical theory as well as matrix theory. Hannu Oja is an academy professor and a professor in biometry in the University of Tampere. He has authored and coauthored numerous research articles in multivariate nonparametrical and robust methods as well as in biostatistics.},
  isbn = {978-1-4419-0467-6},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Oja_2010_Multivariate_Nonparametric_Methods_with_R.pdf;/home/dede/Zotero/storage/AEGXDWJT/9781441904676.html}
}

@inproceedings{olwell1997,
  title = {Predictive {{Statistical Process Control}} for the {{Military}}},
  booktitle = {Proceedings of the {{Third Annual U}}.{{S}}. {{Army Conference}} on {{Applied Statistics}}},
  author = {Olwell, D. H.},
  year = {1997},
  pages = {70},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Olwell_1997_Predictive_Statistical_Process_Control_for_the_Military.pdf}
}

@article{opensciencecollaboration2015,
  title = {Estimating the Reproducibility of Psychological Science},
  author = {{Open Science Collaboration}},
  year = {2015},
  month = aug,
  journal = {Science},
  volume = {349},
  number = {6251},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aac4716},
  abstract = {Empirically analyzing empirical evidence One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts et al. describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study. Science, this issue 10.1126/science.aac4716 Structured Abstract INTRODUCTIONReproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error. RATIONALEThere is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science. RESULTSWe conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and P values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (Mr = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (Mr = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (P {$<$} .05). Thirty-six percent of replications had significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams. CONCLUSIONNo single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original P value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here.Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that ``we already know this'' belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know. {$<$}img class="fragment-image" aria-describedby="F1-caption" src="https://science.sciencemag.org/content/sci/349/6251/aac4716/F1.medium.gif"/{$>$} Download high-res image Open in new tab Download Powerpoint Original study effect size versus replication effect size (correlation coefficients).Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects. Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams. A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired. A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired.},
  chapter = {Research Article},
  copyright = {Copyright \textcopyright{} 2015, American Association for the Advancement of Science},
  langid = {english},
  pmid = {26315443},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Open_Science_Collaboration_2015_Estimating_the_reproducibility_of_psychological_science.pdf;/home/dede/Zotero/storage/5MF4RQUR/aac4716.html}
}

@unpublished{orbanz2014,
  title = {Lecture {{Notes}} on {{Bayesian Nonparametrics}}},
  author = {Orbanz, Peter},
  year = {2014},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Orbanz_2014_Lecture_Notes_on_Bayesian_Nonparametrics2.pdf}
}

@book{ortegaygasset1994,
  title = {The {{Revolt}} of the {{Masses}}},
  author = {{Ortega y Gasset}, Jos{\'e}},
  year = {1994},
  month = feb,
  edition = {Revised ed. edition},
  publisher = {{W. W. Norton \& Company}},
  address = {{New York}},
  isbn = {978-0-393-31095-5},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Gasset_1994_The_Revolt_of_the_Masses.epub}
}

@book{pace1997,
  title = {{Principles of Statistical Inference: From a Neo-Fisherian Perspective}},
  shorttitle = {{Principles of Statistical Inference}},
  author = {Pace, Luigi and Salvan, Alessandra},
  year = {1997},
  publisher = {{World Scientific Pub}},
  address = {{Singapore}},
  abstract = {In this book, an integrated introduction to statistical inference is provided from a frequentist likelihood-based viewpoint. Classical results are presented together with recent developments, largely built upon ideas due to R.A. Fisher. The term "neo-Fisherian" highlights this.After a unified review of background material (statistical models, likelihood, data and model reduction, first-order asymptotics) and inference in the presence of nuisance parameters (including pseudo-likelihoods), a self-contained introduction is given to exponential families, exponential dispersion models, generalized linear models, and group families. Finally, basic results of higher-order asymptotics are introduced (index notation, asymptotic expansions for statistics and distributions, and major applications to likelihood inference).The emphasis is more on general concepts and methods than on regularity conditions. Many examples are given for specific statistical models. Each chapter is supplemented with problems and bibliographic notes. This volume can serve as a textbook in intermediate-level undergraduate and postgraduate courses in statistical inference.},
  isbn = {978-981-02-3066-1},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Pace_Salvan_1997_Principles_of_Statistical_Inference.pdf;/home/dede/Zotero/storage/N3DXF5QD/ERRATA 2014.pdf}
}

@book{pace2001,
  title = {Introduzione Alla Statistica: {{Inferenza}}, Verosimiglianza, Modelli},
  author = {Pace, Luigi and Salvan, Alessandra},
  year = {2001},
  volume = {2},
  publisher = {{CEDAM}}
}

@article{pace2020,
  title = {Likelihood, {{Replicability}} and {{Robbins Confidence Sequences}}},
  author = {Pace, Luigi and Salvan, Alessandra},
  year = {2020},
  journal = {International Statistical Review},
  volume = {88},
  number = {3},
  pages = {599--615},
  issn = {0306-7734, 1751-5823},
  doi = {10.1111/insr.12355},
  langid = {english},
  keywords = {doing},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Pace_Salvan_2020_Likelihood,_Replicability_and_Robbins_Confidence_Sequences.pdf}
}

@article{padilla2016,
  title = {Sequential Nonparametric Tests for a Change in Distribution: An Application to Detecting Radiological Anomalies},
  shorttitle = {Sequential Nonparametric Tests for a Change in Distribution},
  author = {Padilla, Oscar Hernan Madrid and Athey, Alex and Reinhart, Alex and Scott, James G.},
  year = {2016},
  month = dec,
  journal = {arXiv:1612.07867 [stat]},
  eprint = {1612.07867},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {We propose a sequential nonparametric test for detecting a change in distribution, based on windowed Kolmogorov--Smirnov statistics. The approach is simple, robust, highly computationally efficient, easy to calibrate, and requires no parametric assumptions about the underlying null and alternative distributions. We show that both the false-alarm rate and the power of our procedure are amenable to rigorous analysis, and that the method outperforms existing sequential testing procedures in practice. We then apply the method to the problem of detecting radiological anomalies, using data collected from measurements of the background gamma-radiation spectrum on a large university campus. In this context, the proposed method leads to substantial improvements in time-to-detection for the kind of radiological anomalies of interest in law-enforcement and border-security applications.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Padilla_et_al_2016_Sequential_nonparametric_tests_for_a_change_in_distribution.pdf;/home/dede/Zotero/storage/CB7S5J8W/1612.html}
}

@article{paganin2021,
  title = {Centered {{Partition Processes}}: {{Informative Priors}} for {{Clustering}} (with {{Discussion}})},
  shorttitle = {Centered {{Partition Processes}}},
  author = {Paganin, Sally and Herring, Amy H. and Olshan, Andrew F. and Dunson, David B.},
  year = {2021},
  month = mar,
  journal = {Bayesian Analysis},
  volume = {16},
  number = {1},
  pages = {301--370},
  publisher = {{International Society for Bayesian Analysis}},
  issn = {1936-0975, 1931-6690},
  doi = {10.1214/20-BA1197},
  abstract = {There is a very rich literature proposing Bayesian approaches for clustering starting with a prior probability distribution on partitions. Most approaches assume exchangeability, leading to simple representations in terms of Exchangeable Partition Probability Functions (EPPF). Gibbs-type priors encompass a broad class of such cases, including Dirichlet and Pitman-Yor processes. Even though there have been some proposals to relax the exchangeability assumption, allowing covariate-dependence and partial exchangeability, limited consideration has been given on how to include concrete prior knowledge on the partition. For example, we are motivated by an epidemiological application, in which we wish to cluster birth defects into groups and we have prior knowledge of an initial clustering provided by experts. As a general approach for including such prior knowledge, we propose a Centered Partition (CP) process that modifies the EPPF to favor partitions close to an initial one. Some properties of the CP prior are described, a general algorithm for posterior computation is developed, and we illustrate the methodology through simulation examples and an application to the motivating epidemiology study of birth defects.},
  keywords = {Bayesian clustering,Bayesian nonparametrics,centered process,Dirichlet process,exchangeable probability partition function,mixture model,product partition model},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Paganin_et_al_2021_Centered_Partition_Processes.pdf;/home/dede/Zotero/storage/AQJT32EB/20-BA1197.html}
}

@article{page1954,
  title = {Continuous {{Inspection Schemes}}},
  author = {Page, E. S.},
  year = {1954},
  journal = {Biometrika},
  volume = {41},
  number = {1/2},
  pages = {100},
  issn = {00063444},
  doi = {10.2307/2333009},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Page_1954_Continuous_Inspection_Schemes.pdf}
}

@incollection{palmgren2014,
  title = {Transition ({{Markov}}) {{Models}}},
  booktitle = {Wiley {{StatsRef}}: {{Statistics Reference Online}}},
  author = {Palmgren, Juni},
  editor = {Balakrishnan, N. and Colton, Theodore and Everitt, Brian and Piegorsch, Walter and Ruggeri, Fabrizio and Teugels, Jozef L.},
  year = {2014},
  month = sep,
  pages = {stat00360},
  publisher = {{John Wiley \& Sons, Ltd}},
  address = {{Chichester, UK}},
  doi = {10.1002/9781118445112.stat00360},
  isbn = {978-1-118-44511-2},
  langid = {english},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Palmgren_2014_Transition_(Markov)_Models.pdf}
}

@article{pan2010,
  title = {A {{Survey}} on {{Transfer Learning}}},
  author = {Pan, Sinno Jialin and Yang, Qiang},
  year = {2010},
  month = oct,
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {22},
  number = {10},
  pages = {1345--1359},
  issn = {1558-2191},
  doi = {10.1109/TKDE.2009.191},
  abstract = {A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data-labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression, and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as covariate shift. We also explore some potential future issues in transfer learning research.},
  keywords = {Data mining,data mining.,done,Knowledge engineering,Knowledge transfer,Labeling,Learning systems,machine learning,Machine learning,Machine learning algorithms,Space technology,survey,Testing,Training data,Transfer learning},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Pan_Yang_2010_A_Survey_on_Transfer_Learning.pdf;/home/dede/Zotero/storage/JN8UWQBF/5288526.html}
}

@article{panaretos2019,
  title = {Statistical {{Aspects}} of {{Wasserstein Distances}}},
  author = {Panaretos, Victor M. and Zemel, Yoav},
  year = {2019},
  month = mar,
  journal = {Annual Review of Statistics and Its Application},
  volume = {6},
  number = {1},
  eprint = {1806.05500},
  eprinttype = {arxiv},
  pages = {405--431},
  issn = {2326-8298, 2326-831X},
  doi = {10.1146/annurev-statistics-030718-104938},
  abstract = {Wasserstein distances are metrics on probability distributions inspired by the problem of optimal mass transportation. Roughly speaking, they measure the minimal effort required to reconfigure the probability mass of one distribution in order to recover the other distribution. They are ubiquitous in mathematics, with a long history that has seen them catalyse core developments in analysis, optimization, and probability. Beyond their intrinsic mathematical richness, they possess attractive features that make them a versatile tool for the statistician: they can be used to derive weak convergence and convergence of moments, and can be easily bounded; they are well-adapted to quantify a natural notion of perturbation of a probability distribution; and they seamlessly incorporate the geometry of the domain of the distributions in question, thus being useful for contrasting complex objects. Consequently, they frequently appear in the development of statistical theory and inferential methodology, and have recently become an object of inference in themselves. In this review, we provide a snapshot of the main concepts involved in Wasserstein distances and optimal transportation, and a succinct overview of some of their many statistical aspects.},
  archiveprefix = {arXiv},
  keywords = {62-00 (primary); 62G99; 62M99 (secondary),deformation map,done,empirical optimal transport,FrÃ©chet mean,goodness-of-fit,inference,Kantorovich distance,Mongeâ€“Kantorovich problem,optimal coupling,probability metric,Statistics - Methodology,transportation of measure,warping and registration,Wasserstein distance,Wasserstein space},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Panaretos_Zemel_2019_Statistical_Aspects_of_Wasserstein_Distances.pdf;/home/dede/Zotero/storage/F6FQW63G/1806.html}
}

@book{paolella2007,
  title = {{Intermediate Probability: A Computational Approach}},
  shorttitle = {{Intermediate Probability}},
  author = {Paolella, Marc S.},
  year = {2007},
  publisher = {{Wiley-Interscience}},
  abstract = {Intermediate Probability is the natural extension of the author's Fundamental Probability. It details several highly important topics, from standard ones such as order statistics, multivariate normal, and convergence concepts, to more advanced ones which are usually not addressed at this mathematical level, or have never previously appeared in textbook form. The author adopts a computational approach throughout, allowing the reader to directly implement the methods, thus greatly enhancing the learning experience and clearly illustrating the applicability, strengths, and weaknesses of the theory. The book:  Places great emphasis on the numeric computation of convolutions of random variables, via numeric integration, inversion theorems, fast Fourier transforms, saddlepoint approximations, and simulation. Provides introductory material to required mathematical topics such as complex numbers, Laplace and Fourier transforms, matrix algebra, confluent hypergeometric functions, digamma functions, and Bessel functions. Presents full derivation and numerous computational methods of the stable Paretian and the singly and doubly non-central distributions. A whole chapter is dedicated to mean-variance mixtures, NIG, GIG, generalized hyperbolic and numerous related distributions. A whole chapter is dedicated to nesting, generalizing, and asymmetric extensions of popular distributions, as have become popular in empirical finance and other applications. Provides all essential programming code in Matlab and R.  The user-friendly style of writing and attention to detail means that self-study is easily possible, making the book ideal for senior undergraduate and graduate students of mathematics, statistics, econometrics, finance, insurance, and computer science, as well as researchers and professional statisticians working in these fields.},
  isbn = {978-0-470-02637-3},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Paolella_2007_Intermediate_Probability.pdf}
}

@article{paparoditis2001,
  title = {Tapered {{Block Bootstrap}}},
  author = {Paparoditis, Efstathios and Politis, Dimitris N.},
  year = {2001},
  journal = {Biometrika},
  volume = {88},
  number = {4},
  pages = {1105--1119},
  publisher = {{[Oxford University Press, Biometrika Trust]}},
  issn = {0006-3444},
  abstract = {We introduce and study tapered block bootstrap methodology that yields an improvement over the well-known block bootstrap for time series of Kunsch (1989). The asymptotic validity and the favourable bias properties of the tapered block bootstrap are shown. The important practical issues of optimally choosing the window shape and the block size are addressed in detail, while some finite-sample simulations are presented validating the good performance of the tapered block bootstrap.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Paparoditis_Politis_2001_Tapered_Block_Bootstrap.pdf}
}

@article{park2008,
  title = {The {{Bayesian Lasso}}},
  author = {Park, Trevor and Casella, George},
  year = {2008},
  month = jun,
  journal = {Journal of the American Statistical Association},
  volume = {103},
  number = {482},
  pages = {681--686},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1198/016214508000000337},
  abstract = {The Lasso estimate for linear regression parameters can be interpreted as a Bayesian posterior mode estimate when the regression parameters have independent Laplace (i.e., double-exponential) priors. Gibbs sampling from this posterior is possible using an expanded hierarchy with conjugate normal priors for the regression parameters and independent exponential priors on their variances. A connection with the inverse-Gaussian distribution provides tractable full conditional distributions. The Bayesian Lasso provides interval estimates (Bayesian credible intervals) that can guide variable selection. Moreover, the structure of the hierarchical model provides both Bayesian and likelihood methods for selecting the Lasso parameter. Slight modifications lead to Bayesian versions of other Lasso-related estimation methods, including bridge regression and a robust variant.},
  keywords = {Empirical Bayes,Gibbs sampler,Hierarchical model,Inverse Gaussian,Linear regression,Penalized regression,Scale mixture of normals,todo},
  annotation = {\_eprint: https://doi.org/10.1198/016214508000000337},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Park_Casella_2008_The_Bayesian_Lasso.pdf;/home/dede/Zotero/storage/3DVULC95/016214508000000337.html}
}

@article{parker2006,
  title = {Unit Root Testing via the Stationary Bootstrap},
  author = {Parker, Cameron and Paparoditis, Efstathios and Politis, Dimitris N.},
  year = {2006},
  month = aug,
  journal = {Journal of Econometrics},
  volume = {133},
  number = {2},
  pages = {601--638},
  issn = {0304-4076},
  doi = {10.1016/j.jeconom.2005.06.008},
  abstract = {A nonparametric, residual-based stationary bootstrap procedure is proposed for unit root testing in a time series. The procedure generates a pseudoseries which mimics the original, but ensures the presence of a unit root. Unlike many others in the literature, the proposed test is valid for a wide class of weakly dependent processes and is not based on parametric assumptions on the data-generating process. Large sample theory is developed and asymptotic validity is shown via a bootstrap functional central limit theorem. The case of a least squares statistic is discussed in detail, including simulations to investigate the procedure's finite sample performance.},
  langid = {english},
  keywords = {Autocorrelation,Integrated time series,Resampling,Stationary bootstrap,Unit root testing},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Parker_et_al_2006_Unit_root_testing_via_the_stationary_bootstrap.pdf;/home/dede/Zotero/storage/DGAX835D/S0304407605001284.html}
}

@book{parsi2021,
  title = {{The Wrecking of the Liberal World Order: La Fine Dell'ordine Liberale Internazionale}},
  shorttitle = {{The Wrecking of the Liberal World Order}},
  author = {Parsi, Vittorio Emanuele},
  translator = {Parsi, Malvina},
  year = {2021},
  edition = {1st ed. 2021 edizione},
  publisher = {{Palgrave Macmillan}},
  address = {{Cham, Switzerland}},
  isbn = {978-3-030-72042-1},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Parsi_2021_The_Wrecking_of_the_Liberal_World_Order.pdf}
}

@article{parzen1962,
  title = {On {{Estimation}} of a {{Probability Density Function}} and {{Mode}}},
  author = {Parzen, Emanuel},
  year = {1962},
  month = sep,
  journal = {The Annals of Mathematical Statistics},
  volume = {33},
  number = {3},
  pages = {1065--1076},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177704472},
  abstract = {The Annals of Mathematical Statistics},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Parzen_1962_On_Estimation_of_a_Probability_Density_Function_and_Mode.pdf;/home/dede/Zotero/storage/KCYUX7NS/1177704472.html}
}

@book{passingham2016,
  title = {Cognitive {{Neuroscience}}: {{A Very Short Introduction}}},
  shorttitle = {Cognitive {{Neuroscience}}},
  author = {Passingham, Richard E.},
  year = {2016},
  series = {Very Short Introductions 489},
  edition = {First edition},
  publisher = {{Oxford University Press}},
  isbn = {978-0-19-878622-1},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Passingham_2016_Cognitive_Neuroscience.epub}
}

@article{patton2009,
  title = {Correction to ``{{Automatic Block-Length Selection}} for the {{Dependent Bootstrap}}'' by {{D}}. {{Politis}} and {{H}}. {{White}}},
  author = {Patton, Andrew and Politis, Dimitris N. and White, Halbert},
  year = {2009},
  journal = {Econometric Reviews},
  volume = {28},
  number = {4},
  pages = {372--375},
  publisher = {{Taylor \& Francis}},
  issn = {0747-4938},
  doi = {10.1080/07474930802459016},
  abstract = {A correction on the optimal block size algorithms of Politis and White (2004) is given following a correction of Lahiri's (Lahiri 1999) theoretical results by Nordman (2008).},
  keywords = {Block bootstrap,Block size,C14,C22,Circular bootstrap,Stationary bootstrap},
  annotation = {\_eprint: https://doi.org/10.1080/07474930802459016},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Patton_et_al_2009_Correction_to_â€œAutomatic_Block-Length_Selection_for_the_Dependent_Bootstrapâ€_by.pdf;/home/dede/Zotero/storage/LRSAFXWG/07474930802459016.html}
}

@article{paul2021,
  title = {Globalization, Deglobalization and Reglobalization: Adapting Liberal International Order},
  shorttitle = {Globalization, Deglobalization and Reglobalization},
  author = {Paul, T V},
  year = {2021},
  month = sep,
  journal = {International Affairs},
  volume = {97},
  number = {5},
  pages = {1599--1620},
  issn = {0020-5850},
  doi = {10.1093/ia/iiab072},
  abstract = {Liberalism has been the most successful political ideology during the past two centuries in withstanding challenges and adapting to new environments. The liberal international order, set up after the Second World War and strengthened at the end of the Cold War, is going through a series of crises, propelled by deglobalization pressures, and the rise of illiberal and populist leaders, all challenging the three pillars of the liberal order: democracy, economic interdependence and international institutions. Two critical reasons for the decline of the liberal order are internal in terms of income distribution and institutional malaise. The article argues that the demise of the liberal order is not inevitable provided liberal states take remedial measures and adapt to the new environment as they did in 1919, 1930s, the second half of the 1940s, 1960s and 1991. Reformed globalization, or re-globalization is essential for facing the geopolitical challenges emanating from China and other illiberal states. The inability of other systems to offer both prosperity and freedom that the liberal order can provide is its main attractiveness. The connection between internal reforms in liberal states to address deepening inequalities and wealth distribution, a by-product of intensified globalization, and the prospects of liberal order's success is highlighted. The need for a refined welfare state taking into account the new realities to tackle the internal challenges is proposed.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Paul_2021_Globalization,_deglobalization_and_reglobalization.pdf}
}

@article{paynabar2016,
  title = {A {{Change-Point Approach}} for {{Phase-I Analysis}} in {{Multivariate Profile Monitoring}} and {{Diagnosis}}},
  author = {Paynabar, Kamran and Zou, Changliang and Qiu, Peihua},
  year = {2016},
  month = apr,
  journal = {Technometrics},
  volume = {58},
  number = {2},
  pages = {191--204},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.2015.1042168},
  abstract = {Process monitoring and fault diagnosis using profile data remains an important and challenging problem in statistical process control (SPC). Although the analysis of profile data has been extensively studied in the SPC literature, the challenges associated with monitoring and diagnosis of multichannel (multiple) nonlinear profiles are yet to be addressed. Motivated by an application in multioperation forging processes, we propose a new modeling, monitoring, and diagnosis framework for phase-I analysis of multichannel profiles. The proposed framework is developed under the assumption that different profile channels have similar structure so that we can gain strength by borrowing information from all channels. The multidimensional functional principal component analysis is incorporated into change-point models to construct monitoring statistics. Simulation results show that the proposed approach has good performance in identifying change-points in various situations compared with some existing methods. The codes for implementing the proposed procedure are available in the supplementary material.},
  keywords = {Functional data analysis,Functional principal component analysis,Multichannel profiles,Nonlinear profile,Statistical process control,toRead},
  annotation = {\_eprint: https://doi.org/10.1080/00401706.2015.1042168},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Paynabar_et_al_2016_A_Change-Point_Approach_for_Phase-I_Analysis_in_Multivariate_Profile_Monitoring.pdf;/home/dede/Zotero/storage/X884KK7K/00401706.2015.html}
}

@article{pazhayamadom2013a,
  title = {Self-Starting {{CUSUM}} Approach for Monitoring Data Poor Fisheries},
  author = {Pazhayamadom, Deepak and Kelly, Ciaran and Rogan, Emer and Codling, Edward},
  year = {2013},
  month = aug,
  journal = {Fisheries Research},
  volume = {145},
  pages = {114--127},
  doi = {10.1016/j.fishres.2013.02.002},
  abstract = {This study attempts to determine whether a fish stock can be monitored and assessed if no historical fisheries data are available. Many existing methods require a time series of population and fishing pressure observations to estimate reference points to trigger decision rules. We demonstrate here the self-starting cumulative sum control chart (SS-CUSUM) where reference points are calibrated from indicator observations sequentially in real time as they are monitored. We used SS-CUSUM to monitor catch-based indicators from a simulated fishery where no previous scientific data are available. In the scenarios considered, the SS-CUSUM was successful in producing responses to fishing impacts with all indicators. A qualitative assessment on performance measures showed that the method worked best with indicators that represented the large fish component in landed catches (large fish indicators). Our study implies that neither a reference point nor a formal fish stock assessment is necessarily required to detect the impact of fishing on stock biomass. We discuss how SS-CUSUM could be incorporated into the assessment process for data poor fisheries.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Pazhayamadom_et_al_2013_Self-starting_CUSUM_approach_for_monitoring_data_poor_fisheries.pdf}
}

@article{pazhayamadom2016,
  title = {Self-Starting Cumulative Sum Harvest Control Rule ({{SS-CUSUM-HCR}}) for Status-Quo Management of Data-Limited Fisheries},
  author = {Pazhayamadom, Deepak George and Kelly, Ciar{\'a}n J. and Rogan, Emer and Codling, Edward A.},
  year = {2016},
  month = mar,
  journal = {Canadian Journal of Fisheries and Aquatic Sciences},
  volume = {73},
  number = {3},
  pages = {366--381},
  publisher = {{NRC Research Press}},
  issn = {0706-652X},
  doi = {10.1139/cjfas-2015-0039},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Pazhayamadom_et_al_2016_Self-starting_cumulative_sum_harvest_control_rule_(SS-CUSUM-HCR)_for_status-quo.pdf;/home/dede/Documents/MEGA/universita/zotero-pdf/Pazhayamadom_et_al_2016_Self-starting_cumulative_sum_harvest_control_rule_(SS-CUSUM-HCR)_for_status-quo2.pdf}
}

@article{peano1890,
  title = {{Sur une courbe, qui remplit toute une aire plane}},
  author = {Peano, G.},
  year = {1890},
  month = mar,
  journal = {Mathematische Annalen},
  volume = {36},
  number = {1},
  pages = {157--160},
  issn = {0025-5831, 1432-1807},
  doi = {10.1007/BF01199438},
  langid = {french},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Peano_1890_Sur_une_courbe,_qui_remplit_toute_une_aire_plane.pdf}
}

@article{pearson2019,
  title = {Extremism and Toxic Masculinity: The Man Question Re-Posed},
  shorttitle = {Extremism and Toxic Masculinity},
  author = {Pearson, Elizabeth},
  year = {2019},
  month = nov,
  journal = {International Affairs},
  volume = {95},
  number = {6},
  pages = {1251--1270},
  issn = {0020-5850},
  doi = {10.1093/ia/iiz177},
  abstract = {It is more than 20 years since Marysia Zalewski and feminist scholars posed `the man question' in International Relations, repositioning the gaze from female subjectivities to a problematization of the subjecthood of man. The field of masculinity studies has developed this initial question to a deep interrogation of the relationship between maleness and violence. Yet public and policy discourse often reduce the complexity of masculinities within extremism to issues of crisis and toxicity. Governments have prioritized the prevention of extremism, particularly violent Islamism, and in so doing have produced as `risk' particular racialized and marginalized men. This article asks, what are the effects of the toxic masculinity discourse in understanding the British radical right? It argues that current understandings of extremism neglect the central aim of Zalewski's `man' question to destabilize the field and deconstruct patriarchy. They instead position Islamophobia\textemdash which is institutionalized in state discourse\textemdash as the responsibility of particular `extreme' and `toxic' groups. In particular, the article outlines two ways in which `toxic masculinity' is an inadequate concept to describe activism in the anti-Islam(ist) movement the English Defence League (EDL). First, the term `toxic masculinity' occludes the continuities of EDL masculinities with wider patriarchal norms; second, it neglects the role of women as significant actors in the movement. Using an ethnographic and empathetic approach to this case-study, the article explores how Zalewski's theoretical position offers a route to analysis of the ways in which masculinities and patriarchy entwine in producing power and violence; and to a discussion of masculinities that need not equate manhood with threat.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Pearson_2019_Extremism_and_toxic_masculinity.pdf;/home/dede/Zotero/storage/AXV74XNA/5613459.html}
}

@article{perdikis2019,
  title = {A Survey on Multivariate Adaptive Control Charts: {{Recent}} Developments and Extensions},
  shorttitle = {A Survey on Multivariate Adaptive Control Charts},
  author = {Perdikis, Theodoros and Psarakis, Stelios},
  year = {2019},
  journal = {Quality and Reliability Engineering International},
  volume = {35},
  number = {5},
  pages = {1342--1362},
  issn = {1099-1638},
  doi = {10.1002/qre.2521},
  abstract = {In the world of business, quality improvement is of high importance for the manufacturing industries. Statistical process control via control charts provides an online monitoring of the product's characteristic. The adaptive feature is being widely used in the design parameters of a control chart, which allows at least one of them to change during the process monitoring. Specifically, a control chart is considered adaptive if at least one of the chart's parameters (sample size, sampling interval, or control limit coefficient) is allowed to change in real time on the basis of the actual values of the sample statistics. In this paper, recent developments in the design of multivariate adaptive control schemes are presented and discussed.},
  langid = {english},
  keywords = {adaptive control charts,done,multivariate control charts,statistical process monitoring},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/qre.2521},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Perdikis_Psarakis_2019_A_survey_on_multivariate_adaptive_control_charts.pdf;/home/dede/Zotero/storage/WUV5C55H/qre.html}
}

@article{peres2018,
  title = {Variable Selection Methods in Multivariate Statistical Process Control: {{A}} Systematic Literature Review},
  shorttitle = {Variable Selection Methods in Multivariate Statistical Process Control},
  author = {Peres, Fernanda Araujo Pimentel and Fogliatto, F. S.},
  year = {2018},
  journal = {Comput. Ind. Eng.},
  doi = {10.1016/j.cie.2017.12.006},
  abstract = {The current state-of-the-art of the integration of VS in multivariate statistical process control (MSPC) methods was presented, with five topics for future research proposed, exploring the opportunities identified in the literature. Abstract Technological advances led to increasingly larger industrial quality-related datasets calling for process monitoring methods able to handle them. In such context, the application of variable selection (VS) in quality control methods emerges as a promising research topic. This review aims at presenting the current state-of-the-art of the integration of VS in multivariate statistical process control (MSPC) methods. Proposals aligned with the objective were identified, classified according to VS approach, and briefly presented. Research on the topic has considerably increased in the past five years. Thirty methods were identified and categorized in 10 clusters, according to the objective of improvement in MSPC and the step of process monitoring they were aimed to improve. The majority of the propositions were either targeted at exclusively monitoring potential out-of-control variables or improving the monitoring of in-control variables. MSPC improvements were centered in principal component analysis (PCA) projection methods, while VS was mainly carried out using the Least Absolute Shrinkage and Selection Operator (LASSO) method and genetic algorithms. Fault isolation was the most addressed step in process monitoring. We close the paper proposing five topics for future research, exploring the opportunities identified in the literature.},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Peres_Fogliatto_2018_Variable_selection_methods_in_multivariate_statistical_process_control.pdf}
}

@book{petris2009,
  title = {{Dynamic Linear Models with R}},
  author = {Petris, Giovanni and Petrone, Sonia and Campagnoli, Patrizia},
  year = {2009},
  month = jun,
  edition = {2009 ed. edizione},
  publisher = {{Springer}},
  address = {{Dordrecht ; New York}},
  abstract = {This text introduces general state space models in detail before focusing on dynamic linear models, emphasizing their Bayesian analysis. It illustrates all the fundamental steps needed to use dynamic linear models in practice, using R.},
  isbn = {978-0-387-77237-0},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Petris_et_al_2009_Dynamic_Linear_Models_with_R.pdf}
}

@article{petzold2004,
  title = {Surveillance in {{Longitudinal Models}}: {{Detection}} of {{Intrauterine Growth Restriction}}},
  shorttitle = {Surveillance in {{Longitudinal Models}}},
  author = {Petzold, Max and Sonesson, Christian and Bergman, Eva and Kieler, Helle},
  year = {2004},
  journal = {Biometrics},
  volume = {60},
  number = {4},
  pages = {1025--1033},
  issn = {1541-0420},
  doi = {10.1111/j.0006-341X.2004.00258.x},
  abstract = {A new methodology for online detection of intrauterine growth restriction (IUGR) is proposed where traditional methods for statistical surveillance are applied. Here, deficient growth rate is used to detect IUGR instead of the common surrogate measure ``small for gestational age'' (SGA). Fetal growth is estimated by repeated measurements of symphysis-fundus (SF) height. At each time point the new method, based on the Shiryaev\textendash Roberts method, is used to evaluate the growth in SF height. We use Swedish data to model a normal growth pattern, which is used to evaluate the capability of the new method to detect IUGR in comparison with a method used in practice today. Results from simulations indicate that the new method performs considerably better than the method used today. We also illustrate the effect of some important factors which influence the detection ability and illuminate the tendency of the method used today to misclassify SGA cases as IUGR.},
  langid = {english},
  keywords = {Growth,IUGR,Monitoring,Shiryaevâ€“Roberts method,Surveillance,Symphysis-fundus,todo},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.0006-341X.2004.00258.x},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Petzold_et_al_2004_Surveillance_in_Longitudinal_Models.pdf;/home/dede/Zotero/storage/VXDH7UES/j.0006-341X.2004.00258.html}
}

@article{pewsey2020,
  title = {Recent Advances in Directional Statistics},
  author = {Pewsey, Arthur and {Garc{\'i}a-Portugu{\'e}s}, Eduardo},
  year = {2020},
  month = may,
  journal = {arXiv:2005.06889 [stat]},
  eprint = {2005.06889},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Mainstream statistical methodology is generally applicable to data observed in Euclidean space. There are, however, numerous contexts of considerable scientific interest in which the natural supports for the data under consideration are Riemannian manifolds like the unit circle, torus, sphere and their extensions. Typically, such data can be represented using one or more directions, and directional statistics is the branch of statistics that deals with their analysis. In this paper we provide a review of the many recent developments in the field since the publication of Mardia and Jupp (1999), still the most comprehensive text on directional statistics. Many of those developments have been stimulated by interesting applications in fields as diverse as astronomy, medicine, genetics, neurology, aeronautics, acoustics, image analysis, text mining, environmetrics, and machine learning. We begin by considering developments for the exploratory analysis of directional data before progressing to distributional models, general approaches to inference, hypothesis testing, regression, nonparametric curve estimation, methods for dimension reduction, classification and clustering, and the modelling of time series, spatial, and spatio-temporal data. We end with an overview of presently available software for analyzing directional data.},
  archiveprefix = {arXiv},
  keywords = {62H11,Statistics - Methodology,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Pewsey_GarcÃ­a-PortuguÃ©s_2020_Recent_advances_in_directional_statistics.pdf;/home/dede/Zotero/storage/Y58JHDMA/2005.html}
}

@article{peyre2020,
  title = {Computational {{Optimal Transport}}},
  author = {Peyr{\'e}, Gabriel and Cuturi, Marco},
  year = {2020},
  month = mar,
  journal = {arXiv:1803.00567 [stat]},
  eprint = {1803.00567},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Optimal transport (OT) theory can be informally described using the words of the French mathematician Gaspard Monge (1746-1818): A worker with a shovel in hand has to move a large pile of sand lying on a construction site. The goal of the worker is to erect with all that sand a target pile with a prescribed shape (for example, that of a giant sand castle). Naturally, the worker wishes to minimize her total effort, quantified for instance as the total distance or time spent carrying shovelfuls of sand. Mathematicians interested in OT cast that problem as that of comparing two probability distributions, two different piles of sand of the same volume. They consider all of the many possible ways to morph, transport or reshape the first pile into the second, and associate a "global" cost to every such transport, using the "local" consideration of how much it costs to move a grain of sand from one place to another. Recent years have witnessed the spread of OT in several fields, thanks to the emergence of approximate solvers that can scale to sizes and dimensions that are relevant to data sciences. Thanks to this newfound scalability, OT is being increasingly used to unlock various problems in imaging sciences (such as color or texture processing), computer vision and graphics (for shape manipulation) or machine learning (for regression, classification and density fitting). This short book reviews OT with a bias toward numerical methods and their applications in data sciences, and sheds lights on the theoretical properties of OT that make it particularly useful for some of these applications.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Machine Learning,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/PeyrÃ©_Cuturi_2020_Computational_Optimal_Transport.pdf}
}

@article{pfeffermann1993,
  title = {The {{Role}} of {{Sampling Weights When Modeling Survey Data}}},
  author = {Pfeffermann, Danny},
  year = {1993},
  journal = {International Statistical Review},
  volume = {61},
  number = {2},
  pages = {317--337},
  publisher = {{[Wiley, International Statistical Institute (ISI)]}},
  issn = {0306-7734},
  doi = {10.2307/1403631},
  abstract = {The purpose of this paper is to provide a critical survey of the literature, directed at answering two main questions. i) Can the use of the sampling weights be justified for analytic inference about model parameters and if so, under what circumstances? ii) Can guidelines be developed for how to incorporate the weights in the analysis? The general conclusion of this study is that the weights can be used to test and protect against informative sampling designs and against misspecification of the model holding in the population. Six approaches for incorporating the weights in the inference process are considered. The first four approaches are intended to yield design consistent estimators for corresponding descriptive population quantities of the model parameters. The other two approaches attempt to incorporate the weights into the model. /// Le but de cet expos\'e est de fournir un examen critique des recherches, pour r\'espondre \`a deux questions: (i) Est-ce que l'emploi des poids de sondage peut \^etre justifi\'e pour l'inf\'erence analytique sur les param\`etres d'un mod\`ele et, dans ce cas, dans quelles circonstances? (ii) Peut-on d\'eveloper des lignes directrices pour l'introduction des poids dans l'analyse? La conclusion g\'en\'erale de cette \'etude est que les poids peuvent \^etre utilis\'es pour tester et pour prot\'eger contre des plans de sondage informatifs et contre la sp\'ecification fausse du mod\`ele de la population. On condi\`ere six approaches differentes pour introduire les poids dans le processus d'inf\'erence. Les quatre premi\`eres ont pour but de produire des estimateurs qui sont consistants selon le plan de sondage pour les quantit\'es d\'escriptives de la population qui correspondent aux param\`etres du mod\`ele. Les deux autres approches essaient d'introduire les poids dans le mod\`ele.},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Pfeffermann_1993_The_Role_of_Sampling_Weights_When_Modeling_Survey_Data.pdf}
}

@article{pierce1982,
  title = {The {{Asymptotic Effect}} of {{Substituting Estimators}} for {{Parameters}} in {{Certain Types}} of {{Statistics}}},
  author = {Pierce, Donald A.},
  year = {1982},
  journal = {The Annals of Statistics},
  volume = {10},
  number = {2},
  pages = {475--478},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/aos/1176345788},
  abstract = {In a variety of statistical problems, one is interested in the limiting distribution of statistics \$\textbackslash hat\{T\}\_n = T\_n(y\_1, y\_2, \textbackslash cdots, y\_n; \textbackslash hat\{\textbackslash lambda\}\_n)\$, where \$\textbackslash hat\{\textbackslash lambda\}\_n\$ is an estimator of a parameter in the distribution of the \$y\_i\$ and where the limiting distribution of \$T\_n = T\_n(y\_1, y\_2, \textbackslash cdots, y\_n; \textbackslash lambda)\$ is relatively easy to find. For cases in which the limiting distribution of \$T\_n\$ is normal with mean independent of \$\textbackslash lambda\$, a useful method is given for finding the limiting distribution of \$\textbackslash hat\{T\}\_n\$. A simple application to testing normality in regression models is given.},
  keywords = {62E20,62F05,Asymptotic distributions,Goodness-of-fit tests,nuisance parameters,reference only,residuals},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Pierce_1982_The_Asymptotic_Effect_of_Substituting_Estimators_for_Parameters_in_Certain.pdf;/home/dede/Zotero/storage/W72XCGMU/1176345788.html}
}

@article{piironen2017,
  title = {Sparsity Information and Regularization in the Horseshoe and Other Shrinkage Priors},
  author = {Piironen, Juho and Vehtari, Aki},
  year = {2017},
  journal = {Electronic Journal of Statistics},
  volume = {11},
  number = {2},
  eprint = {1707.01694},
  eprinttype = {arxiv},
  pages = {5018--5051},
  issn = {1935-7524},
  doi = {10.1214/17-EJS1337SI},
  abstract = {The horseshoe prior has proven to be a noteworthy alternative for sparse Bayesian estimation, but has previously suffered from two problems. First, there has been no systematic way of specifying a prior for the global shrinkage hyperparameter based on the prior information about the degree of sparsity in the parameter vector. Second, the horseshoe prior has the undesired property that there is no possibility of specifying separately information about sparsity and the amount of regularization for the largest coefficients, which can be problematic with weakly identified parameters, such as the logistic regression coefficients in the case of data separation. This paper proposes solutions to both of these problems. We introduce a concept of effective number of nonzero parameters, show an intuitive way of formulating the prior for the global hyperparameter based on the sparsity assumptions, and argue that the previous default choices are dubious based on their tendency to favor solutions with more unshrunk parameters than we typically expect a priori. Moreover, we introduce a generalization to the horseshoe prior, called the regularized horseshoe, that allows us to specify a minimum level of regularization to the largest values. We show that the new prior can be considered as the continuous counterpart of the spike-and-slab prior with a finite slab width, whereas the original horseshoe resembles the spike-and-slab with an infinitely wide slab. Numerical experiments on synthetic and real world data illustrate the benefit of both of these theoretical advances.},
  archiveprefix = {arXiv},
  keywords = {done,Statistics - Methodology},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Piironen_Vehtari_2017_Sparsity_information_and_regularization_in_the_horseshoe_and_other_shrinkage.pdf;/home/dede/Zotero/storage/UL8WLWAM/1707.html}
}

@article{piironen2020,
  title = {Projective {{Inference}} in {{High-dimensional Problems}}: {{Prediction}} and {{Feature Selection}}},
  shorttitle = {Projective {{Inference}} in {{High-dimensional Problems}}},
  author = {Piironen, Juho and Paasiniemi, Markus and Vehtari, Aki},
  year = {2020},
  month = jan,
  journal = {Electronic Journal of Statistics},
  volume = {14},
  number = {1},
  eprint = {1810.02406},
  eprinttype = {arxiv},
  issn = {1935-7524},
  doi = {10.1214/20-EJS1711},
  abstract = {This paper discusses predictive inference and feature selection for generalized linear models with scarce but high-dimensional data. We argue that in many cases one can benefit from a decision theoretically justified two-stage approach: first, construct a possibly non-sparse model that predicts well, and then find a minimal subset of features that characterize the predictions. The model built in the first step is referred to as the \textbackslash emph\{reference model\} and the operation during the latter step as predictive \textbackslash emph\{projection\}. The key characteristic of this approach is that it finds an excellent tradeoff between sparsity and predictive accuracy, and the gain comes from utilizing all available information including prior and that coming from the left out features. We review several methods that follow this principle and provide novel methodological contributions. We present a new projection technique that unifies two existing techniques and is both accurate and fast to compute. We also propose a way of evaluating the feature selection process using fast leave-one-out cross-validation that allows for easy and intuitive model size selection. Furthermore, we prove a theorem that helps to understand the conditions under which the projective approach could be beneficial. The benefits are illustrated via several simulated and real world examples.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Piironen_et_al_2020_Projective_Inference_in_High-dimensional_Problems.pdf;/home/dede/Zotero/storage/9UZPKAB4/1810.html}
}

@book{pinheiro2009,
  title = {{Mixed-Effects Models in S and S-Plus}},
  author = {Pinheiro, Jose C. and Bates, Douglas M.},
  year = {2009},
  month = apr,
  edition = {2000. 2nd Print edizione},
  publisher = {{Springer Nature}},
  address = {{New York Berlin Heidelberg}},
  abstract = {An overview of the theory and application of linear and nonlinear mixed-effects models in the analysis of grouped data, such as longitudinal data, repeated measures, and multilevel data. The authors present a unified model-building strategy for both models and apply this to the analysis of over 20 real datasets from a wide variety of areas, including pharmacokinetics, agriculture, and manufacturing. Much emphasis is placed on the use of graphical displays at the various phases of the model-building process, starting with exploratory plots of the data and concluding with diagnostic plots to assess the adequacy of a fitted model. The NLME library for analyzing mixed-effects models in S and S-PLUS, developed by the authors, provides the underlying software for implementing the methods presented. This balanced mix of real data examples, modeling software, and theory makes the book a useful reference for practitioners who use, or intend to use, mixed-effects models in their data analyses. It can also be used as a text for a one-semester graduate-level applied course.},
  isbn = {978-1-4419-0317-4},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Pinheiro_Bates_2009_Mixed-Effects_Models_in_S_and_S-Plus.pdf}
}

@article{pitman1997,
  title = {The Two-Parameter {{Poisson-Dirichlet}} Distribution Derived from a Stable Subordinator},
  author = {Pitman, Jim and Yor, Marc},
  year = {1997},
  month = apr,
  journal = {The Annals of Probability},
  volume = {25},
  number = {2},
  pages = {855--900},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0091-1798, 2168-894X},
  doi = {10.1214/aop/1024404422},
  abstract = {The two-parameter Poisson-Dirichlet distribution, denoted \$\textbackslash mathsf\{PD\}(\textbackslash alpha, \textbackslash theta)\$ is a probability distribution on the set of decreasing positive sequences with sum 1. The usual Poisson-Dirichlet distribution with a single parameter \$\textbackslash theta\$, introduced by Kingman, is \$\textbackslash mathsf\{PD\}(0, \textbackslash theta)\$. Known properties of \$\textbackslash mathsf\{PD\}(0, \textbackslash theta)\$, including the Markov chain description due to Vershik, Shmidt and Ignatov, are generalized to the two-parameter case. The size-biased random permutation of \$\textbackslash mathsf\{PD\}(\textbackslash alpha, \textbackslash theta)\$ is a simple residual allocation model proposed by Engen in the context of species diversity, and rediscovered by Perman and the authors in the study of excursions of Brownian motion and Bessel processes. For \$0 {$<$} \textbackslash alpha {$<$} 1, \textbackslash mathsf\{PD\}(\textbackslash alpha, 0)\$ is the asymptotic distribution of ranked lengths of excursions of a Markov chain away from a state whose recurrence time distribution is in the domain of attraction of a stable law of index \$\textbackslash alpha\$. Formulae in this case trace back to work of Darling, Lamperti and Wendel in the 1950s and 1960s. The distribution of ranked lengths of excursions of a one-dimensional Brownian motion is \$\textbackslash mathsf\{PD\}(1/2, 0)\$, and the corresponding distribution for a Brownian bredge is \$\textbackslash mathsf\{PD\}(1/2, 1/2)\$. The \$\textbackslash mathsf\{PD\}(\textbackslash alpha, 0)\$ and \$\textbackslash mathsf\{PD\}(\textbackslash alpha, \textbackslash alpha)\$ distributions admit a similar interpretation in terms of the ranked lengths of excursions of a semistable Markov process whose zero set is the range of a stable subordinator of index \$\textbackslash alpha\$.},
  keywords = {60E99,60G57,60J30,Local time,Poisson point process,ranked lengths of excursions,semistable Markov process,Zero set},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Pitman_Yor_1997_The_two-parameter_Poisson-Dirichlet_distribution_derived_from_a_stable.pdf;/home/dede/Zotero/storage/PGKVASIY/1024404422.html}
}

@book{poldrack2011,
  title = {{Handbook of Functional MRI Data Analysis}},
  author = {Poldrack, Russell A. and Mumford, Jeanette A. and Nichols, Thomas E.},
  year = {2011},
  month = aug,
  publisher = {{Cambridge University Press}},
  abstract = {Functional magnetic resonance imaging (fMRI) has become the most popular method for imaging brain function. Handbook of Functional MRI Data Analysis provides a comprehensive and practical introduction to the methods used for fMRI data analysis. Using minimal jargon, this book explains the concepts behind processing fMRI data, focusing on the techniques that are most commonly used in the field. This book provides background about the methods employed by common data analysis packages including FSL, SPM and AFNI. Some of the newest cutting-edge techniques, including pattern classification analysis, connectivity modeling and resting state network analysis, are also discussed. Readers of this book, whether newcomers to the field or experienced researchers, will obtain a deep and effective knowledge of how to employ fMRI analysis to ask scientific questions and become more sophisticated users of fMRI analysis software.},
  isbn = {978-0-521-51766-9},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Poldrack_et_al_2011_Handbook_of_Functional_MRI_Data_Analysis.pdf}
}

@article{politis1994,
  title = {The {{Stationary Bootstrap}}},
  author = {Politis, Dimitris N. and Romano, Joseph P.},
  year = {1994},
  journal = {Journal of the American Statistical Association},
  volume = {89},
  number = {428},
  pages = {1303--1313},
  publisher = {{[American Statistical Association, Taylor \& Francis, Ltd.]}},
  issn = {0162-1459},
  doi = {10.2307/2290993},
  abstract = {This article introduces a resampling procedure called the stationary bootstrap as a means of calculating standard errors of estimators and constructing confidence regions for parameters based on weakly dependent stationary observations. Previously, a technique based on resampling blocks of consecutive observations was introduced to construct confidence intervals for a parameter of the m-dimensional joint distribution of m consecutive observations, where m is fixed. This procedure has been generalized by constructing a "blocks of blocks" resampling scheme that yields asymptotically valid procedures even for a multivariate parameter of the whole (i.e., infinite-dimensional) joint distribution of the stationary sequence of observations. These methods share the construction of resampling blocks of observations to form a pseudo-time series, so that the statistic of interest may be recalculated based on the resampled data set. But in the context of applying this method to stationary data, it is natural to require the resampled pseudo-time series to be stationary (conditional on the original data) as well. Although the aforementioned procedures lack this property, the stationary procedure developed here is indeed stationary and possesses other desirable properties. The stationary procedure is based on resampling blocks of random length, where the length of each block has a geometric distribution. In this article, fundamental consistency and weak convergence properties of the stationary resampling scheme are developed.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Politis_Romano_1994_The_Stationary_Bootstrap.pdf}
}

@article{politis2004,
  title = {Automatic {{Block-Length Selection}} for the {{Dependent Bootstrap}}},
  author = {Politis, Dimitris N. and White, Halbert},
  year = {2004},
  journal = {Econometric Reviews},
  volume = {23},
  number = {1},
  pages = {53--70},
  publisher = {{Taylor \& Francis}},
  issn = {0747-4938},
  doi = {10.1081/ETC-120028836},
  abstract = {We review the different block bootstrap methods for time series, and present them in a unified framework. We then revisit a recent result of Lahiri [Lahiri, S. N. (1999b). Theoretical comparisons of block bootstrap methods, Ann. Statist. 27:386\textendash 404] comparing the different methods and give a corrected bound on their asymptotic relative efficiency; we also introduce a new notion of finite-sample ``attainable'' relative efficiency. Finally, based on the notion of spectral estimation via the flat-top lag-windows of Politis and Romano [Politis, D. N., Romano, J. P. (1995). Bias-corrected nonparametric spectral estimation. J. Time Series Anal. 16:67\textendash 103], we propose practically useful estimators of the optimal block size for the aforementioned block bootstrap methods. Our estimators are characterized by the fastest possible rate of convergence which is adaptive on the strength of the correlation of the time series as measured by the correlogram.},
  keywords = {Bandwidth choice,Block bootstrap,Resampling,Subsampling,Time series,Variance estimation},
  annotation = {\_eprint: https://doi.org/10.1081/ETC-120028836},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Politis_White_2004_Automatic_Block-Length_Selection_for_the_Dependent_Bootstrap.pdf;/home/dede/Zotero/storage/HB9TZ3SN/ETC-120028836.html}
}

@article{polson2012,
  title = {Local Shrinkage Rules, {{L\'evy}} Processes and Regularized Regression},
  author = {Polson, Nicholas G. and Scott, James G.},
  year = {2012},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {74},
  number = {2},
  pages = {287--311},
  issn = {1467-9868},
  doi = {10.1111/j.1467-9868.2011.01015.x},
  abstract = {Summary. We use L\'evy processes to generate joint prior distributions, and therefore penalty functions, for a location parameter as p grows large. This generalizes the class of local\textendash global shrinkage rules based on scale mixtures of normals, illuminates new connections between disparate methods and leads to new results for computing posterior means and modes under a wide class of priors. We extend this framework to large-scale regularized regression problems where p{$>$}n, and we provide comparisons with other methodologies.},
  langid = {english},
  keywords = {LÃ©vy processes,Normal scale mixtures,Partial least squares,Principal components regression,Shrinkage,Sparsity},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9868.2011.01015.x},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Polson_Scott_2012_Local_shrinkage_rules,_Levy_processes_and_regularized_regression.pdf;/home/dede/Zotero/storage/U2VSPA92/j.1467-9868.2011.01015.html}
}

@article{polson2013,
  title = {Bayesian Inference for Logistic Models Using {{Polya-Gamma}} Latent Variables},
  author = {Polson, Nicholas G. and Scott, James G. and Windle, Jesse},
  year = {2013},
  month = jul,
  journal = {arXiv:1205.0310 [stat]},
  eprint = {1205.0310},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {We propose a new data-augmentation strategy for fully Bayesian inference in models with binomial likelihoods. The approach appeals to a new class of Polya-Gamma distributions, which are constructed in detail. A variety of examples are presented to show the versatility of the method, including logistic regression, negative binomial regression, nonlinear mixed-effects models, and spatial models for count data. In each case, our data-augmentation strategy leads to simple, effective methods for posterior inference that: (1) circumvent the need for analytic approximations, numerical integration, or Metropolis-Hastings; and (2) outperform other known data-augmentation strategies, both in ease of use and in computational efficiency. All methods, including an efficient sampler for the Polya-Gamma distribution, are implemented in the R package BayesLogit. In the technical supplement appended to the end of the paper, we provide further details regarding the generation of Polya-Gamma random variables; the empirical benchmarks reported in the main manuscript; and the extension of the basic data-augmentation framework to contingency tables and multinomial outcomes.},
  archiveprefix = {arXiv},
  keywords = {bayes,data augmentation,latent variables,logistic regression,Polya-Gamma,Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Polson_et_al_2013_Bayesian_inference_for_logistic_models_using_Polya-Gamma_latent_variables.pdf;/home/dede/Zotero/storage/YFQE2T82/1205.html}
}

@article{polunchenko2012,
  title = {State-of-the-{{Art}} in {{Sequential Change-Point Detection}}},
  author = {Polunchenko, Aleksey S. and Tartakovsky, A.},
  year = {2012},
  doi = {10.1007/s11009-011-9256-5},
  abstract = {We provide an overview of the state-of-the-art in the area of sequential change-point detection assuming discrete time and known pre- and post-change distributions. The overview spans over all major formulations of the underlying optimization problem, namely, Bayesian, generalized Bayesian, and minimax. We pay particular attention to the latest advances in each. Also, we link together the generalized Bayesian problem with multi-cyclic disorder detection in a stationary regime when the change occurs at a distant time horizon. We conclude with two case studies to illustrate the cutting edge of the field at work.},
  keywords = {doing},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Polunchenko_Tartakovsky_2012_State-of-the-Art_in_Sequential_Change-Point_Detection.pdf}
}

@book{polya2004,
  title = {{How to Solve It: A New Aspect of Mathematical Method}},
  shorttitle = {{How to Solve It}},
  author = {Polya, George and Conway, John Horton},
  year = {2004},
  month = apr,
  publisher = {{Princeton Univ Pr}},
  address = {{Princeton N.J.}},
  isbn = {978-0-691-11966-3},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Polya_Conway_2004_How_to_Solve_It.epub}
}

@article{polyak1992,
  title = {Acceleration of Stochastic Approximation by Averaging},
  author = {Polyak, B. and Juditsky, A.},
  year = {1992},
  journal = {Siam Journal on Control and Optimization},
  doi = {10.1137/0330046},
  abstract = {A new recursive algorithm of stochastic approximation type with the averaging of trajectories is investigated. Convergence with probability one is proved for a variety of classical optimization and identification problems. It is also demonstrated for these problems that the proposed algorithm achieves the highest possible rate of convergence.},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Polyak_Juditsky_1992_Acceleration_of_stochastic_approximation_by_averaging.pdf}
}

@book{powell2011,
  title = {{Approximate Dynamic Programming: Solving the Curses of Dimensionality}},
  shorttitle = {{Approximate Dynamic Programming}},
  author = {Powell, Warren B.},
  year = {2011},
  month = nov,
  edition = {2\textdegree{} edizione},
  publisher = {{John Wiley \& Sons Inc}},
  address = {{Hoboken, N.J}},
  isbn = {978-0-470-60445-8},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Powell_2011_Approximate_Dynamic_Programming.pdf}
}

@book{prado2010,
  title = {{Time Series: Modeling, Computation, and Inference}},
  shorttitle = {{Time Series}},
  author = {Prado, Raquel and West, Mike},
  year = {2010},
  month = may,
  publisher = {{Chapman and Hall/CRC}},
  address = {{Boca Raton}},
  abstract = {Focusing on Bayesian approaches and computations using simulation-based methods for inference, Time Series: Modeling, Computation, and Inference integrates mainstream approaches for time series modeling with significant recent developments in methodology and applications of time series analysis. It encompasses a graduate-level account of Bayesian time series modeling and analysis, a broad range of references to state-of-the-art approaches to univariate and multivariate time series analysis, and emerging topics at research frontiers.  The book presents overviews of several classes of models and related methodology for inference, statistical computation for model fitting and assessment, and forecasting. The authors also explore the connections between time- and frequency-domain approaches and develop various models and analyses using Bayesian tools, such as Markov chain Monte Carlo (MCMC) and sequential Monte Carlo (SMC) methods. They illustrate the models and methods with examples and case studies from a variety of fields, including signal processing, biomedicine, and finance. Data sets, R and MATLAB\textregistered{} code, and other material are available on the authors' websites.  Along with core models and methods, this text offers sophisticated tools for analyzing challenging time series problems. It also demonstrates the growth of time series analysis into new application areas.},
  isbn = {978-1-4200-9336-0},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Prado_West_2010_Time_Series.pdf}
}

@incollection{prado2016,
  title = {Dynamic {{Bayesian Models}}: {{Inference}} and {{Forecasting}}},
  shorttitle = {Dynamic {{Bayesian Models}}},
  booktitle = {Wiley {{StatsRef}}: {{Statistics Reference Online}}},
  author = {Prado, Raquel},
  year = {2016},
  pages = {1--18},
  publisher = {{American Cancer Society}},
  doi = {10.1002/9781118445112.stat00219.pub2},
  abstract = {Bayesian forecasting encompasses statistical theory and methods in time-series analysis and time-series forecasting, particularly approaches using dynamic and state-space models, although the underlying concepts and theoretical foundation relate to probability modeling and inference more generally. This entry focuses specifically on Bayesian time-series modeling, inference, and forecasting via dynamic models, with mention of related areas.},
  copyright = {Copyright \textcopyright{} 2016 John Wiley \& Sons, Ltd. All rights reserved.},
  isbn = {978-1-118-44511-2},
  langid = {english},
  keywords = {Bayesian inference,dynamic linear models,forecasting,time series,todo},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118445112.stat00219.pub2},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Prado_2016_Dynamic_Bayesian_Models.pdf;/home/dede/Zotero/storage/IIB6GUPR/9781118445112.stat00219.html}
}

@article{priebe2005,
  title = {Scan {{Statistics}} on {{Enron Graphs}}},
  author = {Priebe, Carey and Conroy, John and Marchette, David and Park, Youngser},
  year = {2005},
  month = oct,
  journal = {Computational \& Mathematical Organization Theory},
  volume = {11},
  pages = {229--247},
  doi = {10.1007/s10588-005-5378-z},
  abstract = {We introduce a theory of scan statistics on graphs and apply the ideas to the problem of anomaly detection in a time series of Enron email graphs.},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Priebe_et_al_2005_Scan_Statistics_on_Enron_Graphs.pdf}
}

@article{psarakis2014,
  title = {Some {{Recent Developments}} on the {{Effects}} of {{Parameter Estimation}} on {{Control Charts}}},
  author = {Psarakis, Stelios and Vyniou, Angeliki and Castagliola, Philippe},
  year = {2014},
  journal = {Quality and Reliability Engineering International},
  volume = {30},
  doi = {10.1002/qre.1556},
  abstract = {Statistical process control plays a key role in today's highly competitive industrial environment since it allows quality practitioners to timely detect out-of-control situations and take actions whenever necessary in order to ensure that the products or services produced correspond to certain quality standards. Control charts are the tools quality practitioners use, and their monitoring performance is of major importance in practical applications. Since the values of the parameters used for the design of the charts' control limits are usually unknown in practice, the practitioners need to estimate them using an in-control retrospective sample. It has been shown that parameter estimation severely affects the control charts' properties. Many recent studies focused on investigating the impact of parameter estimates on the performance of control charts and on ways of diminishing this impact. This paper aims to provide an up-to-date critical review on the methodologies that have recently been developed in this area. Copyright \textcopyright{} 2013 John Wiley \& Sons, Ltd.},
  keywords = {reference only}
}

@article{qamar2014,
  title = {Additive {{Gaussian Process Regression}}},
  author = {Qamar, Shaan and Tokdar, Surya T.},
  year = {2014},
  month = nov,
  journal = {arXiv:1411.7009 [stat]},
  eprint = {1411.7009},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Additive-interactive regression has recently been shown to offer attractive minimax error rates over traditional nonparametric multivariate regression in a wide variety of settings, including cases where the predictor count is much larger than the sample size and many of the predictors have important effects on the response, potentially through complex interactions. We present a Bayesian implementation of additive-interactive regression using an additive Gaussian process (AGP) prior and develop an efficient Markov chain sampler that extends stochastic search variable selection in this setting. Careful prior and hyper-parameter specification are developed in light of performance and computational considerations, and key innovations address difficulties in exploring a joint posterior distribution over multiple subsets of high dimensional predictor inclusion vectors. The method offers state-of-the-art support and interaction recovery while improving dramatically over competitors in terms of prediction accuracy on a diverse set of simulated and real data. Results from real data studies provide strong evidence that the additive-interactive framework is an attractive modeling platform for high-dimensional nonparametric regression.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Computation,Statistics - Methodology,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Qamar_Tokdar_2014_Additive_Gaussian_Process_Regression.pdf;/home/dede/Zotero/storage/M29LRKRT/1411.html}
}

@article{qiu2003,
  title = {A {{Nonparametric Multivariate Cumulative Sum Procedure}} for {{Detecting Shifts}} in {{All Directions}}},
  author = {Qiu, Peihua and Hawkins, Douglas},
  year = {2003},
  journal = {Journal of the Royal Statistical Society. Series D (The Statistician)},
  volume = {52},
  number = {2},
  pages = {151--164},
  publisher = {{[Royal Statistical Society, Wiley]}},
  issn = {0039-0526},
  abstract = {The fairly limited range of tools for multivariate statistical process control generally rests on the assumption that the data vectors follow a multivariate normal distribution-an assumption that is rarely satisfied. We discuss detecting possible shifts in the mean vector of a multivariate measurement of a statistical process when the multivariate distribution of the measurement is non-Gaussian. A nonparametric cumulative sum procedure is suggested which is based both on the order information among the measurement components and on the order information between the measurement components and their in-control means. It is shown that this procedure is effective in detecting a wide range of possible shifts. Several numerical examples are presented to evaluate its performance. This procedure is also applied to a data set from an aluminium smelter.},
  keywords = {todo}
}

@book{qiu2013,
  title = {Introduction to {{Statistical Process Control}}},
  author = {Qiu, Peihua},
  year = {2013},
  publisher = {{CRC Press}},
  abstract = {A major tool for quality control and management, statistical process control (SPC) monitors sequential processes, such as production lines and Internet traffic, to ensure that they work stably and satisfactorily. Along with covering traditional methods, Introduction to Statistical Process Control describes many recent SPC methods that improve upon the more established techniques. The author\textemdash a leading researcher on SPC\textemdash shows how these methods can handle new applications.After exploring the role of SPC and other statistical methods in quality control and management, the book covers basic statistical concepts and methods useful in SPC. It then systematically describes traditional SPC charts, including the Shewhart, CUSUM, and EWMA charts, as well as recent control charts based on change-point detection and fundamental multivariate SPC charts under the normality assumption. The text also introduces novel univariate and multivariate control charts for cases when the normality assumption is invalid and discusses control charts for profile monitoring. All computations in the examples are solved using R, with R functions and datasets available for download on the author's website. Offering a systematic description of both traditional and newer SPC methods, this book is ideal as a primary textbook for a one-semester course in disciplines concerned with process quality control, such as statistics, industrial and systems engineering, and management sciences. It can also be used as a supplemental textbook for courses on quality improvement and system management. In addition, the book provides researchers with many useful, recent research results on SPC and gives quality control practitioners helpful guidelines on implementing up-to-date SPC techniques.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Qiu_2014_Introduction_to_Statistical_Process_Control.pdf}
}

@incollection{qiu2019,
  title = {Some {{Recent Studies}} in {{Statistical Process Control}}},
  author = {Qiu, Peihua},
  year = {2019},
  month = aug,
  pages = {3--19},
  doi = {10.1007/978-3-030-20709-0_1},
  abstract = {Statistical process control (SPC) charts are widely used in manufacturing industries for quality control and management. They are used in more and more other applications, such as internet traffic monitoring, disease surveillance, and environmental protection. Traditional SPC charts designed for monitoring production lines in manufacturing industries are based on the assumptions that observed data are independent and identically distributed with a parametric in-control distribution. These assumptions, however, are rarely valid in practice. Therefore, recent SPC research focuses mainly on development of new control charts that are appropriate to use without these assumptions. In this article, we briefly introduce some recent studies on nonparametric SPC, control charts for monitoring dynamic processes, and spatio-temporal process monitoring. Control charts developed in these directions have found broad applications in practice.},
  isbn = {978-3-030-20708-3},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Qiu_2019_Some_Recent_Studies_in_Statistical_Process_Control.pdf}
}

@article{qiu2019a,
  title = {Big {{Data}}? {{Statistical Process Control Can Help}}!},
  shorttitle = {Big {{Data}}?},
  author = {Qiu, Peihua},
  year = {2019},
  month = dec,
  journal = {The American Statistician},
  volume = {0},
  number = {0},
  pages = {1--16},
  publisher = {{Taylor \& Francis}},
  issn = {0003-1305},
  doi = {10.1080/00031305.2019.1700163},
  abstract = {``Big data'' is a buzzword these days due to an enormous amount of data-rich applications in different industries and research projects. In practice, big data often take the form of data streams in the sense that new batches of data keep being collected over time. One fundamental research problem when analyzing big data in a given application is to monitor the underlying sequential process of the observed data to see whether it is longitudinally stable, or how its distribution changes over time. To monitor a sequential process, one major statistical tool is the statistical process control (SPC) charts, which have been developed and used mainly for monitoring production lines in the manufacturing industries during the past several decades. With many new and versatile SPC methods developed in the recent research, it is our belief that SPC can become a powerful tool for handling many big data applications that are beyond the production line monitoring. In this article, we introduce some recent SPC methods, and discuss their potential to solve some big data problems. Certain challenges in the interface between the current SPC research and some big data applications are also discussed.},
  keywords = {Covariates,Data-rich applications,done,Dynamic processes,Feature extraction,Image data,Spatio-temporal data},
  annotation = {\_eprint: https://doi.org/10.1080/00031305.2019.1700163},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Qiu_2019_Big_Data.pdf;/home/dede/Zotero/storage/2ZAN93BL/00031305.2019.html}
}

@article{qiu2019b,
  title = {Nonparametric Monitoring of Multiple Count Data},
  author = {Qiu, Peihua and He, Zhen and Wang, Zhiqiong},
  year = {2019},
  month = sep,
  journal = {IISE Transactions},
  volume = {51},
  number = {9},
  pages = {972--984},
  publisher = {{Taylor \& Francis}},
  issn = {2472-5854},
  doi = {10.1080/24725854.2018.1530486},
  abstract = {Process monitoring of multiple count data has recently received considerable attention in the statistical process control literature. Most existing methods on this topic are based on parametric modeling of the observed process data. However, the assumed parametric models are often invalid in practice, leading to unreliable performance of the related control charts. In this article, we first show the consequence of using a parametric control chart in cases where the underlying parametric distribution is invalid. Then, we thoroughly investigate the performance of some parametric and nonparametric control charts in monitoring multiple count data. Our numerical results show that nonparametric methods can provide a more reliable and effective process monitoring in such cases. A real-data example about the crime log of the University of Florida Police Department is used for illustrating the implementation of the related control charts.},
  keywords = {Distribution-free,log-linear modeling,multiple count data,nonparametric procedures,Poisson distribution,statistical process control,todo},
  annotation = {\_eprint: https://doi.org/10.1080/24725854.2018.1530486},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Qiu_et_al_2019_Nonparametric_monitoring_of_multiple_count_data.pdf;/home/dede/Zotero/storage/E3I55EHZ/24725854.2018.html}
}

@article{qiu2020,
  title = {A {{New Process Control Chart}} for {{Monitoring Short-Range Serially Correlated Data}}},
  author = {Qiu, Peihua and Li, Wendong and Li, Jun},
  year = {2020},
  month = jan,
  journal = {Technometrics},
  volume = {62},
  number = {1},
  pages = {71--83},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.2018.1562988},
  abstract = {\textendash Statistical process control (SPC) charts are critically important for quality control and management in manufacturing industries, environmental monitoring, disease surveillance, and many other applications. Conventional SPC charts are designed for cases when process observations are independent at different observation times. In practice, however, serial data correlation almost always exists in sequential data. It has been well demonstrated in the literature that control charts designed for independent data are unstable for monitoring serially correlated data. Thus, it is important to develop control charts specifically for monitoring serially correlated data. To this end, there is some existing discussion in the SPC literature. Most existing methods are based on parametric time series modeling and residual monitoring, where the data are often assumed to be normally distributed. In applications, however, the assumed parametric time series model with a given order and the normality assumption are often invalid, resulting in unstable process monitoring. Although there is some nice discussion on robust design of such residual monitoring control charts, the suggested designs can only handle certain special cases well. In this article, we try to make another effort by proposing a novel control chart that makes use of the restarting mechanism of a CUSUM chart and the related spring length concept. Our proposed chart uses observations within the spring length of the current time point and ignores all history data that are beyond the spring length. It does not require any parametric time series model and/or a parametric process distribution. It only requires the assumption that process observation at a given time point is associated with nearby observations and independent of observations that are far away in observation times, which should be reasonable for many applications. Numerical studies show that it performs well in different cases.},
  keywords = {Covariance matrix decomposition,Data correlation,Decorrelation,Process monitoring,Spring length,Statistical process control,todo},
  annotation = {\_eprint: https://doi.org/10.1080/00401706.2018.1562988},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Qiu_et_al_2020_A_New_Process_Control_Chart_for_Monitoring_Short-Range_Serially_Correlated_Data.pdf;/home/dede/Zotero/storage/T9ZALDUJ/00401706.2018.html}
}

@article{qiu2021,
  title = {Transparent {{Sequential Learning}} for {{Statistical Process Control}} of {{Serially Correlated Data}}},
  author = {Qiu, Peihua and Xie, Xiulin},
  year = {2021},
  month = may,
  journal = {Technometrics},
  volume = {0},
  number = {0},
  pages = {1--15},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.2021.1929493},
  abstract = {Machine learning methods have been widely used in different applications, including process control and monitoring. For handling statistical process control (SPC) problems, conventional supervised machine learning methods (e.g., artificial neural networks and support vector machines) would have some difficulties. For instance, a training dataset containing both in-control and out-of-control (OC) process observations is required by a supervised machine learning method, but it is rarely available in SPC applications. Furthermore, many machine learning methods work like black boxes. It is often difficult to interpret their learning mechanisms and the resulting decision rules in the context of an application. In the SPC literature, there have been some existing discussions on how to handle the lack of OC observations in the training data, using the one-class classification, artificial contrast, real-time contrast, and some other novel ideas. However, these approaches have their own limitations to handle SPC problems. In this article, we extend the self-starting process monitoring idea that has been employed widely in modern SPC research to a general learning framework for monitoring processes with serially correlated data. Under the new framework, process characteristics to learn are well specified in advance, and process learning is sequential in the sense that the learned process characteristics keep being updated during process monitoring. The learned process characteristics are then incorporated into a control chart for detecting process distributional shift based on all available data by the current observation time. Numerical studies show that process monitoring based on the new learning framework is more reliable and effective than some representative existing machine learning SPC approaches.},
  keywords = {Data correlation,doing,Machine learning,Recursive computation,Self-starting charts,Sequential learning,Statistical process control},
  annotation = {\_eprint: https://doi.org/10.1080/00401706.2021.1929493},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Qiu_Xie_2021_Transparent_Sequential_Learning_for_Statistical_Process_Control_of_Serially.pdf;/home/dede/Zotero/storage/627E5QRV/00401706.2021.html}
}

@article{qu2019,
  title = {Boundary {{Detection Using}} a {{Bayesian Hierarchical Model}} for {{Multiscale Spatial Data}}},
  author = {Qu, Kai and Bradley, Jonathan R. and Niu, Xufeng},
  year = {2019},
  journal = {Technometrics},
  volume = {63},
  number = {1},
  pages = {64--76},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.2019.1677268},
  abstract = {Spatial boundary analysis has attained considerable attention in several disciplines including engineering, shape analysis, spatial statistics, and computer science. The inferential question of interest is often to identify rapid surface change of an unobserved latent process. Curvilinear wombling and crisp wombling (or fuzzy) are two major approaches that have emerged in Bayesian spatial statistics literature. These methods are limited to a single spatial scale even though data with multiple spatial scales are often accessible. Thus, we propose a multiscale representation of the directional derivative Karhunen\textendash Lo\'eve expansion to perform directionally based boundary detection. Taking a multiscale spatial perspective allows us, for the first time, to consider the concept of curvilinear boundary fallacy (CBF) error, which is a boundary detection analog to the ecological fallacy that is often studied in spatial change of support literature. Furthermore, we propose a directionally based multiscale curvilinear boundary error criterion to quantify CBF. We refer to this metric as the criterion for boundary aggregation error (BAGE), and use it to perform boundary detection. Several theoretical results are derived to motivate BAGE. In particular, we show that no BAGE exists when the directional derivatives of eigenfunctions of a KL expansion are constant across spatial scales. We illustrate the use of our model through a simulated example and an analysis of Mediterranean wind measurements data. Supplementary materials for this article are available online.},
  keywords = {Change of support,Gradient,Image segmentation,todo},
  annotation = {\_eprint: https://doi.org/10.1080/00401706.2019.1677268},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Qu_et_al_2019_Boundary_Detection_Using_a_Bayesian_Hierarchical_Model_for_Multiscale_Spatial.pdf;/home/dede/Zotero/storage/WUF3S99U/00401706.2019.html}
}

@article{quesenberry1991c,
  title = {{{SPC Q Charts}} for {{Start-Up Processes}} and {{Short}} or {{Long Runs}}},
  author = {Quesenberry, Charles P.},
  year = {1991},
  journal = {Journal of Quality Technology},
  volume = {23},
  number = {3},
  pages = {213--224},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.1991.11979327},
  abstract = {Classical control charts are designed for processes where data to estimate the process parameters and compute the control limits are available before a production run. For many processes, especially in a job-shop setting, production runs are not necessarily long and charting techniques are required that do not depend upon knowing the process parameters in advance of the run. It is desirable to begin charting at or very near the beginning of the run in these cases. We present here the needed formulas so that charts for both the process mean and variance can be maintained from the start of production, whether or not prior information for estimating the parameters is available. These Q charts are all plotted in a standardized normal scale, and therefore permit the plotting of different statistics on the same chart. This will sometimes permit savings in the chart management program.},
  keywords = {Control Charts,Corrigenda,Job Shops,reference only,Short Runs,Start-Up Processes,Unknown Parameters},
  annotation = {\_eprint: https://doi.org/10.1080/00224065.1991.11979327},
  file = {/home/dede/Zotero/storage/572XJP8E/00224065.1991.html}
}

@article{quesenberry1995a,
  title = {Geometric {{Q Charts}} for {{High Quality Processes}}},
  author = {Quesenberry, Charles P.},
  year = {1995},
  month = oct,
  journal = {Journal of Quality Technology},
  volume = {27},
  number = {4},
  pages = {304--315},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.1995.11979610},
  abstract = {The geometric distribution based on the inverse binomial method of sampling is used to define Q statistics for the case when the process fallout p is known and when it is not known in advance of sampling. The effectiveness to detect shifts in p of four tests made on Shewhart Q charts of these statistics as well as both exponentially weighted moving average and cumulative sum Q charts computed from these statistics is studied. The classic test of one point outside 3-sigma control limits is found to have poor sensitivity to detect a one-step permanent shift in p. The test consisting of four-out-of-five points beyond one standard deviation is found to be a good general omnibus test on a Shewhart Q chart to detect an increase or decrease in p.},
  keywords = {Attributes Control Charts,Corrigenda,Cumulative Sum Control Charts,Exponentially Weighted Moving Average Control Charts,Q Charts,Shewhart Control Charts},
  annotation = {\_eprint: https://doi.org/10.1080/00224065.1995.11979610},
  file = {/home/dede/Zotero/storage/ID8MR3F8/00224065.1995.html}
}

@article{quesenberry1995b,
  title = {On {{Properties}} of {{Q Charts}} for {{Variables}}},
  author = {Quesenberry, Charles P.},
  year = {1995},
  journal = {Journal of Quality Technology},
  volume = {27},
  number = {3},
  pages = {184--203},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.1995.11979592},
  abstract = {The sensitivity of four tests on Shewhart type Q charts and of specially designed EWMA and CUSUM Q charts to detect one-step permanent shifts of either a normal mean or standard deviation has been studied. The usual test that signals for one point beyond three standard deviations on a Shewhart chart is found to have poor sensitivity, generally. The test that signals when four out of five consecutive points are beyond one standard deviation in the same direction is found to be a good omnibus test. The EWMA and CUSUM Q charts are the most sensitive and are about comparable in overall performance.},
  keywords = {Corrigenda,Q Charts,reference only,Variables Control Charts},
  annotation = {\_eprint: https://doi.org/10.1080/00224065.1995.11979592},
  file = {/home/dede/Zotero/storage/WC6QLGD5/00224065.1995.html}
}

@book{quesenberry1997,
  title = {{{SPC Methods}} for {{Quality Improvement}}},
  author = {Quesenberry, Charles P.},
  year = {1997},
  edition = {1st edition},
  publisher = {{Wiley}},
  address = {{New York}},
  isbn = {978-0-471-13087-1},
  langid = {english}
}

@article{quesenberry2001,
  title = {The {{Multivariate Short-Run Snapshot Q Chart}}},
  author = {Quesenberry, Charles P.},
  year = {2001},
  month = jun,
  journal = {Quality Engineering},
  volume = {13},
  number = {4},
  pages = {679--683},
  publisher = {{Taylor \& Francis}},
  issn = {0898-2112},
  doi = {10.1080/08982110108918699},
  keywords = {Multivariate,Q Charts,Quesenberry,Short runs,Snapshot chart,SPC},
  annotation = {\_eprint: https://doi.org/10.1080/08982110108918699},
  file = {/home/dede/Zotero/storage/WW2KG4GH/08982110108918699.html}
}

@inproceedings{rahimi2007,
  title = {Random Features for Large-Scale Kernel Machines},
  booktitle = {Proceedings of the 20th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Rahimi, Ali and Recht, Benjamin},
  year = {2007},
  month = dec,
  series = {{{NIPS}}'07},
  pages = {1177--1184},
  publisher = {{Curran Associates Inc.}},
  address = {{Red Hook, NY, USA}},
  abstract = {To accelerate the training of kernel machines, we propose to map the input data to a randomized low-dimensional feature space and then apply existing fast linear methods. The features are designed so that the inner products of the transformed data are approximately equal to those in the feature space of a user specified shift-invariant kernel. We explore two sets of random features, provide convergence bounds on their ability to approximate various radial basis kernels, and show that in large-scale classification and regression tasks linear machine learning algorithms applied to these features outperform state-of-the-art large-scale kernel machines.},
  isbn = {978-1-60560-352-0},
  keywords = {todo}
}

@book{raiffa2000,
  title = {{Applied Statistical Decision Theory}},
  author = {Raiffa, Howard and Schlaifer, Robert},
  year = {2000},
  edition = {1. edizione},
  publisher = {{Wiley-Interscience}},
  address = {{New York}},
  isbn = {978-0-471-38349-9},
  langid = {Inglese}
}

@book{raj1968,
  title = {Sampling Theory},
  author = {Raj, Des},
  year = {1968},
  month = jan,
  publisher = {{McGraw-Hill}},
  langid = {english}
}

@article{ramsay1988,
  title = {Monotone {{Regression Splines}} in {{Action}}},
  author = {Ramsay, J. O.},
  year = {1988},
  journal = {Statistical Science},
  volume = {3},
  number = {4},
  pages = {425--441},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0883-4237},
  abstract = {Piecewise polynomials or splines extend the advantages of polynomials to include greater flexibility, local effects of parameter changes and the possibility of imposing useful constraints on estimated functions. Among these constraints is monotonicity, which can be an important property in many curve estimation problems. This paper shows the virtues of monotone splines through a number of statistical applications, including response variable transformation in nonlinear regression, transformation of variables in multiple regression, principal components and canonical correlation, and the use of monotone splines to model a dose-response function and to perform item analysis. Computational and inferential issues are discussed and illustrated.},
  file = {/home/dede/Zotero/storage/5M768WWF/Ramsay - 1988 - Monotone Regression Splines in Action.pdf}
}

@book{ramsay2002,
  title = {Applied {{Functional Data Analysis}}: {{Methods}} and {{Case Studies}}},
  shorttitle = {Applied {{Functional Data Analysis}}},
  author = {Ramsay, J. O. and Silverman, B. W.},
  year = {2002},
  series = {Springer {{Series}} in {{Statistics}}},
  publisher = {{Springer-Verlag}},
  address = {{New York}},
  doi = {10.1007/b98886},
  abstract = {Almost as soon as we had completed our previous book Functional Data Analysis in 1997, it became clear that potential interest in the ?eld was far wider than the audience for the thematic presentation we had given there. At the same time, both of us rapidly became involved in relevant new research involving many colleagues in ?elds outside statistics. This book treats the ?eld in a di?erent way, by considering case st- ies arising from our own collaborative research to illustrate how functional data analysis ideas work out in practice in a diverse range of subject areas. These include criminology, economics, archaeology, rheumatology, psych- ogy, neurophysiology, auxology (the study of human growth), meteorology, biomechanics, and education\textemdash and also a study of a juggling statistician. Obviously such an approach will not cover the ?eld exhaustively, and in any case functional data analysis is not a hard-edged closed system of thought. Nevertheless we have tried to give a ?avor of the range of meth- ology we ourselves have considered. We hope that our personal experience, including the fun we had working on these projects, will inspire others to extend ``functional'' thinking to many other statistical contexts. Of course, manyofourcasestudiesrequireddevelopmentofexistingmethodology,and readersshouldgaintheabilitytoadaptmethodstotheirownproblemstoo.},
  isbn = {978-0-387-95414-1},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Ramsay_Silverman_2002_Applied_Functional_Data_Analysis.pdf;/home/dede/Zotero/storage/3ULIIKB7/9780387954141.html}
}

@book{ramsay2005,
  title = {Functional {{Data Analysis}}},
  author = {Ramsay, James and Silverman, B. W.},
  year = {2005},
  series = {Springer {{Series}} in {{Statistics}}},
  edition = {Second},
  publisher = {{Springer-Verlag}},
  address = {{New York}},
  doi = {10.1007/b98888},
  abstract = {Scientists and others today often collect samples of curves and other functional observations. This monograph presents many ideas and techniques for such data. Included are expressions in the functional domain of such classics as linear regression, principal components analysis, linear modeling, and canonical correlation analysis, as well as specifically functional techniques such as curve registration and principal differential analysis. Data arising in real applications are used throughout for both motivation and illustration, showing how functional approaches allow us to see new things, especially by exploiting the smoothness of the processes generating the data. The data sets exemplify the wide scope of functional data analysis; they are drawn from growth analysis, meteorology, biomechanics, equine science, economics, and medicine. The book presents novel statistical technology, much of it based on the authors' own research work, while keeping the mathematical level widely accessible. It is designed to appeal to students, to applied data analysts, and to experienced researchers; it will have value both within statistics and across a broad spectrum of other fields. This second edition is aimed at a wider range of readers, and especially those who would like to apply these techniques to their research problems. It complements the authors' other recent volume Applied Functional Data Analysis: Methods and Case Studies. In particular, there is an extended coverage of data smoothing and other matters arising in the preliminaries to a functional data analysis. The chapters on the functional linear model and modeling of the dynamics of systems through the use of differential equations and principal differential analysis have been completely rewritten and extended to include new developments. Other chapters have been revised substantially, often to give more weight to examples and practical considerations. Jim Ramsay is Professor of Psychology at McGill University and is an international authority on many aspects of multivariate analysis. He was President of the Statistical Society of Canada in 2002-3 and holds the Society's Gold Medal for his work in functional data analysis. Bernard Silverman is Master of St Peter's College and Professor of Statistics at Oxford University. He was President of the Institute of Mathematical Statistics in 2000\textendash 1. He is a Fellow of the Royal Society. His main specialty is in computational statistics, and he is the author or editor of several highly regarded books in this area.},
  isbn = {978-0-387-40080-8},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Ramsay_Silverman_2005_Functional_Data_Analysis.pdf;/home/dede/Zotero/storage/6FCI636S/9780387400808.html}
}

@article{ranganath2013,
  title = {Black {{Box Variational Inference}}},
  author = {Ranganath, Rajesh and Gerrish, Sean and Blei, David M.},
  year = {2013},
  month = dec,
  journal = {arXiv:1401.0118 [cs, stat]},
  eprint = {1401.0118},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Variational inference has become a widely used method to approximate posteriors in complex latent variables models. However, deriving a variational inference algorithm generally requires significant model-specific analysis, and these efforts can hinder and deter us from quickly developing and exploring a variety of models for a problem at hand. In this paper, we present a "black box" variational inference algorithm, one that can be quickly applied to many models with little additional derivation. Our method is based on a stochastic optimization of the variational objective where the noisy gradient is computed from Monte Carlo samples from the variational distribution. We develop a number of methods to reduce the variance of the gradient, always maintaining the criterion that we want to avoid difficult model-based derivations. We evaluate our method against the corresponding black box sampling based methods. We find that our method reaches better predictive likelihoods much faster than sampling methods. Finally, we demonstrate that Black Box Variational Inference lets us easily explore a wide space of models by quickly constructing and evaluating several models of longitudinal healthcare data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Ranganath_et_al_2013_Black_Box_Variational_Inference.pdf;/home/dede/Zotero/storage/ABHQLQZT/1401.html}
}

@article{rao2007,
  title = {Has Statistics a Future? {{If}} so in What Form?},
  shorttitle = {Has Statistics a Future?},
  author = {Rao, C.Radhakrishna},
  year = {2007},
  month = jan,
  journal = {Journal of the Indian Society of Agricultural Statistics},
  volume = {61},
  doi = {10.1142/9789812776372_0022},
  abstract = {The mathematical foundations of statistics as a separate discipline were laid by Fisher, Neyman and Wald during the second quarter of the last century. Subsequent research in statistics and the courses taught in the universities are mostly based on the guidelines set by these pioneers. Statistics is used in some form or other in all areas of human endeavor from scientific research to optimum use of resources for social welfare, prediction and decision-making. However, there are controversies in statistics, especially in the choice of a model for data, use of prior probabilities and subject-matter judgments by experts. The same data analyzed by different consulting statisticians may lead to different conclusions. What is the future of statistics in the present millennium dominated by information technology encompassing the whole of communications, interaction with intelligent systems, massive data bases, and complex information processing networks? The current statistical methodology based on simple probabilistic models developed for the analysis of small data sets appears to be inadequate to meet the needs of customers for quick on line processing of data and making the information available for practical use. Some methods are being put forward in the name of data mining for such purposes. A broad review of the current state of the art in statistics, its merits and demerits, and possible future developments is presented.},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Rao_2007_Has_statistics_a_future.pdf}
}

@book{rapport2013,
  title = {The {{Napoleonic Wars}}: {{A Very Short Introduction}}},
  shorttitle = {The {{Napoleonic Wars}}},
  author = {Rapport, Mike},
  year = {2013},
  series = {Very {{Short Introductions}}},
  publisher = {{Oxford University Press}},
  isbn = {978-0-19-959096-4},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Rapport_2013_The_Napoleonic_Wars.epub}
}

@book{rasmussen2005,
  title = {{Gaussian Processes for Machine Learning}},
  author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
  year = {2005},
  month = nov,
  publisher = {{Mit Pr}},
  address = {{Cambridge, Mass}},
  abstract = {A comprehensive and self-contained introduction to Gaussian processes, which provide a principled, practical, probabilistic approach to learning in kernel machines.Gaussian processes (GPs) provide a principled, practical, probabilistic approach to learning in kernel machines. GPs have received increased attention in the machine-learning community over the past decade, and this book provides a long-needed systematic and unified treatment of theoretical and practical aspects of GPs in machine learning. The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics. The book deals with the supervised-learning problem for both regression and classification, and includes detailed algorithms. A wide variety of covariance (kernel) functions are presented and their properties discussed. Model selection is discussed both from a Bayesian and a classical perspective. Many connections to other well-known techniques from machine learning and statistics are discussed, including support-vector machines, neural networks, splines, regularization networks, relevance vector machines and others. Theoretical issues including learning curves and the PAC-Bayesian framework are treated, and several approximation methods for learning with large datasets are discussed. The book contains illustrative examples and exercises, and code and datasets are available on the Web. Appendixes provide mathematical background and a discussion of Gaussian Markov processes.},
  isbn = {978-0-262-18253-9},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Rasmussen_Williams_2005_Gaussian_Processes_for_Machine_Learning.pdf}
}

@article{raubenheimer2015,
  title = {Bayesian {{Control Chart}} for {{Nonconformities}}},
  author = {Raubenheimer, Lizanne and Merwe, A.J.},
  year = {2015},
  month = dec,
  journal = {Quality and Reliability Engineering International},
  volume = {31},
  pages = {1359--1366},
  doi = {10.1002/qre.1668},
  abstract = {The c-chart or the control chart for nonconformities is designed for the case where one deals with the number of defects or nonconformities observed. A control chart can be developed for the total or average number of nonconformities per unit, which is well modeled by the Poisson distribution. In this paper the c-chart will be studied, where the usual operation of the c-chart will be extended by introducing a Bayesian approach for the c-chart. Control chart limits, average run lengths, and false alarm rates will be determined by using a Bayesian method. These results will be compared with the results obtained when using the classical (frequentist) method. Copyright \textcopyright{} 2014 John Wiley \& Sons, Ltd.},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Raubenheimer_Merwe_2015_Bayesian_Control_Chart_for_Nonconformities.pdf}
}

@article{ravichandran2019,
  title = {Self-Starting {{X-bar}} Control Chart Based on {{Six Sigma}} Quality and Sometimes Pooling Procedure},
  author = {Ravichandran, J.},
  year = {2019},
  month = jan,
  journal = {Journal of Statistical Computation and Simulation},
  volume = {89},
  number = {2},
  pages = {362--377},
  publisher = {{Taylor \& Francis}},
  issn = {0094-9655},
  doi = {10.1080/00949655.2018.1551394},
  abstract = {Self-starting control charts have been proposed in the literature to allow process monitoring when only a small amount of relevant data is available. In fact, self-starting charts are useful in monitoring a process quickly, without having to collect a sizable Phase I sample for estimating the in-control process parameters. In this paper, a new self-starting control charting procedure is proposed in which first an effective initial sample is chosen from the perspective of Six Sigma quality, then the successive sample means are either pooled or not pooled (sometimes pooling procedure) for computing next Q-statistics depending upon its signal. It is observed that the sample statistics obtained so from this in-control Phase I situation can serve as more efficient estimators of unknown parameters for Phase II monitoring. An example is considered to illustrate the construction of the proposed chart and to compare its performance with the existing ones.},
  keywords = {DPMO,Q-statistics,self-starting charts,Six Sigma quality,sometimes pooling},
  annotation = {\_eprint: https://doi.org/10.1080/00949655.2018.1551394},
  file = {/home/dede/Zotero/storage/92YYKL4M/00949655.2018.html}
}

@article{raymaekers2022,
  title = {Class {{Maps}} for {{Visualizing Classification Results}}},
  author = {Raymaekers, Jakob and Rousseeuw, Peter J. and Hubert, Mia},
  year = {2022},
  month = apr,
  journal = {Technometrics},
  volume = {64},
  number = {2},
  pages = {151--165},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.2021.1927849},
  abstract = {Classification is a major tool of statistics and machine learning. A classification method first processes a training set of objects with given classes (labels), with the goal of afterward assigning new objects to one of these classes. When running the resulting prediction method on the training data or on test data, it can happen that an object is predicted to lie in a class that differs from its given label. This is sometimes called label bias, and raises the question whether the object was mislabeled. The proposed class map reflects the probability that an object belongs to an alternative class, how far it is from the other objects in its given class, and whether some objects lie far from all classes. The goal is to visualize aspects of the classification results to obtain insight in the data. The display is constructed for discriminant analysis, the k-nearest neighbor classifier, support vector machines, logistic regression, and coupling pairwise classifications. It is illustrated on several benchmark datasets, including some about images and texts.},
  keywords = {Discriminant analysis,done,k-Nearest neighbors,Mislabeling,Pairwise coupling,Support vector machines},
  annotation = {\_eprint: https://doi.org/10.1080/00401706.2021.1927849},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Raymaekers_et_al_2022_Class_Maps_for_Visualizing_Classification_Results.pdf;/home/dede/Zotero/storage/5K3EA5RG/00401706.2021.html}
}

@article{reich2009,
  title = {Variable {{Selection}} in {{Bayesian Smoothing Spline ANOVA Models}}: {{Application}} to {{Deterministic Computer Codes}}},
  shorttitle = {Variable {{Selection}} in {{Bayesian Smoothing Spline ANOVA Models}}},
  author = {Reich, Brian and Storlie, Curtis and Bondell, Howard},
  year = {2009},
  month = may,
  journal = {Technometrics},
  volume = {51},
  pages = {110--120},
  doi = {10.1198/TECH.2009.0013},
  abstract = {With many predictors, choosing an appropriate subset of the covariates is a crucial, and difficult, step in nonparametric regression. We propose a Bayesian nonparametric regression model for curve-fitting and variable selection. We use the smoothing spline ANOVA framework to decompose the regression function into interpretable main effect and interaction functions. Stochastic search variable selection via MCMC sampling is used to search for models that fit the data well. Also, we show that variable selection is highly-sensitive to hyperparameter choice and develop a technique to select hyperparameters that control the long-run false positive rate. The method is used to build an emulator for a complex computer model for two-phase fluid flow.},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Reich_et_al_2009_Variable_Selection_in_Bayesian_Smoothing_Spline_ANOVA_Models.pdf}
}

@article{reid2015,
  title = {On {{Some Principles}} of {{Statistical Inference}}},
  author = {Reid, Nancy and Cox, David R.},
  year = {2015},
  journal = {International Statistical Review},
  volume = {83},
  number = {2},
  pages = {293--308},
  issn = {1751-5823},
  doi = {10.1111/insr.12067},
  abstract = {SummaryStatistical theory aims to provide a foundation for studying the collection and interpretation of data, a foundation that does not depend on the particular details of the substantive field in which the data are being considered. This gives a systematic way to approach new problems, and a common language for summarising results; ideally, the foundations and common language ensure that statistical aspects of one study, or of several studies on closely related phenomena, can be broadly accessible. We discuss some principles of statistical inference, to outline how these are, or could be, used to inform the interpretation of results, and to provide a greater degree of coherence for the foundations of statistics.},
  langid = {english},
  keywords = {Ancillary,Bayesian,conditional,done,likelihood,models,p-values,sufficient},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/insr.12067},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Reid_Cox_2015_On_Some_Principles_of_Statistical_Inference.pdf;/home/dede/Zotero/storage/URLSSFJS/insr.html}
}

@article{reimherr2021,
  title = {Prior Sample Size Extensions for Assessing Prior Impact and Prior-Likelihood Discordance},
  author = {Reimherr, Matthew and Meng, Xiao-Li and Nicolae, Dan L.},
  year = {2021},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {n/a},
  number = {n/a},
  issn = {1467-9868},
  doi = {10.1111/rssb.12414},
  abstract = {This paper outlines a framework for quantifying the prior's contribution to posterior inference in the presence of prior-likelihood discordance, a broader concept than the usual notion of prior-likelihood conflict. We achieve this dual purpose by extending the classic notion of prior sample size, M, in three directions: (I) estimating M beyond conjugate families; (II) formulating M as a relative notion that is as a function of the likelihood sample size k, M(k), which also leads naturally to a graphical diagnosis; and (III) permitting negative M, as a measure of prior-likelihood conflict, that is, harmful discordance. Our asymptotic regime permits the prior sample size to grow with the likelihood data size, hence making asymptotic arguments meaningful for investigating the impact of the prior relative to that of likelihood. It leads to a simple asymptotic formula for quantifying the impact of a proper prior that only involves computing a centrality and a spread measure of the prior and the posterior. We use simulated and real data to illustrate the potential of the proposed framework, including quantifying how weak is a `weakly informative' prior adopted in a study of lupus nephritis. Whereas we take a pragmatic perspective in assessing the impact of a prior on a given inference problem under a specific evaluative metric, we also touch upon conceptual and theoretical issues such as using improper priors and permitting priors with asymptotically non-vanishing influence.},
  copyright = {\textcopyright{} 2021 Royal Statistical Society},
  langid = {english},
  keywords = {conjugate prior,prior-likelihood conflict,todo,weakly informative prior},
  annotation = {\_eprint: https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/rssb.12414},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Reimherr_et_al_Prior_sample_size_extensions_for_assessing_prior_impact_and_prior-likelihood.pdf;/home/dede/Zotero/storage/SJKJYXAY/rssb.html}
}

@article{renz2019,
  title = {Russian Responses to the Changing Character of War},
  author = {Renz, Bettina},
  year = {2019},
  month = jul,
  journal = {International Affairs},
  volume = {95},
  number = {4},
  pages = {817--834},
  issn = {0020-5850},
  doi = {10.1093/ia/iiz100},
  abstract = {For much of the post-Soviet era there was a widespread belief that improving capabilities required for dealing with local small wars and insurgencies was the central focus of Russian military reforms. As a result, Moscow's military assertiveness and return to geopolitical rivalries since 2014 came as a surprise to many in the West. The article argues that small wars were never the major focus of Russian military transformation and reforms. Tracing the country's experience of war and conflict regionally and internationally since the end of the Cold War, and the impact this had on Moscow's views on what kind of armed forces it required, the article shows that the Kremlin's military ambitions started to diverge dramatically from western expectations as early as the mid-1990s. Russia never really saw armed forces geared towards small and `new war'-type scenarios as sufficient for upholding its regional and international status ambitions. Moreover, the Kremlin's growing preoccupation with internal order and regime stability has increasingly reinforced the rhetoric of a hostile West, which is used to justify the increasing centralization of domestic politics. Russia's military revival has been long in the making and poses serious challenges to its neighbours and to the West.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Renz_2019_Russian_responses_to_the_changing_character_of_war.pdf;/home/dede/Zotero/storage/UFJN3T7L/5499314.html}
}

@book{resnick2013,
  title = {{Adventures in Stochastic Processes}},
  author = {Resnick, Sidney I.},
  year = {2013},
  month = dec,
  edition = {Reprint edizione},
  publisher = {{Birkhauser}},
  address = {{Erscheinungsort nicht ermittelbar}},
  abstract = {Stochastic processes are necessary ingredients for building models of a wide variety of phenomena exhibiting time varying randomness. This text offers easy access to this fundamental topic for many students of applied sciences at many levels. It includes examples, exercises, applications, and computational procedures. It is uniquely useful for beginners and non-beginners in the field. No knowledge of measure theory is presumed.},
  isbn = {978-1-4612-6738-6},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Resnick_2013_Adventures_in_Stochastic_Processes.pdf}
}

@article{reynolds2006,
  title = {Multivariate {{Control Charts}} for {{Monitoring}} the {{Mean Vector}} and {{Covariance Matrix}} with {{Variable Sampling Intervals}}},
  author = {Reynolds, Marion R. Jr. and Cho, Gyo-Young},
  year = {2006},
  month = jan,
  journal = {Sequential Analysis},
  volume = {30},
  number = {1},
  pages = {1--40},
  publisher = {{Taylor \& Francis}},
  issn = {0747-4946},
  doi = {10.1080/07474946.2010.520627},
  abstract = {Traditional control charts for process monitoring are fixed sampling rate (FSR) control charts based on taking samples of fixed size with a fixed sampling interval between samples. Variable sampling rate (VSR) control charts vary the sampling rate as a function of the data from the process, and can detect most process changes significantly faster than traditional FSR control charts. The main focus of this article is on a type of VSR control chart called a variable sampling interval (VSI) control chart, but another type of VSR control chart based on sequential sampling is also considered. We consider the problem of detecting sustained or transient shifts in the mean or variability of a process. We investigate Shewhart control charts and multivariate exponentially weighted moving average (MEWMA) control charts based on sample means and on the sum of the squared deviations from target. We show that the combination of the standard MEWMA chart and an MEWMA-type chart based on squared deviations from target gives excellent overall performance. When the VSI feature is used with this combination, the performance is dramatically better than the performance of the standard FSR multivariate control charts that have traditionally been used.},
  keywords = {62D99,62L10,62P30,Average time to signal,done,Exponentially weighted moving average control chart,Multivariate exponentially weighted moving average control chart,Regression adjustment of variables,Sequential sampling,Squared deviations from target,Statistical process control,Steady-state average time to signal},
  annotation = {\_eprint: https://doi.org/10.1080/07474946.2010.520627},
  file = {/home/dede/Zotero/storage/RHSU3SKS/07474946.2010.html}
}

@article{rhudy2017,
  title = {A {{Kalman Filtering Tutorial}} for {{Undergraduate Students}}},
  author = {Rhudy, Matthew B and Salguero, Roger A and Holappa, Keaton},
  year = {2017},
  month = feb,
  journal = {International Journal of Computer Science \& Engineering Survey},
  volume = {08},
  number = {01},
  pages = {01--18},
  issn = {09763252, 09762760},
  doi = {10.5121/ijcses.2017.8101},
  langid = {english},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Rhudy_et_al_2017_A_Kalman_Filtering_Tutorial_for_Undergraduate_Students.pdf}
}

@book{rice2006,
  title = {{Mathematical Statistics And Data Analysis}},
  author = {Rice, John A.},
  year = {2006},
  month = apr,
  edition = {3\textdegree{} edizione},
  publisher = {{Duxbury Pr}},
  address = {{New Delhi}},
  isbn = {978-81-315-1954-7},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Rice_2006_Mathematical_Statistics_And_Data_Analysis.pdf}
}

@article{richardson1972,
  title = {Cold-{{War Revisionism}}: {{A Critique}}},
  shorttitle = {Cold-{{War Revisionism}}},
  author = {Richardson, J. L.},
  editor = {Kolko, Gabriel and Alperovitz, Gar and Horowitz, David},
  year = {1972},
  journal = {World Politics},
  volume = {24},
  number = {4},
  pages = {579--612},
  publisher = {{Cambridge University Press}},
  issn = {0043-8871},
  doi = {10.2307/2010458},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Richardson_1972_Cold-War_Revisionism.pdf}
}

@book{riker1988,
  title = {Liberalism {{Against Populism}}: {{A Confrontation Between}} the {{Theory}} of {{Democracy}} and the {{Theory}} of {{Social Choice}}},
  shorttitle = {Liberalism {{Against Populism}}},
  author = {Riker, William H.},
  year = {1988},
  month = jul,
  publisher = {{Waveland Pr Inc}},
  address = {{Prospect Heights, Ill}},
  abstract = {The discoveries of social choice theory have undermined the simple and unrealistic 19th century notions of democracy, especially the expectation that electoral institutions smoothly translate popular will directly into public policy. One response to these discoveries is to reject democracy out of hand. Another, which is the program of this book, is to save democracy by formulating more realistic expectations. Hence, this book first summarizes social choice theory in order to explain the full force of its critique. Then it explains, in terms of social choice theory, how politics and public issues change and develop. Finally, it reconciles democratic ideals with this new understanding of politics.},
  isbn = {978-0-88133-367-1},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Riker_1988_Liberalism_Against_Populism.pdf}
}

@misc{rimella2017,
  title = {{{RGeode}}: {{Geometric Density Estimation}}},
  shorttitle = {{{RGeode}}},
  author = {Rimella, Lorenzo},
  year = {2017},
  month = sep,
  abstract = {Provides the hybrid Bayesian method Geometric Density Estimation. On the one hand, it scales the dimension of our data, on the other it performs inference. The method is fully described in the paper "Scalable Geometric Density Estimation" by Y. Wang, A. Canale, D. Dunson (2016) {$<$}http://proceedings.mlr.press/v51/wang16e.pdf{$>$}.},
  copyright = {GPL-2 | GPL-3 [expanded from: GPL ({$\geq$} 2)]},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Rimella_2017_RGeode.pdf}
}

@misc{rinaldi2020,
  title = {Notes on {{Convex Optimization}}},
  author = {Rinaldi, Francesco},
  year = {2020},
  file = {/home/dede/Zotero/storage/IWLN4UC6/Notes.pdf}
}

@book{rinne2008,
  title = {The {{Weibull Distribution}}: {{A Handbook}}},
  shorttitle = {The {{Weibull Distribution}}},
  author = {Rinne, Horst},
  year = {2008},
  month = nov,
  edition = {1st edition},
  publisher = {{Chapman and Hall/CRC}},
  address = {{Boca Raton}},
  isbn = {978-1-4200-8743-7},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Rinne_2008_The_Weibull_Distribution.pdf}
}

@book{ripley2008,
  title = {{Pattern Recognition and Neural Networks}},
  author = {Ripley, Brian D.},
  year = {2008},
  month = jan,
  edition = {1 edizione},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  abstract = {This 1996 book is a reliable account of the statistical framework for pattern recognition and machine learning. With unparalleled coverage and a wealth of case-studies this book gives valuable insight into both the theory and the enormously diverse applications (which can be found in remote sensing, astrophysics, engineering and medicine, for example). So that readers can develop their skills and understanding, many of the real data sets used in the book are available from the author's website: www.stats.ox.ac.uk/\textasciitilde ripley/PRbook/. For the same reason, many examples are included to illustrate real problems in pattern recognition. Unifying principles are highlighted, and the author gives an overview of the state of the subject, making the book valuable to experienced researchers in statistics, machine learning/artificial intelligence and engineering. The clear writing style means that the book is also a superb introduction for non-specialists.},
  isbn = {978-0-521-71770-0},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Ripley_2008_Pattern_Recognition_and_Neural_Networks.pdf}
}

@article{rippl2016,
  title = {Limit Laws of the Empirical {{Wasserstein}} Distance: {{Gaussian}} Distributions},
  shorttitle = {Limit Laws of the Empirical {{Wasserstein}} Distance},
  author = {Rippl, Thomas and Munk, Axel and Sturm, Anja},
  year = {2016},
  month = oct,
  journal = {Journal of Multivariate Analysis},
  volume = {151},
  pages = {90--109},
  issn = {0047-259X},
  doi = {10.1016/j.jmva.2016.06.005},
  abstract = {We derive central limit theorems for the Wasserstein distance between the empirical distributions of Gaussian samples. The cases are distinguished whether the underlying laws are the same or different. Results are based on the (quadratic) Fr\'echet differentiability of the Wasserstein distance in the gaussian case. Extensions to elliptically symmetric distributions are discussed as well as several applications such as bootstrap and statistical testing.},
  langid = {english},
  keywords = {Bootstrap,Delta method,Elliptically symmetric distribution,FrÃ©chet derivative,Goodness-of-fit,Limit theorem,Mallowâ€™s metric,Resolvent operator,todo,Transport metric},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Rippl_et_al_2016_Limit_laws_of_the_empirical_Wasserstein_distance.pdf;/home/dede/Zotero/storage/CDKS5GN9/rippl2016.pdf}
}

@article{ro2015,
  title = {Outlier Detection for High-Dimensional Data},
  author = {Ro, Kwangil and Zou, Changliang and Wang, Zhaojun and Yin, Guosheng},
  year = {2015},
  month = sep,
  journal = {Biometrika},
  volume = {102},
  number = {3},
  pages = {589--599},
  issn = {0006-3444},
  doi = {10.1093/biomet/asv021},
  abstract = {Outlier detection is an integral component of statistical modelling and estimation. For high-dimensional data, classical methods based on the Mahalanobis distance are usually not applicable. We propose an outlier detection procedure that replaces the classical minimum covariance determinant estimator with a high-breakdown minimum diagonal product estimator. The cut-off value is obtained from the asymptotic distribution of the distance, which enables us to control the Type I error and deliver robust outlier detection. Simulation studies show that the proposed method behaves well for high-dimensional data.},
  keywords = {toRead},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Ro_et_al_2015_Outlier_detection_for_high-dimensional_data.pdf;/home/dede/Zotero/storage/I5ABTZL6/2365528.html}
}

@article{robbins1951,
  title = {A {{Stochastic Approximation Method}}},
  author = {Robbins, Herbert and Monro, Sutton},
  year = {1951},
  month = sep,
  journal = {The Annals of Mathematical Statistics},
  volume = {22},
  number = {3},
  pages = {400--407},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177729586},
  abstract = {Let \$M(x)\$ denote the expected value at level \$x\$ of the response to a certain experiment. \$M(x)\$ is assumed to be a monotone function of \$x\$ but is unknown to the experimenter, and it is desired to find the solution \$x = \textbackslash theta\$ of the equation \$M(x) = \textbackslash alpha\$, where \$\textbackslash alpha\$ is a given constant. We give a method for making successive experiments at levels \$x\_1,x\_2,\textbackslash cdots\$ in such a way that \$x\_n\$ will tend to \$\textbackslash theta\$ in probability.},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Robbins_Monro_1951_A_Stochastic_Approximation_Method.pdf;/home/dede/Zotero/storage/HGYIJNZP/1177729586.html}
}

@article{robbins1970,
  title = {Statistical {{Methods Related}} to the {{Law}} of the {{Iterated Logarithm}}},
  author = {Robbins, Herbert},
  year = {1970},
  month = oct,
  journal = {The Annals of Mathematical Statistics},
  volume = {41},
  number = {5},
  pages = {1397--1409},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177696786},
  abstract = {The Annals of Mathematical Statistics},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Robbins_1970_Statistical_Methods_Related_to_the_Law_of_the_Iterated_Logarithm.pdf;/home/dede/Zotero/storage/PR3QC5XN/1177696786.html}
}

@article{robbins1970a,
  title = {Statistical {{Methods Related}} to the {{Law}} of the {{Iterated Logarithm}}},
  author = {Robbins, Herbert},
  year = {1970},
  journal = {The Annals of Mathematical Statistics},
  volume = {41},
  number = {5},
  pages = {1397--1409},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177696786},
  abstract = {The Annals of Mathematical Statistics},
  keywords = {doing},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Robbins_1970_Statistical_Methods_Related_to_the_Law_of_the_Iterated_Logarithm2.pdf;/home/dede/Zotero/storage/5JCRKEA8/1177696786.html}
}

@article{robbins1974,
  title = {The {{Expected Sample Size}} of {{Some Tests}} of {{Power One}}},
  author = {Robbins, H. and Siegmund, D.},
  year = {1974},
  month = may,
  journal = {The Annals of Statistics},
  volume = {2},
  number = {3},
  pages = {415--436},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/aos/1176342704},
  abstract = {Asymptotic approximations to the expected sample size are given for a class of tests of power one introduced in [10]. Comparisons are made with the method of mixtures of likelihood ratios, and an application is given to Breiman's gambling theory for favorable games.},
  keywords = {60G40,62L10,expected sample size,sequential tests,tests of power one},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Robbins_Siegmund_1974_The_Expected_Sample_Size_of_Some_Tests_of_Power_One.pdf;/home/dede/Zotero/storage/2N5KW4EJ/1176342704.html}
}

@book{robert2004,
  title = {{Monte Carlo Statistical Methods}},
  author = {Robert, Christian P. and Casella, George},
  year = {2004},
  month = sep,
  publisher = {{Springer Nature}},
  address = {{New York}},
  abstract = {We have sold 4300 copies worldwide of the first edition (1999).This new edition contains five completely new chapters covering new developments.},
  isbn = {978-0-387-21239-5},
  langid = {Inglese}
}

@book{robert2009,
  title = {{Introducing Monte Carlo Methods with R}},
  author = {Robert, Christian and Casella, George},
  year = {2009},
  month = dec,
  publisher = {{Springer}},
  address = {{New York}},
  abstract = {This book covers the main tools used in statistical simulation from a programmer's point of view, explaining the R implementation of each simulation technique and providing the output for better understanding and comparison.},
  isbn = {978-1-4419-1575-7},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Robert_Casella_2009_Introducing_Monte_Carlo_Methods_with_R.pdf}
}

@unpublished{robert2020,
  title = {Notes on {{Functional Analysis}}},
  author = {Robert, Fr{\'e}d{\'e}ric},
  year = {2020},
  abstract = {Notes from lectures given in SMI Perugia 2020},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Robert_2020_Notes_on_Functional_Analysis.pdf}
}

@article{roberts1996,
  title = {Exponential Convergence of {{Langevin}} Distributions and Their Discrete Approximations},
  author = {Roberts, Gareth O. and Tweedie, Richard L.},
  year = {1996},
  month = dec,
  journal = {Bernoulli},
  volume = {2},
  number = {4},
  pages = {341--363},
  publisher = {{Bernoulli Society for Mathematical Statistics and Probability}},
  issn = {1350-7265},
  abstract = {In this paper we consider a continuous-time method of approximating a given distribution {$\pi$} using the Langevin diffusion d L t=dW t+1 2 {$\nabla$}log{$\pi$}(L t)dt . We find conditions under which this diffusion converges exponentially quickly to {$\pi$} or does not: in one dimension, these are essentially that for distributions with exponential tails of the form {$\pi$} (x){$\propto$}exp(-{$\gamma$}|x| {$\beta$} ) , 0 {$<\beta<\infty$} , exponential convergence occurs if and only if {$\beta$} {$\geq$}1 . We then consider conditions under which the discrete approximations to the diffusion converge. We first show that even when the diffusion itself converges, naive discretizations need not do so. We then consider a 'Metropolis-adjusted' version of the algorithm, and find conditions under which this also converges at an exponential rate: perhaps surprisingly, even the Metropolized version need not converge exponentially fast even if the diffusion does. We briefly discuss a truncated form of the algorithm which, in practice, should avoid the difficulties of the other forms.},
  keywords = {Diffusions,discrete approximations,geometric ergodicity,Hastings algorithms,irreducible Markov processes,Langevin models,Markov chain Monte Carlo,Metropolis algorithms,posterior distributions,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Roberts_Tweedie_1996_Exponential_convergence_of_Langevin_distributions_and_their_discrete.pdf;/home/dede/Zotero/storage/XAAM9EN9/1178291835.html}
}

@article{robinson2020,
  title = {Tree-{{SNE}}: {{Hierarchical Clustering}} and {{Visualization Using}} t-{{SNE}}},
  shorttitle = {Tree-{{SNE}}},
  author = {Robinson, Isaac and {Pierce-Hoffman}, Emma},
  year = {2020},
  month = feb,
  journal = {arXiv:2002.05687 [cs, stat]},
  eprint = {2002.05687},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {t-SNE and hierarchical clustering are popular methods of exploratory data analysis, particularly in biology. Building on recent advances in speeding up t-SNE and obtaining finer-grained structure, we combine the two to create tree-SNE, a hierarchical clustering and visualization algorithm based on stacked one-dimensional t-SNE embeddings. We also introduce alpha-clustering, which recommends the optimal cluster assignment, without foreknowledge of the number of clusters, based off of the cluster stability across multiple scales. We demonstrate the effectiveness of tree-SNE and alpha-clustering on images of handwritten digits, mass cytometry (CyTOF) data from blood cells, and single-cell RNA-sequencing (scRNA-seq) data from retinal cells. Furthermore, to demonstrate the validity of the visualization, we use alpha-clustering to obtain unsupervised clustering results competitive with the state of the art on several image data sets. Software is available at https://github.com/isaacrob/treesne.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,data visualization,done,hierarchical clustering,nearest neighbors,Statistics - Machine Learning,t-SNE},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Robinson_Pierce-Hoffman_2020_Tree-SNE.pdf;/home/dede/Zotero/storage/6RAP4FA3/2002.html}
}

@book{rocha-arteaga2019,
  title = {Topics in {{Infinitely Divisible Distributions}} and {{L\'evy Processes}}},
  author = {{Rocha-Arteaga}, Alfonso and Sato, Ken-iti},
  year = {2019},
  series = {{{SpringerBriefs}} in {{Probability}} and {{Mathematical Statistics}}},
  edition = {Revised edition},
  publisher = {{Springer International Publishing}},
  doi = {10.1007/978-3-030-22700-5},
  abstract = {This book deals with topics in the area of L\'evy processes and infinitely divisible distributions such as Ornstein-Uhlenbeck type processes, selfsimilar additive processes and multivariate subordination. These topics are developed around a decreasing chain of classes of distributions Lm, m = 0,1,...,{$\infty$}, from the class L0 of selfdecomposable distributions to the class L{$\infty$} generated by stable distributions through convolution and convergence.The book is divided into five chapters. Chapter 1 studies basic properties of Lm classes needed for the subsequent chapters. Chapter 2 introduces Ornstein-Uhlenbeck type processes generated by a L\'evy process through stochastic integrals based on L\'evy processes. Necessary and sufficient conditions are given for a generating L\'evy process so that the OU type process has a limit distribution of Lm class.Chapter 3 establishes the correspondence between selfsimilar additive processes and selfdecomposable distributions and makes a close inspection of the Lamperti transformation, which transforms selfsimilar additive processes and stationary type OU processes to each other. Chapter 4 studies multivariate subordination of a cone-parameter L\'evy process by a cone-valued L\'evy process. Finally, Chapter 5 studies strictly stable and Lm properties inherited by the subordinated process in multivariate subordination.In this revised edition, new material is included on advances in these topics. It is rewritten as self-contained as possible. Theorems, lemmas, propositions, examples and remarks were reorganized; some were deleted and others were newly added. The historical notes at the end of each chapter were enlarged.This book is addressed to graduate students and researchers in probability and mathematical statistics who are interested in learning more on L\'evy processes and infinitely divisible distributions.},
  isbn = {978-3-030-22699-2},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Rocha-Arteaga_Sato_2019_Topics_in_Infinitely_Divisible_Distributions_and_LÃ©vy_Processes,_Revised_Edition.pdf;/home/dede/Zotero/storage/H7NL3IC8/9783030226992.html}
}

@article{rockova2014,
  title = {{{EMVS}}: {{The EM Approach}} to {{Bayesian Variable Selection}}},
  shorttitle = {{{EMVS}}},
  author = {Ro{\v c}kov{\'a}, Veronika and George, Edward I.},
  year = {2014},
  month = apr,
  journal = {Journal of the American Statistical Association},
  volume = {109},
  number = {506},
  pages = {828--846},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.2013.869223},
  abstract = {Despite rapid developments in stochastic search algorithms, the practicality of Bayesian variable selection methods has continued to pose challenges. High-dimensional data are now routinely analyzed, typically with many more covariates than observations. To broaden the applicability of Bayesian variable selection for such high-dimensional linear regression contexts, we propose EMVS, a deterministic alternative to stochastic search based on an EM algorithm which exploits a conjugate mixture prior formulation to quickly find posterior modes. Combining a spike-and-slab regularization diagram for the discovery of active predictor sets with subsequent rigorous evaluation of posterior model probabilities, EMVS rapidly identifies promising sparse high posterior probability submodels. External structural information such as likely covariate groupings or network topologies is easily incorporated into the EMVS framework. Deterministic annealing variants are seen to improve the effectiveness of our algorithms by mitigating the posterior multimodality associated with variable selection priors. The usefulness of the EMVS approach is demonstrated on real high-dimensional data, where computational complexity renders stochastic search to be less practical.},
  keywords = {Dynamic posterior exploration,High dimensionality,Regularization plots,Sparsity,SSVS},
  annotation = {\_eprint: https://doi.org/10.1080/01621459.2013.869223},
  file = {/home/dede/Zotero/storage/YIVE9DAI/01621459.2013.html}
}

@article{rockova2018,
  title = {The {{Spike-and-Slab LASSO}}},
  author = {Ro{\v c}kov{\'a}, Veronika and George, Edward I.},
  year = {2018},
  month = jan,
  journal = {Journal of the American Statistical Association},
  volume = {113},
  number = {521},
  pages = {431--444},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.2016.1260469},
  abstract = {Despite the wide adoption of spike-and-slab methodology for Bayesian variable selection, its potential for penalized likelihood estimation has largely been overlooked. In this article, we bridge this gap by cross-fertilizing these two paradigms with the Spike-and-Slab LASSO procedure for variable selection and parameter estimation in linear regression. We introduce a new class of self-adaptive penalty functions that arise from a fully Bayes spike-and-slab formulation, ultimately moving beyond the separable penalty framework. A virtue of these nonseparable penalties is their ability to borrow strength across coordinates, adapt to ensemble sparsity information and exert multiplicity adjustment. The Spike-and-Slab LASSO procedure harvests efficient coordinate-wise implementations with a path-following scheme for dynamic posterior exploration. We show on simulated data that the fully Bayes penalty mimics oracle performance, providing a viable alternative to cross-validation. We develop theory for the separable and nonseparable variants of the penalty, showing rate-optimality of the global mode as well as optimal posterior concentration when p {$>$} n. Supplementary materials for this article are available online.},
  keywords = {High-dimensional regression,LASSO,Penalized likelihood,Posterior concentration,skimmed,Spike-and-Slab,Variable selection},
  annotation = {\_eprint: https://doi.org/10.1080/01621459.2016.1260469},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/RoÄkovÃ¡_George_2018_The_Spike-and-Slab_LASSO.pdf;/home/dede/Zotero/storage/BTSFVHX5/01621459.2016.html}
}

@article{rockova2019,
  title = {Posterior {{Concentration}} for {{Bayesian Regression Trees}} and {{Forests}}},
  author = {Rockova, Veronika and {van der Pas}, Stephanie},
  year = {2019},
  month = jun,
  journal = {arXiv:1708.08734 [math, stat]},
  eprint = {1708.08734},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  abstract = {Since their inception in the 1980's, regression trees have been one of the more widely used non-parametric prediction methods. Tree-structured methods yield a histogram reconstruction of the regression surface, where the bins correspond to terminal nodes of recursive partitioning. Trees are powerful, yet susceptible to over-fitting. Strategies against overfitting have traditionally relied on pruning greedily grown trees. The Bayesian framework offers an alternative remedy against overfitting through priors. Roughly speaking, a good prior charges smaller trees where overfitting does not occur. While the consistency of random histograms, trees and their ensembles has been studied quite extensively, the theoretical understanding of the Bayesian counterparts has been missing. In this paper, we take a step towards understanding why/when do Bayesian trees and their ensembles not overfit. To address this question, we study the speed at which the posterior concentrates around the true smooth regression function. We propose a spike-and-tree variant of the popular Bayesian CART prior and establish new theoretical results showing that regression trees (and their ensembles) (a) are capable of recovering smooth regression surfaces, achieving optimal rates up to a log factor, (b) can adapt to the unknown level of smoothness and (c) can perform effective dimension reduction when p{$>$}n. These results provide a piece of missing theoretical evidence explaining why Bayesian trees (and additive variants thereof) have worked so well in practice.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Rockova_van_der_Pas_2019_Posterior_Concentration_for_Bayesian_Regression_Trees_and_Forests.pdf;/home/dede/Zotero/storage/FTCFJAWG/1708.html}
}

@article{rockova2021,
  title = {Dynamic {{Variable Selection}} with {{Spike-and-Slab Process Priors}}},
  author = {Rockova, Veronika and McAlinn, Kenichiro},
  year = {2021},
  month = mar,
  journal = {Bayesian Analysis},
  volume = {16},
  number = {1},
  pages = {233--269},
  publisher = {{International Society for Bayesian Analysis}},
  issn = {1936-0975, 1931-6690},
  doi = {10.1214/20-BA1199},
  abstract = {We address the problem of dynamic variable selection in time series regression with unknown residual variances, where the set of active predictors is allowed to evolve over time. To capture time-varying variable selection uncertainty, we introduce new dynamic shrinkage priors for the time series of regression coefficients. These priors are characterized by two main ingredients: smooth parameter evolutions and intermittent zeroes for modeling predictive breaks. More formally, our proposed Dynamic Spike-and-Slab (DSS) priors are constructed as mixtures of two processes: a spike process for the irrelevant coefficients and a slab autoregressive process for the active coefficients. The mixing weights are themselves time-varying and depend on lagged values of the series. Our DSS priors are probabilistically coherent in the sense that their stationary distribution is fully known and characterized by spike-and-slab marginals. For posterior sampling over dynamic regression coefficients, model selection indicators as well as unknown dynamic residual variances, we propose a Dynamic SSVS algorithm based on forward-filtering and backward-sampling. To scale our method to large data sets, we develop a Dynamic EMVS algorithm for MAP smoothing. We demonstrate, through simulation and a topical macroeconomic dataset, that DSS priors are very effective at separating active and noisy coefficients. Our fast implementation significantly extends the reach of spike-and-slab methods to big time series data.},
  keywords = {Autoregressive mixture processes,Dynamic sparsity,MAP smoothing,spike and slab,stationarity,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Rockova_McAlinn_2021_Dynamic_Variable_Selection_with_Spike-and-Slab_Process_Priors.pdf;/home/dede/Zotero/storage/D8WAMAC9/20-BA1199.html}
}

@article{rodriguez2011,
  title = {Nonparametric {{Bayesian}} Models through Probit Stick-Breaking Processes},
  author = {Rodr{\'i}guez, Abel and Dunson, David B.},
  year = {2011},
  month = mar,
  journal = {Bayesian Analysis},
  volume = {6},
  number = {1},
  pages = {145--177},
  publisher = {{International Society for Bayesian Analysis}},
  issn = {1936-0975, 1931-6690},
  doi = {10.1214/11-BA605},
  abstract = {We describe a novel class of Bayesian nonparametric priors based on stick-breaking constructions where the weights of the process are constructed as probit transformations of normal random variables. We show that these priors are extremely flexible, allowing us to generate a great variety of models while preserving computational simplicity. Particular emphasis is placed on the construction of rich temporal and spatial processes, which are applied to two problems in finance and ecology.},
  langid = {english},
  mrnumber = {MR2781811},
  zmnumber = {1330.62120},
  keywords = {Data Augmentation,done,Mixture Model,Nonparametric Bayes,Random Probability Measure,Spatial Data,Stick-breaking Prior,Time Series},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/RodrÃ­guez_Dunson_2011_Nonparametric_Bayesian_models_through_probit_stick-breaking_processes.pdf;/home/dede/Zotero/storage/DS43TGRB/1339611944.html}
}

@article{roes1999,
  title = {Effective Application of {{Q}}({{R}}) Charts in Low-Volume Manufacturing},
  author = {Roes, Kit C. B. and Does, Ronald J. M. M. and Jonkers, Bas S.},
  year = {1999},
  journal = {Quality and Reliability Engineering International},
  volume = {15},
  number = {3},
  pages = {175--190},
  issn = {1099-1638},
  doi = {10.1002/(SICI)1099-1638(199905/06)15:3<175::AID-QRE231>3.0.CO;2-S},
  abstract = {Q charts provide means for statistical process control in low-volume processes and start-up phases of production. Concerns on their performance have led to research into different types of enhancements and much discussion on the appropriateness of these. Driven by the aim to implement control charts in the low-volume production of advanced wafer steppers, we investigate the performance of additional run rules and tightening control limits on the traditional Q chart compared with an exponentially weighted moving average (EWMA). Furthermore, we develop an alternative QR chart based on the mean moving range as estimator of the process standard deviation and consider the economics of low-volume processes by means of a specific cost model. The comparisons are based on the run length distributions after a permanent shift and trend, both with an onset early in the process. Real life examples are given for various important variables in wafer stepper production. It is concluded that the EWMA based on QR statistics provides the best performance throughout. Competing alternatives with almost equal performance are the EWMA of Q statistics and the combination of four tests of special causes (1-of-1, 2-of-3, 4-of-5 and 8-of-8) applied on either the Q or QR chart. Overall, the mean moving range performs better. Copyright \textcopyright{} 1999 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {control charts,exponentially weighted moving average,moving range,Q chart,statistical process control},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/\%28SICI\%291099-1638\%28199905/06\%2915\%3A3\%3C175\%3A\%3AAID-QRE231\%3E3.0.CO\%3B2-S},
  file = {/home/dede/Zotero/storage/EWC8GE3G/06)153175AID-QRE2313.0.html}
}

@book{rogge2010,
  title = {Can {{Capitalism Survive}}?},
  author = {Rogge, Benjamin A.},
  year = {2010},
  month = feb,
  publisher = {{Liberty Fund Inc.}},
  address = {{Indianapolis}},
  isbn = {978-0-913966-46-4},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Rogge_2010_Can_Capitalism_Survive.pdf}
}

@article{rohe2011,
  title = {Spectral Clustering and the High-Dimensional Stochastic Blockmodel},
  author = {Rohe, Karl and Chatterjee, Sourav and Yu, Bin},
  year = {2011},
  month = aug,
  journal = {The Annals of Statistics},
  volume = {39},
  number = {4},
  pages = {1878--1915},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/11-AOS887},
  abstract = {Networks or graphs can easily represent a diverse set of data sources that are characterized by interacting units or actors. Social networks, representing people who communicate with each other, are one example. Communities or clusters of highly connected actors form an essential feature in the structure of several empirical networks. Spectral clustering is a popular and computationally feasible method to discover these communities. The stochastic blockmodel [Social Networks 5 (1983) 109\textendash 137] is a social network model with well-defined communities; each node is a member of one community. For a network generated from the Stochastic Blockmodel, we bound the number of nodes ``misclustered'' by spectral clustering. The asymptotic results in this paper are the first clustering results that allow the number of clusters in the model to grow with the number of nodes, hence the name high-dimensional. In order to study spectral clustering under the stochastic blockmodel, we first show that under the more general latent space model, the eigenvectors of the normalized graph Laplacian asymptotically converge to the eigenvectors of a ``population'' normalized graph Laplacian. Aside from the implication for spectral clustering, this provides insight into a graph visualization technique. Our method of studying the eigenvectors of random matrices is original.},
  keywords = {60B20,62H25,62H30,clustering,convergence of eigenvectors,Latent space model,principal components analysis,spectral clustering,stochastic blockmodel,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Rohe_et_al_2011_Spectral_clustering_and_the_high-dimensional_stochastic_blockmodel.pdf;/home/dede/Zotero/storage/2PTNFLKE/11-AOS887.html}
}

@article{rosenblatt2018,
  title = {All-{{Resolutions Inference}} for Brain Imaging},
  author = {Rosenblatt, Jonathan D. and Finos, Livio and Weeda, Wouter D. and Solari, Aldo and Goeman, Jelle J.},
  year = {2018},
  month = nov,
  journal = {NeuroImage},
  volume = {181},
  pages = {786--796},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2018.07.060},
  abstract = {The most prevalent approach to activation localization in neuroimaging is to identify brain regions as contiguous supra-threshold clusters, check their significance using random field theory, and correct for the multiple clusters being tested. Besides recent criticism on the validity of the random field assumption, a spatial specificity paradox remains: the larger the detected cluster, the less we know about the location of activation within that cluster. This is because cluster inference implies ``there exists at least one voxel with an evoked response in the cluster'', and not that ``all the voxels in the cluster have an evoked response''. Inference on voxels within selected clusters is considered bad practice, due to the voxel-wise false positive rate inflation associated with this circular inference. Here, we propose a remedy to the spatial specificity paradox. By applying recent results from the multiple testing statistical literature, we are able to quantify the proportion of truly active voxels within selected clusters, an approach we call All-Resolutions Inference (ARI). If this proportion is high, the paradox vanishes. If it is low, we can further ``drill down'' from the cluster level to sub-regions, and even to individual voxels, in order to pinpoint the origin of the activation. In fact, ARI allows inference on the proportion of activation in all voxel sets, no matter how large or small, however these have been selected, all from the same data. We use two fMRI datasets to demonstrate the non-triviality of the spatial specificity paradox, and its resolution using ARI. We verify that the endless circularity permitted by ARI does not render its estimates overly conservative using both simulation, and a data split.},
  langid = {english},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Rosenblatt_et_al_2018_All-Resolutions_Inference_for_brain_imaging.pdf;/home/dede/Zotero/storage/8M58TVF6/S105381191830675X.html}
}

@article{ross2011,
  title = {Nonparametric {{Monitoring}} of {{Data Streams}} for {{Changes}} in {{Location}} and {{Scale}}},
  author = {Ross, Gordon J. and Tasoulis, Dimitris K. and Adams, Niall M.},
  year = {2011},
  journal = {Technometrics},
  volume = {53},
  number = {4},
  pages = {379--389},
  publisher = {{Taylor \& Francis, Ltd.}},
  issn = {0040-1706},
  abstract = {The analysis of data streams requires methods which can cope with a very high volume of data points. Under the requirement that algorithms must have constant computational complexity and a fixed amount of memory, we develop a framework for detecting changes in data streams when the distributional form of the stream variables is unknown. We consider the general problem of detecting a change in the location and/or scale parameter of a stream of random variables, and adapt several nonparametric hypothesis tests to create a streaming change detection algorithm. This algorithm uses a test statistic with a null distribution independent of the data. This allows a desired rate of false alarms to be maintained for any stream even when its distribution is unknown. Our method is based on hypothesis tests which involve ranking data points, and we propose a method for calculating these ranks online in a manner which respects the constraints of data stream analysis.}
}

@article{rothermel2020,
  title = {Global\textendash Local Dynamics in Anti-Feminist Discourses: An Analysis of {{Indian}}, {{Russian}} and {{US}} Online Communities},
  shorttitle = {Global\textendash Local Dynamics in Anti-Feminist Discourses},
  author = {Rothermel, Ann-Kathrin},
  year = {2020},
  month = sep,
  journal = {International Affairs},
  volume = {96},
  number = {5},
  pages = {1367--1385},
  issn = {0020-5850},
  doi = {10.1093/ia/iiaa130},
  abstract = {Women's rights are a core part of a global consensus on human rights. However, we are currently experiencing an increasing popularity of anti-feminist and misogynist politics threatening to override feminist gains. In order to help explain this current revival and appeal, in this article I analyse how anti-feminist communities construct their collective identities at the intersection of local and global trends and affiliations. Through an in-depth analysis of representations in the collective identities of six popular online anti-feminist communities based in India, Russia and the United States, I shed light on how anti-feminists discursively construct their anti-feminist `self' and the feminist `other' between narratives of localized resistance to change and backlash against the results of broader societal developments associated with globalization. The results expose a complex set of global\textendash local dynamics, which provide a nuanced understanding of the differences and commonalities of anti-feminist collective identity-building and mobilization processes across contexts. By explicitly focusing on the role of discursively produced locations for anti-feminist identity-building and providing new evidence on anti-feminist communities across three different continents, the article contributes to current discussions on transnational anti-feminist mobilizations in both social movement studies and feminist International Relations.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Rothermel_2020_Globalâ€“local_dynamics_in_anti-feminist_discourses.pdf;/home/dede/Zotero/storage/MZB7FRJT/5875744.html}
}

@book{rudin1976,
  title = {Principles of {{Mathematical Analysis}}},
  author = {Rudin, Walter},
  year = {1976},
  month = jan,
  edition = {3 edition},
  publisher = {{McGraw-Hill Education}},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Rudin_1976_Principles_of_Mathematical_Analysis.pdf}
}

@book{rudin1986,
  title = {Real and {{Complex Analysis}}},
  author = {Rudin, Walter},
  year = {1986},
  month = may,
  edition = {3rd edition},
  publisher = {{McGraw-Hill Education}},
  address = {{New York}},
  abstract = {This is an advanced text for the one- or two-semester course in analysis taught primarily to math, science, computer science, and electrical engineering majors at the junior, senior or graduate level. The basic techniques and theorems of analysis are presented in such a way that the intimate connections between its various branches are strongly emphasized. The traditionally separate subjects of 'real analysis' and 'complex analysis' are thus united in one volume. Some of the basic ideas from functional analysis are also included. This is the only book to take this unique approach. The third edition includes a new chapter on differentiation. Proofs of theorems presented in the book are concise and complete and many challenging exercises appear at the end of each chapter. The book is arranged so that each chapter builds upon the other, giving students a gradual understanding of the subject.  This text is part of the Walter Rudin Student Series in Advanced Mathematics.},
  isbn = {978-0-07-054234-1},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Rudin_1986_Real_and_Complex_Analysis.pdf}
}

@article{rue2017,
  title = {Bayesian {{Computing}} with {{INLA}}: {{A Review}}},
  shorttitle = {Bayesian {{Computing}} with {{INLA}}},
  author = {Rue, H{\aa}vard and Riebler, Andrea and S{\o}rbye, Sigrunn H. and Illian, Janine B. and Simpson, Daniel P. and Lindgren, Finn K.},
  year = {2017},
  journal = {Annual Review of Statistics and Its Application},
  volume = {4},
  number = {1},
  pages = {395--421},
  doi = {10.1146/annurev-statistics-060116-054045},
  abstract = {The key operation in Bayesian inference is to compute high-dimensional integrals. An old approximate technique is the Laplace method or approximation, which dates back to Pierre-Simon Laplace (1774). This simple idea approximates the integrand with a second-order Taylor expansion around the mode and computes the integral analytically. By developing a nested version of this classical idea, combined with modern numerical techniques for sparse matrices, we obtain the approach of integrated nested Laplace approximations (INLA) to do approximate Bayesian inference for latent Gaussian models (LGMs). LGMs represent an important model abstraction for Bayesian inference and include a large proportion of the statistical models used today. In this review, we discuss the reasons for the success of the INLA approach, the R-INLA package, why it is so accurate, why the approximations are very quick to compute, and why LGMs make such a useful concept for Bayesian computing.},
  keywords = {approximate Bayesian inference,doing,Gaussian Markov random fields,Laplace approximations,latent Gaussian models,numerical integration,sparse matrices},
  annotation = {\_eprint: https://doi.org/10.1146/annurev-statistics-060116-054045},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Rue_et_al_2017_Bayesian_Computing_with_INLA.pdf}
}

@article{ruppert1995,
  title = {An {{Effective Bandwidth Selector}} for {{Local Least Squares Regression}}},
  author = {Ruppert, D. and Sheather, S. J. and Wand, M. P.},
  year = {1995},
  journal = {Journal of the American Statistical Association},
  volume = {90},
  number = {432},
  pages = {1257--1270},
  publisher = {{[American Statistical Association, Taylor \& Francis, Ltd.]}},
  issn = {0162-1459},
  doi = {10.2307/2291516},
  abstract = {Local least squares kernel regression provides an appealing solution to the nonparametric regression, or "scatterplot smoothing," problem, as demonstrated by Fan, for example. The practical implementation of any scatterplot smoother is greatly enhanced by the availability of a reliable rule for automatic selection of the smoothing parameter. In this article we apply the ideas of plug-in bandwidth selection to develop strategies for choosing the smoothing parameter of local linear squares kernel estimators. Our results are applicable to odd-degree local polynomial fits and can be extended to other settings, such as derivative estimation and multiple nonparametric regression. An implementation in the important case of local linear fits with univariate predictors is shown to perform well in practice. A by-product of our work is the development of a class of nonparametric variance estimators, based on local least squares ideas, and plug-in rules for their implementation.}
}

@article{ruppert1997,
  title = {Empirical-{{Bias Bandwidths}} for {{Local Polynomial Nonparametric Regression}} and {{Density Estimation}}},
  author = {Ruppert, David},
  year = {1997},
  month = sep,
  journal = {Journal of the American Statistical Association},
  volume = {92},
  number = {439},
  pages = {1049--1062},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.1997.10474061},
  abstract = {A data-based local bandwidth selector is proposed for nonparametric regression by local fitting of polynomials. The estimator, called the empirical-bias bandwidth selector (EBBS), is rather simple and easily allows multivariate predictor variables and estimation of any order derivative of the regression function. EBBS minimizes an estimate of mean squared error consisting of a squared bias term plus a variance term. The variance term used is exact, not asymptotic, though it involves the conditional variance of the response given the predictors that must be estimated. The bias term is estimated empirically, not from an asymptotic expression. Thus EBBS is similar to the ``double smoothing'' approach of H\"ardle, Hall, and Marron and a local bandwidth selector of Schucany, but is developed here for a far wider class of estimation problems than what those authors considered. EBBS is tested on simulated data, and its performance seems quite satisfactory. Local polynomial smoothing of a histogram is a highly effective technique for density estimation, and several of the examples involve density estimation by EBBS applied to binned data.},
  keywords = {Curve and surface fitting,Derivative estimation,Heteroscedasticity,Local bandwidth,Local regression,Variance function estimation},
  annotation = {\_eprint: https://doi.org/10.1080/01621459.1997.10474061},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Ruppert_1997_Empirical-Bias_Bandwidths_for_Local_Polynomial_Nonparametric_Regression_and.pdf;/home/dede/Zotero/storage/EIAGEZGQ/01621459.1997.html}
}

@article{russo2020,
  title = {A {{Tutorial}} on {{Thompson Sampling}}},
  author = {Russo, Daniel and Van Roy, Benjamin and Kazerouni, Abbas and Osband, Ian and Wen, Zheng},
  year = {2020},
  month = jul,
  journal = {arXiv:1707.02038 [cs]},
  eprint = {1707.02038},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Thompson sampling is an algorithm for online decision problems where actions are taken sequentially in a manner that must balance between exploiting what is known to maximize immediate performance and investing to accumulate new information that may improve future performance. The algorithm addresses a broad range of problems in a computationally efficient manner and is therefore enjoying wide use. This tutorial covers the algorithm and its application, illustrating concepts through a range of examples, including Bernoulli bandit problems, shortest path problems, product recommendation, assortment, active learning with neural networks, and reinforcement learning in Markov decision processes. Most of these problems involve complex information structures, where information revealed by taking an action informs beliefs about other actions. We will also discuss when and why Thompson sampling is or is not effective and relations to alternative algorithms.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Russo_et_al_2020_A_Tutorial_on_Thompson_Sampling2.pdf;/home/dede/Zotero/storage/RK2NTQ9Q/1707.html}
}

@article{ryan2019,
  title = {The Disciplined Sea: A History of Maritime Security and Zonation},
  shorttitle = {The Disciplined Sea},
  author = {Ryan, Barry J.},
  year = {2019},
  month = sep,
  journal = {International Affairs},
  volume = {95},
  number = {5},
  pages = {1055--1073},
  issn = {0020-5850},
  doi = {10.1093/ia/iiz098},
  abstract = {This article details the evolution of maritime security from the perspective of its impact on the historical architecture of sea space. It argues that, as the fundamental unit of governance, zoning provides keen insight into the mechanics of maritime security. The article observes that Britain's Hovering Acts in the late eighteenth century represent the earliest example of modern zonation at sea and that they exhibit a shift from early modern territorial claims based on imperium and dominium. The article explores the way these hovering zones shaped the rationale underlying contemporary maritime security. It finds that maritime security has effectively relegated national security to a minor spatial belt of state power, while elevating non-traditional understandings of security to the level of global existential threat. The future of maritime security is under construction. Increasingly segmented by interconnecting, overlapping, multi-functional zones that seek to regulate all free movement and usage of the sea, security developments are reorganizing the maritime sphere. Nonetheless, the article argues, despite the novelty of this development, a historical military logic persists in new formations of security-oriented practices of maritime governance.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Ryan_2019_The_disciplined_sea.pdf}
}

@book{rynne2007,
  title = {Linear {{Functional Analysis}}},
  author = {Rynne, Bryan and Youngson, M. A. and Youngson, Martin A.},
  year = {2007},
  month = dec,
  edition = {2 edition},
  publisher = {{Springer}},
  abstract = {This introduction to the ideas and methods of linear functional analysis shows how familiar and useful concepts from finite-dimensional linear algebra can be extended or generalized to infinite-dimensional spaces. Aimed at advanced undergraduates in mathematics and physics, the book assumes a standard background of linear algebra, real analysis (including the theory of metric spaces), and Lebesgue integration, although an introductory chapter summarizes the requisite material. A highlight of the second edition is a new chapter on the Hahn-Banach theorem and its applications to the theory of duality.},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Rynne_et_al_2007_Linear_Functional_Analysis.pdf}
}

@book{sagan1994,
  title = {Space-{{Filling Curves}}},
  author = {Sagan, Hans},
  year = {1994},
  series = {Universitext},
  publisher = {{Springer-Verlag}},
  doi = {10.1007/978-1-4612-0871-6},
  abstract = {The subject of space-filling curves has fascinated mathematicians for over a century and has intrigued many generations of students of mathematics. Working in this area is like skating on the edge of reason. Unfortunately, no comprehensive treatment has ever been attempted other than the gallant effort by W. Sierpiriski in 1912. At that time, the subject was still in its infancy and the most interesting and perplexing results were still to come. Besides, Sierpiriski's paper was written in Polish and published in a journal that is not readily accessible (Sierpiriski [2]). Most of the early literature on the subject is in French, German, and Polish, providing an additional raison d'etre for a comprehensive treatment in English. While there was, understandably, some intensive research activity on this subject around the turn of the century, contributions have, nevertheless, continued up to the present and there is no end in sight, indicating that the subject is still very much alive. The recent interest in fractals has refocused interest on space\- filling curves, and the study of fractals has thrown some new light on this small but venerable part of mathematics. This monograph is neither a textbook nor an encyclopedic treatment of the subject nor a historical account, but it is a little of each. While it may lend structure to a seminar or pro-seminar, or be useful as a supplement in a course on topology or mathematical analysis, it is primarily intended for self-study by the aficionados of classical analysis.},
  isbn = {978-0-387-94265-0},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Sagan_1994_Space-Filling_Curves.pdf;/home/dede/Zotero/storage/DNFM5ZAJ/9780387942650.html}
}

@article{saghir2015,
  title = {Phase-{{I Design Scheme}} for {{X-chart Based}} on {{Posterior Distribution}}},
  author = {Saghir, Aamir},
  year = {2015},
  month = feb,
  journal = {Communications in Statistics - Theory and Methods},
  volume = {44},
  number = {3},
  pages = {644--655},
  publisher = {{Taylor \& Francis}},
  issn = {0361-0926},
  doi = {10.1080/03610926.2012.752846},
  abstract = {This article develops a Phase I design structure of -Chart, namely Bayesian -Chart, based on Bayesian (posterior distribution) framework assuming the normality of the quality characteristic to incorporate parameter uncertainty. Our approach consists of two stages: (i) construction of the control limits for -Chart based on posterior distribution of unknown mean {$\mu$} and (ii) evaluation of the performance of the proposed design structure. The proposed design structure of -Chart is compared with the frequents design structure of - Chart in terms of (i) width of the control region and (ii) power of detecting a shift in the location parameter of the process. It has been observed that the proposed design structure of -Chart is performs better than the usual design structure to detecting shifts in the parameter of the process when the prior mean is close to the unknown target value.},
  keywords = {Bayesian Analysis,Control charts,Posterior distribution,Power Curve,Primary 62P30,Secondary 62F10,todo},
  annotation = {\_eprint: https://doi.org/10.1080/03610926.2012.752846},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Saghir_2015_Phase-I_Design_Scheme_for_X-chart_Based_on_Posterior_Distribution.pdf;/home/dede/Zotero/storage/G6YDXA2V/03610926.2012.html}
}

@article{sakwa2015,
  title = {The Death of {{Europe}}? {{Continental}} Fates after {{Ukraine}}},
  shorttitle = {The Death of {{Europe}}?},
  author = {SAKWA, RICHARD},
  year = {2015},
  month = may,
  journal = {International Affairs},
  volume = {91},
  number = {3},
  pages = {553--579},
  issn = {0020-5850},
  doi = {10.1111/1468-2346.12281},
  abstract = {The unravelling of the post-Cold War security order in Europe was both cause and consequence of the crisis in Ukraine. The crisis was a symptom of the three-fold failure to achieve the aspirations to create a `Europe whole and free' enunciated by the Charter of Paris in 1990, the drift in the European Union's behaviour from normative to geopolitical concerns, and the failure to institutionalize some form of pan-continental unity. The structural failure to create a framework for normative and geopolitical pluralism on the continent meant that Russia was excluded from the new European order. No mode of reconciliation was found between the Brussels-centred wider Europe and various ideas for greater European continental unification. Russia's relations with the EU became increasingly tense in the context of the Eastern Partnership and the Association Agreement with Ukraine. The EU and the Atlantic alliance moved towards a more hermetic and universal form of Atlanticism. Although there remain profound differences between the EU and its trans-Atlantic partner and tensions between member states, the new Atlanticism threatens to subvert the EU's own normative principles. At the same time, Russia moved from a relatively complaisant approach to Atlanticism towards a more critical neo-revisionism, although it does not challenge the legal or normative intellectual foundations of international order. This raises the question of whether we can speak of the `death of Europe' as a project intended to transcend the logic of conflict on the continent.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/SAKWA_2015_The_death_of_Europe.pdf;/home/dede/Zotero/storage/QCJ3KHKG/2326843.html}
}

@article{saleh2016,
  title = {{{CUSUM}} Charts with Controlled Conditional Performance under Estimated Parameters},
  author = {Saleh, Nesma A. and Zwetsloot, Inez M. and Mahmoud, Mahmoud A. and Woodall, William H.},
  year = {2016},
  journal = {Quality Engineering},
  volume = {28},
  number = {4},
  pages = {402--415},
  publisher = {{Taylor \& Francis}},
  issn = {0898-2112},
  doi = {10.1080/08982112.2016.1144072},
  abstract = {We study the effect of the Phase I estimation error on the cumulative sum (CUSUM) chart. Impractically large amounts of Phase I data are needed to sufficiently reduce the variation in the in-control average run lengths (ARL) between practitioners. To reduce the effect of estimation error on the chart's performance we design the CUSUM chart such that the in-control ARL exceeds a desired value with a specified probability. This is achieved by adjusting the control limits using a bootstrap-based design technique. Such approach does affect the out-of-control performance of the chart; however, we find that this effect is relatively small.},
  keywords = {average of ARL (AARL),bootstrap,done,effect of estimation error,standard deviation of average run length (SDARL),statistical process control (SPC),statistical process monitoring (SPM)},
  annotation = {\_eprint: https://doi.org/10.1080/08982112.2016.1144072},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Saleh_et_al_2016_CUSUM_charts_with_controlled_conditional_performance_under_estimated_parameters.pdf;/home/dede/Zotero/storage/DABUCYPL/08982112.2016.html}
}

@article{saleh2021,
  title = {A {{Review}} and {{Critique}} of {{Auxiliary Information-Based Process Monitoring Methods}}},
  author = {Saleh, Nesma A. and Mahmoud, Mahmoud A. and Woodall, William H. and Knoth, Sven},
  year = {2021},
  month = oct,
  abstract = {We review the rapidly growing literature on auxiliary information-based (AIB) process monitoring methods. Under this approach, there is an assumption that the auxiliary variable, which is correlated with the quality variable of interest, has a known mean, or some other parameter, which cannot change over time. We demonstrate that violations of this assumption can have serious adverse effects both when the process is stable and when there has been a process shift. Some process shifts can become undetectable. We also show that the basic AIB approach is a special case of simple linear regression profile monitoring. The AIB charting techniques require strong assumptions. Based on our results, we warn against the use of AIB approach in quality control applications.},
  langid = {english},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Saleh_et_al_2021_A_Review_and_Critique_of_Auxiliary_Information-Based_Process_Monitoring_Methods.pdf}
}

@book{salvan2020,
  title = {{Modelli Lineari Generalizzati}},
  author = {Salvan, Alessandra and Sartori, Nicola and Pace, Luigi},
  year = {2020},
  series = {{La Matematica per il 3+2}},
  publisher = {{Springer-Verlag}},
  address = {{Mailand}},
  doi = {10.1007/978-88-470-4002-1},
  abstract = {Il volume fornisce un'introduzione a teoria e applicazioni dei modelli lineari generalizzati. Si presentano modelli di regressione per risposte continue, binarie, categoriali e di conteggio. Si offre anche un'introduzione ai modelli per risposte correlate. Utilizzando il software statistico R, vengono forniti gli strumenti per l'analisi dei dati tramite i diversi modelli parametrici e semiparametrici. Gli esempi con R alla fine di ciascun capitolo rappresentano una guida ad esercitazioni con il computer e richiedono una partecipazione attiva nello svolgere le analisi proposte. Numerosi esercizi concludono ogni capitolo. Il taglio adottato \`e funzionale ad approfondire in modo integrato aspetti teorici e applicativi. Unico nel suo genere, \`e rivolto agli studenti di Scienze Statistiche.},
  isbn = {978-88-470-4001-4},
  langid = {italian},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Salvan_et_al_2020_Modelli_Lineari_Generalizzati.pdf;/home/dede/Zotero/storage/TCMCS5XW/9788847040014.html}
}

@article{sanderson2016,
  title = {Armadillo: A Template-Based {{C}}++ Library for Linear Algebra},
  shorttitle = {Armadillo},
  author = {Sanderson, Conrad and Curtin, Ryan},
  year = {2016},
  month = jun,
  journal = {Journal of Open Source Software},
  volume = {1},
  number = {2},
  pages = {26},
  issn = {2475-9066},
  doi = {10.21105/joss.00026},
  abstract = {Sanderson et al, (2016), Armadillo: a template-based C++ library for linear algebra, Journal of Open Source Software, 1(2), 26, doi:10.21105/joss.00026},
  langid = {english},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Sanderson_Curtin_2016_Armadillo.pdf;/home/dede/Zotero/storage/8BY5T383/joss.html}
}

@article{sanderson2019,
  title = {A {{User-Friendly Hybrid Sparse Matrix Class}} in {{C}}++},
  author = {Sanderson, Conrad and Curtin, Ryan},
  year = {2019},
  month = oct,
  journal = {arXiv:1805.03380 [cs]},
  eprint = {1805.03380},
  eprinttype = {arxiv},
  primaryclass = {cs},
  doi = {10.1007/978-3-319-96418-8_50},
  abstract = {When implementing functionality which requires sparse matrices, there are numerous storage formats to choose from, each with advantages and disadvantages. To achieve good performance, several formats may need to be used in one program, requiring explicit selection and conversion between the formats. This can be both tedious and error-prone, especially for non-expert users. Motivated by this issue, we present a user-friendly sparse matrix class for the C++ language, with a high-level application programming interface deliberately similar to the widely used MATLAB language. The class internally uses two main approaches to achieve efficient execution: (i) a hybrid storage framework, which automatically and seamlessly switches between three underlying storage formats (compressed sparse column, coordinate list, Red-Black tree) depending on which format is best suited for specific operations, and (ii) template-based meta-programming to automatically detect and optimise execution of common expression patterns. To facilitate relatively quick conversion of research code into production environments, the class and its associated functions provide a suite of essential sparse linear algebra functionality (eg., arithmetic operations, submatrix manipulation) as well as high-level functions for sparse eigendecompositions and linear equation solvers. The latter are achieved by providing easy-to-use abstractions of the low-level ARPACK and SuperLU libraries. The source code is open and provided under the permissive Apache 2.0 license, allowing unencumbered use in commercial products.},
  archiveprefix = {arXiv},
  keywords = {65F50; 97H60; 68N99; 68P05; 97N80,Computer Science - Mathematical Software,E.1,G.1.3,G.4,H.3.4,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Sanderson_Curtin_2019_A_User-Friendly_Hybrid_Sparse_Matrix_Class_in_C++.pdf;/home/dede/Zotero/storage/45RMS5H9/1805.html}
}

@book{santambrogio2015,
  title = {Optimal {{Transport}} for {{Applied Mathematicians}}},
  author = {Santambrogio, Filippo},
  year = {2015},
  series = {Progress in {{Nonlinear Differential Equations}} and {{Their Applications}}},
  volume = {87},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-20828-2},
  isbn = {978-3-319-20827-5 978-3-319-20828-2},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Santambrogio_2015_Optimal_Transport_for_Applied_Mathematicians.pdf}
}

@article{santambrogio2018,
  title = {A Short Story on Optimal Transport and Its Many Applications},
  author = {Santambrogio, Filippo},
  year = {2018},
  journal = {Snapshots of modern mathematics from Oberwolfach;2018},
  pages = {13},
  publisher = {{Mathematisches Forschungsinstitut Oberwolfach}},
  doi = {10.14760/SNAP-2018-013-EN},
  abstract = {We present some examples of optimal transport problems and of applications to different sciences (logistics, economics, image processing, and a little bit of evolution equations) through the crazy story of an industrial dynasty regularly asking advice from an exotic mathematician.},
  collaborator = {Bruschi, David Edward and Kronberg, Danny and Cederbaum, Carla},
  copyright = {Creative Commons Attribution-ShareAlike 4.0 International License},
  langid = {english},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Santambrogio_2018_A_short_story_on_optimal_transport_and_its_many_applications.pdf}
}

@book{sarndal2003,
  title = {Model {{Assisted Survey Sampling}}},
  author = {S{\"a}rndal, Carl-Erik and Swensson, Bengt and Wretman, Jan},
  year = {2003},
  month = oct,
  publisher = {{Springer}},
  address = {{New York Berlin Heidelberg}},
  isbn = {978-0-387-40620-6},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/SÃ¤rndal_et_al_2003_Model_Assisted_Survey_Sampling.pdf}
}

@book{savage1972,
  title = {The Foundations of Statistics},
  author = {Savage, Leonard J.},
  year = {1972},
  publisher = {{Courier Corporation}},
  abstract = {Classic analysis of the foundations of statistics and development of personal probability, one of the greatest controversies in modern statistical thought. Revised edition. Calculus, probability, statistics, and Boolean algebra are recommended.},
  googlebooks = {zSv6dBWneMEC},
  isbn = {978-0-486-62349-8},
  langid = {english},
  keywords = {Mathematics / General,Mathematics / Probability \& Statistics / General,Social Science / Statistics}
}

@inproceedings{saxena2020,
  title = {Thompson {{Sampling}} for {{Linearly Constrained Bandits}}},
  booktitle = {23rd {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}} ({{AISTATS}}), 26-28 {{August}} 2020},
  author = {Saxena, Vidit and Gonzalez, Joseph E. and Jald{\'e}n, Joakim},
  year = {2020},
  pages = {1999--2009},
  publisher = {{ML Research Press}},
  abstract = {We address multi-armed bandits (MAB) where the objective is to maximize the cumulative reward under a probabilistic linear constraint. For a few real-world instances of this problem, constrained ex ...},
  langid = {english},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Saxena_et_al_2020_Thompson_Sampling_for_Linearly_Constrained_Bandits2.pdf;/home/dede/Zotero/storage/SVM4DC4B/record.html}
}

@article{scardapane2018,
  title = {Bayesian {{Random Vector Functional-Link Networks}} for {{Robust Data Modeling}}},
  author = {Scardapane, Simone and Wang, Dianhui and Uncini, Aurelio},
  year = {2018},
  month = jul,
  journal = {IEEE Transactions on Cybernetics},
  volume = {48},
  number = {7},
  pages = {2049--2059},
  issn = {2168-2275},
  doi = {10.1109/TCYB.2017.2726143},
  abstract = {Random vector functional-link (RVFL) networks are randomized multilayer perceptrons with a single hidden layer and a linear output layer, which can be trained by solving a linear modeling problem. In particular, they are generally trained using a closed-form solution of the (regularized) least-squares approach. This paper introduces several alternative strategies for performing full Bayesian inference (BI) of RVFL networks. Distinct from standard or classical approaches, our proposed Bayesian training algorithms allow to derive an entire probability distribution over the optimal output weights of the network, instead of a single pointwise estimate according to some given criterion (e.g., least-squares). This provides several known advantages, including the possibility of introducing additional prior knowledge in the training process, the availability of an uncertainty measure during the test phase, and the capability of automatically inferring hyper-parameters from given data. In this paper, two BI algorithms for regression are first proposed that, under some practical assumptions, can be implemented by a simple iterative process with closed-form computations. Simulation results show that one of the proposed algorithms, Bayesian RVFL, is able to outperform standard training algorithms for RVFL networks with a proper regularization factor selected carefully via a line search procedure. A general strategy based on variational inference is also presented, with an application to data modeling problems with noisy outputs or outliers. As we discuss in this paper, using recent advances in automatic differentiation this strategy can be applied to a wide range of additional situations in an immediate fashion.},
  keywords = {automatic differentiation this strategy,Bayes methods,Bayesian inference,Bayesian inference (BI),Bayesian random vector functional-link networks,Bayesian RVFL,Bayesian training algorithms,BI algorithms,closed-form computations,Computational modeling,data handling,Data models,done,Inference algorithms,inference mechanisms,iterative methods,iterative process,learning (artificial intelligence),least squares approximations,least-squares approach,linear output layer,multilayer perceptrons,probability distribution,random vector functional-link (RVFL),relevance vector machine (RVM),robust data modeling,Robustness,single hidden layer,standard training algorithms,Standards,statistical distributions,Training,variational inference,vectors},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Scardapane_et_al_2018_Bayesian_Random_Vector_Functional-Link_Networks_for_Robust_Data_Modeling.pdf;/home/dede/Zotero/storage/NL75U6EV/7990373.html}
}

@article{schifano2016,
  title = {Online {{Updating}} of {{Statistical Inference}} in the {{Big Data Setting}}},
  author = {Schifano, Elizabeth D. and Wu, Jing and Wang, Chun and Yan, Jun and Chen, Ming-Hui},
  year = {2016},
  journal = {Technometrics: A Journal of Statistics for the Physical, Chemical, and Engineering Sciences},
  volume = {58},
  number = {3},
  pages = {393--403},
  issn = {0040-1706},
  doi = {10.1080/00401706.2016.1142900},
  abstract = {We present statistical methods for big data arising from online analytical processing, where large amounts of data arrive in streams and require fast analysis without storage/access to the historical data. In particular, we develop iterative estimating algorithms and statistical inferences for linear models and estimating equations that update as new data arrive. These algorithms are computationally efficient, minimally storage-intensive, and allow for possible rank deficiencies in the subset design matrices due to rare-event covariates. Within the linear model setting, the proposed online-updating framework leads to predictive residual tests that can be used to assess the goodness-of-fit of the hypothesized model. We also propose a new online-updating estimator under the estimating equation setting. Theoretical properties of the goodness-of-fit tests and proposed estimators are examined in detail. In simulation studies and real data applications, our estimator compares favorably with competing approaches under the estimating equation setting.},
  langid = {english},
  pmcid = {PMC5179229},
  pmid = {28018007},
  keywords = {data compression,data streams,estimating equations,linear regression models,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Schifano_et_al_2016_Online_Updating_of_Statistical_Inference_in_the_Big_Data_Setting.pdf}
}

@article{schmidt-hieber2017,
  title = {Nonparametric Regression Using Deep Neural Networks with {{ReLU}} Activation Function},
  author = {{Schmidt-Hieber}, Johannes},
  year = {2017},
  month = aug,
  journal = {Annals of Statistics},
  volume = {48},
  doi = {10.1214/19-AOS1875},
  abstract = {Consider the multivariate nonparametric regression model. It is shown that estimators based on sparsely connected deep neural networks with ReLU activation function and properly chosen network architecture achieve the minimax rates of convergence (up to log n-factors) under a general composition assumption on the regression function. The framework includes many well-studied structural constraints such as (generalized) additive models. While there is a lot of flexibility in the network architecture, the tuning parameter is the sparsity of the network. Specifically, we consider large networks with number of potential parameters being much bigger than the sample size. The analysis gives some insights why multilayer feedforward neural networks perform well in practice. Interestingly, the depth (number of layers) of the neural network architectures plays an important role and our theory suggests that scaling the network depth with the logarithm of the sample size is natural.},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Schmidt-Hieber_2017_Nonparametric_regression_using_deep_neural_networks_with_ReLU_activation.pdf}
}

@article{schmidt2013,
  title = {Nonparametric {{Bayesian}} Modeling of Complex Networks: An Introduction},
  shorttitle = {Nonparametric {{Bayesian}} Modeling of Complex Networks},
  author = {Schmidt, Mikkel N. and Morup, Morten},
  year = {2013},
  month = may,
  journal = {IEEE Signal Processing Magazine},
  volume = {30},
  number = {3},
  pages = {110--128},
  issn = {1558-0792},
  doi = {10.1109/MSP.2012.2235191},
  abstract = {Modeling structure in complex networks using Bayesian nonparametrics makes it possible to specify flexible model structures and infer the adequate model complexity from the observed data. This article provides a gentle introduction to nonparametric Bayesian modeling of complex networks: Using an infinite mixture model as running example, we go through the steps of deriving the model as an infinite limit of a finite parametric model, inferring the model parameters by Markov chain Monte Carlo, and checking the model?s fit and predictive performance. We explain how advanced nonparametric models for complex networks can be derived and point out relevant literature.},
  keywords = {Adaptation models,Bayes methods,Complex networks,Learning systems,Markov processes,Modeling,Monte Carlo methods,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Schmidt_Morup_2013_Nonparametric_Bayesian_modeling_of_complex_networks.pdf;/home/dede/Zotero/storage/NMS5WF49/6494690.html}
}

@article{schmitt2020,
  title = {Wartime Paradigms and the Future of Western Military Power},
  author = {Schmitt, Olivier},
  year = {2020},
  month = mar,
  journal = {International Affairs},
  volume = {96},
  number = {2},
  pages = {401--418},
  issn = {0020-5850},
  doi = {10.1093/ia/iiaa005},
  abstract = {From the perception of the imminence of threats at the political level to the seizing of initiative through proper timing at the tactical level, temporality is directly related to war and warfare. Yet, despite some analyses of the importance of time at the political/grand strategic level (usually by scholars) and at the tactical level (usually by military professionals) there is surprisingly little discussion of the impact of time on the preparation and the conduct of warfare. This article introduces the concept of `wartime paradigm' as a heuristic device to understand the relationship between the perception of time and the conduct of warfare, and argues that after the Cold War, a specific `wartime paradigm' combining an optimization for speed and an understanding of war as risk management has guided western warfare, from force structure to the conduct of actual operations. It shows how the changing character of warfare directly challenges this wartime paradigm and why, if western forces want to prevail in future conflicts, the establishment of a new wartime paradigm guiding technological improvements and operational concepts is critical.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Schmitt_2020_Wartime_paradigms_and_the_future_of_western_military_power.pdf;/home/dede/Zotero/storage/FLINUJ7J/5716263.html}
}

@book{searle2017,
  title = {{Matrix Algebra Useful for Statistics}},
  author = {Searle, Shayle R. and Khuri, Andre I.},
  year = {2017},
  month = jun,
  publisher = {{John Wiley \& Sons Inc}},
  address = {{Hoboken, New Jersey}},
  abstract = {A thoroughly updated guide to matrix algebra and it uses in statistical analysis and features SAS\&reg;, MATLAB\&reg;, and R throughout This Second Edition addresses matrix algebra that is useful in the statistical analysis of data as well as within statistics as a whole. The material is presented in an explanatory style rather than a formal theorem-proof format and is self-contained. Featuring numerous applied illustrations, numerical examples, and exercises, the book has been updated to include the use of SAS, MATLAB, and R for the execution of matrix computations. In addition, Andr\&eacute; I. Khuri, who has extensive research and teaching experience in the field, joins this new edition as co-author. The Second Edition also:  Contains new coverage on vector spaces and linear transformations and discusses computational aspects of matrices Covers the analysis of balanced linear models using direct products of matrices Analyzes multiresponse linear models where several responses can be of interest Includes extensive use of SAS, MATLAB, and R throughout Contains over 400 examples and exercises to reinforce understanding along with select solutions Includes plentiful new illustrations depicting the importance of geometry as well as historical interludes  Matrix Algebra Useful for Statistics, Second Edition is an ideal textbook for advanced undergraduate and first-year graduate level courses in statistics and other related disciplines. The book is also appropriate as a reference for independent readers who use statistics and wish to improve their knowledge of matrix algebra. THE LATE SHAYLE R. SEARLE, PHD, was professor emeritus of biometry at Cornell University. He was the author of Linear Models for Unbalanced Data and Linear Models and co-author of Generalized, Linear, and Mixed Models, Second Edition, Matrix Algebra for Applied Economics, and Variance Components, all published by Wiley. Dr. Searle received the Alexander von Humboldt Senior Scientist Award, and he was an honorary fellow of the Royal Society of New Zealand. ANDR\&Eacute; I. KHURI, PHD, is Professor Emeritus of Statistics at the University of Florida. He is the author of Advanced Calculus with Applications in Statistics, Second Edition and co-author of Statistical Tests for Mixed Linear Models, all published by Wiley. Dr. Khuri is a member of numerous academic associations, among them the American Statistical Association and the Institute of Mathematical Statistics.},
  isbn = {978-1-118-93514-9},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Searle_Khuri_2017_Matrix_Algebra_Useful_for_Statistics.pdf}
}

@book{sengupta2019,
  title = {{Julia High Performance: Optimizations, distributed computing, multithreading, and GPU programming with Julia 1.0 and beyond, 2nd Edition}},
  shorttitle = {{Julia High Performance}},
  author = {Sengupta, Avik and Edelman, Alan},
  year = {2019},
  edition = {2nd Revised edition},
  publisher = {{Packt Publishing}},
  address = {{Birmingham Mumbai}},
  isbn = {978-1-78829-811-7},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Sengupta_Edelman_2019_Julia_High_Performance.pdf}
}

@book{senn2003,
  title = {Dicing with {{Death}}: {{Chance}}, {{Risk And Health}}},
  shorttitle = {Dicing with {{Death}}},
  author = {Senn, Stephen},
  year = {2003},
  month = nov,
  edition = {1 edition},
  publisher = {{Cambridge University Press}},
  address = {{New York}},
  abstract = {If you think that statistics has nothing to say about what you do or how you could do it better, then you are either wrong or in need of a more interesting job. Stephen Senn explains here how statistics determines many decisions about medical care--from allocating resources for health, to determining which drugs to license, to cause-and-effect in relation to disease. He tackles big themes: clinical trials and the development of medicines, life tables, vaccines and their risks or lack of them, smoking and lung cancer and even the power of prayer. He entertains with puzzles and paradoxes and covers the lives of famous statistical pioneers. By the end of the book the reader will see how reasoning with probability is essential to making rational decisions in medicine, and how and when it can guide us when faced with choices that impact our health and/or life. Stephen Senn has been a Professor of Pharmaceutical and Health Statistics at the University College of London since 1995. In 2001 he won George C. Challis Award of the University of Florida for contributions to biostatistics. Senn's previous two books are Statistical Issues in Drug Development (Wiley, 1997) and Cross-over Trials in Clinical Research (Wiley, 1993). He is the member of seven editorial boards including Statistics in Medicine and Pharmaceutical Statistics.},
  isbn = {978-0-521-54023-0},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Senn_2003_Dicing_with_Death.pdf}
}

@book{serfling1980,
  title = {Approximation {{Theorems}} of {{Mathematical Statistics}}},
  author = {Serfling, Robert J.},
  year = {1980},
  edition = {1st edition},
  publisher = {{Wiley-Interscience}},
  address = {{New York, NY}},
  isbn = {978-0-471-21927-9},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Serfling_2001_Approximation_Theorems_of_Mathematical_Statistics.pdf}
}

@article{sergin2021,
  title = {Toward a Better Monitoring Statistic for Profile Monitoring via Variational Autoencoders},
  author = {Sergin, Nurettin Dorukhan and Yan, Hao},
  year = {2021},
  month = apr,
  journal = {Journal of Quality Technology},
  volume = {0},
  number = {0},
  pages = {1--46},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2021.1903821},
  abstract = {Variational autoencoders have been recently proposed for the problem of process monitoring. While these works show impressive results over classical methods, the proposed monitoring statistics often ignore the inconsistencies in learned lower-dimensional representations and computational limitations in high-dimensional approximations. In this work, we first manifest these issues and then overcome them with a novel statistic formulation that increases out-of-control detection accuracy without compromising computational efficiency. We demonstrate our results on a simulation study with explicit control over latent variations, and a real-life example of image profiles obtained from a hot steel rolling process.},
  keywords = {Deep learning,high-dimensional nonlinear profile,latent variable model,profile monitoring,variational autoencoder},
  annotation = {\_eprint: https://doi.org/10.1080/00224065.2021.1903821},
  file = {/home/dede/Zotero/storage/Q26UAKAG/00224065.2021.html}
}

@article{sethuraman1994,
  title = {A {{Constructive Definition}} of {{Dirichlet Priors}}},
  author = {Sethuraman, Jayaram},
  year = {1994},
  journal = {Statistica Sinica},
  volume = {4},
  number = {2},
  pages = {639--650},
  publisher = {{Institute of Statistical Science, Academia Sinica}},
  issn = {1017-0405},
  abstract = {In this paper we give a simple and new constructive definition of Dirichlet measures removing the restriction that the basic space should be Rk. We also give complete, self contained proofs of the three basic results for Dirichlet measures: 1. The Dirichlet measure is a probability measure on the space of all probability measures. 2. It gives probability one to the subset of discrete probability measures. 3. The posterior distribution is also a Dirichlet measure.},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Sethuraman_1994_A_Constructive_Definition_of_Dirichlet_Priors.pdf}
}

@book{severini2000,
  title = {{Likelihood Methods in Statistics}},
  author = {Severini, Thomas A.},
  year = {2000},
  month = nov,
  edition = {New edizione},
  publisher = {{OUP Oxford}},
  address = {{Oxford ; New York}},
  abstract = {This book provides an introduction to the modern theory of likelihood-based statistical inference. This theory is characterized by several important features. One is the recognition that it is desirable to condition on relevant ancillary statistics. Another is that probability approximations are based on saddlepoint and closely related approximations that generally have very high accuracy. A third aspect is that, for models with nuisance parameters, inference is often based on marginal or conditional likelihoods, or approximations to these likelihoods. These methods have been shown often to yield substantial improvements over classical methods. The book also provides an up-to-date account of recent results in the field, which has been undergoing rapid development.},
  isbn = {978-0-19-850650-8},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Severini_2000_Likelihood_Methods_in_Statistics.pdf}
}

@book{severini2005,
  title = {{Elements of Distribution Theory}},
  author = {Severini, Thomas A.},
  year = {2005},
  edition = {New edizione},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge; New York}},
  abstract = {This detailed introduction to distribution theory uses no measure theory, making it suitable for students in statistics and econometrics as well as for researchers who use statistical methods. Good backgrounds in calculus and linear algebra are important and a course in elementary mathematical analysis is useful, but not required. An appendix gives a detailed summary of the mathematical definitions and results that are used in the book. Topics covered range from the basic distribution and density functions, expectation, conditioning, characteristic functions, cumulants, convergence in distribution and the central limit theorem to more advanced concepts such as exchangeability, models with a group structure, asymptotic approximations to integrals, orthogonal polynomials and saddlepoint approximations. The emphasis is on topics useful in understanding statistical methodology; thus, parametric statistical models and the distribution theory associated with the normal distribution are covered comprehensively.},
  isbn = {978-1-107-63073-4},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Severini_2012_Elements_of_Distribution_Theory.pdf}
}

@article{shahriari2016,
  title = {Taking the {{Human Out}} of the {{Loop}}: {{A Review}} of {{Bayesian Optimization}}},
  shorttitle = {Taking the {{Human Out}} of the {{Loop}}},
  author = {Shahriari, Bobak and Swersky, Kevin and Wang, Ziyu and Adams, Ryan P. and {de Freitas}, Nando},
  year = {2016},
  month = jan,
  journal = {Proceedings of the IEEE},
  volume = {104},
  number = {1},
  pages = {148--175},
  issn = {0018-9219, 1558-2256},
  doi = {10.1109/JPROC.2015.2494218},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Shahriari_et_al_2016_Taking_the_Human_Out_of_the_Loop.pdf}
}

@article{shamp2020,
  title = {Computationally Efficient {{Bayesian}} Sequential Function Monitoring},
  author = {Shamp, Wright and Varbanov, Roumen and Chicken, Eric and Linero, Antonio and Yang, Yun},
  year = {2020},
  month = aug,
  journal = {Journal of Quality Technology},
  volume = {0},
  number = {0},
  pages = {1--19},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2020.1801366},
  abstract = {In functional sequential process monitoring, a process is characterized by sequences of observations called profiles which are monitored over time for stability. The goal is to halt a process when the process generating these observations deviates from a specified in control standard. We propose a Bayesian sequential process control (SPC) methodology which uses wavelets to monitor the functional responses and detect out of control profiles. Our contribution is to propose a solution to the growing computational cost by constructing an efficient and accurate approximation to the posterior distribution of the wavelet coefficients, without recourse to Markov chain Monte Carlo.},
  keywords = {Bayesian,Phase II,profile monitoring,statistical process control,todo,wavelets},
  annotation = {\_eprint: https://doi.org/10.1080/00224065.2020.1801366},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Shamp_et_al_2020_Computationally_efficient_Bayesian_sequential_function_monitoring.pdf;/home/dede/Zotero/storage/GS3SFVAF/00224065.2020.html}
}

@article{shao2010,
  title = {Extended {{Tapered Block Bootstrap}}},
  author = {Shao, Xiaofeng},
  year = {2010},
  journal = {Statistica Sinica},
  volume = {20},
  number = {2},
  pages = {807--821},
  publisher = {{Institute of Statistical Science, Academia Sinica}},
  issn = {1017-0405},
  abstract = {We propose a new block bootstrap procedure for time series, called the extended tapered block bootstrap, to estimate the variance and approximate the sampling distribution of a large class of approximately linear statistics. Our proposal differs from the existing tapered block bootstrap (Paparoditis and Politis (2001, 2002)) in that the tapering is applied to the random weights in the bootstrapped empirical distribution. Under the smooth function model, we obtain asymptotic bias and variance expansions for the variance estimator and establish the consistency of the distribution approximation. The extended tapered block bootstrap has wider applicability than the tapered block bootstrap, while preserving the favorable bias and mean squared error properties of the tapered block bootstrap over the moving block bootstrap. A small simulation study is performed to compare the finite-sample performance of the block-based bootstrap methods.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Shao_2010_Extended_Tapered_Block_Bootstrap.pdf}
}

@article{shen2013,
  title = {Monitoring Poisson Count Data with Probability Control Limits When Sample Sizes Are Time Varying},
  author = {Shen, Xiaobei and Zou, Changliang and Jiang, Wei and Tsung, Fugee},
  year = {2013},
  journal = {Naval Research Logistics (NRL)},
  volume = {60},
  number = {8},
  pages = {625--636},
  issn = {1520-6750},
  doi = {10.1002/nav.21557},
  abstract = {This article considers the problem of monitoring Poisson count data when sample sizes are time varying without assuming a priori knowledge of sample sizes. Traditional control charts, whose control limits are often determined before the control charts are activated, are constructed based on perfect knowledge of sample sizes. In practice, however, future sample sizes are often unknown. Making an inappropriate assumption of the distribution function could lead to unexpected performance of the control charts, for example, excessive false alarms in the early runs of the control charts, which would in turn hurt an operator's confidence in valid alarms. To overcome this problem, we propose the use of probability control limits, which are determined based on the realization of sample sizes online. The conditional probability that the charting statistic exceeds the control limit at present given that there has not been a single alarm before can be guaranteed to meet a specified false alarm rate. Simulation studies show that our proposed control chart is able to deliver satisfactory run length performance for any time-varying sample sizes. The idea presented in this article can be applied to any effective control charts such as the exponentially weighted moving average or cumulative sum chart. \textcopyright{} 2013 Wiley Periodicals, Inc. Naval Research Logistics 60: 625\textendash 636, 2013},
  langid = {english},
  keywords = {average run length,done,exponentially weighted moving average,false alarm rate,healthcare,run length distribution,statistical process control},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/nav.21557},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Shen_et_al_2013_Monitoring_poisson_count_data_with_probability_control_limits_when_sample_sizes.pdf;/home/dede/Zotero/storage/XZKUMNA4/nav.html}
}

@article{shen2014,
  title = {Detection of Multiple Undocumented Change-Points Using Adaptive {{Lasso}}},
  author = {Shen, Jie and Gallagher, Colin M. and Lu, QiQi},
  year = {2014},
  month = jun,
  journal = {Journal of Applied Statistics},
  volume = {41},
  number = {6},
  pages = {1161--1173},
  publisher = {{Taylor \& Francis}},
  issn = {0266-4763},
  doi = {10.1080/02664763.2013.862220},
  abstract = {The problem of detecting multiple undocumented change-points in a historical temperature sequence with simple linear trend is formulated by a linear model. We apply adaptive least absolute shrinkage and selection operator (Lasso) to estimate the number and locations of change-points. Model selection criteria are used to choose the Lasso smoothing parameter. As adaptive Lasso may overestimate the number of change-points, we perform post-selection on change-points detected by adaptive Lasso using multivariate t simultaneous confidence intervals. Our method is demonstrated on the annual temperature data (year: 1902\textendash 2000) from Tuscaloosa, Alabama.},
  keywords = {adaptive Lasso,model selection criterion,multiple undocumented change-points,multivariate t simultaneous confidence intervals,successive GLRT,todo},
  annotation = {\_eprint: https://doi.org/10.1080/02664763.2013.862220},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Shen_et_al_2014_Detection_of_multiple_undocumented_change-points_using_adaptive_Lasso.pdf}
}

@article{shen2016,
  title = {Self-{{Starting Monitoring Scheme}} for {{Poisson Count Data With Varying Population Sizes}}},
  author = {Shen, Xiaobei and Tsui, Kwok-Leung and Zou, Changliang and Woodall, William H.},
  year = {2016},
  journal = {Technometrics},
  volume = {58},
  number = {4},
  pages = {460--471},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.2015.1075423},
  abstract = {In this article, we consider the problem of monitoring Poisson rates when the population sizes are time-varying and the nominal value of the process parameter is unavailable. Almost all previous control schemes for the detection of increases in the Poisson rate in Phase II are constructed based on assumed knowledge of the process parameters, for example, the expectation of the count of a rare event when the process of interest is in control. In practice, however, this parameter is usually unknown and not able to be estimated with a sufficiently large number of reference samples. A self-starting exponentially weighted moving average (EWMA) control scheme based on a parametric bootstrap method is proposed. The success of the proposed method lies in the use of probability control limits, which are determined based on the observations during rather than before monitoring. Simulation studies show that our proposed scheme has good in-control and out-of-control performance under various situations. In particular, our proposed scheme is useful in rare event studies during the start-up stage of a monitoring process. Supplementary materials for this article are available online.},
  keywords = {Average run length,done,Healthcare surveillance,Poisson process,Probability control limits.},
  annotation = {\_eprint: https://doi.org/10.1080/00401706.2015.1075423},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Shen_et_al_2016_Self-Starting_Monitoring_Scheme_for_Poisson_Count_Data_With_Varying_Population.pdf;/home/dede/Zotero/storage/AX2M2MN3/00401706.2015.html}
}

@article{sherman1950,
  title = {Adjustment of an {{Inverse Matrix Corresponding}} to a {{Change}} in {{One Element}} of a {{Given Matrix}}},
  author = {Sherman, Jack and Morrison, Winifred J.},
  year = {1950},
  month = mar,
  journal = {The Annals of Mathematical Statistics},
  volume = {21},
  number = {1},
  pages = {124--127},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177729893},
  abstract = {The Annals of Mathematical Statistics},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Sherman_Morrison_1950_Adjustment_of_an_Inverse_Matrix_Corresponding_to_a_Change_in_One_Element_of_a.pdf;/home/dede/Zotero/storage/DVTS8GHZ/1177729893.html}
}

@article{shevtsova2007,
  title = {Post-Communist {{Russia}}: A Historic Opportunity Missed},
  shorttitle = {Post-Communist {{Russia}}},
  author = {SHEVTSOVA, LILIA},
  year = {2007},
  month = sep,
  journal = {International Affairs},
  volume = {83},
  number = {5},
  pages = {891--912},
  issn = {0020-5850},
  doi = {10.1111/j.1468-2346.2007.00661.x},
  abstract = {This article analyses the nature of the current Russian system and its future trajectory. First, the continuity between the Yeltsin and Putin presidencies is made clear. The nature of the Russian system has, to a great extent, been influenced by Yeltsin, who strengthened demands not for independent institutions but for a new and more powerful authoritarian leadership. Putin has consolidated the system, based on personalized power. But despite signs of economic growth and outward stability there is evidence that the Russian system is unsustainable in the long-term. The current system is based on a modification of the petro-economy that reproduces the merger between power and business with the rentier class. Thus far, however, the model has not been able to solve social conflicts or stop the degradation of `human capital'. Nor is it likely to do so in the future.},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/SHEVTSOVA_2007_Post-communist_Russia.pdf;/home/dede/Zotero/storage/H6AEGAHL/2417854.html}
}

@article{shi2018,
  title = {{{CRPS}} Chart: {{Simultaneously}} Monitoring Location and Scale under Data-Rich Environment},
  shorttitle = {{{CRPS}} Chart},
  author = {Shi, Liangxing and Gong, Ling and Lin, Dennis K.J.},
  year = {2018},
  journal = {Quality and Reliability Engineering International},
  volume = {34},
  number = {4},
  pages = {681--697},
  issn = {1099-1638},
  doi = {10.1002/qre.2280},
  abstract = {The detection performance of a conventional control chart is usually degraded by a large sample size as in Wang and Tsung. This paper proposes a new control chart under data-rich environment. The proposed chart is based on the continuous ranked probability score and aims to simultaneously monitor the location and the scale parameters of any continuous process. We simulate different monitoring schemes with various shift patterns to examine the chart performance. Both in-control and out-of-control performances are studied through simulation studies in terms of the mean, the standard deviation, the median, and some percentiles of the average run length distribution. Simulation results show that the proposed chart keeps a high sensitivity to shifts in location and/or scale without any distributional assumptions, and the outperformance improves, as the sample size becomes larger. Examples are given for illustration.},
  langid = {english},
  keywords = {average run length,CRPS,data-rich environment,doing,large sample size,location and scale},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/qre.2280},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Shi_et_al_2018_CRPS_chart2.pdf;/home/dede/Zotero/storage/RI7N4E8X/qre.html}
}

@article{shmueli2010,
  title = {To {{Explain}} or to {{Predict}}?},
  author = {Shmueli, Galit},
  year = {2010},
  month = aug,
  journal = {Statistical Science},
  volume = {25},
  number = {3},
  pages = {289--310},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/10-STS330},
  abstract = {Statistical modeling is a powerful tool for developing and testing theories by way of causal explanation, prediction, and description. In many disciplines there is near-exclusive use of statistical modeling for causal explanation and the assumption that models with high explanatory power are inherently of high predictive power. Conflation between explanation and prediction is common, yet the distinction must be understood for progressing scientific knowledge. While this distinction has been recognized in the philosophy of science, the statistical literature lacks a thorough discussion of the many differences that arise in the process of modeling for an explanatory versus a predictive goal. The purpose of this article is to clarify the distinction between explanatory and predictive modeling, to discuss its sources, and to reveal the practical implications of the distinction to each step in the modeling process.},
  keywords = {causality,data mining,Explanatory modeling,predictive modeling,predictive power,scientific research,statistical strategy,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Shmueli_2010_To_Explain_or_to_Predict2.pdf;/home/dede/Zotero/storage/IWXPLFXC/10-STS330.html}
}

@article{shu2008,
  title = {A {{Weighted CUSUM Chart}} for {{Detecting Patterned Mean Shifts}}},
  author = {Shu, Lianjie and Jiang, Wei and Tsui, Kwok-Leung},
  year = {2008},
  month = apr,
  journal = {Journal of Quality Technology},
  volume = {40},
  number = {2},
  pages = {194--213},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2008.11917725},
  abstract = {Most conventional control charts focus on detecting a constant mean shift. In reality, however, it is often important to deal with situations where the mean of the monitoring sequence has a dynamic shift pattern, e.g., residuals from a time series. In these cases, the conventional control charts may perform poorly, as they do not consider the information due to what is known as ``forecast recovery'' contained in the patterned mean shift. This paper proposes a new weighted cumulative sum (WCUSUM) procedure for monitoring a sequence with a patterned mean shift. This method first estimates the dynamic mean of the sequence, then uses the estimates for weighting the incremental in the conventional CUSUM chart. Guidelines for designing the WCUSUM chart are proposed, and the performance is compared with that of the conventional CUSUM chart and other alternatives for detecting patterned mean shifts. It is found that the WCUSUM chart performs far superior to other charts for detecting small to moderate shifts when forecast recovery is present and performs competitively otherwise.},
  keywords = {Automatic Process Control,Change-Point Detection,done,EWMA,Statistical Process Control},
  annotation = {\_eprint: https://doi.org/10.1080/00224065.2008.11917725},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Shu_et_al_2008_A_Weighted_CUSUM_Chart_for_Detecting_Patterned_Mean_Shifts.pdf;/home/dede/Zotero/storage/HHJMF4Q3/00224065.2008.html}
}

@book{shumway2017,
  title = {Time {{Series Analysis}} and {{Its Applications}}: {{With R Examples}}},
  shorttitle = {Time {{Series Analysis}} and {{Its Applications}}},
  author = {Shumway, Robert H. and Stoffer, David S.},
  year = {2017},
  month = apr,
  edition = {4th ed.},
  publisher = {{Springer}},
  address = {{New York, NY}},
  abstract = {The fourth edition of this popular graduate textbook, like its predecessors, presents a balanced and comprehensive treatment of both time and frequency domain methods with accompanying theory. Numerous examples using nontrivial data illustrate solutions to problems such as discovering natural and anthropogenic climate change, evaluating pain perception experiments using functional magnetic resonance imaging, and monitoring a nuclear test ban treaty.The book is designed as a textbook for graduate level students in the physical, biological, and social sciences and as a graduate level text in statistics. Some parts may also serve as an undergraduate introductory course. Theory and methodology are separated to allow presentations on different levels. In addition to coverage of classical methods of time series regression, ARIMA models, spectral analysis and state-space models, the text includes modern developments including categorical time series analysis, multivariate spectral methods, long memory series, nonlinear models, resampling techniques, GARCH models, ARMAX models, stochastic volatility, wavelets, and Markov chain Monte Carlo integration methods.This edition includes R code for each numerical example in addition to Appendix R, which provides a reference for the data sets and R scripts used in the text in addition to a tutorial on basic R commands and R time series.~An additional file is available on the book's website for download, making all the data sets and scripts easy to load into R.},
  isbn = {978-3-319-52451-1},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Shumway_Stoffer_2017_Time_Series_Analysis_and_Its_Applications.pdf}
}

@article{sidiropoulos2017,
  title = {Tensor {{Decomposition}} for {{Signal Processing}} and {{Machine Learning}}},
  author = {Sidiropoulos, Nicholas D. and De Lathauwer, Lieven and Fu, Xiao and Huang, Kejun and Papalexakis, Evangelos E. and Faloutsos, Christos},
  year = {2017},
  month = jul,
  journal = {IEEE Transactions on Signal Processing},
  volume = {65},
  number = {13},
  pages = {3551--3582},
  issn = {1941-0476},
  doi = {10.1109/TSP.2017.2690524},
  abstract = {Tensors or multiway arrays are functions of three or more indices (i, j, k, . . . )-similar to matrices (two-way arrays), which are functions of two indices (r, c) for (row, column). Tensors have a rich history, stretching over almost a century, and touching upon numerous disciplines; but they have only recently become ubiquitous in signal and data analytics at the confluence of signal processing, statistics, data mining, and machine learning. This overview article aims to provide a good starting point for researchers and practitioners interested in learning about and working with tensors. As such, it focuses on fundamentals and motivation (using various application examples), aiming to strike an appropriate balance of breadth and depth that will enable someone having taken first graduate courses in matrix algebra and probability to get started doing research and/or developing tensor algorithms and software. Some background in applied optimization is useful but not strictly required. The material covered includes tensor rank and rank decomposition; basic tensor factorization models and their relationships and properties (including fairly good coverage of identifiability); broad coverage of algorithms ranging from alternating optimization to stochastic gradient; statistical performance analysis; and applications ranging from source separation to collaborative filtering, mixture and topic modeling, classification, and multilinear subspace learning.},
  keywords = {alternating direction method of multipliers,alternating optimization,canonical polyadic decomposition (CPD),classification,collaborative filtering,communications,CramÃ©râ€“Rao bound,Gaussâ€“Newton,gradient descent,harmonic retrieval,higher-order singular value decomposition (HOSVD),Matrix decomposition,mixture modeling,multilinear singular value decomposition (MLSVD),NP-hard problems,Optimization,parallel factor analysis (PARAFAC),rank,Signal processing,Signal processing algorithms,source separation,speech separation,stochastic gradient,subspace learning,Tensile stress,Tensor decomposition,tensor factorization,todo,topic modeling,Tucker model,Tutorials,uniqueness},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Sidiropoulos_et_al_2017_Tensor_Decomposition_for_Signal_Processing_and_Machine_Learning.pdf;/home/dede/Zotero/storage/5EJ5FQ7J/7891546.html}
}

@article{siegmund1995,
  title = {Using the {{Generalized Likelihood Ratio Statistic}} for {{Sequential Detection}} of a {{Change-Point}}},
  author = {Siegmund, D. and Venkatraman, E. S.},
  year = {1995},
  month = feb,
  journal = {The Annals of Statistics},
  volume = {23},
  number = {1},
  pages = {255--271},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/aos/1176324466},
  abstract = {We study sequential detection of a change-point using the generalized likelihood ratio statistic. For the special case of detecting a change in a normal mean with known variance, we give approximations to the average run lengths and compare our procedure to standard CUSUM tests and combined CUSUM-Shewhart tests. Several examples indicating extensions to problems involving multiple parameters are discussed.},
  keywords = {62L10,Change-point,likelihood ratio,sequential detection},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Siegmund_Venkatraman_1995_Using_the_Generalized_Likelihood_Ratio_Statistic_for_Sequential_Detection_of_a.pdf;/home/dede/Zotero/storage/MXK4BNUV/1176324466.html}
}

@article{simon2013,
  title = {A {{Sparse-Group Lasso}}},
  author = {Simon, Noah and Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
  year = {2013},
  month = apr,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {22},
  number = {2},
  pages = {231--245},
  publisher = {{Taylor \& Francis}},
  issn = {1061-8600},
  doi = {10.1080/10618600.2012.681250},
  abstract = {For high-dimensional supervised learning problems, often using problem-specific assumptions can lead to greater accuracy. For problems with grouped covariates, which are believed to have sparse effects both on a group and within group level, we introduce a regularized model for linear regression with {$\mathscr{l}$}1 and {$\mathscr{l}$}2 penalties. We discuss the sparsity and other regularization properties of the optimal fit for this model, and show that it has the desired effect of group-wise and within group sparsity. We propose an algorithm to fit the model via accelerated generalized gradient descent, and extend this model and algorithm to convex loss functions. We also demonstrate the efficacy of our model and the efficiency of our algorithm on simulated data. This article has online supplementary material.},
  keywords = {Model,Nesterov,Penalize,Regression,Regularize},
  annotation = {\_eprint: https://doi.org/10.1080/10618600.2012.681250},
  file = {/home/dede/Zotero/storage/I6VV4A4J/10618600.2012.html}
}

@article{singpurwalla2020,
  title = {What {{Does}} the "{{Mean}}" {{Really Mean}}?},
  author = {Singpurwalla, Nozer D. and Lai, Boya},
  year = {2020},
  month = mar,
  journal = {arXiv:2003.01973 [stat]},
  eprint = {2003.01973},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {The arithmetic average of a collection of observed values of a homogeneous collection of quantities is often taken to be the most representative observation. There are several arguments supporting this choice the moment of inertia being the most familiar. But what does this mean? In this note, we bring forth the Kolmogorov-Nagumo point of view that the arithmetic average is a special case of a sequence of functions of a special kind, the quadratic and the geometric means being some of the other cases. The median fails to belong to this class of functions. The Kolmogorov-Nagumo interpretation is the most defensible and the most definitive one for the arithmetic average, but its essence boils down to the fact that this average is merely an abstraction which has meaning only within its mathematical set-up.},
  archiveprefix = {arXiv},
  keywords = {Chisiniâ€™s Equation,done,Kolmogorov-Nagumo Functions,Statistics - Other Statistics,Weighted Means},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Singpurwalla_Lai_2020_What_Does_the_Mean_Really_Mean.pdf;/home/dede/Zotero/storage/9M4QMP44/2003.html}
}

@book{sisson2018,
  title = {Handbook of {{Approximate Bayesian Computation}}},
  editor = {Sisson, Scott A. and Fan, Yanan and Beaumont, Mark},
  year = {2018},
  month = aug,
  edition = {1st edition},
  publisher = {{Chapman and Hall/CRC}},
  address = {{Boca Raton}},
  abstract = {As the world becomes increasingly complex, so do the statistical models required to analyse the challenging problems ahead. For the very first time in a single volume, the Handbook of Approximate Bayesian Computation (ABC) presents an extensive overview of the theory, practice and application of ABC methods. These simple, but powerful statistical techniques, take Bayesian statistics beyond the need to specify overly simplified models, to the setting where the model is defined only as a process that generates data. This process can be arbitrarily complex, to the point where standard Bayesian techniques based on working with tractable likelihood functions would not be viable. ABC methods finesse the problem of model complexity within the Bayesian framework by exploiting modern computational power, thereby permitting approximate Bayesian analyses of models that would otherwise be impossible to implement. The Handbook of ABC provides illuminating insight into the world of Bayesian modelling for intractable models for both experts and newcomers alike. It is an essential reference book for anyone interested in learning about and implementing ABC techniques to analyse complex models in the modern world.},
  isbn = {978-1-4398-8150-7},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Sisson_et_al_2018_Handbook_of_Approximate_Bayesian_Computation.pdf}
}

@article{skilling2004,
  title = {Programming the {{Hilbert}} Curve},
  author = {Skilling, John and Erickson, Gary J and Zhai, Yuxiang},
  year = {2004},
  month = apr,
  journal = {AIP Conference Proceedings},
  volume = {707},
  number = {1},
  pages = {381--387},
  publisher = {{American Institute of Physics}},
  issn = {0094-243X},
  doi = {10.1063/1.1751381},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Skilling_et_al_2004_Programming_the_Hilbert_curve.pdf;/home/dede/Zotero/storage/QC5JFVEG/1.html}
}

@article{slivkins2021,
  title = {Introduction to {{Multi-Armed Bandits}}},
  author = {Slivkins, Aleksandrs},
  year = {2021},
  month = jun,
  journal = {arXiv:1904.07272 [cs, stat]},
  eprint = {1904.07272},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Multi-armed bandits a simple but very powerful framework for algorithms that make decisions over time under uncertainty. An enormous body of work has accumulated over the years, covered in several books and surveys. This book provides a more introductory, textbook-like treatment of the subject. Each chapter tackles a particular line of work, providing a self-contained, teachable technical introduction and a brief review of the further developments; many of the chapters conclude with exercises. The book is structured as follows. The first four chapters are on IID rewards, from the basic model to impossibility results to Bayesian priors to Lipschitz rewards. The next three chapters cover adversarial rewards, from the full-feedback version to adversarial bandits to extensions with linear rewards and combinatorially structured actions. Chapter 8 is on contextual bandits, a middle ground between IID and adversarial bandits in which the change in reward distributions is completely explained by observable contexts. The last three chapters cover connections to economics, from learning in repeated games to bandits with supply/budget constraints to exploration in the presence of incentives. The appendix provides sufficient background on concentration and KL-divergence. The chapters on "bandits with similarity information", "bandits with knapsacks" and "bandits and agents" can also be consumed as standalone surveys on the respective topics.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Data Structures and Algorithms,Computer Science - Machine Learning,Statistics - Machine Learning,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Slivkins_2021_Introduction_to_Multi-Armed_Bandits.pdf;/home/dede/Zotero/storage/JIAE6KWS/1904.html}
}

@article{smith1975,
  title = {A {{Bayesian Approach}} to {{Inference}} about a {{Change-Point}} in a {{Sequence}} of {{Random Variables}}},
  author = {Smith, A. F. M.},
  year = {1975},
  journal = {Biometrika},
  volume = {62},
  number = {2},
  pages = {407--416},
  publisher = {{[Oxford University Press, Biometrika Trust]}},
  issn = {0006-3444},
  doi = {10.2307/2335381},
  abstract = {A Bayesian approach is considered to the problem of making inferences about the point in a sequence of random variables at which the underlying distribution changes. Inferences are based on the posterior probabilities of the possible change-points. Detailed analyses are given for cases in which the distributions are binomial and normal, and numerical illustrations are provided. An informal sequential procedure is also noted.}
}

@book{snyder2013,
  title = {{Thinking the Twentieth Century}},
  author = {Snyder, Timothy and Judt, Tony},
  year = {2013},
  month = feb,
  publisher = {{Vintage}},
  address = {{London}},
  isbn = {978-0-09-956355-6},
  langid = {Inglese},
  keywords = {todo}
}

@article{sogandi2019,
  title = {Risk-Adjusted {{Bernoulli}} Chart in Multi-Stage Healthcare Processes Based on State-Space Model with a Latent Risk Variable and Dynamic Probability Control Limits},
  author = {Sogandi, Fatemeh and Aminnayeri, Majid and Mohammadpour, Adel and Amiri, Amirhossein},
  year = {2019},
  month = feb,
  journal = {Computers \& Industrial Engineering},
  volume = {130},
  doi = {10.1016/j.cie.2019.02.030},
  abstract = {Healthcare-related processes are usually multistage in nature, and the output of each stage is the input of the subsequent stage. However, most existing monitoring schemes consider the individual stages of the healthcare process without considering both inter-stage and intra-stage links. The modeling of multistage healthcare processes based on the state-space model describes the cascade property of the risks and outcomes in subsequent stages. The present paper proposes a Bernoulli state-space model in which unmeasurable risks of processes are considered as a latent risk variable to give a realistic estimate of potential risks. In the proposed monitoring scheme for multistage medical processes, in addition to Parsonnet scores, the other categorical operational covariates are considered. Moreover, dynamic probability control limits are applied to remove the effect of patient risk distributions on in-control average run length performance. Simulation results reveal that the proposed risk-adjusted Bernoulli group exponentially weighted moving average chart for multistage healthcare processes performs satisfactorily under different shifts. Moreover, the proposed monitoring scheme helps to identify the out-of-control stage of the healthcare process to remove the corresponding assignable cause.},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Sogandi_et_al_2019_Risk-adjusted_Bernoulli_chart_in_multi-stage_healthcare_processes_based_on.pdf}
}

@article{sommerfeld2018,
  title = {Inference for Empirical {{Wasserstein}} Distances on Finite Spaces},
  author = {Sommerfeld, Max and Munk, Axel},
  year = {2018},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {80},
  number = {1},
  pages = {219--238},
  issn = {1467-9868},
  doi = {10.1111/rssb.12236},
  abstract = {The Wasserstein distance is an attractive tool for data analysis but statistical inference is hindered by the lack of distributional limits. To overcome this obstacle, for probability measures supported on finitely many points, we derive the asymptotic distribution of empirical Wasserstein distances as the optimal value of a linear programme with random objective function. This facilitates statistical inference (e.g. confidence intervals for sample-based Wasserstein distances) in large generality. Our proof is based on directional Hadamard differentiability. Failure of the classical bootstrap and alternatives are discussed. The utility of the distributional results is illustrated on two data sets.},
  langid = {english},
  keywords = {Bootstrap,Central limit theorem,Directional Hadamard derivative,Hypothesis testing,Optimal transport,todo,Wasserstein distance},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/rssb.12236},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Sommerfeld_Munk_2018_Inference_for_empirical_Wasserstein_distances_on_finite_spaces.pdf;/home/dede/Zotero/storage/NBWGVUPG/sommerfeld2017.pdf;/home/dede/Zotero/storage/P8XCRN42/rssb.html}
}

@inproceedings{song2020,
  title = {Research on Statistical Process Control Method for Multi-Variety and Small Batch Production Mode},
  booktitle = {2020 {{Chinese Control And Decision Conference}} ({{CCDC}})},
  author = {Song, Huaming and Xu, Rui and Wang, Chun},
  year = {2020},
  month = aug,
  pages = {2377--2381},
  issn = {1948-9447},
  doi = {10.1109/CCDC49329.2020.9163978},
  abstract = {Aiming at the problem that the traditional statistical process control method is difficult to be applied into the multi-variety and small batch production mode, this paper proposes a T- EWMA control chart to monitor the mean value of this particular production process. Then, the Monte-Carlo simulation method is used to compare the performance of this proposed control chart with the traditional T control chart. The simulation results show that the anomaly recognition ability of the T- EWMA control chart is significantly better than the traditional T control chart. Finally, based on the Markov Chain theory, this paper presents a parameter optimization method for the T- EWMA control chart.},
  keywords = {Control chart,Control charts,Gaussian distribution,Markov processes,Monitoring,Multi-variety and small batch production,Parameter optimization,Process control,Standards,Statistical process control},
  file = {/home/dede/Zotero/storage/D3ININ2M/9163978.html}
}

@article{spall1992,
  title = {Multivariate Stochastic Approximation Using a Simultaneous Perturbation Gradient Approximation},
  author = {Spall, J.C.},
  year = {1992},
  month = mar,
  journal = {IEEE Transactions on Automatic Control},
  volume = {37},
  number = {3},
  pages = {332--341},
  issn = {00189286},
  doi = {10.1109/9.119632},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Spall_1992_Multivariate_stochastic_approximation_using_a_simultaneous_perturbation.pdf}
}

@article{spall2000,
  title = {Adaptive Stochastic Approximation by the Simultaneous Perturbation Method},
  author = {Spall, J. C.},
  year = {2000},
  month = oct,
  journal = {IEEE Transactions on Automatic Control},
  volume = {45},
  number = {10},
  pages = {1839--1853},
  issn = {1558-2523},
  doi = {10.1109/TAC.2000.880982},
  abstract = {Stochastic approximation (SA) has long been applied for problems of minimizing loss functions or root finding with noisy input information. As with all stochastic search algorithms, there are adjustable algorithm coefficients that must be specified, and that can have a profound effect on algorithm performance. It is known that choosing these coefficients according to an SA analog of the deterministic Newton-Raphson algorithm provides an optimal or near-optimal form of the algorithm. However, directly determining the required Hessian matrix (or Jacobian matrix for root finding) to achieve this algorithm form has often been difficult or impossible in practice. The paper presents a general adaptive SA algorithm that is based on a simple method for estimating the Hessian matrix, while concurrently estimating the primary parameters of interest. The approach applies in both the gradient-free optimization (Kiefer-Wolfowitz) and root-finding/stochastic gradient-based (Robbins-Monro) settings, and is based on the "simultaneous perturbation (SP)" idea introduced previously. The algorithm requires only a small number of loss function or gradient measurements per iteration-independent of the problem dimension-to adaptively estimate the Hessian and parameters of primary interest. Aside from introducing the adaptive SP approach, the paper presents practical implementation guidance, asymptotic theory, and a nontrivial numerical evaluation. Also included is a discussion and numerical analysis comparing the adaptive SP approach with the iterate-averaging approach to accelerated SA.},
  keywords = {Acceleration,Adaptive control,Backpropagation algorithms,Constraint optimization,Jacobian matrices,Least squares approximation,Noise measurement,Parameter estimation,Perturbation methods,Stochastic processes,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Spall_2000_Adaptive_stochastic_approximation_by_the_simultaneous_perturbation_method.pdf;/home/dede/Documents/MEGA/universita/zotero-pdf/Spall_2000_Adaptive_stochastic_approximation_by_the_simultaneous_perturbation_method2.pdf;/home/dede/Zotero/storage/5LNFV86T/880982.html}
}

@book{spall2003,
  title = {{Introduction to Stochastic Search and Optimization: Estimation, Simulation, and Control}},
  shorttitle = {{Introduction to Stochastic Search and Optimization}},
  author = {Spall, James C.},
  year = {2003},
  month = mar,
  edition = {1. edizione},
  publisher = {{Wiley-Interscience}},
  address = {{Hoboken, N.J}},
  abstract = {* Unique in its survey of the range of topics. * Contains a strong, interdisciplinary format that will appeal to both students and researchers. * Features exercises and web links to software and data sets.},
  isbn = {978-0-471-33052-3},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Spall_2003_Introduction_to_Stochastic_Search_and_Optimization.pdf}
}

@incollection{sparapani2019,
  title = {Bayesian {{Additive Regression Trees}} ({{BART}})},
  booktitle = {Wiley {{StatsRef}}: {{Statistics Reference Online}}},
  author = {Sparapani, Rodney and McCulloch, Robert},
  year = {2019},
  pages = {1--9},
  publisher = {{American Cancer Society}},
  doi = {10.1002/9781118445112.stat08251},
  abstract = {This article introduces the nonparametric machine learning technique known as Bayesian additive regression trees (BART) for continuous, dichotomous, categorical, and time-to-event outcomes.},
  copyright = {Copyright \textcopyright{} 2019 John Wiley \& Sons, Ltd. All rights reserved.},
  isbn = {978-1-118-44511-2},
  langid = {english},
  keywords = {black-box models,categorical outcomes,competing risks,continuous outcomes,dichotomous outcomes,ensemble predictive modeling,machine learning,nonparametric,recurrent events,survival analysis,todo},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118445112.stat08251},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Sparapani_McCulloch_2019_Bayesian_Additive_Regression_Trees_(BART).pdf;/home/dede/Zotero/storage/6YSVCJSE/9781118445112.html}
}

@book{spirtes2001,
  title = {Causation, {{Prediction}}, and {{Search}}},
  author = {Spirtes, Peter and Glymour, Clark and Scheines, Richard},
  editor = {Bach, Francis},
  year = {2001},
  month = jan,
  series = {Adaptive {{Computation}} and {{Machine Learning}} Series},
  edition = {Second},
  publisher = {{A Bradford Book}},
  address = {{Cambridge, MA, USA}},
  abstract = {The authors address the assumptions and methods that allow us to turn observations into causal knowledge, and use even incomplete causal knowledge in planning and prediction to influence and control our environment.},
  isbn = {978-0-262-19440-2},
  langid = {english}
}

@article{srivastava1986,
  title = {Likelihood {{Ratio Tests}} for a {{Change}} in the {{Multivariate Normal Mean}}},
  author = {Srivastava, M. S. and Worsley, K. J.},
  year = {1986},
  journal = {Journal of the American Statistical Association},
  volume = {81},
  number = {393},
  pages = {199--204},
  publisher = {{[American Statistical Association, Taylor \& Francis, Ltd.]}},
  issn = {0162-1459},
  doi = {10.2307/2287990},
  abstract = {A sequence of independent multivariate normal vectors with equal but possibly unknown variance matrices are hypothesized to have equal mean vectors, and we wish to test that the mean vectors have changed after an unknown point in the sequence. The likelihood ratio test is based on the maximum Hotelling T2 for the sequences before and after the change point. The main result is a conservative approximation for its null distribution based on an improved Bonferroni inequality. If the change is judged significant, then further changes are estimated by splitting the two subsequences formed by the first change point. The methods can also be used to test for a change in row probabilities of a contingency table, allowing for extramultinomial variation. The results are used to find changes in a set of geological data previously analyzed by Chernoff (1973) by the "faces" method and to find changes in the frequencies of pronouns in the plays of Shakespeare.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Srivastava_Worsley_1986_Likelihood_Ratio_Tests_for_a_Change_in_the_Multivariate_Normal_Mean.pdf}
}

@inproceedings{srivastava2015,
  title = {{{WASP}}: {{Scalable Bayes}} via Barycenters of Subset Posteriors},
  shorttitle = {{{WASP}}},
  booktitle = {Artificial {{Intelligence}} and {{Statistics}}},
  author = {Srivastava, Sanvesh and Cevher, Volkan and Dinh, Quoc and Dunson, David},
  year = {2015},
  month = feb,
  pages = {912--920},
  publisher = {{PMLR}},
  issn = {1938-7228},
  abstract = {The promise of Bayesian methods for big data sets has not fully been realized due to the lack of scalable computational algorithms. For massive data, it is necessary to store and process subsets on...},
  langid = {english},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Srivastava_et_al_2015_WASP.pdf;/home/dede/Zotero/storage/2MPLC4PS/srivastava15.html}
}

@article{srivastava2018,
  title = {Scalable {{Bayes}} via {{Barycenter}} in {{Wasserstein Space}}},
  author = {Srivastava, Sanvesh and Li, Cheng and Dunson, David},
  year = {2018},
  journal = {Journal of Machine Learning Research},
  volume = {19},
  abstract = {We propose a novel approach WASP for Bayesian inference when massive size of the data prohibits posterior computations. WASP is estimated in three steps. First, data are divided into smaller computationally tractable subsets. Second, posterior draws of parameters are obtained for every subset after modifying subset posteriors using stochastic approximation. Finally, the empirical measures of samples from each subset posterior are combined through their barycenter in the Wasserstein space of probability measures. Stochastic approximation ensures that posterior uncertainty quantification of the barycenter matches with that of the full data posterior distribution. The combining step can be conducted efficiently through a sparse linear program, which takes negligible time relative to sampling from subset posteriors, facilitating scaling to massive data. WASP is very general and allows application of existing sampling algorithms to massive data with minimal modifications. We provide theoretical conditions under which rate of convergence of WASP to the delta measure centered at the true parameter coincides with the optimal parametric rate up to a logarithmic factor. WASP is applied for scalable Bayesian computations in a nonparametric mixture model and a movie recommender database containing tens of millions of ratings.},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Srivastava_et_al_2018_Scalable_Bayes_via_Barycenter_in_Wasserstein_Space.pdf}
}

@book{steele2004,
  title = {{The Cauchy-Schwarz Master Class}},
  author = {Steele, J. Michael},
  year = {2004},
  month = jul,
  publisher = {{Cambridge University Press}},
  address = {{Cambridge ; New York}},
  abstract = {This lively, problem-oriented text, first published in 2004, is designed to coach readers toward mastery of the most fundamental mathematical inequalities. With the Cauchy-Schwarz inequality as the initial guide, the reader is led through a sequence of fascinating problems whose solutions are presented as they might have been discovered - either by one of history's famous mathematicians or by the reader. The problems emphasize beauty and surprise, but along the way readers will find systematic coverage of the geometry of squares, convexity, the ladder of power means, majorization, Schur convexity, exponential sums, and the inequalities of H\"older, Hilbert, and Hardy. The text is accessible to anyone who knows calculus and who cares about solving problems. It is well suited to self-study, directed study, or as a supplement to courses in analysis, probability, and combinatorics.},
  isbn = {978-0-521-54677-5},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Steele_2004_The_Cauchy-Schwarz_Master_Class.pdf}
}

@article{stefanucci2018,
  title = {{{PCA-based}} Discrimination of Partially Observed Functional Data, with an Application to {{AneuRisk65}} Data Set},
  author = {Stefanucci, Marco and Sangalli, Laura M. and Brutti, Pierpaolo},
  year = {2018},
  journal = {Statistica Neerlandica},
  volume = {72},
  number = {3},
  pages = {246--264},
  publisher = {{Wiley Online Library}},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Stefanucci_et_al_2018_PCA-based_discrimination_of_partially_observed_functional_data,_with_an.pdf}
}

@article{stefanucci2021,
  title = {Multiscale Stick-Breaking Mixture Models},
  author = {Stefanucci, Marco and Canale, Antonio},
  year = {2021},
  month = jan,
  journal = {Statistics and Computing},
  volume = {31},
  number = {2},
  issn = {1573-1375},
  doi = {10.1007/s11222-020-09991-1},
  abstract = {Bayesian nonparametric density estimation is dominated by single-scale methods, typically exploiting mixture model specifications, exception made for P\'olya trees prior and allied approaches. In this paper we focus on developing a novel family of multiscale stick-breaking mixture models that inherits some of the advantages of both single-scale nonparametric mixtures and P\'olya trees. Our proposal is based on a mixture specification exploiting an infinitely deep binary tree of random weights that grows according to a multiscale generalization of a large class of stick-breaking processes; this multiscale stick-breaking is paired with specific stochastic processes generating sequences of parameters that induce stochastically ordered kernel functions. Properties of this family of multiscale stick-breaking mixtures are described. Focusing on a Gaussian specification, a Markov Chain Monte Carlo algorithm for posterior computation is introduced. The performance of the method is illustrated analyzing both synthetic and real datasets consistently showing competitive results both in scenarios favoring single-scale and multiscale methods. The results suggest that the method is well suited to estimate densities with varying degree of smoothness and local features.},
  langid = {english},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Stefanucci_Canale_2021_Multiscale_stick-breaking_mixture_models.pdf}
}

@article{steigleder2003,
  title = {Generalized {{Stratified Sampling Using}} the {{Hilbert Curve}}},
  author = {Steigleder, Mauro and McCool, Michael D.},
  year = {2003},
  month = jan,
  journal = {Journal of Graphics Tools},
  volume = {8},
  number = {3},
  pages = {41--47},
  publisher = {{Taylor \& Francis}},
  issn = {1086-7651},
  doi = {10.1080/10867651.2003.10487589},
  abstract = {Stratified sampling is a widely used strategy to improve convergence in Monte Carlo techniques. The efficiency of a stratification technique mainly depends on the coherence of the strata. This paper presents an approach to generate an arbitrary number of coherent strata, independently of the dimensionality of the domain, using the Hilbert space-filling curve. Using this approach, it is possible to draw an arbitrary number of stratified samples from higher dimensional spaces using only one-dimensional stratification. This technique can also be used to generalize nonuniform stratified sampling. Source code is available online.},
  keywords = {done},
  annotation = {\_eprint: https://doi.org/10.1080/10867651.2003.10487589},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Steigleder_McCool_2003_Generalized_Stratified_Sampling_Using_the_Hilbert_Curve.pdf;/home/dede/Zotero/storage/ULP6WAC7/10867651.2003.html}
}

@article{steiner2000,
  title = {Monitoring Surgical Performance Using Risk-Adjusted Cumulative Sum Charts},
  author = {Steiner, Stefan H. and Cook, Richard J. and Farewell, Vern T. and Treasure, Tom},
  year = {2000},
  journal = {Biostatistics},
  volume = {1},
  number = {4},
  pages = {441--452},
  publisher = {{Oxford Academic}},
  issn = {1465-4644},
  doi = {10.1093/biostatistics/1.4.441},
  abstract = {Abstract. The cumulative sum (CUSUM) procedure is a graphical method that is widely used for quality monitoring in industrial settings. More recently it has bee},
  langid = {english},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Steiner_et_al_2000_Monitoring_surgical_performance_using_risk-adjusted_cumulative_sum_charts3.pdf;/home/dede/Zotero/storage/9UTBJ2FL/238348.html}
}

@unpublished{steinsson2021,
  title = {Money, {{Inflation}}, and {{Output}}: {{A Quantity-Theoretic Introduction}}},
  author = {Steinsson, Jon},
  year = {2021},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Steinsson_2021_Money, Inflation, and Output.pdf}
}

@unpublished{steinsson2021a,
  title = {How {{Did Growth Begin}}? {{The Industrial Revolution}} and Its {{Antecedents}}},
  author = {Steinsson, Jon},
  year = {2021},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Steinsson_2021_How Did Growth Begin.pdf}
}

@unpublished{steinsson2021b,
  title = {Malthus and {{Pre-Industrial Stagnation}}},
  author = {Steinsson, Jon},
  year = {2021},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Steinsson_2021_Malthus and Pre-Industrial Stagnation.pdf}
}

@unpublished{steinsson2021c,
  title = {Market {{Efficiency}} and {{Market Failure}}},
  author = {Steinsson, Jon},
  year = {2021},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Steinsson_2021_Market Efficiency and Market Failure.pdf}
}

@article{steorts2014,
  title = {A {{Comparison}} of {{Blocking Methods}} for {{Record Linkage}}},
  author = {Steorts, Rebecca C. and Ventura, Samuel L. and Sadinle, Mauricio and Fienberg, Stephen E.},
  year = {2014},
  month = jul,
  journal = {arXiv:1407.3191 [cs, stat]},
  eprint = {1407.3191},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Record linkage seeks to merge databases and to remove duplicates when unique identifiers are not available. Most approaches use blocking techniques to reduce the computational complexity associated with record linkage. We review traditional blocking techniques, which typically partition the records according to a set of field attributes, and consider two variants of a method known as locality sensitive hashing, sometimes referred to as "private blocking." We compare these approaches in terms of their recall, reduction ratio, and computational complexity. We evaluate these methods using different synthetic datafiles and conclude with a discussion of privacy-related issues.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Databases,Statistics - Applications,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Steorts_et_al_2014_A_Comparison_of_Blocking_Methods_for_Record_Linkage.pdf;/home/dede/Zotero/storage/P8R9AFB6/1407.html}
}

@article{steorts2015,
  title = {A {{Bayesian Approach}} to {{Graphical Record Linkage}} and {{De-duplication}}},
  author = {Steorts, Rebecca C. and Hall, Rob and Fienberg, Stephen E.},
  year = {2015},
  month = oct,
  journal = {arXiv:1312.4645 [stat]},
  eprint = {1312.4645},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {We propose an unsupervised approach for linking records across arbitrarily many files, while simultaneously detecting duplicate records within files. Our key innovation involves the representation of the pattern of links between records as a bipartite graph, in which records are directly linked to latent true individuals, and only indirectly linked to other records. This flexible representation of the linkage structure naturally allows us to estimate the attributes of the unique observable people in the population, calculate transitive linkage probabilities across records (and represent this visually), and propagate the uncertainty of record linkage into later analyses. Our method makes it particularly easy to integrate record linkage with post-processing procedures such as logistic regression, capture-recapture, etc. Our linkage structure lends itself to an efficient, linear-time, hybrid Markov chain Monte Carlo algorithm, which overcomes many obstacles encountered by previously record linkage approaches, despite the high-dimensional parameter space. We illustrate our method using longitudinal data from the National Long Term Care Survey and with data from the Italian Survey on Household and Wealth, where we assess the accuracy of our method and show it to be better in terms of error rates and empirical scalability than other approaches in the literature.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Steorts_et_al_2015_A_Bayesian_Approach_to_Graphical_Record_Linkage_and_De-duplication.pdf;/home/dede/Zotero/storage/J66262HG/1312.html}
}

@article{stetco2015,
  title = {Fuzzy {{C-means}}++: {{Fuzzy C-means}} with Effective Seeding Initialization},
  shorttitle = {Fuzzy {{C-means}}++},
  author = {Stetco, Adrian and Zeng, Xiao-Jun and Keane, John},
  year = {2015},
  journal = {Expert Systems with Applications},
  volume = {42},
  number = {21},
  pages = {7541--7548},
  issn = {0957-4174},
  abstract = {Fuzzy C-means has been utilized successfully in a wide range of applications, extending the clustering capability of the K-means to datasets that are uncertain, vague and otherwise hard to cluster. This paper introduces the Fuzzy C-means++ algorithm which, by utilizing the seeding mechanism of the K-means++ algorithm, improves the effectiveness and speed of Fuzzy C-means. By careful seeding that disperses the initial cluster centers through the data space, the resulting Fuzzy C-means++ approach samples starting cluster representatives during the initialization phase. The cluster representatives are well spread in the input space, resulting in both faster convergence times and higher quality solutions. Implementations in R of standard Fuzzy C-means and Fuzzy C-means++ are evaluated on various data sets. We investigate the cluster quality and iteration count as we vary the spreading factor on a series of synthetic data sets. We run the algorithm on real world data sets and to account for the non-determinism inherent in these algorithms we record multiple runs while choosing different k parameter values. The results show that the proposed method gives significant improvement in convergence times (the number of iterations) of up to 40 (2.1 on average) times the standard on synthetic datasets and, in general, an associated lower cost function value and Xie\textendash Beni value. A proof sketch of the logarithmically bounded expected cost function value is given.},
  keywords = {Cluster analysis,done,Fuzzy C-means clustering,Initialization},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Stetco_et_al_2015_Fuzzy_C-means++.pdf;/home/dede/Zotero/storage/Y3AG44XQ/S0957417415003346.html}
}

@book{stoker1989,
  title = {Differential {{Geometry}}},
  author = {Stoker, J. J.},
  year = {1989},
  month = jan,
  edition = {1 edition},
  publisher = {{John Wiley \& Sons}},
  address = {{New York}},
  abstract = {This classic work is now available in an unabridged paperback edition. Stoker makes this fertile branch of mathematics accessible to the nonspecialist by the use of three different notations: vector algebra and calculus, tensor calculus, and the notation devised by Cartan, which employs invariant differential forms as elements in an algebra due to Grassman, combined with an operation called exterior differentiation. Assumed are a passing acquaintance with linear algebra and the basic elements of analysis.},
  isbn = {978-0-471-50403-0},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Stoker_1989_Differential_Geometry.pdf}
}

@article{sukhbaatar2015,
  title = {End-{{To-End Memory Networks}}},
  author = {Sukhbaatar, Sainbayar and Szlam, Arthur and Weston, Jason and Fergus, Rob},
  year = {2015},
  month = nov,
  journal = {arXiv:1503.08895 [cs]},
  eprint = {1503.08895},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We introduce a neural network with a recurrent attention model over a possibly large external memory. The architecture is a form of Memory Network (Weston et al., 2015) but unlike the model in that work, it is trained end-to-end, and hence requires significantly less supervision during training, making it more generally applicable in realistic settings. It can also be seen as an extension of RNNsearch to the case where multiple computational steps (hops) are performed per output symbol. The flexibility of the model allows us to apply it to tasks as diverse as (synthetic) question answering and to language modeling. For the former our approach is competitive with Memory Networks, but with less supervision. For the latter, on the Penn TreeBank and Text8 datasets our approach demonstrates comparable performance to RNNs and LSTMs. In both cases we show that the key concept of multiple computational hops yields improved results.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Sukhbaatar_et_al_2015_End-To-End_Memory_Networks.pdf;/home/dede/Zotero/storage/M8JFHTS5/1503.html}
}

@article{sullivan2002,
  title = {A {{Self-Starting Control Chart}} for {{Multivariate Individual Observations}}},
  author = {Sullivan, Joe H. and Jones, L. Allison},
  year = {2002},
  journal = {Technometrics},
  volume = {44},
  number = {1},
  pages = {24--33},
  publisher = {{[Taylor \& Francis, Ltd., American Statistical Association, American Society for Quality]}},
  issn = {0040-1706},
  abstract = {Multivariate versions of cumulative sum, exponentially weighted moving average (EWMA), and Hotelling's T\textsuperscript{2} charts typically assume knowledge of the in-control process parameters or, with a new or changed process, use parameter estimates from an in-control reference sample of preliminary observations. In contrast, the self-starting chart begins controlling the process without the need for preliminary observations, an advantage when production is slow or when the cost of early out-of-control production is high. Furthermore, the use of estimated parameters substantially degrades the expected performance of conventional charts, a problem avoided by the proposed chart. The self-starting chart uses the deviation of each observation vector from the average of all previous observations. These deviations, or innovations, can be plotted on a T\textsuperscript{2} control chart or accumulated in a multivariate EWMA (MEWMA) chart. The run-length performance is evaluated for step shifts occurring at various points, and the MEWMA charts are shown to outperform T\textsuperscript{2} charts in rapidly detecting process changes. We show that for self-starting charts, performance comparisons based on the average run length can be misleading and introduce a new performance criterion.},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Sullivan_Jones_2002_A_Self-Starting_Control_Chart_for_Multivariate_Individual_Observations.pdf}
}

@book{sutherland2010,
  title = {{Introduction to Metric and Topological Spaces}},
  author = {Sutherland, Wilson A.},
  year = {2010},
  month = sep,
  edition = {2 edizione},
  publisher = {{OUP Oxford}},
  address = {{Oxford}},
  abstract = {One of the ways in which topology has influenced other branches of mathematics in the past few decades is by putting the study of continuity and convergence into a general setting. This new edition of Wilson Sutherland's classic text introduces metric and topological spaces by describing some of that influence. The aim is to move gradually from familiar real analysis to abstract topological spaces, using metric spaces as a bridge between the two. The language of metric and topological spaces is established with continuity as the motivating concept. Several concepts are introduced, first in metric spaces and then repeated for topological spaces, to help convey familiarity. The discussion develops to cover connectedness, compactness and completeness, a trio widely used in the rest of mathematics. Topology also has a more geometric aspect which is familiar in popular expositions of the subject as `rubber-sheet geometry', with pictures of M\"obius bands, doughnuts, Klein bottles and the like; this geometric aspect is illustrated by describing some standard surfaces, and it is shown how all this fits into the same story as the more analytic developments. The book is primarily aimed at second- or third-year mathematics students. There are numerous exercises, many of the more challenging ones accompanied by hints, as well as a companion website, with further explanations and examples as well as material supplementary to that in the book.},
  isbn = {978-0-19-956308-1},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Sutherland_2010_Introduction_to_Metric_and_Topological_Spaces.pdf}
}

@book{sutton2018,
  title = {{Reinforcement Learning: An Introduction}},
  shorttitle = {{Reinforcement Learning, second edition}},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  year = {2018},
  month = nov,
  edition = {Second},
  publisher = {{Bradford Books}},
  address = {{Cambridge, Massachusetts}},
  abstract = {The significantly expanded and updated new edition of a widely used text on reinforcement learning, one of the most active research areas in artificial intelligence.Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms. This second edition has been significantly expanded and updated, presenting new topics and updating coverage of other topics.Like the first edition, this second edition focuses on core online learning algorithms, with the more mathematical material set off in shaded boxes. Part I covers as much of reinforcement learning as possible without going beyond the tabular case for which exact solutions can be found. Many algorithms presented in this part are new to the second edition, including UCB, Expected Sarsa, and Double Learning. Part II extends these ideas to function approximation, with new sections on such topics as artificial neural networks and the Fourier basis, and offers expanded treatment of off-policy learning and policy-gradient methods. Part III has new chapters on reinforcement learning's relationships to psychology and neuroscience, as well as an updated case-studies chapter including AlphaGo and AlphaGo Zero, Atari game playing, and IBM Watson's wagering strategy. The final chapter discusses the future societal impacts of reinforcement learning.},
  isbn = {978-0-262-03924-6},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Sutton_Barto_2018_Reinforcement_Learning,_second_edition.pdf}
}

@article{szucs2017,
  title = {Empirical Assessment of Published Effect Sizes and Power in the Recent Cognitive Neuroscience and Psychology Literature},
  author = {Szucs, Denes and Ioannidis, John P. A.},
  year = {2017},
  month = mar,
  journal = {PLOS Biology},
  volume = {15},
  number = {3},
  pages = {e2000797},
  publisher = {{Public Library of Science}},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.2000797},
  abstract = {We have empirically assessed the distribution of published effect sizes and estimated power by analyzing 26,841 statistical records from 3,801 cognitive neuroscience and psychology papers published recently. The reported median effect size was D = 0.93 (interquartile range: 0.64\textendash 1.46) for nominally statistically significant results and D = 0.24 (0.11\textendash 0.42) for nonsignificant results. Median power to detect small, medium, and large effects was 0.12, 0.44, and 0.73, reflecting no improvement through the past half-century. This is so because sample sizes have remained small. Assuming similar true effect sizes in both disciplines, power was lower in cognitive neuroscience than in psychology. Journal impact factors negatively correlated with power. Assuming a realistic range of prior probabilities for null hypotheses, false report probability is likely to exceed 50\% for the whole literature. In light of our findings, the recently reported low replication success in psychology is realistic, and worse performance may be expected for cognitive neuroscience.},
  langid = {english},
  keywords = {Behavioral neuroscience,Cognitive neuroscience,Cognitive psychology,Experimental psychology,Medical journals,Scientific publishing,Statistical data,Statistical distributions,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Szucs_Ioannidis_2017_Empirical_assessment_of_published_effect_sizes_and_power_in_the_recent.pdf;/home/dede/Zotero/storage/Y4P53FH9/article.html}
}

@article{takahashi2011,
  title = {Discovering {{Emerging Topics}} in {{Social Streams}} via {{Link Anomaly Detection}}},
  author = {Takahashi, Toshimitsu and Tomioka, Ryota and Yamanishi, Kenji},
  year = {2011},
  month = oct,
  journal = {arXiv:1110.2899 [physics, stat]},
  eprint = {1110.2899},
  eprinttype = {arxiv},
  primaryclass = {physics, stat},
  abstract = {Detection of emerging topics are now receiving renewed interest motivated by the rapid growth of social networks. Conventional term-frequency-based approaches may not be appropriate in this context, because the information exchanged are not only texts but also images, URLs, and videos. We focus on the social aspects of theses networks. That is, the links between users that are generated dynamically intentionally or unintentionally through replies, mentions, and retweets. We propose a probability model of the mentioning behaviour of a social network user, and propose to detect the emergence of a new topic from the anomaly measured through the model. We combine the proposed mention anomaly score with a recently proposed change-point detection technique based on the Sequentially Discounting Normalized Maximum Likelihood (SDNML), or with Kleinberg's burst model. Aggregating anomaly scores from hundreds of users, we show that we can detect emerging topics only based on the reply/mention relationships in social network posts. We demonstrate our technique in a number of real data sets we gathered from Twitter. The experiments show that the proposed mention-anomaly-based approaches can detect new topics at least as early as the conventional term-frequency-based approach, and sometimes much earlier when the keyword is ill-defined.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Social and Information Networks,Physics - Physics and Society,Statistics - Machine Learning,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Takahashi_et_al_2011_Discovering_Emerging_Topics_in_Social_Streams_via_Link_Anomaly_Detection.pdf;/home/dede/Zotero/storage/5ZXL7GLU/1110.html}
}

@article{tallberg2004,
  title = {A {{Bayesian Approach}} to {{Modeling Stochastic Blockstructures}} with {{Covariates}}},
  author = {Tallberg, Christian},
  year = {2004},
  month = dec,
  journal = {The Journal of Mathematical Sociology},
  volume = {29},
  number = {1},
  pages = {1--23},
  publisher = {{Routledge}},
  issn = {0022-250X},
  doi = {10.1080/00222500590889703},
  abstract = {We consider social networks in which the relations between actors are governed by latent classes of actors with similar relational structure, i.e., blockmodeling. In Snijders and Nowicki (1997) and Nowicki and Snijders (2001), a Bayesian approach to blockmodels is presented, where the probability of a relation between two actors depends only on the classes to which the actors belong but is independent of the actors. When actors are a priori partitioned into subsets based on actor attributes such as race, sex and income, the model proposed by Nowicki and Snijders completely ignores this extra piece of information. In this paper, a blockmodel that is a simple extension of their model is proposed specifically for such data. The class affiliation probabilities are modeled conditional on the actor attributes via a multinomial probit model. Posterior distributions of the model parameters, and predictive posterior distributions of the class affiliation probabilities are computed by using a straightforward Gibbs sampling algorithm. Applications are illustrated with analysis on real and simulated data sets},
  keywords = {Bayesian analysis,Blockmodels,Gibbs sampling,Multinomial probit,Random graphs,todo},
  annotation = {\_eprint: https://doi.org/10.1080/00222500590889703},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Tallberg_2004_A_Bayesian_Approach_to_Modeling_Stochastic_Blockstructures_with_Covariates.pdf;/home/dede/Zotero/storage/EX2VXEI3/00222500590889703.html}
}

@book{tan2005,
  title = {{Introduction to Data Mining}},
  author = {Tan, Pang-Ning and Steinbach, Michael and Kumar, Vipin},
  year = {2005},
  publisher = {{Pearson College Div}},
  address = {{Boston}},
  abstract = {Introduction to Data Mining presents fundamental concepts and algorithms for those learning data mining for the first time. Each major topic is organized into two chapters, beginning with basic concepts that provide necessary background for understanding each data mining technique, followed by more advanced concepts and algorithms.},
  isbn = {978-0-321-32136-7},
  langid = {Inglese}
}

@article{tancredi2011,
  title = {A Hierarchical {{Bayesian}} Approach to Record Linkage and Population Size Problems},
  author = {Tancredi, Andrea and Liseo, Brunero},
  year = {2011},
  month = jun,
  journal = {The Annals of Applied Statistics},
  volume = {5},
  number = {2B},
  pages = {1553--1585},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {1932-6157, 1941-7330},
  doi = {10.1214/10-AOAS447},
  abstract = {We propose and illustrate a hierarchical Bayesian approach for matching statistical records observed on different occasions. We show how this model can be profitably adopted both in record linkage problems and in capture\textendash recapture setups, where the size of a finite population is the real object of interest. There are at least two important differences between the proposed model-based approach and the current practice in record linkage. First, the statistical model is built up on the actually observed categorical variables and no reduction (to 0\textendash 1 comparisons) of the available information takes place. Second, the hierarchical structure of the model allows a two-way propagation of the uncertainty between the parameter estimation step and the matching procedure so that no plug-in estimates are used and the correct uncertainty is accounted for both in estimating the population size and in performing the record linkage. We illustrate and motivate our proposal through a real data example and simulations.},
  keywords = {Captureâ€“recapture methods,Conditional independence,Gibbs sampling,Metropolisâ€“Hastings,record linkage,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Tancredi_Liseo_2011_A_hierarchical_Bayesian_approach_to_record_linkage_and_population_size_problems.pdf;/home/dede/Zotero/storage/NH6RXB8Z/10-AOAS447.html}
}

@article{tang2015,
  title = {Risk-{{Adjusted Cumulative Sum Charting Procedure Based}} on {{Multiresponses}}},
  author = {Tang, Xu and Gan, Fah F. and Zhang, Lingyun},
  year = {2015},
  month = jan,
  journal = {Journal of the American Statistical Association},
  volume = {110},
  number = {509},
  pages = {16--26},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.2014.960965},
  abstract = {The cumulative sum charting procedure is traditionally used in the manufacturing industry for monitoring the quality of products. Recently, it has been extended to monitoring surgical outcomes. Unlike a manufacturing process where the raw material is usually reasonably homogeneous, patients' risks of surgical failure are usually different. It has been proposed in the literature that the binary outcomes from a surgical procedure be adjusted using the preoperative risk based on a likelihood-ratio scoring method. Such a crude classification of surgical outcome is naive. It is unreasonable to regard a patient who has a full recovery, the same quality outcome as another patient who survived but remained bed-ridden for life. For a patient who survives an operation, there can be many different grades of recovery. Thus, it makes sense to consider a risk-adjusted cumulative sum charting procedure based on more than two outcomes to better monitor surgical performance. In this article, we develop such a chart and study its performance.},
  keywords = {Collocation method,done,Euroscores,Odds ratio,Parsonnet scores,Patient mix,Proportional odds logistic regression,Quality monitoring,Surgical outcomes},
  annotation = {\_eprint: https://doi.org/10.1080/01621459.2014.960965},
  file = {/home/dede/Zotero/storage/3QGXIZ3R/01621459.2014.html}
}

@article{tang2018,
  title = {Development of an {{Immune-Pathology Informed Radiomics Model}} for {{Non-Small Cell Lung Cancer}}},
  author = {Tang, Chad and Hobbs, Brian and Amer, Ahmed and Li, Xiao and Behrens, Carmen and Canales, Jaime Rodriguez and Cuentas, Edwin Parra and Villalobos, Pamela and Fried, David and Chang, Joe Y. and Hong, David S. and Welsh, James W. and Sepesi, Boris and Court, Laurence and Wistuba, Ignacio I. and Koay, Eugene J.},
  year = {2018},
  month = jan,
  journal = {Scientific Reports},
  volume = {8},
  number = {1},
  pages = {1922},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-018-20471-5},
  abstract = {With increasing use of immunotherapy agents, pretreatment strategies for identifying responders and non-responders is useful for appropriate treatment assignment. We hypothesize that the local immune micro-environment of NSCLC is associated with patient outcomes and that these local immune features exhibit distinct radiologic characteristics discernible by quantitative imaging metrics. We assembled two cohorts of NSCLC patients treated with definitive surgical resection and extracted quantitative parameters from pretreatment CT imaging. The excised primary tumors were then quantified for percent tumor PDL1 expression and density of tumor-infiltrating lymphocyte (via CD3 count) utilizing immunohistochemistry and automated cell counting. Associating these pretreatment radiomics parameters with tumor immune parameters, we developed an immune pathology-informed model (IPIM) that separated patients into 4 clusters (designated A-D) utilizing 4 radiomics features. The IPIM designation was significantly associated with overall survival in both training (5 year OS: 61\%, 41\%, 50\%, and 91\%, for clusters A-D, respectively, P\,=\,0.04) and validation (5 year OS: 55\%, 72\%, 75\%, and 86\%, for clusters A-D, respectively, P\,=\,0.002) cohorts and immune pathology (all P\,{$<$}\,0.05). Specifically, we identified a favorable outcome group characterized by low CT intensity and high heterogeneity that exhibited low PDL1 and high CD3 infiltration, suggestive of a favorable immune activated state. We have developed a NSCLC radiomics signature based on the immune micro-environment and patient outcomes. This manuscript demonstrates model creation and validation in independent cohorts.},
  copyright = {2018 The Author(s)},
  langid = {english},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Tang_et_al_2018_Development_of_an_Immune-Pathology_Informed_Radiomics_Model_for_Non-Small_Cell.pdf;/home/dede/Zotero/storage/D3JU2YGC/s41598-018-20471-5.html}
}

@article{tartakovsky2005,
  title = {General Asymptotic {{Bayesian}} Theory of Quickest Change Detection},
  author = {Tartakovsky, A. G. and Veeravalli, V. V.},
  year = {2005},
  journal = {Theory of Probability and its Applications},
  volume = {49},
  number = {3},
  pages = {458--497},
  issn = {0040-585X},
  doi = {10.1137/S0040585X97981202},
  abstract = {The optimal detection procedure for detecting changes in independent and identically distributed (i.i.d.) sequences in a Bayesian setting was derived by Shiryaev in the 1960s. However, the analysis of the performance of this procedure in terms of the average detection delay and false alarm probability has been an open problem. In this paper, we develop a general asymptotic change-point detection theory that is not limited to a restrictive i.i.d. assumption. In particular, we investigate the performance of the Shiryaev procedure for general discrete-time stochastic models in the asymptotic setting, where the false alarm probability approaches zero. We show that the Shiryaev procedure is asymptotically optimal in the general non-i.i.d. case under mild conditions. We also show that the two popular non-Bayesian detection procedures, namely the Page and the Shiryaev-Roberts-Pollak procedures, are generally not optimal (even asymptotically) under the Bayesian criterion. The results of this study are shown to be especially important in studying the asymptotics of decentralized change detection procedures.},
  keywords = {Asymptotic optimality,Change-point detection,Nonlinear renewal theory,Sequential detection,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Tartakovsky_Veeravalli_2005_General_asymptotic_Bayesian_theory_of_quickest_change_detection.pdf}
}

@article{tartakovsky2010,
  title = {State-of-the-{{Art}} in {{Bayesian Changepoint Detection}}},
  author = {Tartakovsky, Alexander G. and Moustakides, George V.},
  year = {2010},
  month = apr,
  journal = {Sequential Analysis},
  volume = {29},
  number = {2},
  pages = {125--145},
  publisher = {{Taylor \& Francis}},
  issn = {0747-4946},
  doi = {10.1080/07474941003740997},
  abstract = {We provide a brief overview of the state-of-the-art in quickest (sequential) changepoint detection and present some new results on asymptotic and numerical analysis of main competitors such as the CUSUM, Shiryaev\textendash Roberts, and Shiryaev detection procedures in a Bayesian context.},
  keywords = {60G40,62F12,62F15,62L15,Changepoint detection,CUSUM test,Fredholm integral equation of the second kind,Numerical analysis,Sequential analysis,Shiryaev procedure,Shiryaevâ€“Roberts procedure,todo},
  annotation = {\_eprint: https://doi.org/10.1080/07474941003740997},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Tartakovsky_Moustakides_2010_State-of-the-Art_in_Bayesian_Changepoint_Detection.pdf;/home/dede/Zotero/storage/FHFAE2ES/07474941003740997.html}
}

@book{tartakovsky2019,
  title = {{Sequential Change Detection and Hypothesis Testing: General Non-i.i.d. Stochastic Models and Asymptotically Optimal Rules}},
  shorttitle = {{Sequential Change Detection and Hypothesis Testing}},
  author = {Tartakovsky, Alexander G.},
  year = {2019},
  edition = {1\textdegree{} edizione},
  publisher = {{Chapman and Hall/CRC}},
  address = {{Boca Raton, FL}},
  isbn = {978-1-4987-5758-4},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Tartakovsky_2019_Sequential_Change_Detection_and_Hypothesis_Testing.pdf}
}

@incollection{teh2010,
  title = {Dirichlet {{Process}}},
  booktitle = {Encyclopedia of {{Machine Learning}}},
  author = {Teh, Yee Whye},
  editor = {Sammut, Claude and Webb, Geoffrey I.},
  year = {2010},
  pages = {280--287},
  publisher = {{Springer US}},
  address = {{Boston, MA}},
  doi = {10.1007/978-0-387-30164-8_219},
  isbn = {978-0-387-30164-8},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Teh_2010_Dirichlet_Process.pdf}
}

@article{tercerogomez2014,
  title = {A {{Self-Starting CUSUM Chart Combined}} with a {{Maximum Likelihood Estimator}} for the {{Time}} of a {{Detected Shift}} in the {{Process Mean}}},
  author = {Tercero G{\'o}mez, V{\'i}ctor and Cordero Franco, Alvaro and P{\'e}rez-Blanco, Angel and Hernandez, Alberto},
  year = {2014},
  month = jun,
  journal = {Quality and Reliability Engineering International},
  volume = {30},
  doi = {10.1002/qre.1511},
  abstract = {Detection of a special cause of variation and the identification of the time it occurs are two important activities in any quality improvement strategy. Detection of changes in a process can be done using control charts. One of these charts, the self-starting CUSUM chart, was created to detect small sustained changes and be implemented without a Phase I or a priori knowledge of the parameters of the process. To estimate the time of a detected change, a CUSUM-based change-point estimator can be used, but experiments show that the corresponding MLE has smaller bias and standard error. This paper proposes the sequential use of the self-starting CUSUM chart and the MLE of a change point in series of independent normal observations. Performance is studied with Monte Carlo simulations showing that the use of the MLE reduces the bias of the change-point estimation. It is also shown how extra observations after a change is detected can be used to improve estimation of the change-point time. Copyright \textcopyright{} 2013 John Wiley \& Sons, Ltd.}
}

@article{theroux2014,
  title = {Control {{Charts}} for {{Short Production Runs}} in {{Aerospace Manufacturing}}},
  author = {Theroux, Eric and Galarneau, Yan and Chen, Mingyuan},
  year = {2014},
  journal = {SAE International Journal of Materials and Manufacturing},
  volume = {7},
  number = {1},
  pages = {65--72},
  publisher = {{SAE International}},
  issn = {1946-3979},
  abstract = {ABSTRACT Statistical process control (SPC) has been extensively used in many different industries including automotive, electronics, and aerospace, among others. SPC tools such as control charts, process capability analysis, sampling inspection, etc., have definitive and powerful impact on quality control and improvement for mass production and similar production systems. In aerospace manufacturing, however, applications of SPC tools are more challenging, especially when these tools are implemented in processes producing products of large sizes with slower production rates. For instance, following a widely accepted rule-of-thumb, about 100 units of products are required in the first phase of implementing a Shewhart type control chart. Once established, it then can be used for process control in the second phase for actual production process monitoring and control. In many aerospace production processes, however, it requires that quality control measures be in place from the time that the first unit of product is produced. Certain types of control charts for self-starting (without the first phase) and for short production runs (as few as 3 units) have been developed by researchers and practitioners. They have been tested and used in places where traditional Shewhart control charts are difficult to apply. In this work, we used Monte-Carlo simulation to study the suitability of applying Q-chart, one of the available self-starting control charts, in comparison with I/MR-X chart for short production runs. The preliminary results suggest that a combination of Cusum Q-chart and Cusum I/MR-X chart be considered for better quality monitoring and control.}
}

@article{thompson1933,
  title = {On the {{Likelihood}} That {{One Unknown Probability Exceeds Another}} in {{View}} of the {{Evidence}} of {{Two Samples}}},
  author = {Thompson, William R.},
  year = {1933},
  journal = {Biometrika},
  volume = {25},
  number = {3/4},
  pages = {285--294},
  publisher = {{[Oxford University Press, Biometrika Trust]}},
  issn = {0006-3444},
  doi = {10.2307/2332286},
  keywords = {todo}
}

@article{thompson1935,
  title = {On the {{Theory}} of {{Apportionment}}},
  author = {Thompson, William R.},
  year = {1935},
  journal = {American Journal of Mathematics},
  volume = {57},
  number = {2},
  pages = {450--456},
  publisher = {{The Johns Hopkins University Press}},
  issn = {0002-9327},
  doi = {10.2307/2371219},
  keywords = {todo}
}

@article{tibshirani1996,
  title = {Regression {{Shrinkage}} and {{Selection}} via the {{Lasso}}},
  author = {Tibshirani, Robert},
  year = {1996},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  volume = {58},
  number = {1},
  pages = {267--288},
  publisher = {{[Royal Statistical Society, Wiley]}},
  issn = {0035-9246},
  abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
  keywords = {Algorithms,done,Error rates,Estimators,Least squares,Linear models,Regression coefficients,Simulations,Standard error},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Tibshirani_1996_Regression_Shrinkage_and_Selection_via_the_Lasso.pdf}
}

@article{tibshirani2005,
  title = {Sparsity and Smoothness via the Fused Lasso},
  author = {Tibshirani, Robert and Saunders, Michael and Rosset, Saharon and Zhu, Ji and Knight, Keith},
  year = {2005},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {67},
  number = {1},
  pages = {91--108},
  issn = {1467-9868},
  doi = {10.1111/j.1467-9868.2005.00490.x},
  abstract = {Summary. The lasso penalizes a least squares regression by the sum of the absolute values (L1-norm) of the coefficients. The form of this penalty encourages sparse solutions (with many coefficients equal to 0). We propose the `fused lasso', a generalization that is designed for problems with features that can be ordered in some meaningful way. The fused lasso penalizes the L1-norm of both the coefficients and their successive differences. Thus it encourages sparsity of the coefficients and also sparsity of their differences\textemdash i.e. local constancy of the coefficient profile. The fused lasso is especially useful when the number of features p is much greater than N, the sample size. The technique is also extended to the `hinge' loss function that underlies the support vector classifier. We illustrate the methods on examples from protein mass spectroscopy and gene expression data.},
  langid = {english},
  keywords = {Fused lasso,Gene expression,Lasso,Least squares regression,Protein mass spectroscopy,Sparse solutions,Support vector classifier},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9868.2005.00490.x},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Tibshirani_et_al_2005_Sparsity_and_smoothness_via_the_fused_lasso.pdf;/home/dede/Zotero/storage/LUBJS65K/j.1467-9868.2005.00490.html}
}

@article{tibshirani2011,
  title = {Regression Shrinkage and Selection via the Lasso: A Retrospective},
  shorttitle = {Regression Shrinkage and Selection via the Lasso},
  author = {Tibshirani, Robert},
  year = {2011},
  journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
  volume = {73},
  number = {3},
  pages = {273--282},
  publisher = {{[Royal Statistical Society, Wiley]}},
  issn = {1369-7412},
  abstract = {In the paper I give a brief review of the basic idea and some history and then discuss some developments since the original paper on regression shrinkage and selection via the lasso.},
  keywords = {A posteriori knowledge,Algorithms,Approximation,Computational statistics,Coordinate systems,done,Estimators,Generalized linear model,Oracles},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Tibshirani_2011_Regression_shrinkage_and_selection_via_the_lasso.pdf}
}

@article{tierney1986,
  title = {Accurate {{Approximations}} for {{Posterior Moments}} and {{Marginal Densities}}},
  author = {Tierney, Luke and Kadane, Joseph B.},
  year = {1986},
  month = mar,
  journal = {Journal of the American Statistical Association},
  volume = {81},
  number = {393},
  pages = {82--86},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.1986.10478240},
  abstract = {This article describes approximations to the posterior means and variances of positive functions of a real or vector-valued parameter, and to the marginal posterior densities of arbitrary (i.e., not necessarily positive) parameters. These approximations can also be used to compute approximate predictive densities. To apply the proposed method, one only needs to be able to maximize slightly modified likelihood functions and to evaluate the observed information at the maxima. Nevertheless, the resulting approximations are generally as accurate and in some cases more accurate than approximations based on third-order expansions of the likelihood and requiring the evaluation of third derivatives. The approximate marginal posterior densities behave very much like saddle-point approximations for sampling distributions. The principal regularity condition required is that the likelihood times prior be unimodal.},
  keywords = {Asymptotic expansions,Bayesian inference,Computation of integrals,done,Laplace method},
  annotation = {\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/01621459.1986.10478240},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Tierney_Kadane_1986_Accurate_Approximations_for_Posterior_Moments_and_Marginal_Densities.pdf;/home/dede/Zotero/storage/E9GLXCQ3/01621459.1986.html}
}

@inproceedings{tighkhorshid2018,
  title = {A Self-Starting {{Risk-adjusted AFT-based}} Control Chart for Monitoring the Survival Time of Patients},
  booktitle = {Proceedings of the 2nd {{IEOM European}} Conference on {{Industrial Engineering}} and {{Operations}} Management.},
  author = {Tighkhorshid, Elahe},
  year = {2018},
  abstract = {Since the processes in the health-care domain deal with humans\&\#39; life, the slightest deviation from the normal state can lead to irreparable damages to individuals and society. Therefore, monitoring the quality of the processes in this area has attracted many researchers' attention. On the other hand, the introduction of new therapies for various diseases has made process monitoring difficult due to the lack of historical data. In this paper, the effect of heterogeneity of patient conditions (including gender, age, etc.) and treatment in different conditions (such as treatment with different therapist groups) has been considered using the accelerated failure time (AFT) regression model and the risk of each patient has been adjusted accordingly. Then, survival times of patients are monitored using a control chart based on the residual values of the AFT regression. Performance of the proposed control chart is evaluated using the average run length (ARL) criterion. The results indicate that the proposed method has a proper performance in identifying out-of-control status in processes in the health-care domain. The results showed that taking into account the therapist groups as well as increasing the number of in-control observations, increase the ability of the proposed control chart to detect shifts in the process.},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Tighkhorshid_2018_A_self-starting_Risk-adjusted_AFT-based_control_chart_for_monitoring_the.pdf;/home/dede/Zotero/storage/G2NNWYEH/390cccd2882e26c672cdb6522be48b9edcb5b99a.html}
}

@misc{tighkhorshid2018a,
  title = {A Self-Starting {{Risk-adjusted AFT-based}} Control Chart for Monitoring the Survival Time of Patients},
  author = {Tighkhorshid, Elahe},
  year = {2018},
  abstract = {Since the processes in the health-care domain deal with humans\&\#39; life, the slightest deviation from the normal state can lead to irreparable damages to individuals and society. Therefore, monitoring the quality of the processes in this area has attracted many researchers' attention. On the other hand, the introduction of new therapies for various diseases has made process monitoring difficult due to the lack of historical data. In this paper, the effect of heterogeneity of patient conditions (including gender, age, etc.) and treatment in different conditions (such as treatment with different therapist groups) has been considered using the accelerated failure time (AFT) regression model and the risk of each patient has been adjusted accordingly. Then, survival times of patients are monitored using a control chart based on the residual values of the AFT regression. Performance of the proposed control chart is evaluated using the average run length (ARL) criterion. The results indicate that the proposed method has a proper performance in identifying out-of-control status in processes in the health-care domain. The results showed that taking into account the therapist groups as well as increasing the number of in-control observations, increase the ability of the proposed control chart to detect shifts in the process.},
  howpublished = {https://www.semanticscholar.org/paper/A-self-starting-Risk-adjusted-AFT-based-control-for-Tighkhorshid/390cccd2882e26c672cdb6522be48b9edcb5b99a},
  langid = {english},
  file = {/home/dede/Zotero/storage/PY2AEHF4/390cccd2882e26c672cdb6522be48b9edcb5b99a.html}
}

@book{tille2020,
  title = {{Sampling and Estimation from Finite Populations}},
  author = {Tille, Yves},
  translator = {Hekimi, Ilya},
  year = {2020},
  month = feb,
  publisher = {{John Wiley \& Sons Inc}},
  address = {{Hoboken, NJ}},
  isbn = {978-0-470-68205-0},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Tille_2020_Sampling_and_Estimation_from_Finite_Populations.pdf}
}

@book{tirole2018,
  title = {{Economics for the Common Good}},
  author = {Tirole, Jean and Davis, Jonathan},
  year = {2018},
  edition = {Unabridged edizione},
  publisher = {{Audible Studios on Brilliance audio}},
  isbn = {978-1-978649-37-8},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Tirole_Davis_2018_Economics_for_the_Common_Good.epub}
}

@article{tomasetti2019,
  title = {Updating {{Variational Bayes}}: {{Fast}} Sequential Posterior Inference},
  shorttitle = {Updating {{Variational Bayes}}},
  author = {Tomasetti, Nathaniel and Forbes, Catherine S. and Panagiotelis, Anastasios},
  year = {2019},
  month = aug,
  journal = {arXiv:1908.00225 [cs, stat]},
  eprint = {1908.00225},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Variational Bayesian (VB) methods produce posterior inference in a time frame considerably smaller than traditional Markov Chain Monte Carlo approaches. Although the VB posterior is an approximation, it has been shown to produce good parameter estimates and predicted values when a rich classes of approximating distributions are considered. In this paper we propose Updating VB (UVB), a recursive algorithm used to update a sequence of VB posterior approximations in an online setting, with the computation of each posterior update requiring only the data observed since the previous update. An extension to the proposed algorithm, named UVB-IS, allows the user to trade accuracy for a substantial increase in computational speed through the use of importance sampling. The two methods and their properties are detailed in two separate simulation studies. Two empirical illustrations of the proposed UVB methods are provided, including one where a Dirichlet Process Mixture model with a novel posterior dependence structure is repeatedly updated in the context of predicting the future behaviour of vehicles on a stretch of the US Highway 101.},
  archiveprefix = {arXiv},
  keywords = {62-04,Computer Science - Machine Learning,Statistics - Computation,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Tomasetti_et_al_2019_Updating_Variational_Bayes.pdf;/home/dede/Zotero/storage/5J6IB7ZI/1908.html}
}

@book{trefethen1997,
  title = {{Numerical Linear Algebra}},
  author = {Trefethen, Lloyd N. and Bau, David},
  year = {1997},
  month = jun,
  edition = {New edizione},
  publisher = {{Society for Industrial and Applied Mathematics}},
  address = {{Philadelphia}},
  abstract = {This is a concise, insightful introduction to the field of numerical linear algebra. The clarity and eloquence of the presentation make it popular with teachers and students alike. The text aims to expand the reader's view of the field and to present standard material in a novel way. All of the most important topics in the field are covered with a fresh perspective, including iterative methods for systems of equations and eigenvalue problems and the underlying principles of conditioning and stability. Presentation is in the form of 40 lectures, which each focus on one or two central ideas. The unity between topics is emphasized throughout, with no risk of getting lost in details and technicalities. The book breaks with tradition by beginning with the QR factorization - an important and fresh idea for students, and the thread that connects most of the algorithms of numerical linear algebra.},
  isbn = {978-0-89871-361-9},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Trefethen_Bau_1997_Numerical_Linear_Algebra.pdf}
}

@book{triantafyllopoulos2021,
  title = {Bayesian {{Inference}} of {{State Space Models}}: {{Kalman Filtering}} and {{Beyond}}},
  shorttitle = {Bayesian {{Inference}} of {{State Space Models}}},
  author = {Triantafyllopoulos, Kostas},
  year = {2021},
  series = {Springer {{Texts}} in {{Statistics}}},
  publisher = {{Springer}},
  isbn = {978-3-030-76123-3},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Triantafyllopoulos_2021_Bayesian_Inference_of_State_Space_Models.pdf}
}

@incollection{trivedi2014,
  title = {Markov {{Modeling}} in {{Reliability}}},
  booktitle = {Wiley {{StatsRef}}: {{Statistics Reference Online}}},
  author = {Trivedi, Kishor S. and Selvamuthu, Dharmaraja},
  editor = {Balakrishnan, N. and Colton, Theodore and Everitt, Brian and Piegorsch, Walter and Ruggeri, Fabrizio and Teugels, Jozef L.},
  year = {2014},
  month = sep,
  pages = {stat03635},
  publisher = {{John Wiley \& Sons, Ltd}},
  address = {{Chichester, UK}},
  doi = {10.1002/9781118445112.stat03635},
  isbn = {978-1-118-44511-2},
  langid = {english},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Trivedi_Selvamuthu_2014_Markov_Modeling_in_Reliability.pdf}
}

@inproceedings{trovo2016,
  title = {Budgeted {{Multi-Armed Bandit}} in {{Continuous Action Space}}},
  booktitle = {{{ECAI}}},
  author = {Trov{\`o}, F. and Paladino, Stefano and Restelli, Marcello and Gatti, N.},
  year = {2016},
  doi = {10.3233/978-1-61499-672-9-560},
  abstract = {A novel algorithm, named B\textendash Zoom, which suffers a regret of \~O(B d+1 d+2 ), where d is the Zooming dimension of the problem and the proposed approach is more robust to adverse settings as compared to existing algorithms designed for BMAB. Multi\textendash Armed Bandits (MABs) have been widely considered in the last decade to model settings in which an agent wants to learn the action providing the highest expected reward among a fixed set of available actions during the operational life of a system. Classical techniques provide solutions that minimize the regret due to learning in settings where selecting an arm has no cost. Though, in many real world applications the learner has to pay some cost for pulling each arm and the learning process is constrained by a fixed budgetB. This problem is addressed in the literature as the Budgeted MAB (BMAB). In this paper, for the first time, we study the problem of Budgeted Continuous\textendash Armed Bandit (BCAB), where the set of the possible actions consists in a continuous set (e.g., a range of prices) and the learner suffers from a random reward and cost at each round. We provide a novel algorithm, named B\textendash Zoom, which suffers a regret of \~O(B d+1 d+2 ), where d is the Zooming dimension of the problem. Finally, we provide an empirical analysis showing that, despite a lower average performance, the proposed approach is more robust to adverse settings as compared to existing algorithms designed for BMAB.},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/TrovÃ²_et_al_2016_Budgeted_Multi-Armed_Bandit_in_Continuous_Action_Space.pdf}
}

@article{truong2020,
  title = {Selective Review of Offline Change Point Detection Methods},
  author = {Truong, Charles and Oudre, Laurent and Vayatis, Nicolas},
  year = {2020},
  month = feb,
  journal = {Signal Processing},
  volume = {167},
  eprint = {1801.00718},
  eprinttype = {arxiv},
  pages = {107299},
  issn = {01651684},
  doi = {10.1016/j.sigpro.2019.107299},
  abstract = {This article presents a selective survey of algorithms for the offline detection of multiple change points in multivariate time series. A general yet structuring methodological strategy is adopted to organize this vast body of work. More precisely, detection algorithms considered in this review are characterized by three elements: a cost function, a search method and a constraint on the number of changes. Each of those elements is described, reviewed and discussed separately. Implementations of the main algorithms described in this article are provided within a Python package called ruptures.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computational Engineering; Finance; and Science,Statistics - Computation,Statistics - Methodology,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Truong_et_al_2020_Selective_review_of_offline_change_point_detection_methods.pdf;/home/dede/Zotero/storage/M85ZQ5U8/1801.html}
}

@book{tsay2010,
  title = {{Analysis of Financial Time Series}},
  author = {Tsay, Ruey S.},
  year = {2010},
  edition = {3\textdegree{} edizione},
  publisher = {{John Wiley \& Sons Inc}},
  address = {{Cambridge, Mass}},
  abstract = {This book provides a broad, mature, and systematic introduction to current financial econometric models and their applications to modeling and prediction of financial time series data. It utilizes real-world examples and real financial data throughout the book to apply the models and methods described.  The author begins with basic characteristics of financial time series data before covering three main topics:  Analysis and application of univariate financial time series The return series of multiple assets Bayesian inference in finance methods  Key features of the new edition include additional coverage of modern day topics such as arbitrage, pair trading, realized volatility, and credit risk modeling; a smooth transition from S-Plus to R; and expanded empirical financial data sets. The overall objective of the book is to provide some knowledge of financial time series, introduce some statistical tools useful for analyzing these series and gain experience in financial applications of various econometric methods..},
  isbn = {978-0-470-41435-4},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Tsay_2010_Analysis_of_Financial_Time_Series.pdf}
}

@book{tsay2013,
  title = {{Multivariate Time Series Analysis: With R and Financial Applications}},
  shorttitle = {{Multivariate Time Series Analysis}},
  author = {Tsay, Ruey S.},
  year = {2013},
  month = nov,
  edition = {1. edizione},
  publisher = {{John Wiley \& Sons Inc}},
  address = {{Hoboken, New Jersey}},
  abstract = {An accessible guide to the multivariate time series tools used in numerous real-world applications Multivariate Time Series Analysis: With R and Financial Applications is the much anticipated sequel coming from one of the most influential and prominent experts on the topic of time series. Through a fundamental balance of theory and methodology, the book supplies readers with a comprehensible approach to financial econometric models and their applications to real-world empirical research. Differing from the traditional approach to multivariate time series, the book focuses on reader comprehension by emphasizing structural specification, which results in simplified parsimonious VAR MA modeling. Multivariate Time Series Analysis: With R and Financial Applications utilizes the freely available R software package to explore complex data and illustrate related computation and analyses. Featuring the techniques and methodology of multivariate linear time series, stationary VAR models, VAR MA time series and models, unitroot process, factor models, and factor-augmented VAR models, the book includes: \textbullet{} Over 300 examples and exercises to reinforce the presented content \textbullet{} User-friendly R subroutines and research presented throughout to demonstrate modern applications \textbullet{} Numerous datasets and subroutines to provide readers with a deeper understanding of the material Multivariate Time Series Analysis is an ideal textbook for graduate-level courses on time series and quantitative finance and upper-undergraduate level statistics courses in time series. The book is also an indispensable reference for researchers and practitioners in business, finance, and econometrics.},
  isbn = {978-1-118-61790-8},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Tsay_2013_Multivariate_Time_Series_Analysis.pdf}
}

@article{tsiamyrtzis2005,
  title = {A {{Bayesian Scheme}} to {{Detect Changes}} in the {{Mean}} of a {{Short-Run Process}}},
  author = {Tsiamyrtzis, Panagiotis and Hawkins, Douglas M},
  year = {2005},
  month = nov,
  journal = {Technometrics},
  volume = {47},
  number = {4},
  pages = {446--456},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1198/004017005000000346},
  abstract = {In this article we propose a model suitable for statistical process control in short production runs. We wish to detect on-line whether the mean of the process has exceeded a prespecified upper threshold value. The theoretical basis of the model is a Bayesian formulation, leading to a mixture of normal distributions. Issues of decisions about whether the process is within specification and forecasting are addressed. The Kalman filter model is shown to be related to a special case of our model. The calculations are illustrated with a clinical chemistry example. The tool wear problem is another potential candidate for our approach.},
  keywords = {Bayesian statistical process control,Kalman filter,Normal mixture,Tool wear},
  annotation = {\_eprint: https://doi.org/10.1198/004017005000000346},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Tsiamyrtzis_Hawkins_2005_A_Bayesian_Scheme_to_Detect_Changes_in_the_Mean_of_a_Short-Run_Process.pdf;/home/dede/Zotero/storage/YV5CSCF6/004017005000000346.html}
}

@article{tsiamyrtzis2008,
  title = {A {{Bayesian EWMA}} Method to Detect Jumps at the Start-up Phase of a Process},
  author = {Tsiamyrtzis, Panagiotis and Hawkins, Douglas M.},
  year = {2008},
  journal = {Quality and Reliability Engineering International},
  volume = {24},
  number = {6},
  pages = {721--735},
  issn = {1099-1638},
  doi = {10.1002/qre.952},
  abstract = {The start-up phase data of a process are the spine of traditional SPC charting and testing methods and are usually assumed to be i.i.d. observations from the in-control distribution. In this work a new method is proposed to model normally distributed start-up phase data where we allow for serial dependence and randomly occurring unidirectional level shifts of the underlying parameter of interest. The theoretic development is based on a Bayesian sequentially updated EWMA model with normal mixture errors. The new approach makes use of available prior information and provides a framework for drawing decisions and making prediction on line, even with a single observation. Copyright \textcopyright{} 2008 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {Bayesian statistical process control (SPC),correlated data,normal mixtures,phase I,short runs},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/qre.952},
  file = {/home/dede/Zotero/storage/CWEW2JWH/qre.html}
}

@article{tsiamyrtzis2010,
  title = {Bayesian {{Startup Phase Mean Monitoring}} of an {{Autocorrelated Process That Is Subject}} to {{Random Sized Jumps}}},
  author = {Tsiamyrtzis, Panagiotis and Hawkins, Douglas M.},
  year = {2010},
  month = nov,
  journal = {Technometrics},
  volume = {52},
  number = {4},
  pages = {438--452},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1198/TECH.2010.08053},
  abstract = {In this work we will provide a monitoring scheme for the mean of an autocorrelated process which can experience bidirectional jumps of random size and occurrence and has a steady state. Our interest focuses in the start up phase and short-run scenarios, where traditional SPC techniques fail to provide formal testing. Furthermore, we will provide a framework where prior information regarding the process can be employed. These will be achieved by adopting a Bayesian sequentially updated scheme that will allow inference in an online fashion. The performance of the proposed model will be compared against other methods which can be applied in similar settings. A simulation study along with a real data application from the dairy business will conclude this work. Supplemental files are available online, with technical details and the code for applying the proposed methodology in R.},
  keywords = {Bayesian statistical process control,Normal mixtures,Online inference,Short runs},
  annotation = {\_eprint: https://doi.org/10.1198/TECH.2010.08053},
  file = {/home/dede/Zotero/storage/ZAPX9JB2/TECH.2010.html}
}

@article{tsiamyrtzis2015,
  title = {Use of Prior Manufacturer Specifications with {{Bayesian}} Logic Eludes Preliminary Phase Issues in Quality Control: An Example in a Hemostasis Laboratory},
  shorttitle = {Use of Prior Manufacturer Specifications with {{Bayesian}} Logic Eludes Preliminary Phase Issues in Quality Control},
  author = {Tsiamyrtzis, Panagiotis and Sobas, Fr{\'e}d{\'e}ric and N{\'e}grier, Claude},
  year = {2015},
  month = jul,
  journal = {Blood Coagulation \& Fibrinolysis: An International Journal in Haemostasis and Thrombosis},
  volume = {26},
  number = {5},
  pages = {590--596},
  issn = {1473-5733},
  doi = {10.1097/MBC.0000000000000314},
  abstract = {The present study seeks to demonstrate the feasibility of avoiding the preliminary phase, which is mandatory in all conventional approaches for internal quality control (IQC) management. Apart from savings on the resources consumed by the preliminary phase, the alternative approach described here is able to detect any analytic problems during the startup and provide a foundation for subsequent conventional assessment. A new dynamically updated predictive control chart (PCC) is used. Being Bayesian in concept, it utilizes available prior information. The manufacturer's prior quality control target value, the manufacturer's maximum acceptable interassay coefficient of variation value and the interassay standard deviation value defined during method validation in each laboratory, allow online IQC management. An Excel template, downloadable from journal website, allows easy implementation of this alternative approach in any laboratory. In the practical case of prothrombin percentage measurement, PCC gave no false alarms with respect to the 1ks rule (with same 5\% false-alarm probability on a single control sample) during an overlap phase between two IQC batches. Moreover, PCCs were as effective as the 1ks rule in detecting increases in both random and systematic error after the minimal preliminary phase required by medical biology guidelines. PCCs can improve efficiency in medical biology laboratories.},
  langid = {english},
  pmid = {25978121},
  keywords = {Bayes Theorem,Hemostasis,Humans,Laboratories,Quality Control},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Tsiamyrtzis_et_al_2015_Use_of_prior_manufacturer_specifications_with_Bayesian_logic_eludes_preliminary.pdf}
}

@article{tsiamyrtzis2018,
  title = {Bayesian Statistical Process Control for {{Phase I}} Count Type Data},
  author = {Tsiamyrtzis, Panagiotis and Hawkins, Douglas},
  year = {2018},
  month = sep,
  journal = {Applied Stochastic Models in Business and Industry},
  volume = {35},
  doi = {10.1002/asmb.2398},
  abstract = {Count data, most often modeled by a Poisson distribution, are common in statistical process control. They are traditionally monitored by frequentist c or u charts, by cumulative sum and by exponentially weighted moving average charts. These charts all assume that the in-control true mean is known, a common fiction that is addressed by gathering a large Phase I sample and using it to estimate the mean. ``Self-starting'' proposals that ameliorate the need for a large Phase I sample have also appeared. All these methods are frequentist, ie, they allow only retrospective inference during Phase I, and they have no coherent way to incorporate less-than-perfect prior information about the in-control mean. In this paper, we introduce a Bayesian procedure that can incorporate prior information, allow online inference, and should be particularly attractive for short-run settings where large Phase I calibration exercises are impossible or unreasonable.},
  keywords = {todo}
}

@article{tsung2002,
  title = {The Dynamic {{T2}} Chart for Monitoring Feedback-Controlled Processes},
  author = {Tsung, Fugee and Apley, Daniel W.},
  year = {2002},
  journal = {IIE Transactions},
  volume = {34},
  number = {12},
  pages = {1043--1053},
  publisher = {{Taylor \& Francis}},
  issn = {0740-817X},
  doi = {10.1080/07408170208928933},
  abstract = {As manufacturing quality has become a decisive factor in global market competition, statistical quality techniques such as Statistical Process Control (SPC) are widely used in industry. With advances in information, sensing, and data collection technology, large volumes of data are routinely available in processes employing Automatic Process Control (APC) and Engineering Process Control (EPC). Although there is a growing need for SPC monitoring in these feedback-controlled environments, an effective implementation scheme is still lacking. This research provides a monitoring method, termed the dynamic T1 chart that improves the detection of assignable causes in feedback-controlled processes.},
  keywords = {reference only},
  annotation = {\_eprint: https://doi.org/10.1080/07408170208928933},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/TSUNG_APLEY_2002_The_dynamic_T_2_chart_for_monitoring_feedback-controlled_processes.pdf;/home/dede/Zotero/storage/N6SYHHFK/07408170208928933.html}
}

@incollection{tsung2010,
  title = {Adaptive {{Charting Techniques}}: {{Literature Review}} and {{Extensions}}},
  shorttitle = {Adaptive {{Charting Techniques}}},
  booktitle = {Frontiers in {{Statistical Quality Control}} 9},
  author = {Tsung, Fujee and Wang, Kaibo},
  editor = {Lenz, Hans-Joachim and Wilrich, Peter-Theodor and Schmid, Wolfgang},
  year = {2010},
  pages = {19--35},
  publisher = {{Physica-Verlag HD}},
  address = {{Heidelberg}},
  doi = {10.1007/978-3-7908-2380-6_2},
  abstract = {The continuous development of SPC is driven by challenges arising from practical applications across diverse industries. Among others, adaptive charts are becoming more and more popular due to their capability in tackling these challenges by learning unknown shifts and tracking time-varying patterns. This chapter reviews recent development of adaptive charts and classifies them into two categories: those with variable sampling parameters and those with variable design parameters. This review focuses on the latter group and compares their charting performance. As an extension to conventional multivariate charts, this work proposes a double-sided directionally variant chart. The proposed chart is capable of detecting shifts having the same or opposite directions as the reference vector and is more robust to processes with unpredictable shift directions.},
  isbn = {978-3-7908-2380-6},
  langid = {english},
  keywords = {Control Chart,done,Exponentially Weighted Move Average,Independent Component Analysis,Quality Technology,Statistical Process Control},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Tsung_Wang_2010_Adaptive_Charting_Techniques.pdf}
}

@article{tsung2018,
  title = {Statistical Transfer Learning: {{A}} Review and Some Extensions to Statistical Process Control},
  shorttitle = {Statistical Transfer Learning},
  author = {Tsung, Fugee and Zhang, Ke and Cheng, Longwei and Song, Zhenli},
  year = {2018},
  month = jan,
  journal = {Quality Engineering},
  volume = {30},
  number = {1},
  pages = {115--128},
  publisher = {{Taylor \& Francis}},
  issn = {0898-2112},
  doi = {10.1080/08982112.2017.1373810},
  abstract = {The rapid development of information technology, together with advances in sensory and data acquisition techniques, has led to the increasing necessity of handling datasets from multiple domains. In recent years, transfer learning has emerged as an effective framework for tackling related tasks in target domains by transferring previously-acquired knowledge from source domains. Statistical models and methodologies are widely involved in transfer learning and play a critical role, which, however, has not been emphasized in most surveys of transfer learning. In this article, we conduct a comprehensive literature review on statistical transfer learning, i.e., transfer learning techniques with a focus on statistical models and statistical methodologies, demonstrating how statistics can be used in transfer learning. In addition, we highlight opportunities for the use of statistical transfer learning to improve statistical process control and quality control. Several potential future issues in statistical transfer learning are discussed.},
  keywords = {3D printing,bayesian modeling,done,landslides,quality control,regularization,statistical process control,statistical transfer learning,transfer learning,urban rail transit},
  annotation = {\_eprint: https://doi.org/10.1080/08982112.2017.1373810},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Tsung_et_al_2018_Statistical_transfer_learning.pdf;/home/dede/Zotero/storage/VFUX6DVY/08982112.2017.html}
}

@article{tucker2020,
  title = {Constrained {{Thompson Sampling}} for {{Real-Time Electricity Pricing}} with {{Grid Reliability Constraints}}},
  author = {Tucker, Nathaniel and Moradipari, Ahmadreza and Alizadeh, Mahnoosh},
  year = {2020},
  month = jun,
  journal = {arXiv:1908.07964 [cs, eess]},
  eprint = {1908.07964},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {We consider the problem of an aggregator attempting to learn customers' load flexibility models while implementing a load shaping program by means of broadcasting daily dispatch signals. We adopt a multi-armed bandit formulation to account for the stochastic and unknown nature of customers' responses to dispatch signals. We propose a constrained Thompson sampling heuristic, Con-TS-RTP, that accounts for various possible aggregator objectives (e.g., to reduce demand at peak hours, integrate more intermittent renewable generation, track a desired daily load profile, etc) and takes into account the operational constraints of a distribution system to avoid potential grid failures as a result of uncertainty in the customers' response. We provide a discussion on the regret bounds for our algorithm as well as a discussion on the operational reliability of the distribution system's constraints being upheld throughout the learning process.},
  archiveprefix = {arXiv},
  keywords = {Electrical Engineering and Systems Science - Systems and Control,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Tucker_et_al_2020_Constrained_Thompson_Sampling_for_Real-Time_Electricity_Pricing_with_Grid.pdf;/home/dede/Zotero/storage/SIGKUTQW/1908.html}
}

@misc{ushey2021,
  title = {Interface to {{Python}}},
  author = {Ushey, Kevin and Allaire, J. J. and Tang, Yuan},
  year = {2021},
  abstract = {Interface to Python modules, classes, and functions. When calling     into Python, R data types are automatically converted to their equivalent Python     types. When values are returned from Python to R they are converted back to R     types. Compatible with all versions of Python {$>$}= 2.7.},
  file = {/home/dede/Zotero/storage/AAKMNJ6H/reticulate.html}
}

@book{valliant2018,
  title = {{Practical Tools for Designing and Weighting Survey Samples}},
  author = {Valliant, Richard and Dever, Jill A. and Kreuter, Frauke},
  year = {2018},
  edition = {Second},
  publisher = {{Springer Nature}},
  address = {{Cham}},
  isbn = {978-3-319-93631-4},
  langid = {Inglese}
}

@book{vanbuuren2018,
  title = {Flexible {{Imputation}} of {{Missing Data}}, {{Second Edition}}},
  author = {{van Buuren}, Stef},
  year = {2018},
  month = jul,
  edition = {2nd edition},
  publisher = {{Chapman and Hall/CRC}},
  address = {{Boca Raton}},
  abstract = {Missing data pose challenges to real-life data analysis. Simple ad-hoc fixes, like deletion or mean imputation, only work under highly restrictive conditions, which are often not met in practice. Multiple imputation replaces each missing value by multiple plausible values. The variability between these replacements reflects our ignorance of the true (but missing) value. Each of the completed data set is then analyzed by standard methods, and the results are pooled to obtain unbiased estimates with correct confidence intervals. Multiple imputation is a general approach that also inspires novel solutions to old problems by reformulating the task at hand as a missing-data problem.  This is the second edition of a popular book on multiple imputation, focused on explaining the application of methods through detailed worked examples using the MICE package as developed by the author. This new edition incorporates the recent developments in this fast-moving field. This class-tested book avoids mathematical and technical details as much as possible: formulas are accompanied by verbal statements that explain the formula in accessible terms. The book sharpens the reader's intuition on how to think about missing data, and provides all the tools needed to execute a well-grounded quantitative analysis in the presence of missing data.},
  isbn = {978-1-138-58831-8},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/van_Buuren_2018_Flexible_Imputation_of_Missing_Data,_Second_Edition.pdf}
}

@article{vandegeer2014,
  title = {On Asymptotically Optimal Confidence Regions and Tests for High-Dimensional Models},
  author = {{Van de Geer}, Sara and B{\"u}hlmann, Peter and Ritov, Ya'acov and Dezeure, Ruben},
  year = {2014},
  month = jun,
  journal = {The Annals of Statistics},
  volume = {42},
  number = {3},
  pages = {1166--1202},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/14-AOS1221},
  abstract = {We propose a general method for constructing confidence intervals and statistical tests for single or low-dimensional components of a large parameter vector in a high-dimensional model. It can be easily adjusted for multiplicity taking dependence among tests into account. For linear models, our method is essentially the same as in Zhang and Zhang [J. R. Stat. Soc. Ser. B Stat. Methodol. 76 (2014) 217\textendash 242]: we analyze its asymptotic properties and establish its asymptotic optimality in terms of semiparametric efficiency. Our method naturally extends to generalized linear models with convex loss functions. We develop the corresponding theory which includes a careful analysis for Gaussian, sub-Gaussian and bounded correlated designs.},
  keywords = {62F25,62J07,62J12,central limit theorem,generalized linear model,Lasso,linear model,multiple testing,Semiparametric efficiency,Sparsity,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Geer_et_al_2014_On_asymptotically_optimal_confidence_regions_and_tests_for_high-dimensional.pdf;/home/dede/Zotero/storage/S6J9JWD5/14-AOS1221.html}
}

@book{vandervaart1998,
  title = {Asymptotic {{Statistics}}},
  author = {{van der Vaart}, A. W.},
  year = {1998},
  series = {Cambridge {{Series}} in {{Statistical}} and {{Probabilistic Mathematics}}},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/CBO9780511802256},
  abstract = {This book is an introduction to the field of asymptotic statistics. The treatment is both practical and mathematically rigorous. In addition to most of the standard topics of an asymptotics course, including likelihood inference, M-estimation, the theory of asymptotic efficiency, U-statistics, and rank procedures, the book also presents recent research topics such as semiparametric models, the bootstrap, and empirical processes and their applications. The topics are organized from the central idea of approximation by limit experiments, which gives the book one of its unifying themes. This entails mainly the local approximation of the classical i.i.d. set up with smooth parameters by location experiments involving a single, normally distributed observation. Thus, even the standard subjects of asymptotic statistics are presented in a novel way. Suitable as a graduate or Master's level statistics text, this book will also give researchers an overview of research in asymptotic statistics.},
  isbn = {978-0-521-78450-4},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/van_der_Vaart_1998_Asymptotic_Statistics.pdf;/home/dede/Zotero/storage/Y658Z2ZD/A3C7DAD3F7E66A1FA60E9C8FE132EE1D.html}
}

@article{vanwieringen2020,
  title = {Lecture Notes on Ridge Regression},
  author = {{van Wieringen}, Wessel N.},
  year = {2020},
  month = jan,
  journal = {arXiv:1509.09169 [stat]},
  eprint = {1509.09169},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {The linear regression model cannot be fitted to high-dimensional data, as the high-dimensionality brings about empirical non-identifiability. Penalized regression overcomes this non-identifiability by augmentation of the loss function by a penalty (i.e. a function of regression coefficients). The ridge penalty is the sum of squared regression coefficients, giving rise to ridge regression. Here many aspect of ridge regression are reviewed e.g. moments, mean squared error, its equivalence to constrained estimation, and its relation to Bayesian regression. Finally, its behaviour and use are illustrated in simulation and on omics data. Subsequently, ridge regression is generalized to allow for a more general penalty. The ridge penalization framework is then translated to logistic regression and its properties are shown to carry over. To contrast ridge penalized estimation, the final chapter introduces its lasso counterpart.},
  archiveprefix = {arXiv},
  keywords = {done,Statistics - Methodology},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/van_Wieringen_2020_Lecture_notes_on_ridge_regression.pdf;/home/dede/Zotero/storage/HH74N7FJ/1509.html}
}

@article{vargas2009,
  title = {On the {{Run Length}} of a {{State-Space Control Chart}} for {{Multivariate Autocorrelated Data}}},
  author = {Vargas, Manuel and Alfaro, Jos{\'e} Luis and Mond{\'e}jar, Jos{\'e}},
  year = {2009},
  month = oct,
  journal = {Communications in Statistics - Simulation and Computation},
  volume = {38},
  number = {9},
  pages = {1823--1833},
  publisher = {{Taylor \& Francis}},
  issn = {0361-0918},
  doi = {10.1080/03610910903147771},
  abstract = {The literature on statistical process control (SPC) describes the negative effects of autocorrelation in terms of the increase in false alarms. This has been treated by the individual modeling of each series or the application of VAR models. In the former case, the analysis of the cross correlation structure between the variables is altered. In the latter, if the cross correlation is not strong, the filtering process may modify the weakest relations. In order to improve these aspects, state-space models have been introduced in multivariate statistical process control (MSPC). This article presents a proposal for building a control chart for innovations, estimating its average run length to highlight its advantages over the VAR approach mentioned above.},
  keywords = {62P30,93C83,Average run length,done,Multivariate control chart,State-space model,Statistical process control},
  annotation = {\_eprint: https://doi.org/10.1080/03610910903147771},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Vargas_et_al_2009_On_the_Run_Length_of_a_State-Space_Control_Chart_for_Multivariate.pdf;/home/dede/Zotero/storage/4HM6FNK8/03610910903147771.html}
}

@article{vaswani2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  month = dec,
  journal = {arXiv:1706.03762 [cs]},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Vaswani_et_al_2017_Attention_Is_All_You_Need.pdf;/home/dede/Zotero/storage/V4CMM9FJ/1706.html}
}

@article{veeravalli2012,
  title = {Quickest {{Change Detection}}},
  author = {Veeravalli, Venugopal V. and Banerjee, Taposh},
  year = {2012},
  month = oct,
  journal = {arXiv:1210.5552 [cs, math, stat]},
  eprint = {1210.5552},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {The problem of detecting changes in the statistical properties of a stochastic system and time series arises in various branches of science and engineering. It has a wide spectrum of important applications ranging from machine monitoring to biomedical signal processing. In all of these applications the observations being monitored undergo a change in distribution in response to a change or anomaly in the environment, and the goal is to detect the change as quickly as possibly, subject to false alarm constraints. In this chapter, two formulations of the quickest change detection problem, Bayesian and minimax, are introduced, and optimal or asymptotically optimal solutions to these formulations are discussed. Then some generalizations and extensions of the quickest change detection problem are described. The chapter is concluded with a discussion of applications and open issues.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Theory,Mathematics - Optimization and Control,Mathematics - Probability,Mathematics - Statistics Theory,Statistics - Applications,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Veeravalli_Banerjee_2012_Quickest_Change_Detection.pdf;/home/dede/Zotero/storage/RNBT4QDM/1210.html}
}

@article{vehtari2012,
  title = {A Survey of {{Bayesian}} Predictive Methods for Model Assessment, Selection and Comparison},
  author = {Vehtari, Aki and Ojanen, Janne},
  year = {2012},
  month = jan,
  journal = {Statistics Surveys},
  volume = {6},
  number = {none},
  pages = {142--228},
  publisher = {{Amer. Statist. Assoc., the Bernoulli Soc., the Inst. Math. Statist., and the Statist. Soc. Canada}},
  issn = {1935-7516},
  doi = {10.1214/12-SS102},
  abstract = {To date, several methods exist in the statistical literature for model assessment, which purport themselves specifically as Bayesian predictive methods. The decision theoretic assumptions on which these methods are based are not always clearly stated in the original articles, however. The aim of this survey is to provide a unified review of Bayesian predictive model assessment and selection methods, and of methods closely related to them. We review the various assumptions that are made in this context and discuss the connections between different approaches, with an emphasis on how each method approximates the expected utility of using a Bayesian model for the purpose of predicting future data.},
  keywords = {Bayesian,cross-validation,decision theory,done,Expected utility,information criteria,Model selection,predictive},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Vehtari_Ojanen_2012_A_survey_of_Bayesian_predictive_methods_for_model_assessment,_selection_and.pdf;/home/dede/Zotero/storage/55RVZM4Q/12-SS102.html}
}

@article{vehtari2016,
  title = {Practical {{Bayesian}} Model Evaluation Using Leave-One-out Cross-Validation and {{WAIC}}},
  author = {Vehtari, Aki and Gelman, Andrew and Gabry, Jonah},
  year = {2016},
  month = sep,
  journal = {arXiv:1507.04544 [stat]},
  eprint = {1507.04544},
  eprinttype = {arxiv},
  primaryclass = {stat},
  doi = {10.1007/s11222-016-9696-4},
  abstract = {Leave-one-out cross-validation (LOO) and the widely applicable information criterion (WAIC) are methods for estimating pointwise out-of-sample prediction accuracy from a fitted Bayesian model using the log-likelihood evaluated at the posterior simulations of the parameter values. LOO and WAIC have various advantages over simpler estimates of predictive error such as AIC and DIC but are less used in practice because they involve additional computational steps. Here we lay out fast and stable computations for LOO and WAIC that can be performed using existing simulation draws. We introduce an efficient computation of LOO using Pareto-smoothed importance sampling (PSIS), a new procedure for regularizing importance weights. Although WAIC is asymptotically equal to LOO, we demonstrate that PSIS-LOO is more robust in the finite case with weak priors or influential observations. As a byproduct of our calculations, we also obtain approximate standard errors for estimated predictive errors and for comparing of predictive errors between two models. We implement the computations in an R package called 'loo' and demonstrate using models fit with the Bayesian inference package Stan.},
  archiveprefix = {arXiv},
  keywords = {done,Statistics - Computation,Statistics - Methodology},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Vehtari_et_al_2016_Practical_Bayesian_model_evaluation_using_leave-one-out_cross-validation_and.pdf;/home/dede/Zotero/storage/7K86R7YZ/1507.html}
}

@article{vehtari2019,
  title = {Expectation Propagation as a Way of Life: {{A}} Framework for {{Bayesian}} Inference on Partitioned Data},
  shorttitle = {Expectation Propagation as a Way of Life},
  author = {Vehtari, Aki and Gelman, Andrew and Sivula, Tuomas and Jyl{\"a}nki, Pasi and Tran, Dustin and Sahai, Swupnil and Blomstedt, Paul and Cunningham, John P. and Schiminovich, David and Robert, Christian},
  year = {2019},
  month = nov,
  journal = {arXiv:1412.4869 [stat]},
  eprint = {1412.4869},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {A common divide-and-conquer approach for Bayesian computation with big data is to partition the data, perform local inference for each piece separately, and combine the results to obtain a global posterior approximation. While being conceptually and computationally appealing, this method involves the problematic need to also split the prior for the local inferences; these weakened priors may not provide enough regularization for each separate computation, thus eliminating one of the key advantages of Bayesian methods. To resolve this dilemma while still retaining the generalizability of the underlying local inference method, we apply the idea of expectation propagation (EP) as a framework for distributed Bayesian inference. The central idea is to iteratively update approximations to the local likelihoods given the state of the other approximations and the prior. The present paper has two roles: we review the steps that are needed to keep EP algorithms numerically stable, and we suggest a general approach, inspired by EP, for approaching data partitioning problems in a way that achieves the computational benefits of parallelism while allowing each local update to make use of relevant information from the other sites. In addition, we demonstrate how the method can be applied in a hierarchical context to make use of partitioning of both data and parameters. The paper describes a general algorithmic framework, rather than a specific algorithm, and presents an example implementation for it.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Vehtari_et_al_2019_Expectation_propagation_as_a_way_of_life.pdf}
}

@article{vehtari2020,
  title = {Rank-Normalization, Folding, and Localization: {{An}} Improved \$\textbackslash widehat\{\vphantom\}{{R}}\vphantom\{\}\$ for Assessing Convergence of {{MCMC}}},
  shorttitle = {Rank-Normalization, Folding, and Localization},
  author = {Vehtari, Aki and Gelman, Andrew and Simpson, Daniel and Carpenter, Bob and B{\"u}rkner, Paul-Christian},
  year = {2020},
  month = jan,
  journal = {arXiv:1903.08008 [stat]},
  eprint = {1903.08008},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Markov chain Monte Carlo is a key computational tool in Bayesian statistics, but it can be challenging to monitor the convergence of an iterative stochastic algorithm. In this paper we show that the convergence diagnostic \$\textbackslash widehat\{R\}\$ of Gelman and Rubin (1992) has serious flaws. Traditional \$\textbackslash widehat\{R\}\$ will fail to correctly diagnose convergence failures when the chain has a heavy tail or when the variance varies across the chains. In this paper we propose an alternative rank-based diagnostic that fixes these problems. We also introduce a collection of quantile-based local efficiency measures, along with a practical approach for computing Monte Carlo error estimates for quantiles. We suggest that common trace plots should be replaced with rank plots from multiple chains. Finally, we give recommendations for how these methods should be used in practice.},
  archiveprefix = {arXiv},
  keywords = {done,Markov Chain mixing,MCMC,posterior convergence,Statistics - Computation,Statistics - Methodology},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Vehtari_et_al_2020_Rank-normalization,_folding,_and_localization.pdf;/home/dede/Zotero/storage/T23SIYGT/1903.html}
}

@article{vehtari2021,
  title = {Pareto {{Smoothed Importance Sampling}}},
  author = {Vehtari, Aki and Simpson, Daniel and Gelman, Andrew and Yao, Yuling and Gabry, Jonah},
  year = {2021},
  month = feb,
  journal = {arXiv:1507.02646 [stat]},
  eprint = {1507.02646},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Importance weighting is a general way to adjust Monte Carlo integration to account for draws from the wrong distribution, but the resulting estimate can be noisy when the importance ratios have a heavy right tail. This routinely occurs when there are aspects of the target distribution that are not well captured by the approximating distribution, in which case more stable estimates can be obtained by modifying extreme importance ratios. We present a new method for stabilizing importance weights using a generalized Pareto distribution fit to the upper tail of the distribution of the simulated importance ratios. The method, which empirically performs better than existing methods for stabilizing importance sampling estimates, includes stabilized effective sample size estimates, Monte Carlo error estimates and convergence diagnostics.},
  archiveprefix = {arXiv},
  keywords = {done,Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Vehtari_et_al_2021_Pareto_Smoothed_Importance_Sampling.pdf;/home/dede/Zotero/storage/9FRL6D36/1507.html}
}

@book{velleman2019,
  title = {How to {{Prove It}}: {{A Structured Approach}}},
  shorttitle = {How to {{Prove It}}},
  author = {Velleman, Daniel J.},
  year = {2019},
  month = aug,
  edition = {3rd edition},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge ; New York, NY}},
  isbn = {978-1-108-43953-4},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Velleman_2019_How_to_Prove_It.pdf}
}

@book{venables2004,
  title = {Modern {{Applied Statistics}} with {{S}}},
  author = {Venables, W. N. and Ripley, B. D.},
  year = {2004},
  series = {Statistics and {{Computing}}},
  edition = {Fourth},
  publisher = {{Springer-Verlag}},
  address = {{New York}},
  doi = {10.1007/978-0-387-21706-2},
  abstract = {S is a powerful environment for the statistical and graphical analysis of data. It provides the tools to implement many statistical ideas that have been made possible by the widespread availability of workstations having good graphics and computational capabilities. This book is a guide to using S environments to perform statistical analyses and provides both an introduction to the use of S and a course in modern statistical methods. Implementations of S are available commercially in S-PLUS(R) workstations and as the Open Source R for a wide range of computer systems. The aim of this book is to show how to use S as a powerful and graphical data analysis system. Readers are assumed to have a basic grounding in statistics, and so the book is intended for would-be users of S-PLUS or R and both students and researchers using statistics. Throughout, the emphasis is on presenting practical problems and full analyses of real data sets. Many of the methods discussed are state of the art approaches to topics such as linear, nonlinear and smooth regression models, tree-based methods, multivariate analysis, pattern recognition, survival analysis, time series and spatial statistics. Throughout modern techniques such as robust methods, non-parametric smoothing and bootstrapping are used where appropriate. This fourth edition is intended for users of S-PLUS 6.0 or R 1.5.0 or later. A substantial change from the third edition is updating for the current versions of S-PLUS and adding coverage of R. The introductory material has been rewritten to emphasis the import, export and manipulation of data. Increased computational power allows even more computer-intensive methods to be used, and methods such as GLMMs,},
  isbn = {978-0-387-95457-8},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Venables_Ripley_2004_Modern_Applied_Statistics_with_S.pdf;/home/dede/Zotero/storage/SAMAJ7IY/9780387954578.html}
}

@article{verbyla2019,
  title = {A Note on Model Selection Using Information Criteria for General Linear Models Estimated Using {{REML}}},
  author = {Verbyla, Arunas Petras},
  year = {2019},
  journal = {Australian \& New Zealand Journal of Statistics},
  volume = {61},
  number = {1},
  pages = {39--50},
  issn = {1467-842X},
  doi = {10.1111/anzs.12254},
  abstract = {It is common practice to compare the fit of non-nested models using the Akaike (AIC) or Bayesian (BIC) information criteria. The basis of these criteria is the log-likelihood evaluated at the maximum likelihood estimates of the unknown parameters. For the general linear model (and the linear mixed model, which is a special case), estimation is usually carried out using residual or restricted maximum likelihood (REML). However, for models with different fixed effects, the residual likelihoods are not comparable and hence information criteria based on the residual likelihood cannot be used. For model selection, it is often suggested that the models are refitted using maximum likelihood to enable the criteria to be used. The first aim of this paper is to highlight that both the AIC and BIC can be used for the general linear model by using the full log-likelihood evaluated at the REML estimates. The second aim is to provide a derivation of the criteria under REML estimation. This aim is achieved by noting that the full likelihood can be decomposed into a marginal (residual) and conditional likelihood and this decomposition then incorporates aspects of both the fixed effects and variance parameters. Using this decomposition, the appropriate information criteria for model selection of models which differ in their fixed effects specification can be derived. An example is presented to illustrate the results and code is available for analyses using the ASReml-R package.},
  copyright = {\textcopyright{} 2019 Australian Statistical Publishing Association Inc. Published by John Wiley \& Sons Australia Pty Ltd.},
  langid = {english},
  keywords = {Akaike information criterion,Bayesian information criterion,residual maximum likelihood,todo},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/anzs.12254},
  file = {/home/dede/Zotero/storage/WLENNN8C/anzs.html}
}

@incollection{vermunt2014,
  title = {Structural {{Equation Modeling}}: {{Mixture Models}}},
  shorttitle = {Structural {{Equation Modeling}}},
  booktitle = {Wiley {{StatsRef}}: {{Statistics Reference Online}}},
  author = {Vermunt, Jeroen K. and Magidson, Jay},
  editor = {Balakrishnan, N. and Colton, Theodore and Everitt, Brian and Piegorsch, Walter and Ruggeri, Fabrizio and Teugels, Jozef L.},
  year = {2014},
  month = sep,
  pages = {stat06478},
  publisher = {{John Wiley \& Sons, Ltd}},
  address = {{Chichester, UK}},
  doi = {10.1002/9781118445112.stat06478},
  isbn = {978-1-118-44511-2},
  langid = {english},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Vermunt_Magidson_2014_Structural_Equation_Modeling.pdf}
}

@book{vershynin2018,
  title = {{High-Dimensional Probability: An Introduction with Applications in Data Science}},
  shorttitle = {{High-Dimensional Probability}},
  author = {Vershynin, Roman},
  year = {2018},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge ; New York, NY}},
  isbn = {978-1-108-41519-4},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Vershynin_2018_High-Dimensional_Probability.pdf}
}

@inproceedings{viappiani2013,
  title = {Thompson {{Sampling}} for {{Bayesian Bandits}} with {{Resets}}},
  booktitle = {Algorithmic {{Decision Theory}}},
  author = {Viappiani, Paolo},
  editor = {Perny, Patrice and Pirlot, Marc and Tsouki{\`a}s, Alexis},
  year = {2013},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {399--410},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-41575-3_31},
  abstract = {Multi-armed bandit problems are challenging sequential decision problems that have been widely studied as they constitute a mathematical framework that abstracts many different decision problems in fields such as machine learning, logistics, industrial optimization, management of clinical trials, etc. In this paper we address a non stationary environment with expected rewards that are dynamically evolving, considering a particular type of drift, that we call resets, in which the arm qualities are re-initialized from time to time. We compare different arm selection strategies with simulations, focusing on a Bayesian method based on Thompson sampling (a simple, yet effective, technique for trading off between exploration and exploitation).},
  isbn = {978-3-642-41575-3},
  langid = {english},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Viappiani_2013_Thompson_Sampling_for_Bayesian_Bandits_with_Resets2.pdf}
}

@book{villani2008,
  title = {Optimal {{Transport}}: {{Old}} and {{New}}},
  shorttitle = {Optimal {{Transport}}},
  author = {Villani, C{\'e}dric},
  year = {2008},
  month = sep,
  edition = {2009th edition},
  publisher = {{Springer}},
  address = {{Aalborg}},
  abstract = {At the close of the 1980s, the independent contributions of Yann Brenier, Mike Cullen and John Mather launched a revolution in the venerable field of optimal transport founded by G. Monge in the 18th century, which has made breathtaking forays into various other domains of mathematics ever since. The author presents a broad overview of this area, supplying complete and self-contained proofs of all the fundamental results of the theory of optimal transport at the appropriate level of generality. Thus, the book encompasses the broad spectrum ranging from basic theory to the most recent research results.   PhD students or researchers can read the entire book without any prior knowledge of the field. A comprehensive bibliography with notes that extensively discuss the existing literature underlines the book's value as a most welcome reference text on this subject.},
  isbn = {978-87-93102-13-2},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Villani_2008_Optimal_Transport.pdf}
}

@article{villanueva-guerra2017,
  title = {A Control Chart for Variance Based on Squared Ranks},
  author = {{Villanueva-Guerra}, Elena Cristina and {Tercero-G{\'o}mez}, V{\`i}ctor Gustavo and {Cordero-Franco}, Alvaro Eduardo and Conover, William Jay},
  year = {2017},
  month = dec,
  journal = {Journal of Statistical Computation and Simulation},
  volume = {87},
  number = {18},
  pages = {3537--3562},
  publisher = {{Taylor \& Francis}},
  issn = {0094-9655},
  doi = {10.1080/00949655.2017.1376327},
  abstract = {A nonparametric control chart for variance is proposed. The chart is constructed following the change-point approach through the recursive use of the squared ranks test for variance. It is capable of detecting changes in the behaviour of individual observations with performance similar to a self-starting CUSUM chart for scale when normality is assumed, and a relatively better power when assessing nonnormal observations. A comparison is also made with two equivalent nonparametric charts based on Mood and Ansari-Bradley statistics. When dealing with symmetrical distributions, the proposed chart shows smaller (better) out-of-control average run length (ARL), and a competing performance otherwise. In addition, sensitivity to changes in mean and variance at the same time was tested. Extensive Monte Carlo simulation was used to measure performance, and a practical example is provided to illustrate how the proposed control chart can be implemented in practice.},
  keywords = {62-07,62G99,change-point,nonnormal,self-starting,SPC,Squared ranks test},
  annotation = {\_eprint: https://doi.org/10.1080/00949655.2017.1376327},
  file = {/home/dede/Zotero/storage/YDKZVLSK/00949655.2017.html}
}

@book{vinci2019,
  title = {{Trattato della pittura}},
  author = {da Vinci, Leonardo and Papa, Rodolfo},
  year = {2019},
  month = jan,
  edition = {Unabridged edizione},
  publisher = {{Demetra}},
  address = {{Firenze}},
  abstract = {Leonardo afferma che la pittura ha il primato su tutte le arti ed \`e la guida di tutte le discipline; assimila la pittura alla filosofia e la pone al vertice dell'architettonica dei saperi, ovvero della struttura enciclopedica della conoscenza. Leonardo \`e contemporaneamente l'ultimo dei medioevali e il primo dei moderni, infatti mantiene la struttura dei saperi con le loro relazioni interne in continuit\`a con la tradizione antica e medioevale, ma ne sovverte la gerarchia inventando una nuova dimensione scientifica: la scienza della pittura. Leonardo rivendica per la pittura un posto nobile tra le arti liberali, al pari della filosofia.},
  isbn = {978-88-440-5354-3},
  langid = {italian}
}

@incollection{voncollani2014,
  title = {Acceptance {{Sampling}} in {{Modern Industrial Environments}}},
  booktitle = {Wiley {{StatsRef}}: {{Statistics Reference Online}}},
  author = {Von Collani, Elart and G{\"o}b, Rainer},
  editor = {Balakrishnan, N. and Colton, Theodore and Everitt, Brian and Piegorsch, Walter and Ruggeri, Fabrizio and Teugels, Jozef L.},
  year = {2014},
  month = sep,
  pages = {stat04217},
  publisher = {{John Wiley \& Sons, Ltd}},
  address = {{Chichester, UK}},
  doi = {10.1002/9781118445112.stat04217},
  isbn = {978-1-118-44511-2},
  langid = {english},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Von_Collani_GÃ¶b_2014_Acceptance_Sampling_in_Modern_Industrial_Environments.pdf}
}

@article{vonhippel2009,
  title = {How to {{Impute Interactions}}, {{Squares}}, and {{Other Transformed Variables}}},
  author = {{von Hippel}, Paul T.},
  year = {2009},
  journal = {Sociological Methodology},
  volume = {39},
  number = {1},
  pages = {265--291},
  issn = {1467-9531},
  doi = {10.1111/j.1467-9531.2009.01215.x},
  abstract = {Researchers often carry out regression analysis using data that have missing values. Missing values can be filled in using multiple imputation, but imputation is tricky if the regression includes interactions, squares, or other transformations of the regressors. In this paper, we examine different approaches to imputing transformed variables; and we find one simple method that works well across a variety of circumstances. Our recommendation is to transform, then impute\textemdash i.e., calculate the interactions or squares in the incomplete data and then impute these transformations like any other variable. The transform-then-impute method yields good regression estimates, even though the imputed values are often inconsistent with one another. It is tempting to try and ``fix'' the inconsistencies in the imputed values, but methods that do so lead to biased regression estimates. Such biased methods include the passive imputation strategy implemented by the popular ice command for Stata.},
  copyright = {\textcopyright{} 2009 by American Sociological Association},
  langid = {english},
  keywords = {todo},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9531.2009.01215.x},
  file = {/home/dede/Zotero/storage/RUHJWFCX/j.1467-9531.2009.01215.html}
}

@article{vonluxburg2007,
  title = {A {{Tutorial}} on {{Spectral Clustering}}},
  author = {{von Luxburg}, Ulrike},
  year = {2007},
  month = nov,
  journal = {arXiv:0711.0189 [cs]},
  eprint = {0711.0189},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In recent years, spectral clustering has become one of the most popular modern clustering algorithms. It is simple to implement, can be solved efficiently by standard linear algebra software, and very often outperforms traditional clustering algorithms such as the k-means algorithm. On the first glance spectral clustering appears slightly mysterious, and it is not obvious to see why it works at all and what it really does. The goal of this tutorial is to give some intuition on those questions. We describe different graph Laplacians and their basic properties, present the most common spectral clustering algorithms, and derive those algorithms from scratch by several different approaches. Advantages and disadvantages of the different spectral clustering algorithms are discussed.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Machine Learning,done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/von_Luxburg_2007_A_Tutorial_on_Spectral_Clustering.pdf;/home/dede/Zotero/storage/NE6XFQRE/0711.html}
}

@article{vose1991,
  title = {A Linear Algorithm for Generating Random Numbers with a given Distribution},
  author = {Vose, M.D.},
  year = {1991},
  month = sep,
  journal = {IEEE Transactions on Software Engineering},
  volume = {17},
  number = {9},
  pages = {972--975},
  issn = {1939-3520},
  doi = {10.1109/32.92917},
  abstract = {Let xi be a random variable over a finite set with an arbitrary probability distribution. Improvements to a fast method of generating sample values for xi in constant time are suggested. The proposed modification reduces the time required for initialization to O(n). For a simple genetic algorithm, this improvement changes an O(g n 1n n) algorithm into an O(g n) algorithm (where g is the number of generations, and n is the population size).{$<>$}},
  keywords = {Computational modeling,Computer science,Genetic algorithms,Probability distribution,Random number generation,Random variables,Roundoff errors,todo},
  file = {/home/dede/Zotero/storage/3DIG72BF/92917.html}
}

@article{waaij2020,
  title = {Uncertainty Quantification in the Stochastic Block Model with an Unknown Number of Classes},
  author = {Waaij, J. V. and Kleijn, B.},
  year = {2020},
  abstract = {We study the frequentist properties of Bayesian statistical inference for the stochastic block model, with an unknown number of classes of varying sizes. We equip the space of vertex labellings with a prior on the number of classes and, conditionally, a prior on the labels. The number of classes may grow to infinity as a function of the number of vertices, depending on the sparsity of the graph. We derive non-asymptotic posterior contraction rates of the form \$P\_\{\textbackslash theta\_\{0,n\}\}\textbackslash Pi\_n(B\_n\textbackslash mid X\^n)\textbackslash le \textbackslash epsilon\_n\$, where \$X\^n\$ is the observed graph, generated according to \$P\_\{\textbackslash theta\_\{0,n\}\}\$, \$B\_n\$ is either \$\textbackslash\{\textbackslash theta\_\{0, n\}\textbackslash\}\$ or, in the very sparse case, a ball around \$\textbackslash theta\_\{0,n\}\$ of known extent, and \$\textbackslash epsilon\_n\$ is an explicit rate of convergence.  These results enable conversion of credible sets to confidence sets. In the sparse case, credible tests are shown to be confidence sets. In the very sparse case, credible sets are enlarged to form confidence sets. Confidence levels are explicit, for each \$n\$, as a function of the credible level and the rate of convergence.  Hypothesis testing between the number of classes is considered with the help of posterior odds, and is shown to be consistent. Explicit upper bounds on errors of the first and second type and an explicit lower bound on the power of the tests are given.},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Waaij_Kleijn_2020_Uncertainty_quantification_in_the_stochastic_block_model_with_an_unknown_number.pdf}
}

@incollection{wackernagel2014,
  title = {Multivariate {{Kriging}}},
  booktitle = {Wiley {{StatsRef}}: {{Statistics Reference Online}}},
  author = {Wackernagel, Hans},
  editor = {Balakrishnan, N. and Colton, Theodore and Everitt, Brian and Piegorsch, Walter and Ruggeri, Fabrizio and Teugels, Jozef L.},
  year = {2014},
  month = sep,
  pages = {stat07732},
  publisher = {{John Wiley \& Sons, Ltd}},
  address = {{Chichester, UK}},
  doi = {10.1002/9781118445112.stat07732},
  isbn = {978-1-118-44511-2},
  langid = {english},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Wackernagel_2014_Multivariate_Kriging.pdf}
}

@article{wade2018,
  title = {Bayesian {{Cluster Analysis}}: {{Point Estimation}} and {{Credible Balls}} (with {{Discussion}})},
  shorttitle = {Bayesian {{Cluster Analysis}}},
  author = {Wade, Sara and Ghahramani, Zoubin},
  year = {2018},
  month = jun,
  journal = {Bayesian Analysis},
  volume = {13},
  number = {2},
  pages = {559--626},
  publisher = {{International Society for Bayesian Analysis}},
  issn = {1936-0975, 1931-6690},
  doi = {10.1214/17-BA1073},
  abstract = {Clustering is widely studied in statistics and machine learning, with applications in a variety of fields. As opposed to popular algorithms such as agglomerative hierarchical clustering or k-means which return a single clustering solution, Bayesian nonparametric models provide a posterior over the entire space of partitions, allowing one to assess statistical properties, such as uncertainty on the number of clusters. However, an important problem is how to summarize the posterior; the huge dimension of partition space and difficulties in visualizing it add to this problem. In a Bayesian analysis, the posterior of a real-valued parameter of interest is often summarized by reporting a point estimate such as the posterior mean along with 95\% credible intervals to characterize uncertainty. In this paper, we extend these ideas to develop appropriate point estimates and credible sets to summarize the posterior of the clustering structure based on decision and information theoretic techniques.},
  keywords = {Binderâ€™s loss,mixture model,random partition,variation of information},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Wade_Ghahramani_2018_Bayesian_Cluster_Analysis.pdf;/home/dede/Zotero/storage/PHI6UKQR/17-BA1073.html}
}

@inproceedings{wager2013,
  title = {Dropout Training as Adaptive Regularization},
  booktitle = {Proceedings of the 26th {{International Conference}} on {{Neural Information Processing Systems}} - {{Volume}} 1},
  author = {Wager, Stefan and Wang, Sida and Liang, Percy},
  year = {2013},
  month = dec,
  series = {{{NIPS}}'13},
  pages = {351--359},
  publisher = {{Curran Associates Inc.}},
  address = {{Red Hook, NY, USA}},
  abstract = {Dropout and other feature noising schemes control overfitting by artificially corrupting the training data. For generalized linear models, dropout performs a form of adaptive regularization. Using this viewpoint, we show that the dropout regularizer is first-order equivalent to an L2 regularizer applied after scaling the features by an estimate of the inverse diagonal Fisher information matrix. We also establish a connection to AdaGrad, an online learning algorithm, and find that a close relative of AdaGrad operates by repeatedly solving linear dropout-regularized problems. By casting dropout as regularization, we develop a natural semi-supervised algorithm that uses unlabeled data to create a better adaptive regularizer. We apply this idea to document classification tasks, and show that it consistently boosts the performance of dropout training, improving on state-of-the-art results on the IMDB reviews dataset.},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Wager_et_al_2013_Dropout_training_as_adaptive_regularization.pdf}
}

@article{wagner2021,
  title = {Kalman {{Bayesian Neural Networks}} for {{Closed-form Online Learning}}},
  author = {Wagner, Philipp and Wu, Xinyang and Huber, Marco F.},
  year = {2021},
  month = oct,
  journal = {arXiv:2110.00944 [cs, stat]},
  eprint = {2110.00944},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Compared to point estimates calculated by standard neural networks, Bayesian neural networks (BNN) provide probability distributions over the output predictions and model parameters, i.e., the weights. Training the weight distribution of a BNN, however, is more involved due to the intractability of the underlying Bayesian inference problem and thus, requires efficient approximations. In this paper, we propose a novel approach for BNN learning via closed-form Bayesian inference. For this purpose, the calculation of the predictive distribution of the output and the update of the weight distribution are treated as Bayesian filtering and smoothing problems, where the weights are modeled as Gaussian random variables. This allows closed-form expressions for training the network's parameters in a sequential/online fashion without gradient descent. We demonstrate our method on several UCI datasets and compare it to the state of the art.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Wagner_et_al_2021_Kalman_Bayesian_Neural_Networks_for_Closed-form_Online_Learning.pdf;/home/dede/Zotero/storage/AJRF5Z4H/2110.html}
}

@book{wainwright2019,
  title = {{High-Dimensional Statistics: A Non-Asymptotic Viewpoint}},
  shorttitle = {{High-Dimensional Statistics}},
  author = {Wainwright, Martin J.},
  year = {2019},
  month = feb,
  publisher = {{Cambridge University Press}},
  address = {{Cambridge ; New York, NY}},
  abstract = {Recent years have witnessed an explosion in the volume and variety of data collected in all scientific disciplines and industrial settings. Such massive data sets present a number of challenges to researchers in statistics and machine learning. This book provides a self-contained introduction to the area of high-dimensional statistics, aimed at the first-year graduate level. It includes chapters that are focused on core methodology and theory - including tail bounds, concentration inequalities, uniform laws and empirical process, and random matrices - as well as chapters devoted to in-depth exploration of particular model classes - including sparse linear models, matrix models with rank constraints, graphical models, and various types of non-parametric models. With hundreds of worked examples and exercises, this text is intended both for courses and for self-study by graduate students and researchers in statistics, machine learning, and related fields who must understand, apply, and adapt modern statistical methods suited to large-scale data.},
  isbn = {978-1-108-49802-9},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Wainwright_2019_High-Dimensional_Statistics.pdf}
}

@article{wald1945,
  title = {Sequential {{Tests}} of {{Statistical Hypotheses}}},
  author = {Wald, A.},
  year = {1945},
  month = jun,
  journal = {The Annals of Mathematical Statistics},
  volume = {16},
  number = {2},
  pages = {117--186},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177731118},
  abstract = {The Annals of Mathematical Statistics},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Wald_1945_Sequential_Tests_of_Statistical_Hypotheses.pdf;/home/dede/Zotero/storage/E2895I5G/1177731118.html}
}

@article{walker1974,
  title = {New Fast Method for Generating Discrete Random Numbers with Arbitrary Frequency Distributions},
  author = {Walker, A. J.},
  year = {1974},
  month = apr,
  journal = {Electronics Letters},
  volume = {10},
  number = {8},
  pages = {127--128},
  publisher = {{IET Digital Library}},
  issn = {1350-911X},
  doi = {10.1049/el:19740097},
  abstract = {A new method for generating discrete random numbers with arbitrary amplitude/frequency distributions is presented. It consists essentially of amplitude manipulation of uniformly distributed, statistically independent sequential numbers. A digital-hardware implementation of the system features fast single-clock operation, with good statistical properties, and is well suited to software implementation.},
  langid = {english},
  keywords = {todo},
  file = {/home/dede/Zotero/storage/AM7QCY6Y/el_19740097.html}
}

@article{walker2007,
  title = {Sampling the {{Dirichlet Mixture Model}} with {{Slices}}},
  author = {Walker, Stephen G.},
  year = {2007},
  month = jan,
  journal = {Communications in Statistics - Simulation and Computation},
  volume = {36},
  number = {1},
  pages = {45--54},
  publisher = {{Taylor \& Francis}},
  issn = {0361-0918},
  doi = {10.1080/03610910601096262},
  abstract = {We provide a new approach to the sampling of the well known mixture of Dirichlet process model. Recent attention has focused on retention of the random distribution function in the model, but sampling algorithms have then suffered from the countably infinite representation these distributions have. The key to the algorithm detailed in this article, which also keeps the random distribution functions, is the introduction of a latent variable which allows a finite number, which is known, of objects to be sampled within each iteration of a Gibbs sampler.},
  keywords = {Bayesian nonparametrics,Density estimation,Dirichlet process,Gibbs sampler,Primary 62G99; 62F15,Secondary 65C60,Slice sampling,todo},
  annotation = {\_eprint: https://doi.org/10.1080/03610910601096262},
  file = {/home/dede/Zotero/storage/GIP5TNUA/03610910601096262.html}
}

@article{wang2013,
  title = {A {{GLR Control Chart}} for {{Monitoring}} the {{Mean Vector}} of a {{Multivariate Normal Process}}},
  author = {Wang, Sai and Reynolds, Marion R.},
  year = {2013},
  month = jan,
  journal = {Journal of Quality Technology},
  volume = {45},
  number = {1},
  pages = {18--33},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2013.11917913},
  abstract = {This paper develops a statistical process control (SPC) chart based on a generalized likelihood ratio (GLR) statistic to monitor the mean vector of a multivariate normal process. The performance of the GLR chart is compared with the performance of the Hotelling {$\chi$}2 chart, the multivariate exponentially weighted moving average (MEWMA) chart, and a multi-MEWMA combination. Results show that the Hotelling {$\chi$}2 chart and the MEWMA chart are only effective for a small range of shift sizes in the mean vector, while the GLR chart and some carefully designed multi-MEWMA combinations can give similarly better overall performance in detecting a wide range of shift magnitudes. Unlike most of these other options, the GLR chart does not require specification of tuning-parameter values by the user. The GLR chart also has the advantage in process diagnostics: at the time of a signal, estimates of change-point and out-of-control mean vector are immediately available to the user. All these advantages of the GLR chart make it a favorable option for practitioners. For the design of the GLR chart, a series of easy-to-use equations are provided to users for calculating the control limit to achieve a desired in-control performance.},
  keywords = {Change Point,done,Generalized Likelihood Ratio,Multivariate Control Chart,Multivariate CUSUM,Multivariate EWMA,Statistical Process Control,Surveillance},
  annotation = {\_eprint: https://doi.org/10.1080/00224065.2013.11917913},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Wang_Reynolds_2013_A_GLR_Control_Chart_for_Monitoring_the_Mean_Vector_of_a_Multivariate_Normal.pdf;/home/dede/Zotero/storage/6LSVLNNC/00224065.2013.html}
}

@article{wang2015,
  title = {Large-{{Scale Multi-Stream Quickest Change Detection}} via {{Shrinkage Post-Change Estimation}}},
  author = {Wang, Yuan and Mei, Yajun},
  year = {2015},
  month = dec,
  journal = {IEEE Transactions on Information Theory},
  volume = {61},
  number = {12},
  eprint = {1308.5738},
  eprinttype = {arxiv},
  pages = {6926--6938},
  issn = {0018-9448, 1557-9654},
  doi = {10.1109/TIT.2015.2495361},
  abstract = {The quickest change detection problem is considered in the context of monitoring large-scale independent normal distributed data streams with possible changes in some of the means. It is assumed that for each individual local data stream, either there are no local changes, or there is a "big" local change that is larger than a pre-specified lower bound. Two different kinds of scenarios are studied: one is the sparse post-change case when the unknown number of affected data streams is much smaller than the total number of data streams, and the other is when all local data streams are affected simultaneously although not necessarily identically. We propose a systematic approach to develop efficient global monitoring schemes for quickest change detection by combining hard thresholding with linear shrinkage estimators to estimating all post-change parameters simultaneously. Our theoretical analysis demonstrates that the shrinkage estimation can balance the tradeoff between the first-order and second-order terms of the asymptotic expression on the detection delays, and our numerical simulation studies illustrate the usefulness of shrinkage estimation and the challenge of Monte Carlo simulation of the average run length to false alarm in the context of online monitoring large-scale data streams.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory,Primary 62L10; 62L15; secondary 62F05,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Wang_Mei_2015_Large-Scale_Multi-Stream_Quickest_Change_Detection_via_Shrinkage_Post-Change.pdf;/home/dede/Zotero/storage/N3933H32/1308.html}
}

@inproceedings{wang2016,
  title = {Scalable Geometric Density Estimation},
  booktitle = {Artificial {{Intelligence}} and {{Statistics}}},
  author = {Wang, Ye and Canale, Antonio and Dunson, David},
  year = {2016},
  month = may,
  pages = {857--865},
  publisher = {{PMLR}},
  issn = {1938-7228},
  abstract = {It is standard to assume a low-dimensional structure in estimating a high-dimensional density.  However, popular methods, such as probabilistic principal component analysis, scale poorly computatio...},
  langid = {english},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Wang_et_al_2016_Scalable_geometric_density_estimation.pdf;/home/dede/Zotero/storage/TSNNM2Q3/wang16e.html}
}

@article{wang2016a,
  title = {Functional {{Data Analysis}}},
  author = {Wang, Jane-Ling and Chiou, Jeng-Min and M{\"u}ller, Hans-Georg},
  year = {2016},
  month = jun,
  journal = {Annual Review of Statistics and Its Application},
  volume = {3},
  number = {1},
  pages = {257--295},
  issn = {2326-8298, 2326-831X},
  doi = {10.1146/annurev-statistics-041715-033624},
  langid = {english},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Wang_et_al_2016_Functional_Data_Analysis.pdf}
}

@article{wang2018,
  title = {A Spatial-Adaptive Sampling Procedure for Online Monitoring of Big Data Streams},
  author = {Wang, Andi and Xian, Xiaochen and Tsung, Fugee and Liu, Kaibo},
  year = {2018},
  month = oct,
  journal = {Journal of Quality Technology},
  volume = {50},
  number = {4},
  pages = {329--343},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2018.1507560},
  abstract = {With the improvement of data-acquisition technology, big data streams that involve continuous observations with high dimensionality and large volume frequently appear in modern applications, which poses significant challenges for statistical process control. In this article we consider the problem of online monitoring a class of big data streams where each data stream is associated with a spatial location. Our goal is to quickly detect shifts occurring in such big data streams when only partial information can be observed at each time and the out-of-control variables are clustered in a small and unknown region. To achieve this goal, we propose a novel spatial-adaptive sampling and monitoring (SASAM) procedure that aims to leverage the spatial information of the data streams for quick change detection. Specifically, the proposed sampling strategy will adaptively and intelligently integrate two seemingly contradictory ideas: (1) random sampling that quickly searches for possible out-of-control variables; and (2) directional sampling that focuses on highly suspicious out-of-control variables that may cluster in a small region. Simulation and real case studies show that the proposed method significantly outperforms the existing sampling strategy without taking the spatial information of the data streams into consideration.},
  keywords = {big data streams,cumulative-sum statistics,done,high-dimensional and high-frequency data,partial information,scalable monitoring schemes,statistical process control},
  annotation = {\_eprint: https://doi.org/10.1080/00224065.2018.1507560},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Wang_et_al_2018_A_spatial-adaptive_sampling_procedure_for_online_monitoring_of_big_data_streams.pdf;/home/dede/Zotero/storage/Q7TV4IQ9/00224065.2018.html}
}

@article{wang2018a,
  title = {A {{Dirichlet Process Gaussian State Machine Model}} for {{Change Detection}} in {{Transient Processes}}},
  author = {Wang, Zimo and Bukkapatnam, Satish T. S.},
  year = {2018},
  month = jul,
  journal = {Technometrics},
  volume = {60},
  number = {3},
  pages = {373--385},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.2017.1371079},
  abstract = {The ability to detect incipient and critical changes in real world process\textemdash esessential for system integrity assurance\textemdash is currently impeded by the mismatch between the key assumption of stationarity underlying most change detection methods and the nonlinear and nonstationary (transient) dynamics of most real-world processes. The current approaches are slow or outright unable to detect qualitative changes in the behaviors that lead to anomalies. We present a Dirichlet process Gaussian state machine (DPGSM) model to represent dynamic intermittency, which is one of the most ubiquitous real-world transient behaviors. The DPGSM model treats a signal as a random walk among a Dirichlet process mixture of Gaussian clusters. Hypothesis tests and a numerical scheme based on this nonparametric representation were developed to detect subtle changes in the transient (intermittent) dynamics. Experimental investigations suggest that the DPGSM approach can consistently detect incipient, critical changes in intermittent signals some 50\textendash 2000 ms (20\textendash 90\%) ahead of competing methods in benchmark test cases as well as a variety of real-world applications, such as in alternation patterns (e.g., ragas) in a music piece, and in the vibration signals capturing the initiation of product defects in an ultraprecision manufacturing process. A supplementary file to this article, available online, includes a Matlab implementation of the presented DPGSM.},
  keywords = {Bayesian nonparametrics,Intermittent nonstationary process,Quality assurance,todo},
  annotation = {\_eprint: https://doi.org/10.1080/00401706.2017.1371079},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Wang_Bukkapatnam_2018_A_Dirichlet_Process_Gaussian_State_Machine_Model_for_Change_Detection_in.pdf;/home/dede/Zotero/storage/5A4Z6JHC/00401706.2017.html}
}

@article{wang2018b,
  title = {Thresholded {{Multivariate Principal Component Analysis}} for {{Phase I Multichannel Profile Monitoring}}},
  author = {Wang, Yuan and Mei, Yajun and Paynabar, Kamran},
  year = {2018},
  month = jul,
  journal = {Technometrics},
  volume = {60},
  number = {3},
  pages = {360--372},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.2017.1375993},
  abstract = {Monitoring multichannel profiles has important applications in manufacturing systems improvement, but it is nontrivial to develop efficient statistical methods because profiles are high-dimensional functional data with intrinsic inner- and interchannel correlations, and that the change might only affect a few unknown features of multichannel profiles. To tackle these challenges, we propose a novel thresholded multivariate principal component analysis (PCA) method for multichannel profile monitoring. Our proposed method consists of two steps of dimension reduction: It first applies the functional PCA to extract a reasonably large number of features under the in-control state, and then uses the soft-thresholding techniques to further select significant features capturing profile information under the out-of-control state. The choice of tuning parameter for soft-thresholding is provided based on asymptotic analysis, and extensive numerical studies are conducted to illustrate the efficacy of our proposed thresholded PCA methodology.},
  keywords = {Change-point,Multichannel profiles,Principal component analysis,Shrinkage estimation,Statistical process control,todo},
  annotation = {\_eprint: https://doi.org/10.1080/00401706.2017.1375993},
  file = {/home/dede/Zotero/storage/5NTD6F56/Wang et al. - 2018 - Thresholded Multivariate Principal Component Analy.pdf;/home/dede/Zotero/storage/DUKFFHFL/00401706.2017.html}
}

@article{wang2020,
  title = {New {{One-Sided EWMA}} t {{Charts}} without and with {{Variable Sampling Intervals}} for {{Monitoring}} the {{Process Mean}}},
  author = {Wang, Yan and Hu, Xuelong and Zhou, Xiaojian and Qiao, Yulong and Wu, Shu},
  year = {2020},
  month = nov,
  journal = {Mathematical Problems in Engineering},
  volume = {2020},
  pages = {e7567215},
  publisher = {{Hindawi}},
  issn = {1024-123X},
  doi = {10.1155/2020/7567215},
  abstract = {In statistical process control (SPC), t charts play a vital role in the monitoring of the process mean, especially when the process variance is unknown. In this paper, two separate upper-sided and lower-sided exponentially weighted moving average (EWMA) t charts are first proposed and the Monte Carlo simulation method is used to obtain their run length (RL) properties. Compared with the traditional one-sided EWMA t charts and several run rules t charts, the proposed charts are proven to have better performance than these competing charts. In addition, by adding the variable sampling interval (VSI) feature to the proposed charts, the new VSI one-sided EWMA t charts are shown to detect different shift sizes in the process more efficient than the chart without VSI feature. Finally, an example of a milk filling process illustrates the use of the charts.},
  langid = {english},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Wang_et_al_2020_New_One-Sided_EWMA_t_Charts_without_and_with_Variable_Sampling_Intervals_for.pdf;/home/dede/Zotero/storage/PJLWKQZI/7567215.html}
}

@article{wang2021,
  title = {Data-{{Driven Determination}} of the {{Number}} of {{Jumps}} in {{Regression Curves}}},
  author = {Wang, Guanghui and Zou, Changliang and Qiu, Peihua},
  year = {2021},
  month = sep,
  journal = {Technometrics},
  volume = {0},
  number = {0},
  pages = {1--11},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.2021.1978551},
  abstract = {In nonparametric regression with jump discontinuities, one major challenge is to determine the number of jumps in a regression curve. Most existing methods to solve that problem are based on either a sequence of hypothesis tests or model selection, by introducing some extra tuning parameters that may not be easy to determine in practice. This article aims to develop a data-driven new methodology for determining the number of jumps, using an order-preserved sample-splitting strategy together with a cross-validation-based criterion. Statistical consistency of the determined number of jumps by our proposed method is established. More interestingly, the proposed method allows us to move beyond just point estimation, and it can quantify uncertainty of the proposed estimate. The key idea behind our method is the construction of a series of statistics with marginal symmetry property and this property can be used for choosing a data-driven threshold to control the false discovery rate of our method. The proposed method is computationally efficient. Numerical experiments indicate that it has a reliable performance in finite-sample cases. An R package jra is developed to implement the proposed method.},
  keywords = {Cross-validation,False discovery rate,Jump discontinuity,Local linear smoothing,Nonparametric regression,skimmed,Uniform convergence},
  annotation = {\_eprint: https://doi.org/10.1080/00401706.2021.1978551},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Wang_et_al_2021_Data-Driven_Determination_of_the_Number_of_Jumps_in_Regression_Curves.pdf;/home/dede/Zotero/storage/CD9TMHF2/00401706.2021.html}
}

@article{wardell1994,
  title = {Run-{{Length Distributions}} of {{Special-Cause Control Charts}} for {{Correlated Processes}}},
  author = {Wardell, Don G. and Moskowitz, Herbert and Plante, Robert D.},
  year = {1994},
  journal = {Technometrics},
  volume = {36},
  number = {1},
  pages = {3--17},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.1994.10485393},
  abstract = {We derive run-length distributions of the special-cause control chart proposed by Alwan and Roberts for correlated observations, given that the assignable cause to be detected is a shift in the process mean. Both recursive and closed-form solutions are derived for the run-length distribution, average run length (ARL), and standard deviation of the run length (SRL) for any AR(p) process, and approximate solutions are derived for the more general ARMA(p,q) processes. The expressions derived do not depend on the type of shift in the process mean. Numerical results are illustrated for the ARL and SRL of the ARMA(l,l) model, given that the shift in the mean is a step shift. These results show that the ARL and SRL of the specialcause control chart are relatively smaller when the process is negatively rather than positively autocorrelated. Regardless of the sign of the autocorrelation, the shape of the probability mass function of the run length reveals that the probability of detecting shifts very early is substantially higher for the special-cause chart than for more traditional control charts. Early detection makes the cause of the signal easier to identify, resulting in a more rapid rate of continuous quality improvement. There are some cases, however, when traditional charts, which are simpler to implement, should be considered even when the process is autocorrelated.},
  keywords = {Autocorrelation,Average run length,reference only,Statistical process control,Time series},
  annotation = {\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/00401706.1994.10485393},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Wardell_et_al_1994_Run-Length_Distributions_of_Special-Cause_Control_Charts_for_Correlated.pdf;/home/dede/Zotero/storage/9WNP9IHB/00401706.1994.html}
}

@article{wasserman1993,
  title = {Short Run Spc Based upon the Second Order Dynamic Linear Model for Trend Detection},
  author = {Wasserman, Gary S. and Sudjianto, Agus},
  year = {1993},
  month = jan,
  journal = {Communications in Statistics - Simulation and Computation},
  volume = {22},
  number = {4},
  pages = {1011--1036},
  publisher = {{Taylor \& Francis}},
  issn = {0361-0918},
  doi = {10.1080/03610919308813140},
  abstract = {Recently, several new applications of control chart procedures for short production runs have been introduced. Bothe (1989) and Burr (1989) proposed the use of control chart statistics which are obtained by scaling the quality characteristic by target values or process estimates of a location and scale parameter. The performance of these control charts can be significantly affected by the use of incorrect scaling parameters, resulting in either an excessive "false alarm rate," or insensitivity to the detection of moderate shifts in the process. To correct for these deficiencies, Quesenberry (1990, 1991) has developed the Q-Chart which is formed from running process estimates of the sample mean and variance. For the case where both the process mean and variance are unknown, the Q-chaxt statistic is formed from the standard inverse Z-transformation of a t-statistic. Q-charts do not perform correctly, however, in the presence of special cause disturbances at process startup. This has recently been supported by results published by Del Castillo and Montgomery (1992), who recommend the use of an alternative control chart procedure which is based upon a first-order adaptive Kalman filter model Consistent with the recommendations by Castillo and Montgomery, we propose an alternative short run control chart procedure which is based upon the second order dynamic linear model (DLM). The control chart is shown to be useful for the early detection of unwanted process trends. Model and control chart parameters are updated sequentially in a Bayesian estimation framework, providing the greatest degree of flexibility in the level of prior information which is incorporated into the model. The result is a weighted moving average control chart statistic which can be used to provide running estimates of process capability. The average run length performance of the control chart is compared to the optimal performance of the exponentially weighted moving average (EWMA) chart, as reported by Gan (1991). Using a simulation approach, the second order DLM control chart is shown to provide better overall performance than the EWMA for short production run applications},
  keywords = {adaptive,Bayesian estimation,dynamic,Kalman filter,linear model,short run statistical process control (SPC)},
  annotation = {\_eprint: https://doi.org/10.1080/03610919308813140},
  file = {/home/dede/Zotero/storage/8NI4EGIW/03610919308813140.html}
}

@article{wasserman1994,
  title = {Short Run Spc Using Dynamic Control Chart},
  author = {Wasserman, Gary S.},
  year = {1994},
  month = sep,
  journal = {Computers \& Industrial Engineering},
  series = {16th {{Annual Conference}} on {{Computers}} and {{Industrial Engineering}}},
  volume = {27},
  number = {1},
  pages = {353--356},
  issn = {0360-8352},
  doi = {10.1016/0360-8352(94)90307-7},
  abstract = {A generalization of the EWMA control chart, referred to within as the Dynamic EWMA control chart, is developed. The control chart is based upon a first-order, constant variance, dynamic linear model. Its applicability for short production run SPC applications described.},
  langid = {english},
  file = {/home/dede/Zotero/storage/8RTRQF6B/0360835294903077.html}
}

@article{wasserman1995,
  title = {An Adaptation of the {{EWMA}} Chart for Short Run {{SPC}}},
  author = {Wasserman, G. S.},
  year = {1995},
  month = oct,
  journal = {International Journal of Production Research},
  volume = {33},
  number = {10},
  pages = {2821--2833},
  publisher = {{Taylor \& Francis}},
  issn = {0020-7543},
  doi = {10.1080/00207549508904847},
  abstract = {The properties of a first-order, constant variance, dynamic linear model when monitoring an i.i.d. process are used to develop a framework for monitoring processes under startup conditions. Under stationary conditions, the statistic used for updating the estimate of the posterior mean of the process is just the exponentially weighted moving average (EWMA) control chart statistic. As such, an adaptation of the EWMA chart, referred to within as a dynamic EWMA chart, is proposed for monitoring short production runs. The model is driven by a specification of a prior distribution in the mean and variance of the process. Simple approximations are developed for the posterior estimates of the model parameters to enable its use by shop floor personnel. It is based upon specifications in the sensitivity of the control chart procedure to special cause phenomena when the chart is operated in its stationary phase. An i.i.d. process is simulated to determine the required control chart standard deviation multiplier to achieve an in-control average run length (ARL) of 500. The results of simulation runs of 5000 replicates are evaluated for a range of settings of the EWMA weighting factor.},
  annotation = {\_eprint: https://doi.org/10.1080/00207549508904847},
  file = {/home/dede/Zotero/storage/RWLIBH33/00207549508904847.html}
}

@book{wasserman2005,
  title = {All of {{Nonparametric Statistics}}},
  author = {Wasserman, Larry},
  year = {2005},
  month = oct,
  publisher = {{Springer}},
  address = {{New York}},
  isbn = {978-0-387-25145-5},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Wasserman_2005_All_of_Nonparametric_Statistics.pdf}
}

@article{watson1964,
  title = {Smooth {{Regression Analysis}}},
  author = {Watson, Geoffrey S.},
  year = {1964},
  journal = {Sankhy\=a: The Indian Journal of Statistics, Series A (1961-2002)},
  volume = {26},
  number = {4},
  pages = {359--372},
  publisher = {{Springer}},
  issn = {0581-572X},
  abstract = {Few would deny that the most powerful statistical tool is graph paper. When however there are many observations (and/or many variables) graphical procedures become tedious. It seems to the author that the most characteristic problem for statisticians at the moment is the development of methods for analyzing the data poured out by electronic observing systems. The present paper gives a simple computer method for obtaining a "graph" from a large number of observations.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Watson_1964_Smooth_Regression_Analysis.pdf}
}

@article{weese2016,
  title = {Statistical {{Learning Methods Applied}} to {{Process Monitoring}}: {{An Overview}} and {{Perspective}}},
  shorttitle = {Statistical {{Learning Methods Applied}} to {{Process Monitoring}}},
  author = {Weese, Maria and Martinez, Waldyn and Megahed, Fadel M. and {Jones-Farmer}, L. Allison},
  year = {2016},
  month = jan,
  journal = {Journal of Quality Technology},
  volume = {48},
  number = {1},
  pages = {4--24},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2016.11918148},
  abstract = {The increasing availability of high-volume, high-velocity data sets, often containing variables of different data types, brings an increasing need for monitoring tools that are designed to handle these big data sets. While the research on multivariate statistical process monitoring tools is vast, the application of these tools for big data sets has received less attention. In this expository paper, we give an overview of the current state of data-driven multivariate statistical process monitoring methodology. We highlight some of the main directions involving statistical learning and dimension reduction techniques applied to control charts in research from supply chain, engineering, computer science, and statistics. The goal of this paper is to bring into better focus some of the monitoring and surveillance methodology informed by data mining techniques that show promise for monitoring large and diverse data sets. We introduce an example using Wikipedia search information and illustrate a few of the complexities of applying the available methods to a high-dimensional monitoring scenario. Throughout, we offer advice to practitioners and some suggestions for future research in this emerging area of research.},
  keywords = {Control Charts,Ensembles,Neural Networks,Regression,skimmed,Support Vector Machines,Variable Selection},
  annotation = {\_eprint: https://doi.org/10.1080/00224065.2016.11918148},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Weese_et_al_2016_Statistical_Learning_Methods_Applied_to_Process_Monitoring.pdf}
}

@book{wei2005,
  title = {{Time Series Analysis: Univariate And Multivariate Methods}},
  shorttitle = {{Time Series Analysis}},
  author = {Wei, William W. S.},
  year = {2005},
  edition = {2\textdegree{} edizione},
  publisher = {{Pearson College Div}},
  address = {{Boston}},
  abstract = {With its broad coverage of methodology, this comprehensive book is a useful learning and reference tool for those in applied sciences where analysis and research of time series is useful. Its plentiful examples show the operational details and purpose of a variety of univariate and multivariate time series methods. Numerous figures, tables and real-life time series data sets illustrate the models and methods useful for analyzing, modeling, and forecasting data collected sequentially in time. The text also offers a balanced treatment between theory and applications. ~    Overview. Fundamental Concepts. Stationary Time Series Models. Nonstationary Time Series Models. Forecasting. Model Identification. Parameter Estimation, Diagnostic Checking, and Model Selection. Seasonal Time Series Models. Testing for a Unit Root. Intervention Analysis and Outlier Detection. Fourier Analysis. Spectral Theory of Stationary Processes. Estimation of the Spectrum. Transfer Function Models. Time Series Regression and GARCH Models. Vector Time Series Models. More on Vector Time Series. State Space Models and the Kalman Filter. Long Memory and Nonlinear Processes. Aggregation and Systematic Sampling in Time Series. ~   For all readers interested in time series analysis.},
  isbn = {978-0-321-32216-6},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Wei_2005_Time_Series_Analysis.pdf}
}

@book{wei2019,
  title = {{Multivariate Time Series Analysis and Applications}},
  author = {Wei, William W. S.},
  year = {2019},
  month = mar,
  edition = {1. edizione},
  publisher = {{John Wiley \& Sons Inc}},
  address = {{Hoboken, NJ}},
  abstract = {An essential guide on high dimensional multivariate time series including all the latest topics from one of the leading experts in the field Following the highly successful and much lauded book, Time Series Analysis\textemdash Univariate and Multivariate Methods, this new work by William W.S. Wei focuses on high dimensional multivariate time series, and is illustrated with numerous high dimensional empirical time series. Beginning with the fundamentalconcepts and issues of multivariate time series analysis,this book covers many topics that are not found in general multivariate time series books. Some of these are repeated measurements, space-time series modelling, and dimension reduction. The book also looks at vector time series models, multivariate time series regression models, and principle component analysis of multivariate time series. Additionally, it provides readers with information on factor analysis of multivariate time series, multivariate GARCH models, and multivariate spectral analysis of time series. With the development of computers and the internet, we have increased potential for data exploration. In the next few years, dimension will become a more serious problem. Multivariate Time Series Analysis and its Applications provides some initial solutions, which may encourage the development of related software needed for the high dimensional multivariate time series analysis.  Written by bestselling author and leading expert in the field Covers topics not yet explored in current multivariate books Features classroom tested material Written specifically for time series courses  Multivariate Time Series Analysis and its Applications is designed for an advanced time series analysis course. It is a must-have for anyone studying time series analysis and is also relevant for students in economics, biostatistics, and engineering.},
  isbn = {978-1-119-50285-2},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Wei_2019_Multivariate_Time_Series_Analysis_and_Applications.pdf}
}

@book{weinberg,
  title = {World {{War II}}: {{A Very Short Introduction}}},
  shorttitle = {World {{War II}}},
  author = {Weinberg, Gerhard L.},
  keywords = {done}
}

@inproceedings{welling2011,
  title = {Bayesian {{Learning}} via {{Stochastic Gradient Langevin Dynamics}}},
  booktitle = {{{ICML}}},
  author = {Welling, M. and Teh, Y.},
  year = {2011},
  abstract = {This paper proposes a new framework for learning from large scale datasets based on iterative learning from small mini-batches by adding the right amount of noise to a standard stochastic gradient optimization algorithm and shows that the iterates will converge to samples from the true posterior distribution as the authors anneal the stepsize. In this paper we propose a new framework for learning from large scale datasets based on iterative learning from small mini-batches. By adding the right amount of noise to a standard stochastic gradient optimization algorithm we show that the iterates will converge to samples from the true posterior distribution as we anneal the stepsize. This seamless transition between optimization and Bayesian posterior sampling provides an inbuilt protection against overfitting. We also propose a practical method for Monte Carlo estimates of posterior statistics which monitors a "sampling threshold" and collects samples after it has been surpassed. We apply the method to three models: a mixture of Gaussians, logistic regression and ICA with natural gradients.},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Welling_Teh_2011_Bayesian_Learning_via_Stochastic_Gradient_Langevin_Dynamics.pdf}
}

@book{welsh1996,
  title = {Aspects of {{Statistical Inference}}},
  author = {Welsh, A. H.},
  year = {1996},
  month = oct,
  publisher = {{John Wiley \& Sons}},
  abstract = {Relevant, concrete, and thorough--the essential data-based text onstatistical inference  The ability to formulate abstract concepts and draw conclusionsfrom data is fundamental to mastering statistics. Aspects ofStatistical Inference equips advanced undergraduate and graduatestudents with a comprehensive grounding in statistical inference,including nonstandard topics such as robustness, randomization, andfinite population inference.  A. H. Welsh goes beyond the standard texts and expertly synthesizesbroad, critical theory with concrete data and relevant topics. Thetext follows a historical framework, uses real-data sets andstatistical graphics, and treats multiparameter problems, yet isultimately about the concepts themselves.  Written with clarity and depth, Aspects of Statistical Inference: * Provides a theoretical and historical grounding in statisticalinference that considers Bayesian, fiducial, likelihood, andfrequentist approaches * Illustrates methods with real-data sets on diabetic retinopathy,the pharmacological effects of caffeine, stellar velocity, andindustrial experiments * Considers multiparameter problems * Develops large sample approximations and shows how to use them * Presents the philosophy and application of robustness theory * Highlights the central role of randomization in statistics * Uses simple proofs to illuminate foundational concepts * Contains an appendix of useful facts concerning expansions,matrices, integrals, and distribution theory  Here is the ultimate data-based text for comparing and presentingthe latest approaches to statistical inference.},
  googlebooks = {4BFq21QpDnQC},
  isbn = {978-0-471-11591-5},
  langid = {english},
  keywords = {Mathematics / Probability \& Statistics / General,Mathematics / Probability \& Statistics / Stochastic Processes},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Welsh_1996_Aspects_of_Statistical_Inference.pdf}
}

@article{west1986,
  title = {Monitoring and {{Adaptation}} in {{Bayesian Forecasting Models}}},
  author = {West, Mike and Harrison, P. Jeff},
  year = {1986},
  month = sep,
  journal = {Journal of the American Statistical Association},
  volume = {81},
  number = {395},
  pages = {741--750},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.1986.10478331},
  abstract = {Practical aspects of a new technique for monitoring and controlling the predictive performance of Bayesian forecasting models are discussed. The basic features of the approach to model monitoring introduced in a general setting in West (1986) are described and extended to a wide class of dynamic, nonnormal, and nonlinear Bayesian forecasting models. An associated method of automatically detecting and rejecting outliers and adapting models to abrupt structural changes in the time series is also discussed. The resulting forecast monitoring and control scheme is simply constructed and applied and is illustrated in two applications.},
  keywords = {Change points,Dynamic models,Forecast control,Outliers,Predictive ability,Tracking signals},
  annotation = {\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/01621459.1986.10478331},
  file = {/home/dede/Zotero/storage/24WJ9A4Z/01621459.1986.html}
}

@article{west1986a,
  title = {Bayesian {{Model Monitoring}}},
  author = {West, Mike},
  year = {1986},
  journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
  volume = {48},
  number = {1},
  pages = {70--78},
  issn = {2517-6161},
  doi = {10.1111/j.2517-6161.1986.tb01391.x},
  abstract = {A simple method of monitoring the predictive performance of a class of Bayesian models is introduced. The models involve sequential analyses of sequences of observations and are appropriate for a variety of monitoring and forecasting applications. For any given model, the monitoring technique is based on a comparison of the predictive ability of the model, measured by the observed values of predictive densities, with that of a single alternative. The alternative is constructed sequentially and is designed, for the class of models considered, as a relatively general yet neutral alternative to the original. The monitor is used as a general diagnostic tool to detect and assess discrepancies between the data and predictions generated from the model; particular sources of such model failure of interest are the occurrence of outliers and structural change in the series. An illustration is provided and a simple method of automatically coping with outliers and adapting to change is outlined.},
  langid = {english},
  keywords = {bayes' factors,change point,cusum,forecasting,model assessment,outlier rejection,predictive ability,sequential bayesian modelling},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.2517-6161.1986.tb01391.x},
  file = {/home/dede/Zotero/storage/PT4F4DV8/j.2517-6161.1986.tb01391.html}
}

@book{west1997,
  title = {Bayesian {{Forecasting}} and {{Dynamic Models}}},
  author = {West, Mike and Harrison, Jeff},
  year = {1997},
  series = {Springer {{Series}} in {{Statistics}}},
  edition = {Second},
  publisher = {{Springer-Verlag}},
  address = {{New York}},
  doi = {10.1007/b98971},
  abstract = {This text is concerned with Bayesian learning, inference and forecasting in dynamic environments. We describe the structure and theory of classes of dynamic models and their uses in forecasting and time series analysis. The principles, models and methods of Bayesian forecasting and time - ries analysis have been developed extensively during the last thirty years. Thisdevelopmenthasinvolvedthoroughinvestigationofmathematicaland statistical aspects of forecasting models and related techniques. With this has come experience with applications in a variety of areas in commercial, industrial, scienti?c, and socio-economic ?elds. Much of the technical - velopment has been driven by the needs of forecasting practitioners and applied researchers. As a result, there now exists a relatively complete statistical and mathematical framework, presented and illustrated here. In writing and revising this book, our primary goals have been to present a reasonably comprehensive view of Bayesian ideas and methods in m- elling and forecasting, particularly to provide a solid reference source for advanced university students and research workers.},
  isbn = {978-0-387-94725-9},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/West_Harrison_1997_Bayesian_Forecasting_and_Dynamic_Models.pdf;/home/dede/Zotero/storage/RPHRGHMX/9780387947259.html}
}

@article{weston2015,
  title = {Memory {{Networks}}},
  author = {Weston, Jason and Chopra, Sumit and Bordes, Antoine},
  year = {2015},
  month = nov,
  journal = {arXiv:1410.3916 [cs, stat]},
  eprint = {1410.3916},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chaining multiple supporting sentences to answer questions that require understanding the intension of verbs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Statistics - Machine Learning,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Weston_et_al_2015_Memory_Networks.pdf;/home/dede/Zotero/storage/K556YFD5/1410.html}
}

@book{whatmore2022,
  title = {The {{History}} of {{Political Thought}}: {{A Very Short Introduction}}},
  shorttitle = {The {{History}} of {{Political Thought}}},
  author = {Whatmore, Richard},
  year = {2022},
  series = {Very {{Short Introductions}}},
  publisher = {{Oxford University Press}},
  isbn = {978-0-19-885372-5},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Whatmore_2022_The_History_of_Political_Thought.epub}
}

@book{wienke2010,
  title = {Frailty {{Models}} in {{Survival Analysis}}},
  author = {Wienke, Andreas},
  year = {2010},
  series = {Chapman \& {{Hall}}/{{CRC Biostatistics Series}}},
  publisher = {{Chapman and Hall/CRC}},
  doi = {10.1201/9781420073911},
  abstract = {The concept of frailty offers a convenient way to introduce unobserved heterogeneity and associations into models for survival data. In its simplest form, frailty is an unobserved random proportionality factor that modifies the hazard function of an individual or a group of related individuals. Frailty Models in Survival Analysis presents a comprehensive overview of the fundamental approaches in the area of frailty models. The book extensively explores how univariate frailty models can represent unobserved heterogeneity. It also emphasizes correlated frailty models as extensions of univariate and shared frailty models. The author analyzes similarities and differences between frailty and copula models; discusses problems related to frailty models, such as tests for homogeneity; and describes parametric and semiparametric models using both frequentist and Bayesian approaches. He also shows how to apply the models to real data using the statistical packages of R, SAS, and Stata. The appendix provides the technical mathematical results used throughout. Written in nontechnical terms accessible to nonspecialists, this book explains the basic ideas in frailty modeling and statistical techniques, with a focus on real-world data application and interpretation of the results. By applying several models to the same data, it allows for the comparison of their advantages and limitations under varying model assumptions. The book also employs simulations to analyze the finite sample size performance of the models.},
  isbn = {978-1-4200-7388-1},
  keywords = {empirical bayes,frailty,hierarchical,multilevel},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Wienke_2010_Frailty_Models_in_Survival_Analysis.pdf}
}

@article{wigell2016,
  title = {Geopolitics versus Geoeconomics: The Case of {{Russia}}'s Geostrategy and Its Effects on the {{EU}}},
  shorttitle = {Geopolitics versus Geoeconomics},
  author = {WIGELL, MIKAEL and VIHMA, ANTTO},
  year = {2016},
  month = may,
  journal = {International Affairs},
  volume = {92},
  number = {3},
  pages = {605--627},
  issn = {0020-5850},
  doi = {10.1111/1468-2346.12600},
  abstract = {Geopolitics and geoeconomics are often addressed together, with the latter seen as a sub-variant of the former. This article shows the usefulness of differentiating them at a conceptual level. By juxtaposing traditional geopolitics and geoeconomics, we suggest that they have remarkably different qualities and implications for their targets, on both national and international levels. Importantly, these include the formation of alliances, and whether they are driven by balancing, bandwagoning or underbalancing dynamics. An analysis of Russia's shifting geostrategy towards Europe shows these differences in practice. Russian geoeconomics has long been successful as a `wedge strategy', dividing the EU. As a result, the EU has underbalanced and its Russia policies have been incoherent. The observable tendencies in 2014\textendash 15 towards a more coherent European approach can be explained by the changing emphasis in Russia's geostrategy. Russia's turn to geopolitics works as a centripetal force, causing a relative increase in EU unity. Centripetal tendencies due to heightened threat perception can be observed in the economic sanctions, emerging German leadership in EU foreign policy, and discussion on energy union. The analysis calls for more attention to the way strategic choices\textemdash geopolitics versus geoeconomics\textemdash affect the coherence of threatened states and alliance patterns.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/WIGELL_VIHMA_2016_Geopolitics_versus_geoeconomics.pdf;/home/dede/Zotero/storage/WTCJK34I/2326953.html}
}

@article{wilhelm2010,
  title = {Tmvtnorm: {{A Package}} for the {{Truncated Multivariate Normal Distribution}}},
  shorttitle = {Tmvtnorm},
  author = {Wilhelm, Stefan and Manjunath, B.},
  year = {2010},
  month = jun,
  journal = {The R Journal},
  volume = {2},
  doi = {10.32614/RJ-2010-005},
  abstract = {In this article we present tmvtnorm , an R package implementation for the truncated multivariate normal distribution. We consider random number generation with rejection and Gibbs sampling, computation of marginal densi- ties as well as computation of the mean and co- variance of the truncated variables. This contri- bution brings together latest research in this field and provides useful methods for both scholars and practitioners when working with truncated normal variables.},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Wilhelm_Manjunath_2010_tmvtnorm.pdf}
}

@misc{wilhelm2015,
  title = {Tmvtnorm: {{Truncated Multivariate Normal}} and {{Student}} t {{Distribution}}},
  shorttitle = {Tmvtnorm},
  author = {Wilhelm, Stefan and Manjunath, B. G.},
  year = {2015},
  month = aug,
  abstract = {Random number generation for the truncated multivariate normal and Student t distribution. Computes probabilities, quantiles and densities, including one-dimensional and bivariate marginal densities. Computes first and second moments (i.e. mean and covariance matrix) for the double-truncated multinormal case.},
  copyright = {GPL-2 | GPL-3 [expanded from: GPL ({$\geq$} 2)]},
  keywords = {Distributions}
}

@book{wilson2015,
  title = {{Models for Dependent Time Series}},
  author = {Wilson, Granville Tunnicliffe and Reale, Marco and Haywood, John},
  year = {2015},
  month = jul,
  publisher = {{Chapman and Hall/CRC}},
  address = {{Boca Raton}},
  abstract = {Models for Dependent Time Series addresses the issues that arise and the methodology that can be applied when the dependence between time series is described and modeled. Whether you work in the economic, physical, or life sciences, the book shows you how to draw meaningful, applicable, and statistically valid conclusions from multivariate (or vector) time series data.  The first four chapters discuss the two main pillars of the subject that have been developed over the last 60 years: vector autoregressive modeling and multivariate spectral analysis. These chapters provide the foundational material for the remaining chapters, which cover the construction of structural models and the extension of vector autoregressive modeling to high frequency, continuously recorded, and irregularly sampled series. The final chapter combines these approaches with spectral methods for identifying causal dependence between time series. Web Resource A supplementary website provides the data sets used in the examples as well as documented MATLAB\textregistered{} functions and other code for analyzing the examples and producing the illustrations. The site also offers technical details on the estimation theory and methods and the implementation of the models.},
  isbn = {978-1-58488-650-1},
  langid = {Inglese}
}

@article{witten2011,
  title = {New {{Insights}} and {{Faster Computations}} for the {{Graphical Lasso}}},
  author = {Witten, Daniela M. and Friedman, Jerome H. and Simon, Noah},
  year = {2011},
  journal = {Journal of Computational and Graphical Statistics},
  volume = {20},
  number = {4},
  pages = {892--900},
  publisher = {{[American Statistical Association, Taylor \& Francis, Ltd., Institute of Mathematical Statistics, Interface Foundation of America]}},
  issn = {1061-8600},
  abstract = {We consider the graphical lasso formulation for estimating a Gaussian graphical model in the high-dimensional setting. This approach entails estimating the inverse covariance matrix under a multivariate normal model by maximizing the {$\mathscr{l}$} 1 -penalized log-likelihood. We present a very simple necessary and sufficient condition that can be used to identify the connected components in the graphical lasso solution. The condition can be employed to determine whether the estimated inverse covariance matrix will be block diagonal, and if so, then to identify the blocks. This in turn can lead to drastic speed improvements, since one can simply apply a standard graphical lasso algorithm to each block separately. Moreover, the necessary and sufficient condition provides insight into the graphical lasso solution: the set of connected nodes at any given tuning parameter value is a superset of the set of connected nodes at any larger tuning parameter value. This article has supplementary material online.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Witten_et_al_2011_New_Insights_and_Faster_Computations_for_the_Graphical_Lasso.pdf}
}

@article{woo2014,
  title = {Cluster-Extent Based Thresholding in {{fMRI}} Analyses: Pitfalls and Recommendations},
  shorttitle = {Cluster-Extent Based Thresholding in {{fMRI}} Analyses},
  author = {Woo, Choong-Wan and Krishnan, Anjali and Wager, Tor D.},
  year = {2014},
  month = may,
  journal = {NeuroImage},
  volume = {91},
  pages = {412--419},
  issn = {1095-9572},
  doi = {10.1016/j.neuroimage.2013.12.058},
  abstract = {Cluster-extent based thresholding is currently the most popular method for multiple comparisons correction of statistical maps in neuroimaging studies, due to its high sensitivity to weak and diffuse signals. However, cluster-extent based thresholding provides low spatial specificity; researchers can only infer that there is signal somewhere within a significant cluster and cannot make inferences about the statistical significance of specific locations within the cluster. This poses a particular problem when one uses a liberal cluster-defining primary threshold (i.e., higher p-values), which often produces large clusters spanning multiple anatomical regions. In such cases, it is impossible to reliably infer which anatomical regions show true effects. From a survey of 814 functional magnetic resonance imaging (fMRI) studies published in 2010 and 2011, we show that the use of liberal primary thresholds (e.g., p{$<$}.01) is endemic, and that the largest determinant of the primary threshold level is the default option in the software used. We illustrate the problems with liberal primary thresholds using an fMRI dataset from our laboratory (N=33), and present simulations demonstrating the detrimental effects of liberal primary thresholds on false positives, localization, and interpretation of fMRI findings. To avoid these pitfalls, we recommend several analysis and reporting procedures, including 1) setting primary p{$<$}.001 as a default lower limit; 2) using more stringent primary thresholds or voxel-wise correction methods for highly powered studies; and 3) adopting reporting practices that make the level of spatial precision transparent to readers. We also suggest alternative and supplementary analysis methods.},
  langid = {english},
  pmcid = {PMC4214144},
  pmid = {24412399},
  keywords = {Cluster Analysis,Cluster-extent thresholding,Computer Simulation,Data Interpretation; Statistical,False discovery rate,False Positive Reactions,Family-wise error rate,fMRI,FSL,Gaussian random fields,Humans,Image Processing; Computer-Assisted,Magnetic Resonance Imaging,Multiple comparisons,Neuroimaging,Normal Distribution,Primary threshold,Signal-To-Noise Ratio,Software,SPM,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Woo_et_al_2014_Cluster-extent_based_thresholding_in_fMRI_analyses.pdf}
}

@article{wood2003,
  title = {Thin Plate Regression Splines},
  author = {Wood, Simon N.},
  year = {2003},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {65},
  number = {1},
  pages = {95--114},
  publisher = {{Wiley Online Library}}
}

@book{wood2017,
  title = {{Generalized Additive Models: An Introduction with R, Second Edition}},
  shorttitle = {{Generalized Additive Models}},
  author = {Wood, Simon N.},
  year = {2017},
  edition = {2\textdegree{} edizione},
  publisher = {{Chapman and Hall/CRC}},
  address = {{Boca Raton}},
  isbn = {978-1-4987-2833-1},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Wood_2017_Generalized_Additive_Models.pdf}
}

@article{woodall1999,
  title = {Research {{Issues}} and {{Ideas}} in {{Statistical Process Control}}},
  author = {Woodall, William H. and Montgomery, Douglas C.},
  year = {1999},
  month = oct,
  journal = {Journal of Quality Technology},
  volume = {31},
  number = {4},
  pages = {376--386},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.1999.11979944},
  abstract = {An overview is given of current research on control charting methods for process monitoring and improvement. The discussion includes a historical perspective along with ideas for future research. Research topics include, for example, variable sample size and sampling interval methods, economic designs, attribute data methods, charts based on autocorrelated observations, multivariate methods, and nonparametric methods. Recommendations and references are provided to those interested in pursuing research ideas in statistical process control (SPC). Some issues regarding the relevance of SPC research are also discussed.},
  keywords = {Control Charting,doing,Statistical Process Control,Statistical Quality Control},
  annotation = {\_eprint: https://doi.org/10.1080/00224065.1999.11979944},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Woodall_Montgomery_1999_Research_Issues_and_Ideas_in_Statistical_Process_Control.pdf;/home/dede/Zotero/storage/Q7YCRD35/00224065.1999.html}
}

@article{woodall2000,
  title = {Controversies and {{Contradictions}} in {{Statistical Process Control}}},
  author = {Woodall, William H.},
  year = {2000},
  month = oct,
  journal = {Journal of Quality Technology},
  volume = {32},
  number = {4},
  pages = {341--350},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2000.11980013},
  abstract = {Statistical process control (SPC) methods are widely used to monitor and improve manufacturing processes and service operations. Disputes over the theory and application of these methods are frequent and often very intense. Some of the controversies and issues discussed are the relationship between hypothesis testing and control charting, the role of theory and the modeling of control chart performance, the relative merits of competing methods, the relevance of research on SPC and even the relevance of SPC itself. One purpose of the paper is to offer a resolution of some of these disagreements in order to improve the communication between practitioners and researchers.},
  keywords = {Average Run Length,Control Charts,Cumulative Sum Control Charts,doing,Exponentially Weighted Moving Average Control Charts},
  annotation = {\_eprint: https://doi.org/10.1080/00224065.2000.11980013},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Woodall_2000_Controversies_and_Contradictions_in_Statistical_Process_Control.pdf}
}

@article{woodall2014,
  title = {Some {{Current Directions}} in the {{Theory}} and {{Application}} of {{Statistical Process Monitoring}}},
  author = {Woodall, William and Montgomery, Douglas},
  year = {2014},
  month = jan,
  journal = {Journal of Quality Technology},
  volume = {46},
  pages = {78--94},
  doi = {10.1080/00224065.2014.11917955},
  abstract = {The purpose of this paper is to provide an overview and our perspective of recent research and applications of statistical process monitoring. The focus is on work done over the past decade or so. We review briefly a number of important areas, including health-related monitoring, spatiotemporal surveillance, profile monitoring, use of autocorrelated data, the effect of estimation error, and high-dimensional monitoring, among others. We briefly discuss the choice of performance metrics. We provide references and offer some directions for further research.},
  keywords = {todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Woodall_Montgomery_2014_Some_Current_Directions_in_the_Theory_and_Application_of_Statistical_Process.pdf}
}

@article{woodall2016,
  title = {An {{Overview}} and {{Perspective}} on {{Social Network Monitoring}}},
  author = {Woodall, William and Zhao, Meng and Paynabar, Kamran and Wilson, James},
  year = {2016},
  month = mar,
  journal = {IISE Transactions},
  volume = {49},
  doi = {10.1080/0740817X.2016.1213468},
  abstract = {In this expository paper we give an overview of some statistical methods for the monitoring of social networks. We discuss the advantages and limitations of various methods as well as some relevant issues. One of our primary contributions is to give the relationships between network monitoring methods and monitoring methods in engineering statistics and public health surveillance. We encourage researchers in the industrial process monitoring area to work on developing and comparing the performance of social network monitoring methods. We also discuss some of the issues in social network monitoring and give a number of research ideas.},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Woodall_et_al_2016_An_Overview_and_Perspective_on_Social_Network_Monitoring.pdf}
}

@article{woodall2019,
  title = {Rethinking Control Chart Design and Evaluation},
  author = {Woodall, William H. and Faltin, Frederick W.},
  year = {2019},
  month = oct,
  journal = {Quality Engineering},
  volume = {31},
  number = {4},
  pages = {596--605},
  publisher = {{Taylor \& Francis}},
  issn = {0898-2112},
  doi = {10.1080/08982112.2019.1582779},
  abstract = {We discuss some practical issues involving the control of the number of false alarms in process monitoring. This topic is of growing importance as the number of variables being monitored and the frequency of measurement increase. An alternative formulation for evaluating and comparing the performance of control charts is given based on defining in-control, indifference and out-of-control regions of the parameter space. Methods are designed so that only changes of practical importance are to be detected quickly. This generalization of the existing framework makes control charting much more useful in practice, especially when many variables are being monitored. It also justifies to a greater extent the use of cumulative sum (CUSUM) methods.},
  keywords = {average run length,cumulative sum chart,done,false alarm rate,hypothesis testing,practical significance,statistical process monitoring,statistical significance},
  annotation = {\_eprint: https://doi.org/10.1080/08982112.2019.1582779},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Woodall_Faltin_2019_Rethinking_control_chart_design_and_evaluation.pdf;/home/dede/Zotero/storage/PICW8XQZ/08982112.2019.html}
}

@article{worsley1979,
  title = {On the {{Likelihood Ratio Test}} for a {{Shift}} in {{Location}} of {{Normal Populations}}},
  author = {Worsley, K. J.},
  year = {1979},
  journal = {Journal of the American Statistical Association},
  volume = {74},
  number = {366},
  pages = {365--367},
  publisher = {{[American Statistical Association, Taylor \& Francis, Ltd.]}},
  issn = {0162-1459},
  doi = {10.2307/2286336},
  abstract = {An alternative to the hypothesis that the sequence X\textsubscript{1}, ..., X\textsubscript{n} are independent and identically distributed normal random variables, with mean {$\mu$} and variance {$\sigma$}\textsuperscript{2}, is that the location parameter {$\mu$} shifts at some unknown instant. The null distributions of likelihood ratio test statistics are given by Hawkins (1977) for the two cases of known and unknown {$\sigma$}\textsuperscript{2}. Unfortunately, the null distribution for unknown {$\sigma$}\textsuperscript{2} obtained in that article is incorrect. In this article the correct null distribution is found and a numerical integration technique is used to obtain standard percentage points for n = 3(1)10. A Monte Carlo method is used to obtain additional standard percentage points for n = 15(5)50.}
}

@article{worsley1986,
  title = {Confidence {{Regions}} and {{Tests}} for a {{Change-Point}} in a {{Sequence}} of {{Exponential Family Random Variables}}},
  author = {Worsley, K. J.},
  year = {1986},
  journal = {Biometrika},
  volume = {73},
  number = {1},
  pages = {91--104},
  publisher = {{[Oxford University Press, Biometrika Trust]}},
  issn = {0006-3444},
  doi = {10.2307/2336275},
  abstract = {Maximum likelihood methods are used to test for a change in a sequence of independent exponential family random variables, with particular emphasis on the exponential distribution. The exact null and alternative distributions of the test statistics are found, and the power is compared with a test based on a linear trend statistic. Exact and approximate confidence regions for the change-point are based on the values accepted by a level {$\alpha$} likelihood ratio test and a modification of the method proposed by Cox \& Spj\o tvoll (1982). The methods are applied to a classical data set on the time intervals between coal mine explosions, and the change in variation of stock market returns. In both cases the confidence regions for the change-point cover historical events that may have caused the changes.}
}

@book{wu2021,
  title = {Experiments: {{Planning}}, {{Analysis}}, and {{Optimization}}},
  shorttitle = {Experiments},
  author = {Wu, C. F. Jeff and Hamada, Michael S.},
  year = {2021},
  month = mar,
  edition = {3rd edition},
  publisher = {{Wiley}},
  address = {{Hoboken, NJ}},
  abstract = {Praise for the First Edition: "If you ... want an up-to-date, definitive reference written by authors who have contributed much to this field, then this book is an essential addition to your library."\rule{1em}{1pt}Journal of the American Statistical Association A COMPREHENSIVE REVIEW OF MODERN EXPERIMENTAL DESIGN Experiments: Planning, Analysis, and Optimization, Third Edition provides a complete discussion of modern experimental design for product and process improvement\rule{1em}{1pt}the design and analysis of experiments and their applications for system optimization, robustness, and treatment comparison. While maintaining the same easy-to-follow style as the previous editions, this book continues to present an integrated system of experimental design and analysis that can be applied across various fields of research including engineering, medicine, and the physical sciences. New chapters provide modern updates on practical optimal design and computer experiments, an explanation of computer simulations as an alternative to physical experiments. Each chapter begins with a real-world example of an experiment followed by the methods required to design that type of experiment. The chapters conclude with an application of the methods to the experiment, bridging the gap between theory and practice. The authors modernize accepted methodologies while refining many cutting-edge topics including robust parameter design, analysis of non-normal data, analysis of experiments with complex aliasing, multilevel designs, minimum aberration designs, and orthogonal arrays. The third edition includes:  Information on the design and analysis of computer experiments A discussion of practical optimal design of experiments An introduction to conditional main effect (CME) analysis and definitive screening designs (DSDs) New exercise problems  This book includes valuable exercises and problems, allowing the reader to gauge their progress and retention of the book's subject matter as they complete each chapter. Drawing on examples from their combined years of working with industrial clients, the authors present many cutting-edge topics in a single, easily accessible source. Extensive case studies, including goals, data, and experimental designs, are also included, and the book's data sets can be found on a related FTP site, along with additional supplemental material. Chapter summaries provide a succinct outline of discussed methods, and extensive appendices direct readers to resources for further study. Experiments: Planning, Analysis, and Optimization, Third Edition is an excellent book for design of experiments courses at the upper-undergraduate and graduate levels. It is also a valuable resource for practicing engineers and statisticians.},
  isbn = {978-1-119-47010-6},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Wu_Hamada_2021_Experiments.pdf}
}

@article{xia2015,
  title = {Thompson {{Sampling}} for {{Budgeted Multi-armed Bandits}}},
  author = {Xia, Yingce and Li, Haifang and Qin, Tao and Yu, Nenghai and Liu, Tie-Yan},
  year = {2015},
  month = may,
  journal = {arXiv:1505.00146 [cs]},
  eprint = {1505.00146},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Thompson sampling is one of the earliest randomized algorithms for multi-armed bandits (MAB). In this paper, we extend the Thompson sampling to Budgeted MAB, where there is random cost for pulling an arm and the total cost is constrained by a budget. We start with the case of Bernoulli bandits, in which the random rewards (costs) of an arm are independently sampled from a Bernoulli distribution. To implement the Thompson sampling algorithm in this case, at each round, we sample two numbers from the posterior distributions of the reward and cost for each arm, obtain their ratio, select the arm with the maximum ratio, and then update the posterior distributions. We prove that the distribution-dependent regret bound of this algorithm is \$O(\textbackslash ln B)\$, where \$B\$ denotes the budget. By introducing a Bernoulli trial, we further extend this algorithm to the setting that the rewards (costs) are drawn from general distributions, and prove that its regret bound remains almost the same. Our simulation results demonstrate the effectiveness of the proposed algorithm.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Xia_et_al_2015_Thompson_Sampling_for_Budgeted_Multi-armed_Bandits2.pdf;/home/dede/Zotero/storage/3IG4UYHY/1505.html}
}

@inproceedings{xia2016,
  title = {Budgeted Multi-Armed Bandits with Multiple Plays},
  booktitle = {Proceedings of the {{Twenty-Fifth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Xia, Yingce and Qin, Tao and Ma, Weidong and Yu, Nenghai and Liu, Tie-Yan},
  year = {2016},
  month = jul,
  series = {{{IJCAI}}'16},
  pages = {2210--2216},
  publisher = {{AAAI Press}},
  address = {{New York, New York, USA}},
  abstract = {We study the multi-play budgeted multi-armed bandit (MP-BMAB) problem, in which pulling an arm receives both a random reward and a random cost, and a player pulls L ({$\geq$} 1) arms at each round. The player targets at maximizing her total expected reward under a budget constraint B for the pulling costs. We present a multiple ratio confidence bound policy: At each round, we first calculate a truncated upper (lower) confidence bound for the expected reward (cost) of each arm, and then pull the L arms with the maximum ratio of the sum of the upper confidence bounds of rewards to the sum of the lower confidence bounds of costs. We design a 0- 1 integer linear fractional programming oracle that can pick such the L arms within polynomial time. We prove that the regret of our policy is sublinear in general and is log-linear for certain parameter settings. We further consider two special cases of MP-BMABs: (1) We derive a lower bound for any consistent policy for MP-BMABs with Bernoulli reward and cost distributions. (2) We show that the proposed policy can also solve conventional budgeted MAB problem (a special case of MP-BMABs with L = 1) and provides better theoretical results than existing UCB-based pulling policies.},
  isbn = {978-1-57735-770-4},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Xia_et_al_2016_Budgeted_multi-armed_bandits_with_multiple_plays.pdf}
}

@article{xia2019,
  title = {A Computationally Efficient Self-Starting Scheme to Monitor General Linear Profiles with Abrupt Changes},
  author = {Xia, Zhiming and Tsung, Fugee},
  year = {2019},
  month = may,
  journal = {Quality Technology \& Quantitative Management},
  volume = {16},
  number = {3},
  pages = {278--296},
  publisher = {{Taylor \& Francis}},
  issn = {null},
  doi = {10.1080/16843703.2017.1396956},
  abstract = {A self-starting monitoring scheme is proposed in this paper for the simultaneous detection of variance and coefficients in linear profiles with unknown error distributions. Based on the global data, we construct a sequential Wald-type charting statistic, obtain the corresponding asymptotical distributions and further provide a recursive algorithm to quickly calculate statistics sequentially. Control limits of our charting statistics are also constructed based on their asymptotical distributions. Finally, we apply our method to analyze both artificial and real data, and numerical results show that our method performs well.},
  keywords = {Linear profile,self-starting,SPC,Wald-type charting statistic},
  annotation = {\_eprint: https://doi.org/10.1080/16843703.2017.1396956},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Xia_Tsung_2019_A_computationally_efficient_self-starting_scheme_to_monitor_general_linear.pdf;/home/dede/Zotero/storage/PTWCBQTA/16843703.2017.html}
}

@article{xian2018,
  title = {A {{Nonparametric Adaptive Sampling Strategy}} for {{Online Monitoring}} of {{Big Data Streams}}},
  author = {Xian, Xiaochen and Wang, Andi and Liu, Kaibo},
  year = {2018},
  month = jan,
  journal = {Technometrics},
  volume = {60},
  number = {1},
  pages = {14--25},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.2017.1317291},
  abstract = {With the rapid advancement of sensor technology, a huge amount of data is generated in various applications, which poses new and unique challenges for statistical process control (SPC). In this article, we propose a nonparametric adaptive sampling (NAS) strategy to online monitor nonnormal big data streams in the context of limited resources, where only a subset of observations are available at each acquisition time. In particular, this proposed method integrates a rank-based CUSUM scheme and an innovative idea that corrects the anti-rank statistics with partial observations, which can effectively detect a wide range of possible mean shifts when data streams are exchangeable and follow arbitrary distributions. Two theoretical properties on the sampling layout of the proposed NAS algorithm are investigated when the process is in control and out of control. Both simulations and case studies are conducted under different scenarios to illustrate and evaluate the performance of the proposed method. Supplementary materials for this article are available online.},
  keywords = {Distribution-free,done,Multivariate CUSUM procedure,Partial observations,Process change detection,Statistical process control},
  annotation = {\_eprint: https://doi.org/10.1080/00401706.2017.1317291},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Xian_et_al_2018_A_Nonparametric_Adaptive_Sampling_Strategy_for_Online_Monitoring_of_Big_Data.pdf;/home/dede/Zotero/storage/EG4APGK8/00401706.2017.html}
}

@article{xue2021,
  title = {A Nonparametric {{CUSUM}} Chart for Monitoring Multivariate Serially Correlated Processes},
  author = {Xue, Li and Qiu, Peihua},
  year = {2021},
  month = aug,
  journal = {Journal of Quality Technology},
  volume = {53},
  number = {4},
  pages = {396--409},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2020.1778430},
  abstract = {In applications, most processes for quality control and management are multivariate. Thus, multivariate statistical process control (MSPC) is an important research problem and has been discussed extensively in the literature. Early MSPC research is based on the assumptions that process observations at different time points are independent and they have a parametric distribution (e.g., Gaussian) when the process is in-control (IC). Recent MSPC research has lifted the ``parametric distribution'' assumption, and some nonparametric MSPC charts have been developed. These nonparametric MSPC charts, however, often requires the ``independent process observations'' assumption, which is rarely valid in practice because serial data correlation is common in a time series data. In the literature, it has been well demonstrated that a control chart who ignores serial data correlation would be unreliable to use when such data correlation exists. So far, we have not found any existing nonparametric MSPC charts that can accommodate serial data correlation properly. In this paper, we suggest a flexible nonparametric MSPC chart which can accommodate stationary serial data correlation properly. Numerical studies show that it performs well in different cases.},
  keywords = {data correlation,decorrelation,doing,moment estimation,nonparametric charts,stationary data correlation,statistical process control},
  annotation = {\_eprint: https://doi.org/10.1080/00224065.2020.1778430},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Xue_Qiu_2021_A_nonparametric_CUSUM_chart_for_monitoring_multivariate_serially_correlated.pdf;/home/dede/Zotero/storage/GW8B27P2/00224065.2020.html}
}

@article{yan2017,
  title = {Anomaly {{Detection}} in {{Images With Smooth Background}} via {{Smooth-Sparse Decomposition}}},
  author = {Yan, Hao and Paynabar, Kamran and Shi, Jianjun},
  year = {2017},
  month = jan,
  journal = {Technometrics},
  volume = {59},
  number = {1},
  pages = {102--114},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.2015.1102764},
  abstract = {In various manufacturing applications such as steel, composites, and textile production, anomaly detection in noisy images is of special importance. Although there are several methods for image denoising and anomaly detection, most of these perform denoising and detection sequentially, which affects detection accuracy and efficiency. Additionally, the low computational speed of some of these methods is a limitation for real-time inspection. In this article, we develop a novel methodology for anomaly detection in noisy images with smooth backgrounds. The proposed method, named smooth-sparse decomposition, exploits regularized high-dimensional regression to decompose an image and separate anomalous regions by solving a large-scale optimization problem. To enable the proposed method for real-time implementation, a fast algorithm for solving the optimization model is proposed. Using simulations and a case study, we evaluate the performance of the proposed method and compare it with existing methods. Numerical results demonstrate the superiority of the proposed method in terms of the detection accuracy as well as computation time. This article has supplementary materials that includes all the technical details, proofs, MATLAB codes, and simulated images used in the article.},
  keywords = {Anomaly detection,Convex optimization,High-dimensional,Image,Regression,Smooth background,todo},
  annotation = {\_eprint: https://doi.org/10.1080/00401706.2015.1102764},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Yan_et_al_2017_Anomaly_Detection_in_Images_With_Smooth_Background_via_Smooth-Sparse.pdf;/home/dede/Zotero/storage/YH68L4LT/00401706.2015.html}
}

@article{yang2015,
  title = {An {{Arithmetic-Analytical Expression}} of the {{Hilbert-Type Space-Filling Curves}} and {{Its Applications}}},
  author = {Yang, Xiaoling and Tan, Ying},
  year = {2015},
  month = apr,
  journal = {Bulletin of the Malaysian Mathematical Sciences Society},
  volume = {38},
  number = {2},
  pages = {841--854},
  issn = {0126-6705, 2180-4206},
  doi = {10.1007/s40840-014-0052-6},
  langid = {english},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Yang_Tan_2015_An_Arithmetic-Analytical_Expression_of_the_Hilbert-Type_Space-Filling_Curves.pdf}
}

@article{yang2017,
  title = {Nonparametric {{Profile Monitoring}} Using {{Dynamic Probability Control Limits}}},
  author = {Yang, Wenwan and Zou, Changliang and Wang, Zhaojun},
  year = {2017},
  journal = {Quality and Reliability Engineering International},
  volume = {33},
  number = {5},
  pages = {1131--1142},
  issn = {1099-1638},
  doi = {10.1002/qre.2104},
  abstract = {This article focuses on monitoring nonparametric profile with time-varying sample sizes and random predictors. Traditional profile monitoring schemes, whose control limits are often determined before the monitoring initiates, are constructed based on perfect knowledge of profile sample sizes and predictors. In practice, however, our foreknowledge about future random sample sizes and predictors is seldom available. An inappropriate assumption or estimation of the sample sizes model and/or predictors distribution function may lead to unexpected performance of traditional control charts. To overcome this problem, we propose a kernel-based nonparametric profile monitoring scheme which integrates the multivariate exponentially weighted moving average procedure with the probability control limits. The success of the proposed chart lies in the use of dynamic control limits which are determined online, essentially aiming at guaranteeing the conditional probability that the charting statistic exceeds the control limit at present given that there is no alarm before the current time point to meet a pre-specified false alarm rate. The simulation studies show that the proposed control scheme has good in-control and out-of-control performances under various scenarios of time-varying sample sizes and random predictors. Copyright \textcopyright{} 2016 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {control charts,kernel estimation,multivariate statistics,profile monitoring,quality control,skimmed},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/qre.2104},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Yang_et_al_2017_Nonparametric_Profile_Monitoring_using_Dynamic_Probability_Control_Limits.pdf;/home/dede/Zotero/storage/YVSVU2AZ/qre.html}
}

@article{yang2021,
  title = {Adaptive {{Process Monitoring Using Covariate Information}}},
  author = {Yang, Kai and Qiu, Peihua},
  year = {2021},
  month = jul,
  journal = {Technometrics},
  volume = {63},
  number = {3},
  pages = {313--328},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.2020.1772115},
  abstract = {Statistical process control (SPC) charts provide a powerful tool for monitoring production lines in manufacturing industries. They are also used widely in other applications, such as sequential monitoring of internet traffic flows, disease incidences, health care systems, and more. In practice, quality/performance variables are often affected in a complex way by many covariates, such as material, labor, weather conditions, social/economic conditions, and so forth. Among all these covariates, some could be observed, some might be difficult to observe, and the others might even be difficult for us to notice their existence. Intuitively, an SPC chart could be improved by using helpful information in covariates. However, because of the complex relationship between the quality/performance variables and the covariates, shifts in the quality/performance variables could be due to certain covariates whose data cannot be collected. On the other hand, shifts in some observable covariates may not necessarily cause shifts in the quality/performance variables. Thus, it is challenging to properly use covariate information for process monitoring in a general setting. This article suggests a method to handle this problem. An effective exponentially weighted moving average chart is developed, in which its weighting parameter is chosen large if the related covariates included in the collected data tend to have a shift and small otherwise. Because the covariate information is used in the weighting parameter only, the chart is designed solely for detecting shifts in the quality/performance variables, but it can react to a future shift in the quality/performance variables quickly because the helpful covariate information has been used in its observation weighting mechanism. Extensive numerical studies show that this method is effective in many different cases.},
  keywords = {todo},
  annotation = {\_eprint: https://doi.org/10.1080/00401706.2020.1772115},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Yang_Qiu_2021_Adaptive_Process_Monitoring_Using_Covariate_Information2.pdf;/home/dede/Zotero/storage/ST7KATD3/00401706.2020.html}
}

@article{yang2022,
  title = {Design Variable-Sampling Control Charts Using Covariate Information},
  author = {Yang, Kai and Qiu, Peihua},
  year = {2022},
  month = may,
  journal = {IISE Transactions},
  volume = {54},
  number = {5},
  pages = {505--519},
  publisher = {{Taylor \& Francis}},
  issn = {2472-5854},
  doi = {10.1080/24725854.2021.1902591},
  abstract = {Statistical Process Control (SPC) charts are widely used in manufacturing industry for monitoring the performance of sequential production processes over time. A common practice in using a control chart is to first collect samples and take measurements of certain quality variables from them at equally-spaced sampling times, and then make decisions about the process status by the chart based on the observed data. In some applications, however, the quality variables are associated with certain covariates, and it should improve the performance of an SPC chart if the covariate information can be used properly. Intuitively, if the covariate information indicates that the process under monitoring is likely to have a distributional shift soon based on the established relationship between the quality variables and the covariates, then it should benefit the process monitoring by collecting the next process observation sooner than usual. Motivated by this idea, we propose a general framework to design a variable-sampling control chart by using covariate information. Our proposed chart is self-starting and can well accommodate stationary short-range serial data correlation. It should be the first variable-sampling control chart in the literature that the sampling intervals are determined by the covariate information. Numerical studies show that the proposed method performs well in different cases considered.},
  keywords = {Auxiliary variables,data correlation,kernel estimation,regression modelling,sampling intervals,self-starting,statistical process control,variable-sampling},
  annotation = {\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/24725854.2021.1902591},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Yang_Qiu_2022_Design_variable-sampling_control_charts_using_covariate_information.pdf;/home/dede/Zotero/storage/XSALDIBG/24725854.2021.html}
}

@article{yashin1995,
  title = {Correlated Individual Frailty:  An Advantageous Approach to Survival Analysis of Bivariate Data},
  shorttitle = {Correlated Individual Frailty},
  author = {Yashin, A. I. and Vaupel, J. W. and Iachine, I. A.},
  year = {1995},
  journal = {Mathematical Population Studies},
  volume = {5},
  number = {2},
  pages = {145--159, 183},
  issn = {0889-8480},
  doi = {10.1080/08898489509525394},
  abstract = {"We develop a new model of bivariate survival based on the notion of correlated individual frailty.  We analyze the properties of this model and suggest a new approach to the analysis of bivariate data that does not require a parametric specification--but permits estimation--of the form of the hazard function for individuals.  We empirically demonstrate the advantages of the model in the statistical analysis of bivariate data."  (SUMMARY IN FRE)},
  langid = {english},
  pmid = {12290053},
  keywords = {correlated frailty,Data Analysis,Demographic Factors,Demography,done,Length Of Life,Longevity,Models; Theoretical,Mortality,Population,Population Dynamics,Research,Research Methodology,Statistical Studies,Statistics as Topic,Studies,survival,Survival Rate,Survivorship,World},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Yashin_et_al_1995_Correlated_individual_frailty.pdf}
}

@article{ye2022,
  title = {Online Nonparametric Monitoring of Heterogeneous Data Streams with Partial Observations Based on {{Thompson}} Sampling},
  author = {Ye, Honghan and Xian, Xiaochen and Cheng, Jing-Ru C. and Hable, Brock and Shannon, Robert W. and Elyaderani, Mojtaba Kadkhodaie and Liu, Kaibo},
  year = {2022},
  month = feb,
  journal = {IISE Transactions},
  volume = {0},
  number = {0},
  pages = {1--13},
  publisher = {{Taylor \& Francis}},
  issn = {2472-5854},
  doi = {10.1080/24725854.2022.2039423},
  abstract = {With the rapid advancement of sensor technology driven by Internet-of-Things-enabled applications, tremendous amounts of measurements of heterogeneous data streams are frequently acquired for online process monitoring. Such massive data, involving a large number of data streams with high sampling frequency, incur high costs on data collection, transmission, and analysis in practice. As a result, the resource constraint often restricts the data observability to only a subset of data streams at each data acquisition time, posing significant challenges in many online monitoring applications. Unfortunately, existing methods do not provide a general framework for monitoring heterogeneous data streams with partial observations. In this article, we propose a nonparametric monitoring and sampling algorithm to quickly detect abnormalities occurring to heterogeneous data streams. In particular, an approximation framework is incorporated with an antirank-based CUSUM procedure to collectively estimate the underlying status of all data streams based on partially observed data. Furthermore, an intelligent sampling strategy based on Thompson sampling is proposed to dynamically observe the informative data streams and balance between exploration and exploitation to facilitate quick anomaly detection. Theoretical justification of the proposed algorithm is also investigated. Both simulations and case studies are conducted to demonstrate the superiority of the proposed method.},
  keywords = {antirank-based CUSUM procedure,doing,Heterogeneous data streams,mean shift detection,partial observations,Thompson sampling},
  annotation = {\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/24725854.2022.2039423},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Ye_et_al_2022_Online_nonparametric_monitoring_of_heterogeneous_data_streams_with_partial.pdf;/home/dede/Zotero/storage/S6UZQSUP/24725854.2022.html}
}

@book{yee2015,
  title = {Vector Generalized Linear and Additive Models: With an Implementation in {{R}}},
  shorttitle = {Vector Generalized Linear and Additive Models},
  author = {Yee, Thomas W.},
  year = {2015},
  series = {Springer Series in Statistics},
  publisher = {{Springer}},
  address = {{New York, NY}},
  isbn = {978-1-4939-2817-0},
  langid = {english},
  lccn = {QA279 .Y44 2015},
  keywords = {Datenverarbeitung,Hochleistungsrechnen,Linear models (Statistics),Lineares Modell,Regression analysis,Statistisches Modell,Vector spaces,Wahrscheinlichkeitstheorie},
  annotation = {OCLC: ocn907271683},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Yee_2015_Vector_generalized_linear_and_additive_models.pdf}
}

@article{yildirim2013,
  title = {An {{Online Expectation}}\textendash{{Maximization Algorithm}} for {{Changepoint Models}}},
  author = {Yildirim, Sinan and Singh, Sumeetpal S. and Doucet, Arnaud},
  year = {2013},
  month = oct,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {22},
  number = {4},
  pages = {906--926},
  publisher = {{Taylor \& Francis}},
  issn = {1061-8600},
  doi = {10.1080/10618600.2012.674653},
  abstract = {Changepoint models are widely used to model the heterogeneity of sequential data. We present a novel sequential Monte Carlo (SMC) online expectation\textendash maximization (EM) algorithm for estimating the static parameters of such models. The SMC online EM algorithm has a cost per time which is linear in the number of particles and could be particularly important when the data is representable as a long sequence of observations, since it drastically reduces the computational requirements for implementation. We present an asymptotic analysis for the stability of the SMC estimates used in the online EM algorithm and demonstrate the performance of this scheme by using both simulated and real data originating from DNA analysis. The supplementary materials for the article are available online.},
  keywords = {Particle filter,Recursive estimation,Smoothing,Stochastic approximation,todo},
  annotation = {\_eprint: https://doi.org/10.1080/10618600.2012.674653},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Yildirim_et_al_2013_An_Online_Expectationâ€“Maximization_Algorithm_for_Changepoint_Models.pdf}
}

@article{yost2006,
  title = {France's New Nuclear Doctrine},
  author = {Yost, David S.},
  year = {2006},
  month = jul,
  journal = {International Affairs},
  volume = {82},
  number = {4},
  pages = {701--721},
  issn = {0020-5850},
  doi = {10.1111/j.1468-2346.2006.00564.x},
  abstract = {The new nuclear deterrence doctrine announced by President Jacques Chirac in January 2006 has rightly been recognized as a milestone, although in fact several of the key changes in policy were set forth in June 2001. While France remains determined to deter major power threats, its main new preoccupation is deterring regional powers by making clear that it has developed more employable nuclear options. The innovations announced in January 2006 include the focus on deterring state sponsors of terrorism, the threat to attach an enemy's `capacity to act', the more discriminate and controllable employment options, the willingness to launch `final warning' strikes, the description of `strategic supplies' as a potential vital interest, and the presentation of nuclear deterrence as the foundation of a strategy of prevention and, when necessary, conventional military intervention. Several factors may have led Chirac to make the speech at this juncture. These include maintaining the credibility of deterrence and presidential power, sustaining the budgetary effort required for the nuclear posture, clarifying French deterrence doctrine for external and internal audiences, and sending a message of autonomy to Iran's and France's key European partners. The new doctrine's implications include its significance for deterrence and non-proliferation and for France's relations with its partners in NATO and the European Union.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Yost_2006_France's_new_nuclear_doctrine.pdf;/home/dede/Zotero/storage/39NRQ3L7/2624347.html}
}

@book{young2005,
  title = {Essentials of {{Statistical Inference}}},
  author = {Young, G. A. and Smith, R. L.},
  year = {2005},
  month = jul,
  edition = {1 edition},
  publisher = {{Cambridge University Press}},
  abstract = {Aimed at advanced undergraduate and graduate students in mathematics and related disciplines, this book presents the concepts and results underlying the Bayesian, frequentist and Fisherian approaches, with particular emphasis on the contrasts between them. Computational ideas are explained, as well as basic mathematical theory. Written in a lucid and informal style, this concise text provides both basic material on the main approaches to inference, as well as more advanced material on  developments in statistical theory, including: material on Bayesian computation, such as MCMC, higher-order likelihood theory, predictive inference, bootstrap methods and conditional inference. It contains numerous extended examples of the application of formal inference techniques to real data, as well as historical commentary on the development of the subject. Throughout, the text concentrates on concepts, rather than mathematical detail, while maintaining appropriate levels of formality. Each chapter ends with a set of accessible problems.},
  langid = {english},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Young_Smith_2005_Essentials_of_Statistical_Inference.pdf}
}

@article{yu2020,
  title = {Hyper-{{Parameter Optimization}}: {{A Review}} of {{Algorithms}} and {{Applications}}},
  shorttitle = {Hyper-{{Parameter Optimization}}},
  author = {Yu, Tong and Zhu, Hong},
  year = {2020},
  month = mar,
  journal = {arXiv:2003.05689 [cs, stat]},
  eprint = {2003.05689},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Since deep neural networks were developed, they have made huge contributions to everyday lives. Machine learning provides more rational advice than humans are capable of in almost every aspect of daily life. However, despite this achievement, the design and training of neural networks are still challenging and unpredictable procedures. To lower the technical thresholds for common users, automated hyper-parameter optimization (HPO) has become a popular topic in both academic and industrial areas. This paper provides a review of the most essential topics on HPO. The first section introduces the key hyper-parameters related to model training and structure, and discusses their importance and methods to define the value range. Then, the research focuses on major optimization algorithms and their applicability, covering their efficiency and accuracy especially for deep learning networks. This study next reviews major services and toolkits for HPO, comparing their support for state-of-the-art searching algorithms, feasibility with major deep learning frameworks, and extensibility for new modules designed by users. The paper concludes with problems that exist when HPO is applied to deep learning, a comparison between optimization algorithms, and prominent approaches for model evaluation with limited computational resources.},
  archiveprefix = {arXiv},
  keywords = {auto-tuning,Computer Science - Machine Learning,deep neural network,Hyper-parameter,Statistics - Machine Learning,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Yu_Zhu_2020_Hyper-Parameter_Optimization.pdf;/home/dede/Zotero/storage/TM8ZBRWV/2003.html}
}

@misc{yu2021,
  title = {{{APTS High-Dimensional Statistics}}},
  author = {Yu, Yi},
  year = {2021},
  langid = {english},
  keywords = {todo},
  file = {/home/dede/Zotero/storage/3G39DGHI/Yu - APTS High-Dimensional Statistics.pdf}
}

@article{zamba2006,
  title = {A {{Multivariate Change-Point Model}} for {{Statistical Process Control}}},
  author = {Zamba, K. D and Hawkins, Douglas M},
  year = {2006},
  month = nov,
  journal = {Technometrics},
  volume = {48},
  number = {4},
  pages = {539--549},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1198/004017006000000291},
  abstract = {Multivariate statistical process control (SPC) carries out ongoing checks to ensure that a process is in control. These checks on the process are traditionally done by T2, multivariate cusum, and multivariate exponentially weighted moving average control charts. These traditional SPC charts assume that the in-control true parameters are known exactly and use these assumed true values to set the control limits. The reality, however, is that true parameter values are seldom if ever known exactly; rather, they are commonly estimated from a Phase I sample. It is increasingly recognized that this Phase I study needs to involve large samples if the parameter estimates are to provide run behavior matching that of the known-parameter situation. But, apart from the general undesirability of large and thus expensive studies preliminary to actual charting, some industrial settings have a paucity of relevant data for estimating the process parameters. An attractive alternative to traditional charting methods when monitoring for a step change in the mean vector is an unknown-parameter likelihood ratio test for a change in mean of p-variate normal data. We have found that this approach description is able to control the run behavior despite the lack of a large Phase I sample.},
  keywords = {Average run length,Change point,Likelihood ratio,Multivariate statistical process control,Phases I and II,T2 statistic},
  annotation = {\_eprint: https://doi.org/10.1198/004017006000000291},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Zamba_Hawkins_2006_A_Multivariate_Change-Point_Model_for_Statistical_Process_Control.pdf;/home/dede/Zotero/storage/6YWEI5NN/004017006000000291.html}
}

@article{zamba2013,
  title = {A Three-State Recursive Sequential {{Bayesian}} Algorithm for Biosurveillance},
  author = {Zamba, K. D. and Tsiamyrtzis, Panagiotis and Hawkins, Douglas M.},
  year = {2013},
  month = feb,
  journal = {Computational Statistics and Data Analysis},
  volume = {58},
  number = {1},
  pages = {82--97},
  issn = {0167-9473},
  doi = {10.1016/j.csda.2011.04.015},
  abstract = {A serial signal detection algorithm is developed to monitor pre-diagnosis and medical diagnosis data pertaining to biosurveillance. The algorithm is three-state sequential, based on Bayesian thinking. It accounts for non-stationarity, irregularity and seasonality, and captures serial structural details of an epidemic curve. At stage n, a trichotomous variable governing the states of an epidemic is defined, and a prior distribution for time-indexed serial readings is set. The technicality consists of finding a posterior state probability based on the observed data history, using the posterior as a prior distribution for stage n+1 and sequentially monitoring surges in posterior state probabilities. A sensitivity analysis for validation is conducted and analytical formulas for the predictive distribution are supplied for error management purposes. The method is applied to syndromic surveillance data gathered in the United States (US) District of Columbia metropolitan area.},
  keywords = {Bayesian sequential update,Dynamic control,Syndromic surveillance}
}

@article{zanella2016,
  title = {Flexible {{Models}} for {{Microclustering}} with {{Application}} to {{Entity Resolution}}},
  author = {Zanella, Giacomo and Betancourt, Brenda and Wallach, Hanna and Miller, Jeffrey and Zaidi, Abbas and Steorts, Rebecca C.},
  year = {2016},
  month = oct,
  journal = {arXiv:1610.09780 [math, stat]},
  eprint = {1610.09780},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  abstract = {Most generative models for clustering implicitly assume that the number of data points in each cluster grows linearly with the total number of data points. Finite mixture models, Dirichlet process mixture models, and Pitman--Yor process mixture models make this assumption, as do all other infinitely exchangeable clustering models. However, for some applications, this assumption is inappropriate. For example, when performing entity resolution, the size of each cluster should be unrelated to the size of the data set, and each cluster should contain a negligible fraction of the total number of data points. These applications require models that yield clusters whose sizes grow sublinearly with the size of the data set. We address this requirement by defining the microclustering property and introducing a new class of models that can exhibit this property. We compare models within this class to two commonly used clustering models using four entity-resolution data sets.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory,Statistics - Applications,Statistics - Machine Learning,Statistics - Methodology,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Zanella_et_al_2016_Flexible_Models_for_Microclustering_with_Application_to_Entity_Resolution.pdf;/home/dede/Zotero/storage/FW482GSV/1610.html}
}

@article{zantek2005,
  title = {Run-Length Distributions of {{Q-chart}} Schemes},
  author = {Zantek, Paul F.},
  year = {2005},
  journal = {IIE Transactions},
  volume = {37},
  number = {11},
  pages = {1037--1045},
  publisher = {{Taylor \& Francis}},
  issn = {0740-817X},
  doi = {10.1080/07408170500232297},
  abstract = {This article studies the performance of the Shewhart chart of Q statistics in the detection of process mean shifts in start-up processes and short runs. We propose an accurate, analytic approximation of this chart's run-length distribution. Our study reveals that the chart has an early detection advantage in that it is more likely than other methods to detect a process mean shift within the first few observations following the shift. This is a desirable property because early detection should make it easier to identify the cause of the shift, increasing the rate of continuous quality improvement. In addition, our analysis illustrates the importance of reacting immediately to out-of-control signals from the chart as compared to waiting for subsequent observations to confirm the presence of a shift.},
  keywords = {doing},
  annotation = {\_eprint: https://doi.org/10.1080/07408170500232297},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Zantek_2005_Run-length_distributions_of_Q-chart_schemes.pdf;/home/dede/Zotero/storage/XH9785UC/07408170500232297.html}
}

@article{zantek2006,
  title = {A Self-Starting Procedure for Monitoring Process Quality in Multistage Manufacturing Systems},
  author = {Zantek, Paul F. and Wright {\textdagger}, Gordon P. and Plante, Robert D.},
  year = {2006},
  month = apr,
  journal = {IIE Transactions},
  volume = {38},
  number = {4},
  pages = {293--308},
  publisher = {{Taylor \& Francis}},
  issn = {0740-817X},
  doi = {10.1080/07408170500208354},
  abstract = {Manufacturing systems typically contain processing and assembly stages whose output quality is significantly affected by the output quality of preceding stages. The deficiencies of using standard statistical process-monitoring procedures in such systems have been highlighted in the literature. This article proposes a procedure to monitor process and product quality in multistage systems. By accounting for the quality of the input to each stage, the procedure not only detects the presence of out-of-control conditions but also helps to identify the stages responsible for such departures. We extend previous research to the common case where the process parameters are unknown. An extensive performance study shows that the procedure is effective in detecting out-of-control conditions and that it convincingly outperforms existing methods. We illustrate the use of the procedure using production line data from a major electronics manufacturer. \textdagger Deceased},
  annotation = {\_eprint: https://doi.org/10.1080/07408170500208354},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Zantek_et_al_2006_A_self-starting_procedure_for_monitoring_process_quality_in_multistage2.pdf;/home/dede/Zotero/storage/45YE6PSE/07408170500208354.html}
}

@article{zeng2011,
  title = {A {{Bayesian}} Approach to Risk-Adjusted Outcome Monitoring in Healthcare},
  author = {Zeng, L. and Zhou, S.},
  year = {2011},
  month = dec,
  journal = {Statistics in Medicine},
  volume = {30},
  number = {29},
  pages = {3431--3446},
  issn = {1097-0258},
  doi = {10.1002/sim.4374},
  abstract = {Clinical outcomes are commonly monitored in healthcare practices to detect changes in care providers' performance. One key challenge in outcome monitoring is the need of adjustment for patient base-line risks. Various control charting methods have been developed to conduct risk-adjusted outcome monitoring, but they all rely on the availability of a large number of historical data. We propose a Bayesian approach to this type of monitoring for cases where historical data are not available. In our approach, detection of change is formulated as a model-selection problem and solved using a popular Bayesian tool for variable selection, the Bayes factor. Issues in decision-making about whether there is a change point in the observed patient outcomes are addressed, including specification of priors and computation of Bayes factors. This approach is applied to a real data set on cardiac surgeries, and its performance under different parameter scenarios is studied through simulations.},
  langid = {english},
  pmid = {21969272},
  keywords = {Bayes Theorem,Cardiac Surgical Procedures,Computer Simulation,Female,Humans,Male,Models; Statistical,Quality Assurance; Health Care,Risk,Treatment Outcome}
}

@article{zeng2020,
  title = {Artificial Intelligence and {{China}}'s Authoritarian Governance},
  author = {Zeng, Jinghan},
  year = {2020},
  month = nov,
  journal = {International Affairs},
  volume = {96},
  number = {6},
  pages = {1441--1459},
  issn = {0020-5850},
  doi = {10.1093/ia/iiaa172},
  abstract = {China has adopted a proactive and strategic approach to embrace the age of artificial intelligence (AI). This article argues that China's bold AI practices are part of its broad and incoherent adaptation strategy to governance by digital means. AI is part of a digital technology package that the Chinese authoritarian regime has actively employed not only to improve public service, but also to strengthen its authoritarian governance. China's digital progress benefits from its huge internet market, strong state power and weak civil awareness, making it more competitive than western democratic societies where privacy concern restricts their AI development. However, China's ambitious AI plan contains considerable risks; its overall impact depends on how AI affects major sources of political legitimacy including economic growth, social stability and ideology. China's approach is gambling on its success in (a) delivering a booming AI economy, (b) ensuring a smooth social transformation towards the age of AI and (c) proving ideological superiority of its authoritarian and communist values.},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Zeng_2020_Artificial_intelligence_and_China's_authoritarian_governance.pdf}
}

@article{zhang2012,
  title = {An Improved Self-Starting Cumulative Count of Conforming Chart for Monitoring High-Quality Processes under Group Inspection},
  author = {Zhang, Cai Wen and Xie, Min and Jin, Tongdan},
  year = {2012},
  month = dec,
  journal = {International Journal of Production Research},
  volume = {50},
  number = {23},
  pages = {7026--7043},
  publisher = {{Taylor \& Francis}},
  issn = {0020-7543},
  doi = {10.1080/00207543.2011.649305},
  abstract = {The cumulative count of conforming (CCC) chart as a main statistical process control tool for monitoring high-quality processes has been widely studied. However, its applicability is limited to situations where units of product are inspected sequentially, or item by item. Motivated by real-life problems, this paper proposes an improved control charting technique for high-quality processes under group inspection. It integrates a self-starting feature and an approximately ARL- (average run length) unbiased design. The chart, named the CCCG chart, monitors the cumulative count of conforming samples until a non-conforming one is encountered. The `G' in the subscript stands for `Group'. The self-starting feature caters to the need in practice to start monitoring a production process as soon as possible. The approximately ARL-unbiased design is developed to significantly improve the chart's sensitivity to process deterioration compared with the conventional design. The performance of the CCCG chart in phase I when the control limits are sequentially updated has been examined. Simulated data examples are presented to demonstrate the use and efficiency of the proposed technique. The CCCG chart is a natural generalisation of the traditional CCC chart and thus includes the latter as a special case.},
  keywords = {ARL-unbiased,control chart,cumulative conformance count,high-quality process,self-starting},
  annotation = {\_eprint: https://doi.org/10.1080/00207543.2011.649305},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Zhang_et_al_2012_An_improved_self-starting_cumulative_count_of_conforming_chart_for_monitoring.pdf;/home/dede/Zotero/storage/S4PMIICL/00207543.2011.html}
}

@article{zhang2015,
  title = {Dynamic Probability Control Limits for Risk-Adjusted {{CUSUM}} Charts Based on Multiresponses},
  author = {Zhang, Xiang and Loda, Justin B. and Woodall, William H.},
  year = {2015},
  journal = {Statistics in Medicine},
  volume = {34},
  number = {25},
  pages = {3336--3348},
  issn = {1097-0258},
  doi = {10.1002/sim.6547},
  abstract = {The risk-adjusted Bernoulli cumulative sum (CUSUM) chart developed by Steiner et al. (2000) is an increasingly popular tool for monitoring clinical and surgical performance. In practice, however, the use of a fixed control limit for the chart leads to a quite variable in-control average run length performance for patient populations with different risk score distributions. To overcome this problem, we determine simulation-based dynamic probability control limits (DPCLs) patient-by-patient for the risk-adjusted Bernoulli CUSUM charts. By maintaining the probability of a false alarm at a constant level conditional on no false alarm for previous observations, our risk-adjusted CUSUM charts with DPCLs have consistent in-control performance at the desired level with approximately geometrically distributed run lengths. Our simulation results demonstrate that our method does not rely on any information or assumptions about the patients' risk distributions. The use of DPCLs for risk-adjusted Bernoulli CUSUM charts allows each chart to be designed for the corresponding particular sequence of patients for a surgeon or hospital. Copyright \textcopyright{} 2015 John Wiley \& Sons, Ltd.},
  copyright = {Copyright \textcopyright{} 2015 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {average run length (ARL),done,false alarm rate,run length distribution,statistical process control,surgical performance},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.6547},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Zhang_Woodall_2015_Dynamic_probability_control_limits_for_risk-adjusted_CUSUM_charts_based_on.pdf;/home/dede/Zotero/storage/YDW2P52L/sim.html}
}

@article{zhang2017,
  title = {Monitoring the Shape Parameter of a {{Weibull}} Renewal Process},
  author = {Zhang, Cai Wen and Ye, Zhisheng and Xie, Min},
  year = {2017},
  month = aug,
  journal = {IISE Transactions},
  volume = {49},
  number = {8},
  pages = {800--813},
  publisher = {{Taylor \& Francis}},
  issn = {2472-5854},
  doi = {10.1080/24725854.2016.1278315},
  abstract = {This research arose from a challenge faced in real practice\textemdash monitoring changes to the Weibull shape parameter. From first-hand experience, we understand that a mechanism for such a purpose is very useful. This article is primarily focused on monitoring the shape parameter of a Weibull renewal process. We derive a novel statistic on the Weibull shape parameter making use of maximum likelihood theory, which is demonstrated to follow an approximately normal distribution. This desirable normality property makes the statistic well suited for use in monitoring the Weibull shape parameter. It also allows for a simple approach to constructing a Shewhart-type control chart, named the Beta chart. The parameter values required to design a Beta chart are provided. A self-starting procedure is also proposed for setting up the Phase I Beta chart. The Average Run Length (ARL) performance of the Beta chart is evaluated through Monte Carlo simulation. A comparison with a moving range exponentially weighted moving average (EWMA) chart from the literature shows that the Beta chart has much better ARL performance when properly designed. Application examples, using both simulated and real data, demonstrate that the Beta chart is effective and makes good sense in real practice.},
  keywords = {ARL unbiased,control chart,shape parameter,time-between-events,Weibull renewal process},
  annotation = {\_eprint: https://doi.org/10.1080/24725854.2016.1278315},
  file = {/home/dede/Zotero/storage/CSCGISF9/24725854.2016.html}
}

@article{zhang2018,
  title = {Multiple Profiles Sensor-Based Monitoring and Anomaly Detection},
  author = {Zhang, Chen and Yan, Hao and Lee, Seungho and Shi, Jianjun},
  year = {2018},
  month = oct,
  journal = {Journal of Quality Technology},
  volume = {50},
  number = {4},
  pages = {344--362},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2018.1508275},
  abstract = {Generally, in an advanced manufacturing system hundreds of sensors are deployed to measure key process variables in real time. Thus it is desirable to develop methodologies to use real-time sensor data for on-line system condition monitoring and anomaly detection. However, there are several challenges in developing an effective process monitoring system: (i) data streams generated by multiple sensors are high-dimensional profiles; (ii) sensor signals are affected by noise due to system-inherent variations; (iii) signals of different sensors have cluster-wise features; and (iv) an anomaly may cause only sparse changes of sensor signals. To address these challenges, this article presents a real-time multiple profiles sensor-based process monitoring system, which includes the following modules: (i) preprocessing sensor signals to remove inherent variations and conduct profile alignments, (ii) using multichannel functional principal component analysis (MFPCA)\textendash based methods to extract sensor features by considering cluster-wise between-sensor correlations, and (iii) constructing a monitoring scheme with the top-R strategy based on the extracted features, which has scalable detection power for different fault patterns. Finally, we implement and demonstrate the proposed framework using data from a real manufacturing system.},
  keywords = {data fusion,done,functional PCA,multichannel profile monitoring,statistical process control},
  annotation = {\_eprint: https://doi.org/10.1080/00224065.2018.1508275},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Zhang_et_al_2018_Multiple_profiles_sensor-based_monitoring_and_anomaly_detection.pdf;/home/dede/Zotero/storage/DSGKKTLH/00224065.2018.html}
}

@inproceedings{zhang2019,
  title = {Partially {{Observable Multi-Sensor Sequential Change Detection}}: {{A Combinatorial Multi-Armed Bandit Approach}}},
  shorttitle = {Partially {{Observable Multi-Sensor Sequential Change Detection}}},
  booktitle = {The {{Thirty-Third AAAI Conference}} on {{Artificial Intelligence}}, {{AAAI}} 2019, {{The Thirty-First Innovative Applications}} of {{Artificial Intelligence Conference}}, {{IAAI}} 2019, {{The Ninth AAAI Symposium}} on {{Educational Advances}} in {{Artificial Intelligence}}, {{EAAI}} 2019, {{Honolulu}}, {{Hawaii}}, {{USA}}, {{January}} 27 - {{February}} 1, 2019},
  author = {Zhang, Chen and Hoi, Steven C. H.},
  year = {2019},
  pages = {5733--5740},
  publisher = {{AAAI Press}},
  doi = {10.1609/aaai.v33i01.33015733},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Zhang_Hoi_2019_Partially_Observable_Multi-Sensor_Sequential_Change_Detection.pdf}
}

@article{zhang2020,
  title = {Bandit {{Change-Point Detection}} for {{Real-Time Monitoring High-Dimensional Data Under Sampling Control}}},
  author = {Zhang, Wanrong and Mei, Yajun},
  year = {2020},
  month = sep,
  journal = {arXiv:2009.11891 [stat]},
  eprint = {2009.11891},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {In many real-world problems of real-time monitoring high-dimensional streaming data, one wants to detect an undesired event or change quickly once it occurs, but under the sampling control constraint in the sense that one might be able to only observe or use selected components data for decision-making per time step in the resource-constrained environments. In this paper, we propose to incorporate multi-armed bandit approaches into sequential change-point detection to develop an efficient bandit change-point detection algorithm. Our proposed algorithm, termed Thompson-Sampling-Shiryaev-Roberts-Pollak (TSSRP), consists of two policies per time step: the adaptive sampling policy applies the Thompson Sampling algorithm to balance between exploration for acquiring long-term knowledge and exploitation for immediate reward gain, and the statistical decision policy fuses the local Shiryaev-Roberts-Pollak statistics to determine whether to raise a global alarm by sum shrinkage techniques. Extensive numerical simulations and case studies demonstrate the statistical and computational efficiency of our proposed TSSRP algorithm.},
  archiveprefix = {arXiv},
  keywords = {done,Statistics - Machine Learning,Statistics - Methodology},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Zhang_Mei_2020_Bandit_Change-Point_Detection_for_Real-Time_Monitoring_High-Dimensional_Data.pdf;/home/dede/Zotero/storage/WN5TMN9F/2009.html}
}

@article{zhang2021,
  title = {Change Point Detection and Issue Localization Based on Fleet-Wide Fault Data},
  author = {Zhang, Zhanpan and Doganaksoy, Necip},
  year = {2021},
  month = jun,
  journal = {Journal of Quality Technology},
  volume = {0},
  number = {0},
  pages = {1--13},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2021.1937409},
  abstract = {Modern industrial assets (e.g., generators, turbines, engines) are outfitted with numerous sensors to monitor key operating and environmental variables. Unusual sensor readings, such as high temperature, excessive vibration, or low current, could trigger rule-based actions (also known as faults) that range from warning alarms to immediate shutdown of the asset to prevent potential damage. In the case study of this article, a wind park experienced a sudden surge in vibration-induced shutdowns. We utilize fault data logs from the park with the goal of detecting common change points across turbines. Another important goal is the localization of fault occurrences to an identifiable set of turbines. The literature on change point detection and localization for multiple assets is highly sparse. Our technical development is based on the generalized linear modeling framework. We combine well-known solutions to change point detection for a single asset with a heuristics-based approach to identify a common change point(s) for multiple assets. The performance of the proposed detection and localization algorithms is evaluated through synthetic (Monte Carlo) fault data streams. Several novel performance metrics are defined to characterize different aspects of a change point detection algorithm for multiple assets. For the case study example, the proposed methodology identified the change point and the subset of affected turbines with a high degree of accuracy. The problem described here warrants further study to accommodate general fault distributions, change point detection algorithms, and very large fleet sizes.},
  keywords = {big data,event data,generalized linear models,Poisson distribution,supervisory control and data acquisition (SCADA) system,todo,wind turbine},
  annotation = {\_eprint: https://doi.org/10.1080/00224065.2021.1937409},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Zhang_Doganaksoy_2021_Change_point_detection_and_issue_localization_based_on_fleet-wide_fault_data.pdf;/home/dede/Zotero/storage/LU583TPP/00224065.2021.html}
}

@article{zhang2021a,
  title = {Pathfinder: {{Parallel}} Quasi-{{Newton}} Variational Inference},
  shorttitle = {Pathfinder},
  author = {Zhang, Lu and Carpenter, Bob and Gelman, Andrew and Vehtari, Aki},
  year = {2021},
  month = aug,
  journal = {arXiv:2108.03782 [cs, stat]},
  eprint = {2108.03782},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We introduce Pathfinder, a variational method for approximately sampling from differentiable log densities. Starting from a random initialization, Pathfinder locates normal approximations to the target density along a quasi-Newton optimization path, with local covariance estimated using the inverse Hessian estimates produced by the optimizer. Pathfinder returns draws from the approximation with the lowest estimated Kullback-Leibler (KL) divergence to the true posterior. We evaluate Pathfinder on a wide range of posterior distributions, demonstrating that its approximate draws are better than those from automatic differentiation variational inference (ADVI) and comparable to those produced by short chains of dynamic Hamiltonian Monte Carlo (HMC), as measured by 1-Wasserstein distance. Compared to ADVI and short dynamic HMC runs, Pathfinder requires one to two orders of magnitude fewer log density and gradient evaluations, with greater reductions for more challenging posteriors. Importance resampling over multiple runs of Pathfinder improves the diversity of approximate draws, reducing 1-Wasserstein distance further and providing a measure of robustness to optimization failures on plateaus, saddle points, or in minor modes. The Monte Carlo KL-divergence estimates are embarrassingly parallelizable in the core Pathfinder algorithm, as are multiple runs in the resampling version, further increasing Pathfinder's speed advantage with multiple cores.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Zhang_et_al_2021_Pathfinder.pdf;/home/dede/Zotero/storage/SJZ55N35/2108.html}
}

@inproceedings{zhao2009,
  title = {Parallel {{K-Means Clustering Based}} on {{MapReduce}}},
  booktitle = {Cloud {{Computing}}},
  author = {Zhao, Weizhong and Ma, Huifang and He, Qing},
  year = {2009},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {674--679},
  abstract = {Data clustering has been received considerable attention in many applications, such as data mining, document retrieval, image segmentation and pattern classification. The enlarging volumes of information emerging by the progress of technology, makes clustering of very large scale of data a challenging task. In order to deal with the problem, many researchers try to design efficient parallel clustering algorithms. In this paper, we propose a parallel k-means clustering algorithm based on MapReduce, which is a simple yet powerful parallel programming technique. The experimental results demonstrate that the proposed algorithm can scale well and efficiently process large datasets on commodity hardware.},
  isbn = {978-3-642-10665-1},
  langid = {english},
  keywords = {Data mining,done,Hadoop,K-means,MapReduce,Parallel clustering},
  file = {/home/dede/Zotero/storage/ZX9ZH4HV/Zhao et al. - 2009 - Parallel K-Means Clustering Based on MapReduce.pdf}
}

@article{zhao2021,
  title = {An {{Intrinsic Geometrical Approach}} for {{Statistical Process Control}} of {{Surface}} and {{Manifold Data}}},
  author = {Zhao, Xueqi and {del Castillo}, Enrique},
  year = {2021},
  month = jul,
  journal = {Technometrics},
  volume = {63},
  number = {3},
  pages = {295--312},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.2020.1772114},
  abstract = {We present a new method for statistical process control (SPC) of a discrete part manufacturing system based on intrinsic geometrical properties of the parts, estimated from three-dimensional sensor data. An intrinsic method has the computational advantage of avoiding the difficult part registration problem, necessary in previous SPC approaches of three-dimensional geometrical data, but inadequate if noncontact sensors are used. The approach estimates the spectrum of the Laplace\textendash Beltrami (LB) operator of the scanned parts and uses a multivariate nonparametric control chart for online process control. Our proposal brings SPC closer to computer vision and computer graphics methods aimed to detect large differences in shape (but not in size). However, the SPC problem differs in that small changes in either shape or size of the parts need to be detected, keeping a controllable false alarm rate and without completely filtering noise. An online or ``Phase II'' method and a scheme for starting up in the absence of prior data (``Phase I'') are presented. Comparison with earlier approaches that require registration shows the LB spectrum method to be more sensitive to rapidly detect small changes in shape and size, including the practical case when the sequence of part datasets is in the form of large, unequal size meshes. A post-alarm diagnostic method to investigate the location of defects on the surface of a part is also presented. While we focus in this article on surface (triangulation) data, the methods can also be applied to point cloud and voxel metrology data.},
  keywords = {Differential geometry,Laplaceâ€“Beltrami operator,Noncontact sensor,Permutation test,Spectral method,todo},
  annotation = {\_eprint: https://doi.org/10.1080/00401706.2020.1772114},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Zhao_del_Castillo_2021_An_Intrinsic_Geometrical_Approach_for_Statistical_Process_Control_of_Surface.pdf;/home/dede/Zotero/storage/QD82JLZ9/00401706.2020.html}
}

@article{zi2018,
  title = {Discussion on ``{{Statistical}} Transfer Learning: {{A}} Review and Some Extensions to Statistical Process Control''},
  shorttitle = {Discussion on ``{{Statistical}} Transfer Learning},
  author = {Zi, Xuemin and Zou, Changliang},
  year = {2018},
  month = jan,
  journal = {Quality Engineering},
  volume = {30},
  number = {1},
  pages = {129--132},
  publisher = {{Taylor \& Francis}},
  issn = {0898-2112},
  doi = {10.1080/08982112.2018.1388053},
  keywords = {done},
  annotation = {\_eprint: https://doi.org/10.1080/08982112.2018.1388053},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Zi_Zou_2018_Discussion_on_â€œStatistical_transfer_learning.pdf;/home/dede/Zotero/storage/GKRLLIYI/08982112.2018.html}
}

@article{zorzetto2020,
  title = {Bayesian Non-Asymptotic Extreme Value Models for Environmental Data},
  author = {Zorzetto, Enrico and Canale, Antonio and Marani, Marco},
  year = {2020},
  month = may,
  journal = {arXiv:2005.12101 [stat]},
  eprint = {2005.12101},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Motivated by the analysis of extreme rainfall data, we introduce a general Bayesian hierarchical model for estimating the probability distribution of extreme values of intermittent random sequences, a common problem in geophysical and environmental science settings. The approach presented here relaxes the asymptotic assumption typical of the traditional extreme value (EV) theory, and accounts for the possible underlying variability in the distribution of event magnitudes and occurrences, which are described through a latent temporal process. Focusing on daily rainfall extremes, the structure of the proposed model lends itself to incorporating prior geo-physical understanding of the rainfall process. By means of an extensive simulation study, we show that this methodology can significantly reduce estimation uncertainty with respect to Bayesian formulations of traditional asymptotic EV methods, particularly in the case of relatively small samples. The benefits of the approach are further illustrated with an application to a large data set of 479 long daily rainfall historical records from across the continental United States. By comparing measures of in-sample and out-of-sample predictive accuracy, we find that the model structure developed here, combined with the use of all available observations for inference, significantly improves robustness with respect to overfitting to the specific sample.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology,todo},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Zorzetto_et_al_2020_Bayesian_non-asymptotic_extreme_value_models_for_environmental_data.pdf;/home/dede/Zotero/storage/S6GXLA6H/2005.html}
}

@book{zotero-undefined,
  title = {Distribution-{{Free Methods}} for {{Statistical Process Monitoring}} and {{Control}}},
  abstract = {This book offers new contributions and introduces several distribution-free monitoring schemes and studies them in detail. In particular, the book focuses on the establishment of nonparametric techniques appropriately designed for monitoring a production process.},
  langid = {english},
  keywords = {doing},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Distribution-Free_Methods_for_Statistical_Process_Monitoring_and_Control.pdf;/home/dede/Zotero/storage/Q5G232D7/978-3-030-25081-2.html}
}

@article{zou2005,
  title = {Regularization and Variable Selection via the Elastic Net},
  author = {Zou, Hui and Hastie, Trevor},
  year = {2005},
  month = apr,
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {67},
  number = {2},
  pages = {301--320},
  issn = {1369-7412, 1467-9868},
  doi = {10.1111/j.1467-9868.2005.00503.x},
  langid = {english},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Zou_Hastie_2005_Regularization_and_variable_selection_via_the_elastic_net.pdf}
}

@article{zou2007,
  title = {Monitoring {{General Linear Profiles Using Multivariate Exponentially Weighted Moving Average Schemes}}},
  author = {Zou, Changliang and Tsung, Fugee and Wang, Zhaojun},
  year = {2007},
  month = nov,
  journal = {Technometrics},
  volume = {49},
  number = {4},
  pages = {395--408},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1198/004017007000000164},
  abstract = {We propose a statistical process control scheme that can be implemented in industrial practice, in which the quality of a process can be characterized by a general linear profile. We start by reviewing the general linear profile model and the existing monitoring methods. Based on this, we propose a novel multivariate exponentially weighted moving average monitoring scheme for such a profile. We introduce two other enhancement features, the variable sampling interval and the parametric diagnostic approach, to further improve the performance of the proposed scheme. Throughout the article, we use a deep reactive ion etching example from semiconductor manufacturing, which has a profile that fits a quadratic polynomial regression model well, to illustrate the implementation of the proposed approach.},
  keywords = {Change point detection,Linear profiles,Statistical process control,todo},
  annotation = {\_eprint: https://doi.org/10.1198/004017007000000164},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Zou_et_al_2007_Monitoring_General_Linear_Profiles_Using_Multivariate_Exponentially_Weighted.pdf;/home/dede/Zotero/storage/HIT79S69/004017007000000164.html}
}

@article{zou2007a,
  title = {A {{Self-Starting Control Chart}} for {{Linear Profiles}}},
  author = {Zou, Changliang and Zhou, Chunguang and Wang, Zhaojun and Tsung, Fugee},
  year = {2007},
  journal = {Journal of Quality Technology},
  volume = {39},
  number = {4},
  pages = {364--375},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2007.11917702},
  abstract = {A self-starting control chart based on recursive residuals is proposed for monitoring linear profiles when the nominal values of the process parameters are unknown. This chart can detect a shift in the intercept, the slope, or the standard deviation. Because of the good properties of the plot statistics, the proposed chart can be easily designed to match any desired in-control average run length. Simulated results show that our approach has good charting performance across a range of possible shifts when the process parameters are unknown and that it is particularly useful during the start-up stage of a process.},
  keywords = {Average Run Length,done,EWMA Charts,Markov Chain,Recursive Residuals},
  annotation = {\_eprint: https://doi.org/10.1080/00224065.2007.11917702},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Zou_et_al_2007_A_Self-Starting_Control_Chart_for_Linear_Profiles.pdf;/home/dede/Zotero/storage/B63VPBKM/00224065.2007.html}
}

@article{zou2007b,
  title = {On the ``Degrees of Freedom'' of the Lasso},
  author = {Zou, Hui and Hastie, Trevor and Tibshirani, Robert},
  year = {2007},
  month = oct,
  journal = {The Annals of Statistics},
  volume = {35},
  number = {5},
  pages = {2173--2192},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/009053607000000127},
  abstract = {We study the effective degrees of freedom of the lasso in the framework of Stein's unbiased risk estimation (SURE). We show that the number of nonzero coefficients is an unbiased estimate for the degrees of freedom of the lasso\textemdash a conclusion that requires no special assumption on the predictors. In addition, the unbiased estimator is shown to be asymptotically consistent. With these results on hand, various model selection criteria\textemdash Cp, AIC and BIC\textemdash are available, which, along with the LARS algorithm, provide a principled and efficient approach to obtaining the optimal lasso fit with the computational effort of a single ordinary least-squares fit.},
  keywords = {62J05,62J07,90C46,Degrees of freedom,LARS algorithm,Lasso,Model selection,SURE,unbiased estimate},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Zou_et_al_2007_On_the_â€œdegrees_of_freedomâ€_of_the_lasso.pdf;/home/dede/Zotero/storage/QPJL7UA9/009053607000000127.html}
}

@article{zou2008,
  title = {Monitoring {{Profiles Based}} on {{Nonparametric Regression Methods}}},
  author = {Zou, Changliang and Tsung, Fugee and Wang, Zhaojun},
  year = {2008},
  month = nov,
  journal = {Technometrics},
  volume = {50},
  number = {4},
  pages = {512--526},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1198/004017008000000433},
  abstract = {The use statistical process control (SPC) in monitoring and diagnosis of process and product quality profiles remains an important problem in various manufacturing industries. The SPC problem with a nonlinear profile is particularly challenging. This article proposes a novel scheme to monitor changes in both the regression relationship and the variation of the profile online. It integrates the multivariate exponentially weighted moving average procedure with the generalized likelihood ratio test based on nonparametric regression. The proposed scheme not only provides an effective SPC solution to handle nonlinear profiles, which are common in industrial practice, but it also resolves the latent problem in popular parametric monitoring methods of being unable to detect certain types of changes due to a misspecified, out-of-control model. Our simulation results demonstrate the effectiveness and efficiency of the proposed monitoring scheme. In addition, a systematic diagnostic approach is provided to locate the change point of the process and identify the type of change in the profile. Finally, a deep reactive ion-etching example from semiconductor manufacturing is used to illustrate the implementation of the proposed monitoring and diagnostic approach.},
  keywords = {done,Exponentially weighted moving average,Generalized likelihood ratio test,Lack-of-fit test,Local linear smoother,Nonlinear profile,Statistical process control},
  annotation = {\_eprint: https://doi.org/10.1198/004017008000000433},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Zou_et_al_2008_Monitoring_Profiles_Based_on_Nonparametric_Regression_Methods.pdf;/home/dede/Zotero/storage/JX4DANLV/004017008000000433.html}
}

@article{zou2009,
  title = {On the Adaptive Elastic-Net with a Diverging Number of Parameters},
  author = {Zou, Hui and Zhang, Hao Helen},
  year = {2009},
  month = aug,
  journal = {The Annals of Statistics},
  volume = {37},
  number = {4},
  pages = {1733--1751},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/08-AOS625},
  abstract = {We consider the problem of model selection and estimation in situations where the number of parameters diverges with the sample size. When the dimension is high, an ideal method should have the oracle property [J. Amer. Statist. Assoc.96 (2001) 1348\textendash 1360] and [Ann. Statist.32 (2004) 928\textendash 961] which ensures the optimal large sample performance. Furthermore, the high-dimensionality often induces the collinearity problem, which should be properly handled by the ideal method. Many existing variable selection methods fail to achieve both goals simultaneously. In this paper, we propose the adaptive elastic-net that combines the strengths of the quadratic regularization and the adaptively weighted lasso shrinkage. Under weak regularity conditions, we establish the oracle property of the adaptive elastic-net. We show by simulations that the adaptive elastic-net deals with the collinearity problem better than the other oracle-like methods, thus enjoying much improved finite sample performance.},
  keywords = {62J05,62J07,Adaptive regularization,done,elastic-net,high dimensionality,Model selection,oracle property,shrinkage methods},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Zou_Zhang_2009_On_the_adaptive_elastic-net_with_a_diverging_number_of_parameters.pdf;/home/dede/Zotero/storage/KFCJ6E4A/08-AOS625.html}
}

@article{zou2015,
  title = {An {{Efficient Online Monitoring Method}} for {{High-Dimensional Data Streams}}},
  author = {Zou, Changliang and Wang, Z. and Zi, Xuemin and Jiang, Wei},
  year = {2015},
  journal = {Technometrics},
  doi = {10.1080/00401706.2014.940089},
  abstract = {A new control chart is developed based on a powerful goodness-of-fit test of the local cumulative sum statistics from each data stream to detect heterogenous mixtures in high-dimensional data streams. Monitoring high-dimensional data streams has become increasingly important for real-time detection of abnormal activities in many data-rich applications. We are interested in detecting an occurring event as soon as possible, but we do not know which subset of data streams is affected by the event. By connecting to the problem of detecting heterogenous mixtures, a new control chart is developed based on a powerful goodness-of-fit test of the local cumulative sum statistics from each data stream. Numerical results show that the proposed method is able to balance the detection of various fractions of affected streams, and generally outperforms existing methods. Supplementary materials for this article are available online.},
  keywords = {done},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Zou_et_al_2015_An_Efficient_Online_Monitoring_Method_for_High-Dimensional_Data_Streams.pdf}
}

@book{zucchini2016,
  title = {Hidden {{Markov Models}} for {{Time Series}}: {{An Introduction Using R}}},
  shorttitle = {Hidden {{Markov Models}} for {{Time Series}}},
  author = {Zucchini, Walter and Macdonald, Iain and Langlock, Ronald},
  year = {2016},
  series = {Monographs on Statistics and Applied Probability 150},
  publisher = {{CRC Press}},
  doi = {10.1201/9781420010893},
  abstract = {Reveals How HMMs Can Be Used as General-Purpose Time Series Models Implements all methods in RHidden Markov Models for Time Series: An Introduction Using R applies hidden Markov models (HMMs) to a wide range of time series types, from continuous-valued, circular, and multivariate series to binary data, bounded and unbounded counts, and categorical observations. It also discusses how to employ the freely available computing environment R to carry out computations for parameter estimation, model selection and checking, decoding, and forecasting. Illustrates the methodology in actionAfter presenting the simple Poisson HMM, the book covers estimation, forecasting, decoding, prediction, model selection, and Bayesian inference. Through examples and applications, the authors describe how to extend and generalize the basic model so it can be applied in a rich variety of situations. They also provide R code for some of the examples, enabling the use of the codes in similar applications. Effectively interpret data using HMMs This book illustrates the wonderful flexibility of HMMs as general-purpose models for time series data. It provides a broad understanding of the models and their uses.},
  isbn = {978-1-4822-5384-9},
  keywords = {hidden markov,markov,time series},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Zucchini_et_al_2016_Hidden_Markov_Models_for_Time_Series.pdf}
}

@article{zwetsloot2017,
  title = {A Head-to-Head Comparative Study of the Conditional Performance of Control Charts Based on Estimated Parameters},
  author = {Zwetsloot, Inez M. and Woodall, William H.},
  year = {2017},
  month = apr,
  journal = {Quality Engineering},
  volume = {29},
  number = {2},
  pages = {244--253},
  publisher = {{Taylor \& Francis}},
  issn = {0898-2112},
  doi = {10.1080/08982112.2016.1237651},
  abstract = {Implementation of the Shewhart, CUSUM, and EWMA charts requires estimates of the in-control process parameters. Many researchers have shown that estimation error strongly influences the performance of these charts. However, a given amount of estimation error may differ in effect across charts. Therefore, we perform a pairwise comparison of the effect of estimation error across these charts. We conclude that the Shewhart chart is more strongly affected by estimation error than the CUSUM and EWMA charts. Furthermore, we show that the general belief that the CUSUM and EWMA charts have similar performance no longer holds under estimated parameters.},
  keywords = {average run length,conditional chart performance,CUSUM control chart,done,estimation effect,EWMA control chart,Shewhart control chart,statistical process control (SPC),statistical process monitoring (SPM)},
  annotation = {\_eprint: https://doi.org/10.1080/08982112.2016.1237651},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Zwetsloot_Woodall_2017_A_head-to-head_comparative_study_of_the_conditional_performance_of_control.pdf;/home/dede/Zotero/storage/GM46JA6V/08982112.2016.html}
}

@article{zwetsloot2021,
  title = {A {{Review}} of {{Some Sampling}} and {{Aggregation Strategies}} for {{Basic Statistical Process Monitoring}}},
  author = {Zwetsloot, Inez M. and Woodall, William H.},
  year = {2021},
  month = jan,
  journal = {Journal of Quality Technology},
  volume = {53},
  number = {1},
  pages = {1--16},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2019.1611354},
  abstract = {We review the long-established rational subgrouping principle for determining an effective sampling plan for process monitoring. We present some other general advice that has been given in the literature and discuss some issues related to sampling as it applies to monitoring. Because it is very common to form samples by aggregating data over fixed time intervals, we review the literature on the effect of temporal aggregation on process monitoring performance and provide our perspective. We offer some practical advice and some directions for future research.},
  keywords = {A review of some sampling and aggregation strategies for basic statistical process monitoring: Discussion by Ross Sparks,average time to signal,Data aggregation in disease surveillance,Discussion of article by Zwetsloot and Woodall: A review of some sampling and aggregation strategies for basic statistical process monitoring,Discussion of Zwetsloot and Woodall; â€œA review of some sampling and aggregation strategies for basic statistical process monitoringâ€,Discussion: A review of some sampling and aggregation strategies for basic statistical process monitoring (I. M. Zwetsloot and W. H. Woodall),Discussion: Process data streams aggregation versus product samples aggregation,public health surveillance,rational subgrouping,social network monitoring,temporal aggregation,time-between-events control chart,timestamp,todo},
  annotation = {\_eprint: https://doi.org/10.1080/00224065.2019.1611354},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Zwetsloot_Woodall_2021_A_Review_of_Some_Sampling_and_Aggregation_Strategies_for_Basic_Statistical.pdf;/home/dede/Zotero/storage/JALGZZNX/00224065.2019.html}
}

@article{zwetsloot2021a,
  title = {Rejoinder},
  author = {Zwetsloot, Inez M. and Woodall, William H.},
  year = {2021},
  month = jan,
  journal = {Journal of Quality Technology},
  volume = {53},
  number = {1},
  pages = {44--46},
  publisher = {{Taylor \& Francis}},
  issn = {0022-4065},
  doi = {10.1080/00224065.2019.1611361},
  keywords = {A review of some sampling and aggregation strategies for basic statistical process monitoring: Discussion by Ross Sparks,Data aggregation in disease surveillance,Discussion of article by Zwetsloot and Woodall: A review of some sampling and aggregation strategies for basic statistical process monitoring,Discussion of Zwetsloot and Woodall; â€œA review of some sampling and aggregation strategies for basic statistical process monitoringâ€,Discussion: A review of some sampling and aggregation strategies for basic statistical process monitoring (I. M. Zwetsloot and W. H. Woodall),Discussion: Process data streams aggregation versus product samples aggregation,done},
  annotation = {\_eprint: https://doi.org/10.1080/00224065.2019.1611361},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Zwetsloot_Woodall_2021_Rejoinder.pdf;/home/dede/Zotero/storage/GXC2NF5J/00224065.2019.html}
}

@book{zwillinger2014,
  title = {{Table of Integrals, Series, and Products}},
  author = {Zwillinger, Daniel},
  year = {2014},
  edition = {8\textdegree{} edizione},
  publisher = {{Academic Press}},
  address = {{Amsterdam ; Boston}},
  isbn = {978-0-12-384933-5},
  langid = {Inglese},
  file = {/home/dede/Documents/MEGA/universita/zotero-pdf/Zwillinger_2014_Table_of_Integrals,_Series,_and_Products.pdf}
}


